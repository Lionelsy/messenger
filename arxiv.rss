<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Arxiv论文推荐</title><link>https://arxiv.org/</link><description>Arxiv论文推荐（自动生成）</description><language>zh-CN</language><lastBuildDate>Fri, 02 Jan 2026 14:33:45 +0800</lastBuildDate><item><guid>2512.24271v1</guid><title>Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</title><link>http://arxiv.org/abs/2512.24271v1</link><author>Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展，但它们存在一个关键漏洞：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。由于文本和视频之间固有的数据不平衡，这一限制很难解决，因为收集和标注反事实数据的成本很高。为了解决这个问题，我们引入了DualityForge，这是一个新颖的反事实数据合成框架，它采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，我们构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，为了充分利用我们配对数据的对比性质，我们提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。我们将开源我们的数据集和代码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展，但它们存在一个关键漏洞：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决多模态大语言模型在处理反事实视频时存在的视觉无根据的幻觉问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入DualityForge，这是一个新颖的反事实数据合成框架，采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入DualityForge和DualityVidQA数据集，以及Duality-Normalized Advantage Training (DNA-Train)训练机制，我们有效地减少了多模态大语言模型在处理反事实视频时的幻觉问题，并展示了强大的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展。然而，它们存在一个关键问题：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。由于文本和视频之间固有的数据不平衡，这一限制很难解决，因为收集和标注反事实数据的成本很高。为了解决这个问题，我们引入了DualityForge，这是一个新颖的反事实数据合成框架，它采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，我们构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，为了充分利用我们配对数据的对比性质，我们提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。我们将开源我们的数据集和代码。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型（MLLMs）在视频理解中存在的过度依赖语言先验的问题，导致在处理反事实视频时出现视觉无根据的幻觉。这个问题在现实研究中很重要，因为MLLMs在视频理解中的应用越来越广泛，但幻觉问题限制了它们的准确性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，如利用AI生成内容（AIGC）和视觉强化学习，设计出了一种新的数据合成框架DualityForge，通过可控的视频编辑将真实视频转换为反事实场景，并构建了DualityVidQA数据集。作者也借鉴了现有的视频理解数据集和视觉强化学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过嵌入结构化上下文信息到视频编辑和问答生成过程中，自动产生高质量的反事实视频和问答对，从而提高MLLMs的视频理解能力。整体实现流程包括使用DualityForge框架生成反事实视频，然后使用这些视频和原始视频构建对比性问答对，最后通过Duality-Normalized Advantage Training（DNA-Train）方法进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括DualityForge框架，它利用可控的视频编辑生成反事实场景，并嵌入结构化上下文信息；DualityVidQA数据集，它包含大量反事实视频和问答对；以及DNA-Train训练方法，它通过对比性问答训练提高MLLMs的视频理解能力。相比之前的工作，这些创新点提供了更系统、更自动化的方法来减少MLLMs的幻觉问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个通过反事实视频生成和对比性问答训练来提高MLLMs视频理解能力的新方法，有效减少了模型在反事实视频上的幻觉问题。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.&lt;/p&gt;</description></item><item><guid>2512.24323v1</guid><title>Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention</title><link>http://arxiv.org/abs/2512.24323v1</link><author>Haijing Liu, Zhiyuan Song, Hefeng Wu, Tao Pu, Keze Wang, Liang Lin</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Ego-RVOS旨在从第一人称视频中根据语言查询分割出积极参与人类动作的特定对象，这对理解人类行为至关重要。然而，由于第一人称视频中的模糊性和训练数据中的偏差，实现鲁棒的分割具有挑战性。现有的方法常常从数据集中学习虚假的相关性，并受到第一人称视角的基本视觉混淆因素的影响，如快速运动和频繁的遮挡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Ego-RVOS任务对于理解人类行为非常重要，但由于第一人称视频的模糊性和训练数据的偏差，实现鲁棒的分割具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决这些限制，引入了Causal Ego-REferring Segmentation (CERES)，这是一个插件式因果框架，将强预训练的RVOS骨干网络适应到第一人称领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CERES实现了双模态因果干预：应用后门调整原则来抵消从数据集统计中学习到的语言表示偏差，并利用前门调整概念来解决视觉混淆，通过智能地整合语义视觉特征与几何深度信息，创建对第一人称扭曲更鲁棒的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CERES在Ego-RVOS基准上实现了最先进的性能，突出了应用因果推理构建更可靠模型以实现更广泛的第一人称视频理解的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CERES通过因果推理方法提高了第一人称视频中对象分割的鲁棒性，展示了其在第一人称视频理解中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Ego-RVOS旨在从第一人称视频中根据语言查询分割出积极参与人类动作的特定对象，这对理解人类行为至关重要。然而，由于第一人称视频中的模糊性和训练数据中的偏差，实现鲁棒的分割具有挑战性。现有的方法常常从数据集中学习虚假的相关性，并受到第一人称视角的基本视觉混淆因素的影响，如快速运动和频繁的遮挡。为了解决这些限制，引入了Causal Ego-REferring Segmentation (CERES)，这是一个插件式因果框架，将强预训练的RVOS骨干网络适应到第一人称领域。CERES实现了双模态因果干预：应用后门调整原则来抵消从数据集统计中学习到的语言表示偏差，并利用前门调整概念来解决视觉混淆，通过智能地整合语义视觉特征与几何深度信息，创建对第一人称扭曲更鲁棒的表示。CERES在Ego-RVOS基准上实现了最先进的性能，突出了应用因果推理构建更可靠模型以实现更广泛的第一人称视频理解的潜力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是在第一人称视频中，根据语言查询分割出特定参与人类动作的对象的问题。这个问题在现实或研究中非常重要，因为它有助于机器更深入地理解人类行为和交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的不足，特别是数据集偏差和视觉混淆因素，设计了CERES框架。这个方法借鉴了因果推理的原则，并参考了现有的RVOS和因果推理相关工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是使用双模态因果干预来减少语言和视觉偏差。整体实现流程包括使用后门调整来处理语言偏差，使用前门调整和视觉-深度中介来处理视觉偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用因果推理原则来解决Ego-RVOS中的鲁棒性问题，使用后门调整来减少语言偏差，以及使用前门调整和视觉-深度中介来减少视觉偏差。与之前的工作相比，这种方法更全面地考虑了语言和视觉偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于因果推理的Ego-RVOS框架，通过双模态因果干预提高了模型的鲁棒性和泛化能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.&lt;/p&gt;</description></item><item><guid>2512.24327v1</guid><title>Topological Spatial Graph Coarsening</title><link>http://arxiv.org/abs/2512.24327v1</link><author>Anna Calissano, Etienne Lasalle</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了空间图的简化问题，旨在找到一个小型的空间图，同时保留初始图的整体结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 空间图是节点在空间中局部化的图，例如公共交通网络、分子和分支生物结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在空间图中进行图简化，同时保留初始图的主要拓扑特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出了一种基于新框架的拓扑空间图粗化方法，该方法通过折叠短边来实现粗化，并适应了经典拓扑描述符（持久图）的构建，以捕获拓扑信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法是无参数的，并且对初始空间图的旋转、平移和缩放具有等变性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在合成和真实空间图上评估了性能，并显示它显著减少了图的大小，同时保留了相关的拓扑信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究了空间图的简化问题，旨在找到一个小型的空间图，同时保留初始图的整体结构。空间图是节点在空间中局部化的图，例如公共交通网络、分子和分支生物结构。在空间图中进行图简化，同时保留初始图的主要拓扑特征。提出了一种基于新框架的拓扑空间图粗化方法，该方法通过折叠短边来实现粗化，并适应了经典拓扑描述符（持久图）的构建，以捕获拓扑信息。该方法是无参数的，并且对初始空间图的旋转、平移和缩放具有等变性。该方法在合成和真实空间图上评估了性能，并显示它显著减少了图的大小，同时保留了相关的拓扑信息。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空间图简化的问题，即如何在减少节点数量的同时保留空间图的主要拓扑特征。这个问题在现实研究中非常重要，因为空间图通常包含大量节点和边，简化它们可以帮助更好地理解和分析复杂的数据结构，如交通网络、分子结构等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴拓扑数据分析（TDA）中的工具，特别是持久图和三角感知图过滤，来设计这个方法。他们考虑了现有工作在图数据上的应用，并提出了一个新的过滤方法来适应空间图数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过合并短边来简化空间图，同时保留其主要的拓扑特征。整体实现流程包括：定义一个参数θ来控制合并的边长阈值，根据这个参数将图中的节点合并成超节点，并重新计算超节点的位置，最后通过比较原始图和简化图的持久图来选择合适的简化程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出了一种新的三角感知图过滤方法来适应空间图数据，并定义了一个基于持久图的评分系统来指导简化程度的选择。与之前的工作相比，这个方法更注重在简化图的同时保留空间图的主要拓扑特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于拓扑数据分析的空间图简化方法，能够在减少节点数量的同时保留空间图的主要拓扑特征。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatial graphs are particular graphs for which the nodes are localized in space (e.g., public transport network, molecules, branching biological structures). In this work, we consider the problem of spatial graph reduction, that aims to find a smaller spatial graph (i.e., with less nodes) with the same overall structure as the initial one. In this context, performing the graph reduction while preserving the main topological features of the initial graph is particularly relevant, due to the additional spatial information. Thus, we propose a topological spatial graph coarsening approach based on a new framework that finds a trade-off between the graph reduction and the preservation of the topological characteristics. The coarsening is realized by collapsing short edges. In order to capture the topological information required to calibrate the reduction level, we adapt the construction of classical topological descriptors made for point clouds (the so-called persistent diagrams) to spatial graphs. This construction relies on the introduction of a new filtration called triangle-aware graph filtration. Our coarsening approach is parameter-free and we prove that it is equivariant under rotations, translations and scaling of the initial spatial graph. We evaluate the performances of our method on synthetic and real spatial graphs, and show that it significantly reduces the graph sizes while preserving the relevant topological information.&lt;/p&gt;</description></item><item><guid>2512.24331v1</guid><title>Spatial-aware Vision Language Model for Autonomous Driving</title><link>http://arxiv.org/abs/2512.24331v1</link><author>Weijie Wei, Zhipeng Luo, Ling Feng, Venice Erin Liong</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中展现出巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。为了解决这个问题，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。实验表明，LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中具有巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决VLMs在自动驾驶中的局限性，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 我们的工作强调了在构建可信的基于VLMs的自动驾驶系统中，明确3D度量数据的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中展现出巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。为了解决这个问题，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。实验表明，LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。我们的工作强调了在构建可信的基于VLMs的自动驾驶系统中，明确3D度量数据的重要性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉语言模型（VLMs）在自动驾驶场景中缺乏精确的3D空间理解能力的问题。这个问题在现实研究中非常重要，因为准确的3D空间理解是自动驾驶安全规划和决策的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有VLMs和LiDAR技术的优势，设计了一个名为LVLDrive的框架，该框架结合了图像和LiDAR点云数据，并引入了Gradual Fusion Q-Former来逐步融合LiDAR特征，同时保持VLMs的现有知识。这个设计借鉴了OmniDrive中的Q-Former 3D块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过Gradual Fusion Q-Former逐步融合LiDAR点云特征，以增强VLMs的3D空间理解能力。整体实现流程包括使用三个预训练编码器处理文本、图像和点云数据，然后通过Gradual Fusion Q-Former融合这些数据，最后通过语言模型生成任务特定的响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括引入Gradual Fusion Q-Former来逐步融合LiDAR特征，以及构建一个空间感知的视觉问答（SA-QA）数据集来增强模型的3D空间推理能力。与之前的工作相比，LVLDrive在融合LiDAR和图像数据方面更加稳定，并且通过SA-QA数据集提供了更精确的空间理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入Gradual Fusion Q-Former和SA-QA数据集，显著提升了视觉语言模型在自动驾驶场景中的3D空间理解能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&amp;#x27;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.&lt;/p&gt;</description></item><item><guid>2512.24373v1</guid><title>Skim-Aware Contrastive Learning for Efficient Document Representation</title><link>http://arxiv.org/abs/2512.24373v1</link><author>Waheed Ahmed Abro, Zied Bouraoui</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; transformer-based models 在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。相比之下，人类通常通过浏览文本，专注于重要部分来理解整体信息。基于这种人类策略，我们引入了一种新的自监督对比学习框架，以增强长文档的表示。我们的方法随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。这模拟了人类综合信息的方式，从而产生了更丰富且计算效率更高的表示。在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然基于 transformer 的模型在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入一种新的自监督对比学习框架，随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过模拟人类综合信息的方式，产生了更丰富且计算效率更高的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然基于 transformer 的模型在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。相比之下，人类通常通过浏览文本，专注于重要部分来理解整体信息。基于这种人类策略，我们引入了一种新的自监督对比学习框架，以增强长文档的表示。我们的方法随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。这模拟了人类综合信息的方式，从而产生了更丰富且计算效率更高的表示。在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决长文档的有效表示问题，特别是在法律和医学领域。这个问题重要，因为现有的方法在处理长文档时效率低且难以捕捉完整文档的上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了人类浏览文本的策略，设计了一种自监督对比学习框架。他们使用了现有的层次化 transformer 模型和 Longformer 模型，并提出了一个新的块预测编码器（CPE）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习来增强长文档的表示。整体流程包括随机遮蔽文档的一部分，并使用基于自然语言推理的对比目标来对齐相关部分，同时与不相关部分保持距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括提出块预测编码器（CPE），使用自监督对比学习来增强文档表示，以及利用自然语言推理（NLI）来模拟人类浏览文本的过程。与之前的工作相比，这种方法更注重捕捉文档中不同部分之间的关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的自监督对比学习框架，通过模拟人类浏览文本的策略来增强长文档的表示，从而在法律和医学领域实现了更高的准确性和效率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.&lt;/p&gt;</description></item><item><guid>2512.24384v1</guid><title>Geometric Multi-Session Map Merging with Learned Local Descriptors</title><link>http://arxiv.org/abs/2512.24384v1</link><author>Yanlong Ma, Nakul S. Joshi, Christa S. Robison, Philip R. Osteen, Brett T. Lopez</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于学习的局部描述符框架GMLD，用于大规模多会话点云地图合并，该框架能够系统地对跨不同会话收集的地图进行对齐，特别是在重叠区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多会话地图合并对于大规模环境中的自主操作至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够有效合并多会话地图的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用关键点感知编码器和基于平面的几何转换器提取判别性特征，用于回环检测和相对位姿估计，并在因子图优化阶段加入会话间扫描匹配成本因素以增强全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在公开数据集和自收集数据上评估，结果显示出准确且鲁棒的地图合并，误差低，学习到的特征在回环检测和相对位姿估计中表现出色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GMLD框架能够有效地进行大规模多会话点云地图合并。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种基于学习的局部描述符框架GMLD，用于大规模多会话点云地图合并，该框架能够系统地对跨不同会话收集的地图进行对齐，特别是在重叠区域。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多会话地图合并的问题，即在大型环境中进行扩展的自主操作时，如何将不同会话收集的地图在重叠区域进行系统地对齐。这个问题在现实或研究中非常重要，因为准确的地图合并能够帮助自主系统在未知环境中进行有效的规划和导航。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的地图合并、位置识别和注册以及位姿图优化工作，设计出了一种基于学习的局部描述符框架。该方法结合了关键点感知编码器和平面几何转换器，以提取具有判别性的特征，用于回环检测和相对位姿估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用学习到的局部描述符来提高地图合并的准确性和鲁棒性。整体实现流程包括三个主要模块：关键点和描述符生成、回环检测和注册、以及地图合并。首先，从密集点云中提取关键点和局部描述符；然后，通过计算局部描述符之间的距离来检测潜在的回环，并估计相对变换；最后，通过位姿图优化来合并地图，确保全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：关键点感知的局部描述符生成、基于平面的几何转换器编码器、以及扫描匹配成本感知的会话间位姿图优化。相比之前的工作，这些创新点提高了地图合并的准确性和鲁棒性，并能够更好地处理大型环境中的复杂情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于学习的局部描述符框架，能够有效地进行多会话地图合并，提高地图的准确性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.&lt;/p&gt;</description></item><item><guid>2512.24385v1</guid><title>Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems</title><link>http://arxiv.org/abs/2512.24385v1</link><author>Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了如何从多模态车载传感器数据中构建真正的空间智能，特别是在自动驾驶车辆和无人机等自主系统中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着自主系统的快速发展，从多模态车载传感器数据中整合能力以创建统一理解的需求日益增加，但这一挑战依然存在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文旨在提出一个全面的框架，用于多模态预训练，并识别推动该目标实现的核心技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文分析了基础传感器特征与学习策略之间的相互作用，评估了平台特定数据集在这些进步中的作用，并提出了一个统一的预训练范式分类法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 本文提出了一个统一的预训练范式分类法，从单模态基线到复杂的统一框架，这些框架学习整体表示以用于高级任务，如3D物体检测和语义占用预测。此外，本文还研究了文本输入和占用表示的集成，以促进开放世界的感知和规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文确定了关键瓶颈，如计算效率和模型可扩展性，并提出了一个路线图，以实现通用多模态基础模型，这些模型能够实现稳健的空间智能，适用于实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文探讨了如何从多模态车载传感器数据中构建真正的空间智能，特别是在自动驾驶车辆和无人机等自主系统中的应用。随着自主系统的快速发展，从多模态车载传感器数据中整合能力以创建统一理解的需求日益增加，但这一挑战依然存在。本文旨在提出一个全面的框架，用于多模态预训练，并识别推动该目标实现的核心技术。本文分析了基础传感器特征与学习策略之间的相互作用，评估了平台特定数据集在这些进步中的作用，并提出了一个统一的预训练范式分类法。本文提出了一个统一的预训练范式分类法，从单模态基线到复杂的统一框架，这些框架学习整体表示以用于高级任务，如3D物体检测和语义占用预测。此外，本文还研究了文本输入和占用表示的集成，以促进开放世界的感知和规划。本文确定了关键瓶颈，如计算效率和模型可扩展性，并提出了一个路线图，以实现通用多模态基础模型，这些模型能够实现稳健的空间智能，适用于实际部署。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从多模态传感器数据中训练出具有空间智能的自主系统。这个问题在现实或研究中非常重要，因为自主系统（如自动驾驶汽车和无人机）需要准确感知和理解环境，才能安全有效地运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有单模态和跨模态学习方法，以及基础模型在多模态场景中的应用，设计了多模态数据预训练框架。他们借鉴了现有工作，特别是自监督学习、跨模态交互和知识蒸馏等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态预训练，从不同传感器（如摄像头和LiDAR）中提取统一的表示，以实现空间智能。整体流程包括单模态预训练、跨模态交互和统一框架预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括提出统一的多模态预训练框架，以及将文本输入和占用表示整合到预训练中。与之前的工作相比，这篇论文更全面地分析了不同模态和平台的数据集，以及预训练方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文为从多模态传感器数据中训练具有空间智能的自主系统提供了一个全面的框架和路线图。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.&lt;/p&gt;</description></item><item><guid>2512.24404v1</guid><title>Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning</title><link>http://arxiv.org/abs/2512.24404v1</link><author>Soham Pahari, M. Srinivas</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多模态智能在视觉理解和高级推理方面取得了显著进展，但大多数推理系统仍依赖文本信息进行推断，这在空间任务（如视觉导航和地理定位）中限制了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大多数推理系统依赖文本信息，这在空间任务中限制了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 讨论该领域的潜在范围，并提出一个视觉推理范式Geo-Consistent Visual Planning，即ViReLoc框架，该框架仅使用视觉表示进行规划和定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ViReLoc学习空间依赖和几何关系，通过在视觉域中编码逐步推理并使用基于强化优化的目标进行规划，同时整合对比学习和自适应特征交互来对齐跨视图视角并减少视角差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验在不同导航和定位场景中显示出空间推理准确性和跨视图检索性能的持续改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 视觉推理作为导航和定位的强大补充方法，表明这些任务可以在没有实时全球定位系统数据的情况下完成，从而实现更安全的导航解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态智能在视觉理解和高级推理方面取得了显著进展，但大多数推理系统仍依赖文本信息进行推断，这在空间任务（如视觉导航和地理定位）中限制了其有效性。本文讨论了该领域的潜在范围，并提出一个视觉推理范式Geo-Consistent Visual Planning，即ViReLoc框架，该框架仅使用视觉表示进行规划和定位。ViReLoc学习空间依赖和几何关系，通过在视觉域中编码逐步推理并使用基于强化优化的目标进行规划，同时整合对比学习和自适应特征交互来对齐跨视图视角并减少视角差异。实验在不同导航和定位场景中显示出空间推理准确性和跨视图检索性能的持续改进。这些结果表明视觉推理作为导航和定位的强大补充方法，表明这些任务可以在没有实时全球定位系统数据的情况下完成，从而实现更安全的导航解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决地面到空中的定位问题，通过视觉推理引导规划。这个问题在现实研究中很重要，因为它可以实现无需GPS的导航，提高导航的安全性和隐私性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过结合视觉推理和强化学习来设计这个方法，借鉴了现有工作中的特征提取、对比学习和强化学习等技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过视觉推理引导规划，整体实现流程包括构建画布、交叉视图地理定位和通过强化学习的视觉规划三个阶段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括统一的架构、视觉推理模块和可微分的规划系统。相比之前的工作，这个方法能够同时进行检索和规划，并且通过视觉推理来引导规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过视觉推理引导规划的方法，实现了地面到空中的定位，为无需GPS的导航提供了一种新的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.&lt;/p&gt;</description></item><item><guid>2512.24428v1</guid><title>Subsecond 3D Mesh Generation for Robot Manipulation</title><link>http://arxiv.org/abs/2512.24428v1</link><author>Qian Wang, Omar Abdellall, Tony Gao, Xiatao Sun, Daniel Rakita</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D网格是计算机科学和工程中的一种基本表示形式，在机器人学中尤其有价值，因为它们能够以与机器人如何与物理世界交互一致的方式捕捉物体，从而实现预测稳定抓取、检测碰撞和模拟动力学等核心功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 尽管自动3D网格生成方法近年来取得了显著进展，但仍然存在两个关键挑战：生成高保真网格的速度过慢，难以满足实时应用需求，通常每个物体需要数十秒；仅生成网格本身是不够的，在机器人学中，网格必须被正确地从场景中分割并注册到适当的尺度和姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 引入一个端到端系统，解决上述挑战，从单个RGB-D图像中在不到一秒的时间内生成高质量的、具有上下文信息的3D网格。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 集成开放词汇对象分割、加速的基于扩散的网格生成和鲁棒的点云注册，每个步骤都针对速度和准确性进行了优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在现实世界的操作任务中展示了其有效性，表明它能够使网格成为机器人感知和规划的实用按需表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该系统有效地解决了3D网格生成中的实时性和上下文信息问题，为机器人学提供了实用的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D网格是计算机科学和工程中的一种基本表示形式，在机器人学中尤其有价值，因为它们能够以与机器人如何与物理世界交互一致的方式捕捉物体，从而实现预测稳定抓取、检测碰撞和模拟动力学等核心功能。尽管自动3D网格生成方法近年来取得了显著进展，但仍然存在两个关键挑战：生成高保真网格的速度过慢，难以满足实时应用需求，通常每个物体需要数十秒；仅生成网格本身是不够的，在机器人学中，网格必须被正确地从场景中分割并注册到适当的尺度和姿态。引入一个端到端系统，解决上述挑战，从单个RGB-D图像中在不到一秒的时间内生成高质量的、具有上下文信息的3D网格。集成开放词汇对象分割、加速的基于扩散的网格生成和鲁棒的点云注册，每个步骤都针对速度和准确性进行了优化。在现实世界的操作任务中展示了其有效性，表明它能够使网格成为机器人感知和规划的实用按需表示。该系统有效地解决了3D网格生成中的实时性和上下文信息问题，为机器人学提供了实用的解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D网格生成在机器人操作中的实时性问题。在现实和研究中，这个问题非常重要，因为3D网格能够帮助机器人更好地理解和交互物理世界，实现抓取、避障和动力学模拟等关键任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合现有技术并加以改进来设计这个方法。他们借鉴了开放词汇对象分割、加速扩散模型和鲁棒点云注册等技术，并对这些技术进行了优化，以实现快速且准确的3D网格生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将开放词汇分割、加速网格生成和鲁棒点云注册三个步骤紧密集成，以实现快速且准确的3D网格生成。整体流程包括：首先使用 Florence-2 和 SAM2 进行开放词汇分割；然后使用 FlashVDM 加速的 Hunyuan3D 2.0 生成高保真网格；最后通过 RANSAC 和 ICP 进行点云注册，对齐网格与观测点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：使用 FlashVDM 加速扩散模型生成网格，减少生成时间；采用分层 SDF 解码和自适应键值选择技术进一步加速推理；结合 Florence-2 和 SAM2 实现开放词汇分割；使用 RANSAC 和 ICP 进行点云注册，无需纹理网格。这些创新点使得该方法在速度和准确性上都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种快速、准确的3D网格生成方法，通过整合开放词汇分割、加速扩散模型和鲁棒点云注册，实现了亚秒级的网格生成，为实时机器人应用提供了新的可能性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.&lt;/p&gt;</description></item><item><guid>2512.24438v1</guid><title>Exploring Compositionality in Vision Transformers using Wavelet Representations</title><link>http://arxiv.org/abs/2512.24438v1</link><author>Akshad Shyam Purushottamdas, Pranav K Nayak, Divya Mehul Rajparia, Deekshith Patel, Yashmitha Gogineni, Konda Reddy Mopuri, Sumohana S. Channappayya</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究通过分析视觉Transformer（ViT）编码器学习到的表示，探讨了其组合性。研究引入了一个类似于先前用于测量表示学习中组合性的框架，以测试ViT编码器的组合性。研究使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 先前对Transformer模型的工作原理的了解主要通过对它们在语言任务上的行为进行分析而获得。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本研究旨在通过组合性的视角，调查ViT编码器学习到的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 研究引入了一个类似于先前用于测量表示学习中组合性的框架，并使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 来自一阶DWT分解的基元在潜在空间中近似地组合，产生了编码器表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ViT编码器在表示空间中尊重组合性，提供了一种新的视角来理解ViT如何结构化信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究通过分析视觉Transformer（ViT）编码器学习到的表示，探讨了其组合性。研究引入了一个类似于先前用于测量表示学习中组合性的框架，以测试ViT编码器的组合性。研究使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想解决如何测试视觉Transformer（ViT）编码器表示的组成性。这个问题在现实或研究中很重要，因为理解ViT如何学习和组织信息可以帮助提高模型的解释性和性能，特别是在计算机视觉领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作中关于表示学习的组成性框架，并使用离散小波变换（DWT）来生成图像的基集（输入特定的原始元素）。这个方法的设计思路是将图像分解为视觉上有意义的原始元素，并检查这些原始元素在ViT编码器中的表示是否具有组成性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是使用DWT将图像分解为原始元素，并检查这些原始元素在ViT编码器中的表示是否可以重新组合以近似原始图像的表示。整体实现流程包括使用DWT分解图像，提取原始元素，然后检查这些原始元素在ViT编码器中的表示是否具有组成性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用DWT来分析ViT编码器的组成性，以及提出一个框架来测试ViT编码器表示的组成性。与之前的工作相比，这篇论文首次将DWT应用于ViT编码器的组成性分析，并提供了实证结果来支持这一方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过使用离散小波变换来分析视觉Transformer编码器的组成性，提供了一种新的视角来理解ViT如何结构化信息。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While insights into the workings of the transformer model have largely emerged by analysing their behaviour on language tasks, this work investigates the representations learnt by the Vision Transformer (ViT) encoder through the lens of compositionality. We introduce a framework, analogous to prior work on measuring compositionality in representation learning, to test for compositionality in the ViT encoder. Crucial to drawing this analogy is the Discrete Wavelet Transform (DWT), which is a simple yet effective tool for obtaining input-dependent primitives in the vision setting. By examining the ability of composed representations to reproduce original image representations, we empirically test the extent to which compositionality is respected in the representation space. Our findings show that primitives from a one-level DWT decomposition produce encoder representations that approximately compose in latent space, offering a new perspective on how ViTs structure information.&lt;/p&gt;</description></item><item><guid>2512.24470v1</guid><title>Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</title><link>http://arxiv.org/abs/2512.24470v1</link><author>Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavoned, Martin Steinert</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 摘要介绍了IMO MASS Code对自主和远程监督的海上船舶的要求，包括检测操作设计域的偏离、进入预定义的回退模式通知操作员、允许立即人工覆盖以及未经批准不更改航行计划。摘要还讨论了如何在这些要求中实现快速、可人工覆盖的回退操作，并介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的海上自主系统在正确操作依赖于语义理解的情况下（例如，潜水下降标志意味着有人在水中，火势附近意味着危险）难以应对。摘要指出，视觉语言模型（VLMs）可以提供语义意识，以应对这些分布外的情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 摘要旨在展示视觉语言模型（VLMs）如何提供语义意识，以及如何通过快速-慢速异常管道和短视距、可人工覆盖的回退操作，使IMO MASS Code的要求在实际操作中可行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 摘要介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器，能够在持续的人工授权下从水有效、世界锚定的轨迹中选择一个谨慎的操作（或保持停泊）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 摘要通过在40个港口场景中测量每调用场景理解、延迟、与人类共识的一致性（模型多数投票）、火灾危险场景的短视距风险缓解以及水上警报-&amp;gt;回退操作-&amp;gt;操作员接管，验证了Semantic Lookout模型的有效性。结果表明，亚10秒的模型保留了大多数较慢的先进模型的意识，回退操作选择器优于仅基于几何学的基线，并在火灾场景中增加了安全距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 摘要支持视觉语言模型作为语义回退操作选择器，符合IMO MASS Code的要求，并在实际延迟预算内有效。摘要还鼓励未来研究在领域适应的混合自主系统中，将基础模型的语义与多传感器鸟瞰感知和短视距重规划相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要介绍了IMO MASS Code对自主和远程监督的海上船舶的要求，包括检测操作设计域的偏离、进入预定义的回退模式通知操作员、允许立即人工覆盖以及未经批准不更改航行计划。摘要还讨论了如何在这些要求中实现快速、可人工覆盖的回退操作，并介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自主船舶在遇到需要语义理解的异常情况时，如何进行安全、有效的Fallback操作的问题。这个问题在现实中非常重要，因为自主船舶在遇到传统几何方法无法处理的情况（如旗帜、火灾等）时，需要能够安全地应对并等待人类接管，以确保航行安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，特别是视觉-语言模型（VLMs）和快速-慢速异常检测管道，设计出了一种基于VLM的Fallback操作选择器。该方法借鉴了Sinha等人的工作，并结合了IMO MASS Code的要求，设计出了一种短时程、可被人类覆盖的Fallback操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用VLM提供语义理解能力，选择一个谨慎的Fallback操作，直到人类接管。整体实现流程包括：使用摄像头获取图像，通过VLM理解图像内容，选择一个安全的轨迹，并在人类接管前保持船舶安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：使用VLM进行语义理解，选择一个谨慎的Fallback操作，并确保人类可以立即接管。相比之前的工作，这个方法更加注重语义理解，并且能够在短时程内做出反应，同时保持人类控制权。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于VLM的Fallback操作选择器，能够在自主船舶遇到需要语义理解的异常情况时，安全、有效地进行Fallback操作，并确保人类可以立即接管。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert-&amp;gt;fallback maneuver-&amp;gt;operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird&amp;#x27;s-eye-view perception and short-horizon replanning.&lt;/p&gt;</description></item><item><guid>2512.24504v1</guid><title>Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments</title><link>http://arxiv.org/abs/2512.24504v1</link><author>Zhiwei Wei, Yuxing Liu, Hua Liao, Wenjia Xu</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了基础模型（FM）代理在符号地图环境中的探索、记忆和推理能力，并提出了一个交互式评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地图环境是表示空间结构的基本媒介，理解基础模型代理如何理解和在地图环境中行动对于实现可靠的基于地图的推理和应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文旨在分析基础模型代理如何在符号地图环境中探索、记忆和推理，并揭示不同组件的功能角色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文提出了一种交互式评估框架，代理在部分可观察的基于网格的地图上进行增量式探索，地图包含道路、交叉口和兴趣点（POI），代理在每个步骤只接收局部观察。使用六种空间任务来评估空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 探索主要影响经验获取，但对最终推理准确性的影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是顺序和基于图的表示）显著提高路径规划等结构密集型任务的性能；推理方案进一步塑造存储的空间知识的使用方式，高级提示支持更有效的多步推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 空间推理性能在模型版本和规模超过一定能力阈值后会饱和，表明改进基于地图的空间理解需要针对空间表示和推理的机制，而不仅仅是规模扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究了基础模型（FM）代理在符号地图环境中的探索、记忆和推理能力，并提出了一个交互式评估框架。地图环境是表示空间结构的基本媒介，理解基础模型代理如何理解和在地图环境中行动对于实现可靠的基于地图的推理和应用至关重要。本文旨在分析基础模型代理如何在符号地图环境中探索、记忆和推理，并揭示不同组件的功能角色。本文提出了一种交互式评估框架，代理在部分可观察的基于网格的地图上进行增量式探索，地图包含道路、交叉口和兴趣点（POI），代理在每个步骤只接收局部观察。使用六种空间任务来评估空间理解能力。探索主要影响经验获取，但对最终推理准确性的影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是顺序和基于图的表示）显著提高路径规划等结构密集型任务的性能；推理方案进一步塑造存储的空间知识的使用方式，高级提示支持更有效的多步推理。空间推理性能在模型版本和规模超过一定能力阈值后会饱和，表明改进基于地图的空间理解需要针对空间表示和推理的机制，而不仅仅是规模扩展。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何评估基础模型（FM）代理在地图环境中的空间认知能力问题。这个问题在现实或研究中很重要，因为理解和评估FM代理在地图环境中的空间认知能力对于开发可靠的基于地图的推理系统和应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有在物理或感知环境、地图环境中的空间能力评估方法，设计了一个交互式评估框架。这个框架让代理逐步探索部分可观察的网格地图，并通过一系列任务评估其空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过交互式探索和推理来评估FM代理的空间认知能力。整体实现流程包括代理逐步探索地图环境，构建内部空间表示，并通过一系列任务（如方向判断、距离估计、路径规划等）评估其空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出一个交互式评估框架，设计一套涵盖多个城市的探测任务，系统地分析不同探索策略、记忆表示和提示方法对空间理解的影响。相比之前的工作，这个方法更注重动态、经验驱动的空间认知评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过交互式评估框架，揭示了基础模型代理在地图环境中探索、记忆和推理空间的能力，为设计更可靠的基于地图的推理系统提供了重要见解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.&lt;/p&gt;</description></item><item><guid>2512.24513v1</guid><title>From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting</title><link>http://arxiv.org/abs/2512.24513v1</link><author>Zhiwei Wei, Mengzi Zhang, Boyan Lu, Zhitao Deng, Nai Yang, Hua Liao</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究探讨了动态元素如行人和车辆对城市感知的影响，通过构建带和不带动态元素的街景图像对，进行感知实验和机器学习分析，发现移除动态元素会显著降低感知的活力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的城市分析研究通常将城市场景视为静态，忽略了动态元素如行人和车辆的作用，这可能导致感知偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一个控制框架，通过构建带和不带动态元素的街景图像对，研究动态元素对城市感知的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用语义分割和MLLM引导的生成性修复技术构建图像对，进行感知实验，并训练机器学习模型分析多模态视觉特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 移除动态元素导致感知活力降低30.97%，其他维度变化较小；光照条件、人类存在和深度变化是导致感知变化的关键因素；65%的参与者对活力感知有显著变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于静态图像的城市感知评估可能低估城市的活力，动态元素对城市感知有显著影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解城市感知从街景图像已成为城市分析和以人为本的城市设计中的核心主题。然而，大多数现有研究将城市场景视为静态，很大程度上忽略了行人、车辆等动态元素的作用，这引发了基于感知的城市分析中潜在偏差的担忧。为了解决这个问题，我们提出一个控制框架，通过构建带和不带行人和车辆的街景图像对，使用语义分割和MLLM引导的生成性修复技术，隔离动态元素的感知效应。基于中国东莞的720对图像，进行感知实验，参与者评估原始和编辑场景的六个感知维度。结果表明，移除动态元素导致感知活力一致降低30.97%，其他维度变化较小且异质；为了进一步探索潜在机制，我们使用多模态视觉特征训练了11个机器学习模型，发现光照条件、人类存在和深度变化是导致感知变化的关键因素。在个体层面，65%的参与者对活力感知有显著变化，而其他维度为35-50%；性别对安全感知有边缘调节作用。除了控制实验，训练的模型被扩展到城市规模的数据集，以预测移除动态元素后的活力变化。城市级别的结果表明，这种感知变化是普遍存在的，具有空间结构，影响73.7%的位置和32.1%的图像，表明仅基于静态图像的城市感知评估可能严重低估城市的活力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有研究大多将城市场景视为静态，忽略了动态元素（如行人和车辆）对城市感知的影响。这个问题在现实中很重要，因为动态元素实际上显著影响人们对城市的感知，而忽略它们可能导致城市分析中的偏见。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作，如使用街景图像、语义分割和机器学习来研究城市感知。他们设计了通过生成式填充去除动态元素的方法，并构建了配对街景图像进行感知实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过控制实验来隔离动态元素对城市感知的影响。整体流程包括收集街景图像，使用语义分割和生成式填充去除动态元素，然后进行感知实验，最后分析结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括使用生成式填充技术去除动态元素，并进行配对图像的感知实验。与之前工作不同，他们特别关注动态元素对城市感知的影响，并进行了大规模的实验和分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过实验和分析揭示了动态元素对城市感知的重要影响，强调了在大型感知驱动城市研究中考虑瞬时城市特征的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding urban perception from street view imagery has become a central topic in urban analytics and human centered urban design. However, most existing studies treat urban scenes as static and largely ignore the role of dynamic elements such as pedestrians and vehicles, raising concerns about potential bias in perception based urban analysis. To address this issue, we propose a controlled framework that isolates the perceptual effects of dynamic elements by constructing paired street view images with and without pedestrians and vehicles using semantic segmentation and MLLM guided generative inpainting. Based on 720 paired images from Dongguan, China, a perception experiment was conducted in which participants evaluated original and edited scenes across six perceptual dimensions. The results indicate that removing dynamic elements leads to a consistent 30.97% decrease in perceived vibrancy, whereas changes in other dimensions are more moderate and heterogeneous. To further explore the underlying mechanisms, we trained 11 machine learning models using multimodal visual features and identified that lighting conditions, human presence, and depth variation were key factors driving perceptual change. At the individual level, 65% of participants exhibited significant vibrancy changes, compared with 35-50% for other dimensions; gender further showed a marginal moderating effect on safety perception. Beyond controlled experiments, the trained model was extended to a city-scale dataset to predict vibrancy changes after the removal of dynamic elements. The city level results reveal that such perceptual changes are widespread and spatially structured, affecting 73.7% of locations and 32.1% of images, suggesting that urban perception assessments based solely on static imagery may substantially underestimate urban liveliness.&lt;/p&gt;</description></item><item><guid>2512.24532v1</guid><title>From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning</title><link>http://arxiv.org/abs/2512.24532v1</link><author>Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 在大型语言模型（LLMs）中进行空间推理的研究由于在导航和规划中的应用而受到越来越多的关注。尽管LLMs具有强大的通用语言能力，但它们在结构化环境中的空间变换和多步规划方面仍然存在困难。我们提出了一种两阶段方法，将空间推理分解为原子构建块及其组合。首先，我们对基本的空间变换（如旋转、平移和缩放）进行监督微调，以使模型具备基本的物理空间知识。然后，我们冻结这个具有物理知识的模型，并在GRPO框架内训练轻量级的LoRA适配器，以在基于谜题的环境中通过闭环方式学习组合这些构建块的多步规划策略。为了支持这一流程，我们合成了一个ASCII艺术数据集，并构建了一个相应的基于ASCII的强化学习环境。我们的方法在动态环境（具有明确的状态更新）和静态环境（模型必须依赖其内部状态跨步骤）中始终优于基线，包括通用主干、具有物理知识的模型和端到端RL模型。此外，与从头开始进行端到端强化学习相比，所提出的方法收敛更快，训练更稳定。最后，我们分析了注意力模式，以评估微调是否在空间理解方面带来了有意义的改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; We analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在大型语言模型（LLMs）中进行空间推理的研究由于在导航和规划中的应用而受到越来越多的关注。尽管LLMs具有强大的通用语言能力，但它们在结构化环境中的空间变换和多步规划方面仍然存在困难。我们提出了一种两阶段方法，将空间推理分解为原子构建块及其组合。首先，我们对基本的空间变换（如旋转、平移和缩放）进行监督微调，以使模型具备基本的物理空间知识。然后，我们冻结这个具有物理知识的模型，并在GRPO框架内训练轻量级的LoRA适配器，以在基于谜题的环境中通过闭环方式学习组合这些构建块的多步规划策略。为了支持这一流程，我们合成了一个ASCII艺术数据集，并构建了一个相应的基于ASCII的强化学习环境。我们的方法在动态环境（具有明确的状态更新）和静态环境（模型必须依赖其内部状态跨步骤）中始终优于基线，包括通用主干、具有物理知识的模型和端到端RL模型。此外，与从头开始进行端到端强化学习相比，所提出的方法收敛更快，训练更稳定。最后，我们分析了注意力模式，以评估微调是否在空间理解方面带来了有意义的改进。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大型语言模型（LLMs）在空间推理和多步规划方面的不足。这个问题在现实或研究中很重要，因为空间推理能力对于机器人导航、语言导航任务等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过将空间推理分解为原子构建块及其组合来设计这个方法，借鉴了现有工作，如使用监督微调来学习基本空间变换，并应用强化学习来优化这些构建块的组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将空间理解分解为一系列基本变换，并使用监督微调来学习这些变换，然后通过强化学习来优化这些变换的组合。整体实现流程包括两个阶段：监督微调阶段和强化学习阶段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括将空间推理分解为原子构建块，使用ASCII艺术数据集和强化学习环境，以及通过LoRA适配器来优化策略。相比之前的工作，这个方法更注重构建块的组合和强化学习的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合监督微调和强化学习的新型空间推理方法，显著提升了LLMs在多步空间规划任务中的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.&lt;/p&gt;</description></item><item><guid>2512.24593v1</guid><title>3D Semantic Segmentation for Post-Disaster Assessment</title><link>http://arxiv.org/abs/2512.24593v1</link><author>Nhut Le, Maryam Rahnemoonfar</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。3D语义分割对于灾害后的评估至关重要，但现有的深度学习模型缺乏专门针对灾害后环境的数据库。为了解决这个问题，我们使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。我们评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现，揭示了现有方法在受灾地区的重大局限性。这些发现强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决现有的深度学习模型缺乏专门针对灾害后环境的数据库的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 现有方法在受灾地区的重大局限性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。3D语义分割对于灾害后的评估至关重要，但现有的深度学习模型缺乏专门针对灾害后环境的数据库。为了解决这个问题，我们使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。我们评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现，揭示了现有方法在受灾地区的重大局限性。这些发现强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有深度学习模型缺乏针对灾后环境设计的3D语义分割数据集的问题。这个问题在现实中非常重要，因为准确的灾后评估对于救援行动和减少经济损失至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的3D重建技术和深度学习模型，设计了一个针对灾后环境的3D语义分割数据集。他们使用了无人机拍摄的数据，并应用了Structure-from-Motion和Multi-View Stereo技术来重建3D点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是创建一个专门针对灾后环境的3D语义分割数据集，并评估现有深度学习模型在该数据集上的性能。整体流程包括数据收集、3D点云重建、生成地面真实验证标签，以及使用Fast Point Transformer、Point Transformer v3和OA-CNNs模型进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点在于构建了一个专门针对灾后环境的3D语义分割数据集，并评估了现有深度学习模型在该数据集上的性能。与之前的工作相比，这篇论文强调了现有模型在灾后环境中的局限性，并提出了改进的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建一个针对灾后环境的3D语义分割数据集，揭示了现有深度学习模型在该场景下的局限性，并强调了改进3D语义分割技术的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses. While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments. To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds. We evaluated the state-of-the-art (SOTA) 3D semantic segmentation models, Fast Point Transformer (FPT), Point Transformer v3 (PTv3), and OA-CNNs on this dataset, exposing significant limitations in existing methods for disaster-stricken regions. These findings underscore the urgent need for advancements in 3D segmentation techniques and the development of specialized 3D benchmark datasets to improve post-disaster scene understanding and response.&lt;/p&gt;</description></item><item><guid>2512.24605v1</guid><title>MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding</title><link>http://arxiv.org/abs/2512.24605v1</link><author>Panquan Yang, Junfei Huang, Zongzhangbao Yin, Yingsong Hu, Anni Xu, Xinyi Luo, Xueqi Sun, Hai Wu, Sheng Ao, Zhaoxing Zhu, Chenglu Wen, Cheng Wang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D视觉定位旨在将自然语言句子中的对象定位到3D点云场景中。这对路边基础设施系统在复杂交通环境中解释自然语言和定位相关目标至关重要。然而，大多数现有的数据集和方法都集中在室内和室外驾驶场景，由于缺乏路边基础设施传感器捕获的配对点云-文本数据，室外监控场景仍然未被探索。本文引入了一个新的任务，即室外监控场景的3D视觉定位，它使基础设施级别的交通场景理解超越了自车视角。为了支持这个任务，我们构建了MoniRefer，这是第一个真实世界的路边级3D视觉定位多模态数据集。该数据集包含约136,018个对象和411,128个自然语言表达，收集自多个复杂的交通十字路口。为了确保数据集的质量和准确性，我们对所有语言描述和3D标签进行了人工验证。此外，我们还提出了一种新的端到端方法，名为Moni3DVG，它利用图像提供的丰富外观信息和几何信息以及点云的光学信息进行多模态特征学习和3D对象定位。在提出的基准上的大量实验和消融研究证明了我们方法的优势和有效性。我们的数据集和代码将被发布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大多数现有的3D视觉定位数据集和方法都集中在室内和室外驾驶场景，而室外监控场景由于缺乏配对点云-文本数据而未被探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 引入室外监控场景的3D视觉定位任务，并构建一个真实世界的路边级3D视觉定位多模态数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出了一种新的端到端方法Moni3DVG，利用图像和点云的多模态特征学习和3D对象定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MoniRefer数据集包含约136,018个对象和411,128个自然语言表达，通过人工验证确保了质量和准确性。Moni3DVG方法在实验中证明了其优势和有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提出的室外监控场景的3D视觉定位任务和Moni3DVG方法为交通场景理解提供了新的途径，并展示了其在真实世界数据集上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决户外监控场景下的3D视觉定位问题，即如何根据自然语言描述在3D点云场景中定位相关对象。这个问题在现实研究中非常重要，因为对于路边基础设施系统来说，能够解释自然语言并定位复杂交通环境中的相关目标至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D视觉定位研究的局限性，特别是户外监控场景中数据稀缺、语言多样性有限和视角不一致等问题，设计出MoniRefer数据集和Moni3DVG方法。该方法借鉴了现有2D视觉定位和3D视觉定位的研究成果，并针对户外监控场景进行了优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; Moni3DVG方法的核心思想是通过整合图像和点云的多模态特征进行3D对象定位。整体实现流程包括数据采集、多模态特征学习和3D对象定位。首先，使用LiDAR和相机采集多模态数据；然后，提取图像和点云的 appearance、几何和光学信息进行特征学习；最后，利用这些特征匹配自然语言描述，定位目标对象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括构建了第一个大规模户外监控场景的多模态数据集MoniRefer，提出了Moni3DVG方法，并实现了在户外监控场景下的高效3D视觉定位。相比之前的工作，MoniRefer数据集更注重户外场景和语言多样性，Moni3DVG方法则更有效地整合了多模态信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建MoniRefer数据集和提出Moni3DVG方法，为户外监控场景下的3D视觉定位任务提供了新的基准和解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;</description></item><item><guid>2512.24708v1</guid><title>BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework</title><link>http://arxiv.org/abs/2512.24708v1</link><author>András Millinghoffer, András Formanek, András Antos, Péter Antal</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; BandiK是一种新的多任务辅助任务子集选择方法，使用多臂老虎机来解决问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 有效地在多个任务之间转移知识非常重要，但在基础模型下游任务中仍然存在挑战。转移的性质，特别是其可传递性和不可传递性，仍然是一个开放的问题，负迁移仍然是一个重大障碍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决选择有益辅助任务集的约束，这些约束通常受到高计算成本、大量合理的候选辅助集以及目标任务之间选择复杂性的阻碍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BandiK引入了一种三阶段的多任务辅助任务子集选择方法，使用多臂老虎机。每个臂拉都通过在一个随机训练-测试数据集分割上训练和测试一个多输出神经网络来评估候选辅助集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; BandiK首先估计任务之间的成对迁移，这有助于识别哪些任务可能从联合学习中受益。在第二阶段，它基于初始估计为每个目标任务构建线性数量的候选辅助任务集。第三阶段，它采用多臂老虎机框架，其中臂对应于候选辅助集的性能，这些辅助集作为多个输出神经网络在训练-测试数据集分割上实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BandiK通过将单个任务特定的MAB集成到多臂老虎机结构中来提高效率。这种解决方案利用了相同的神经网络实现了多个臂，这些臂对应于给定候选集的不同单个老虎机。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 有效地在多个任务之间转移知识非常重要，但在基础模型下游任务中仍然存在挑战。转移的性质，特别是其可传递性和不可传递性，仍然是一个开放的问题，负迁移仍然是一个重大障碍。选择有益辅助任务集的约束通常受到高计算成本、大量合理的候选辅助集以及目标任务之间选择复杂性的阻碍。BandiK引入了一种三阶段的多任务辅助任务子集选择方法，使用多臂老虎机。每个臂拉都通过在一个随机训练-测试数据集分割上训练和测试一个多输出神经网络来评估候选辅助集。BandiK首先估计任务之间的成对迁移，这有助于识别哪些任务可能从联合学习中受益。在第二阶段，它基于初始估计为每个目标任务构建线性数量的候选辅助任务集。第三阶段，它采用多臂老虎机框架，其中臂对应于候选辅助集的性能，这些辅助集作为多个输出神经网络在训练-测试数据集分割上实现。BandiK通过将单个任务特定的MAB集成到多臂老虎机结构中来提高效率。这种解决方案利用了相同的神经网络实现了多个臂，这些臂对应于给定候选集的不同单个老虎机。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多任务学习中如何高效选择辅助任务集的问题。这个问题在现实或研究中非常重要，因为选择合适的辅助任务集可以显著提高多任务学习的效率和性能，同时避免负迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴多臂老虎机（MAB）的方法，设计了一个三阶段的多任务辅助任务子集选择方法。这个方法借鉴了现有工作中关于多臂老虎机在超参数优化和神经架构搜索中的应用，以及多任务学习的相关研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用多臂老虎机来选择最佳的辅助任务集。整体实现流程包括三个阶段：首先估计任务之间的成对迁移效应，并构建正向和负向迁移图；然后基于初始估计构建候选辅助任务集；最后使用多臂老虎机框架选择每个目标任务的最佳多任务神经网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用半重叠臂的多臂老虎机框架，以及将单个任务特定的多臂老虎机集成到多臂老虎机结构中。相比之前的工作，这个方法可以更有效地选择辅助任务集，并且减少了计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于多臂老虎机的高效多任务辅助任务子集选择方法，可以显著提高多任务学习的效率和性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The challenge of effectively transferring knowledge across multiple tasks is of critical importance and is also present in downstream tasks with foundation models. However, the nature of transfer, its transitive-intransitive nature, is still an open problem, and negative transfer remains a significant obstacle. Selection of beneficial auxiliary task sets in multi-task learning is frequently hindered by the high computational cost of their evaluation, the high number of plausible candidate auxiliary sets, and the varying complexity of selection across target tasks.   To address these constraints, we introduce BandiK, a novel three-stage multi-task auxiliary task subset selection method using multi-bandits, where each arm pull evaluates candidate auxiliary sets by training and testing a multiple output neural network on a single random train-test dataset split. Firstly, BandiK estimates the pairwise transfers between tasks, which helps in identifying which tasks are likely to benefit from joint learning. In the second stage, it constructs a linear number of candidate sets of auxiliary tasks (in the number of all tasks) for each target task based on the initial estimations, significantly reducing the exponential number of potential auxiliary task sets. Thirdly, it employs a Multi-Armed Bandit (MAB) framework for each task, where the arms correspond to the performance of candidate auxiliary sets realized as multiple output neural networks over train-test data set splits. To enhance efficiency, BandiK integrates these individual task-specific MABs into a multi-bandit structure. The proposed multi-bandit solution exploits that the same neural network realizes multiple arms of different individual bandits corresponding to a given candidate set. This semi-overlapping arm property defines a novel multi-bandit cost/reward structure utilized in BandiK.&lt;/p&gt;</description></item><item><guid>2512.24763v1</guid><title>UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning</title><link>http://arxiv.org/abs/2512.24763v1</link><author>Ankit Dhiman, Srinath R, Jaswanth Reddy, Lokesh R Boregowda, Venkatesh Babu Radhakrishnan</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。一个关键挑战是 2D 实例标签跨视图的不一致性，导致 3D 预测效果不佳。现有方法采用两阶段方法，一些依赖于对比学习和超参数敏感的聚类，而另一些则预处理标签以实现一致性。我们提出一个统一框架，将这些步骤合并，通过引入可学习的特征嵌入来减少训练时间并提高性能，该嵌入用于高斯原语中的分割。然后通过一种新的“嵌入到标签”过程高效解码成实例标签，有效地集成了优化。尽管这个统一框架提供了显著的好处，但我们观察到在物体边界处存在伪影。为了解决物体边界问题，我们提出沿着这些边界进行硬挖掘样本。然而，直接将硬挖掘应用于特征嵌入被证明是不稳定的。因此，我们在计算三元组损失之前对光栅化特征嵌入应用线性层，这稳定了训练并显著提高了性能。我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决 2D 实例标签跨视图的不一致性，提高 3D 预测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出一个统一框架，将对比学习和标签预处理步骤合并，引入可学习的特征嵌入来减少训练时间并提高性能，通过“嵌入到标签”过程高效解码成实例标签，并应用线性层进行硬挖掘以稳定训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 统一框架提供了显著的好处，但在物体边界处存在伪影。应用线性层进行硬挖掘稳定了训练并显著提高了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。一个关键挑战是 2D 实例标签跨视图的不一致性，导致 3D 预测效果不佳。现有方法采用两阶段方法，一些依赖于对比学习和超参数敏感的聚类，而另一些则预处理标签以实现一致性。我们提出一个统一框架，将这些步骤合并，通过引入可学习的特征嵌入来减少训练时间并提高性能，该嵌入用于高斯原语中的分割。然后通过一种新的“嵌入到标签”过程高效解码成实例标签，有效地集成了优化。尽管这个统一框架提供了显著的好处，但我们观察到在物体边界处存在伪影。为了解决物体边界问题，我们提出沿着这些边界进行硬挖掘样本。然而，直接将硬挖掘应用于特征嵌入被证明是不稳定的。因此，我们在计算三元组损失之前对光栅化特征嵌入应用线性层，这稳定了训练并显著提高了性能。我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从2D图像中不一致的实例分割标签生成一致的三维分割标签的问题。这个问题在现实或研究中非常重要，因为三维场景理解对于增强现实、自动驾驶和路径规划等领域至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的三维表示方法如NeRF和3DGS，并利用对比学习的思想设计出这个方法。他们参考了Contrastive-Lift的工作，但对其进行了改进，以实现更高效的训练和更准确的分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过对比学习将二维分割标签直接解码为三维分割标签。整体实现流程包括：将二维图像渲染为三维高斯点云，为每个三维高斯点云添加一个d维向量嵌入，通过对比损失优化这些嵌入，然后通过线性层和三重损失进一步优化，最后通过“嵌入到标签”过程生成三维分割标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：提出了一种统一的单阶段方法，直接将学习到的三维嵌入解码为一致的三维分割标签；使用基于三重损失的新型对比损失来减少类间方差，从而实现更准确的三维分割。与之前的工作相比，这个方法更高效，因为它不需要额外的后处理步骤，并且具有更快的推理时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于对比学习的统一三维实例分割方法，该方法能够直接从二维分割标签生成一致的三维分割标签，并在多个数据集上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel &amp;quot;Embedding-to-Label&amp;quot; process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.&lt;/p&gt;</description></item><item><guid>2512.24834v1</guid><title>GenZ: Foundational models as latent variable generators within traditional statistical models</title><link>http://arxiv.org/abs/2512.24834v1</link><author>Marko Jojic, Nebojsa Jojic</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; GenZ是一种混合模型，它通过可解释的语义特征连接了基础模型和统计模型。该方法通过迭代过程发现语义特征描述，对比通过统计建模错误识别的项目组，而不是仅仅依赖基础模型的领域理解。该方法被表述为一个通用的EM算法，联合优化语义特征描述和统计模型参数。该方法提示一个冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为对潜在二元特征的噪声观察，这些特征通过学习的统计关系预测实值目标。该方法在两个领域进行了演示：房价预测（享乐回归）和冷启动协同过滤用于电影推荐。在房价预测中，该模型使用从多模态列表数据中发现的语义特征实现了12%的中值相对误差，大大优于依赖LLM的通用领域知识的GPT-5基线（误差为38%）。对于Netflix电影嵌入，该模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式（例如，建筑细节预测当地房地产市场，系列成员身份预测用户偏好），这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型拥有广泛的领域知识，但它们通常无法捕捉对预测任务至关重要的数据集特定模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出GenZ模型，通过可解释的语义特征连接基础模型和统计模型，以解决大型语言模型在捕捉数据集特定模式方面的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过迭代过程对比通过统计建模错误识别的项目组，发现语义特征描述，并使用通用的EM算法联合优化语义特征描述和统计模型参数。提示一个冻结的基础模型根据发现的特征对项目进行分类，并将这些判断视为对潜在二元特征的噪声观察。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在房价预测中，GenZ模型使用发现的语义特征实现了12%的中值相对误差，远优于依赖LLM的通用领域知识的GPT-5基线。对于Netflix电影嵌入，GenZ模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式，这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GenZ模型通过可解释的语义特征有效地连接了基础模型和统计模型，在房价预测和电影推荐领域取得了显著的性能提升，并揭示了数据集特定的模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了GenZ，一种通过可解释的语义特征连接基础模型和统计模型的混合模型。虽然大型语言模型拥有广泛的领域知识，但它们通常无法捕捉对预测任务至关重要的数据集特定模式。我们的方法通过迭代过程对比通过统计建模错误识别的项目组，发现语义特征描述，而不是仅仅依赖基础模型的领域理解。我们将此表述为一个通用的EM算法，联合优化语义特征描述和统计模型参数。该方法提示一个冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为对潜在二元特征的噪声观察，这些特征通过学习的统计关系预测实值目标。我们在两个领域演示了该方法：房价预测（享乐回归）和冷启动协同过滤用于电影推荐。在房价预测中，我们的模型使用发现的语义特征实现了12%的中值相对误差，远优于依赖LLM的通用领域知识的GPT-5基线。对于Netflix电影嵌入，我们的模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式，这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将大型语言模型（LLM）的广泛领域知识与特定数据集的模式相结合，以改进预测任务。这个问题在现实或研究中很重要，因为LLM虽然知识广泛，但往往无法捕捉到特定数据集的独特模式，这限制了它们在预测任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴概念瓶颈模型（CBM）的思想，设计了一个混合模型，该模型通过迭代过程对比通过统计建模错误识别的项目组来发现语义特征描述，而不是仅仅依赖LLM的领域理解。这个方法借鉴了现有工作，但引入了新的元素，如不确定性参数和基于统计模型后验分布的对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用LLM作为解释器，通过对比统计模型识别的项目组来发现语义特征，并使用这些特征来改进预测模型。整体实现流程包括使用LLM对项目进行分类，通过统计模型的后验分布对比项目组，使用这些对比信息来更新语义特征描述，并最终联合优化语义特征和统计模型参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用冻结的LLM作为解释器，通过对比统计模型识别的项目组来发现语义特征，以及支持任意映射的统计模型。与之前的工作相比，这个方法更注重发现数据集特定的模式，而不是仅仅依赖LLM的领域知识，并且能够处理高维实值目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的混合模型，通过结合LLM和统计建模来发现数据集特定的语义特征，从而提高预测任务的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model&amp;#x27;s domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM&amp;#x27;s general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model&amp;#x27;s domain knowledge alone.&lt;/p&gt;</description></item><item><guid>2512.24845v1</guid><title>ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation</title><link>http://arxiv.org/abs/2512.24845v1</link><author>Qiuyi Gu, Yuze Sheng, Jincheng Yu, Jiahao Tang, Xiaolong Shan, Zhaoyang Shen, Tinghao Yi, Xiaodan Liang, Xinlei Chen, Yu Wang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。为弥补这一差距，我们提出了ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。该方法利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。我们将这些运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。大量实际实验表明，ArtiSG在功能元素召回和关节估计精度方面显著优于基线。此外，我们证明构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补3D场景图在功能信息方面的不足，特别是对于关节对象的功能信息，通过构建功能3D场景图来指导机器人在真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。将运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ArtiSG在功能元素召回和关节估计精度方面显著优于基线。构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ArtiSG框架能有效构建功能3D场景图，弥补现有方法的不足，并显著提升机器人在真实环境中的操作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。为弥补这一差距，我们提出了ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。该方法利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。我们将这些运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。大量实际实验表明，ArtiSG在功能元素召回和关节估计精度方面显著优于基线。此外，我们证明构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景图中缺乏功能信息的问题，特别是对于需要物理操作的关节对象。这个问题在现实研究中很重要，因为机器人需要理解物体的功能属性才能进行有效的物理操作和任务执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，如利用人类演示进行机器人学习和3D场景图构建，设计出ArtiSG方法。他们注意到现有方法的局限性，如视觉歧义和缺乏功能细节，因此提出了一个结合人类演示和视觉基础模型的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将人类演示编码到结构化的机器人记忆中，以构建功能3D场景图。整体流程包括三个阶段：初始化功能场景图，利用便携式设备进行视角鲁棒的关节估计，以及交互增强的图细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括视角鲁棒的关节跟踪、交互增强的功能元素检测和开放词汇场景构建。与之前的工作相比，ArtiSG能够更准确地估计关节机制，并识别被视觉感知忽略的细微功能元素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ArtiSG通过结合人类演示和视觉基础模型，构建了功能3D场景图，使机器人能够更好地理解和操作关节对象。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.&lt;/p&gt;</description></item><item><guid>2512.24866v1</guid><title>Characterization of Transfer Using Multi-task Learning Curves</title><link>http://arxiv.org/abs/2512.24866v1</link><author>András Millinghoffer, Bence Bolgár, Péter Antal</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。我们假设通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，可以更根本地描述迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，以更根本地描述迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 我们使用多任务学习曲线来定量地建模迁移效应，这些曲线近似于不同样本数量下的归纳性能。我们描述了一种高效的方法来近似多任务学习曲线，类似于训练期间的任务亲和分组方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 我们的结果表明，学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。我们假设通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，可以更根本地描述迁移效应。为了捕捉这种现象，我们使用多任务学习曲线来定量地建模迁移效应，这些曲线近似于不同样本数量下的归纳性能。我们描述了一种高效的方法来近似多任务学习曲线，类似于训练期间的任务亲和分组方法。我们比较了迁移的统计和计算方法，这表明之前的计算成本要高得多，但具有更好的性能和更广泛的适用性。我们使用基准药物靶点相互作用数据集进行了评估。我们的结果表明，学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多任务学习中迁移效应的表征问题。这个问题在现实或研究中非常重要，因为迁移效应直接影响机器学习模型的性能和泛化能力，特别是在处理大规模、多任务问题时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多任务学习方法，特别是任务亲和分组方法，设计出一种新的方法来表征迁移效应。他们借鉴了现有工作，但提出了新的学习曲线方法来更有效地捕捉迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过学习曲线来表征迁移效应，特别是通过样本数量来观察模型性能的变化。整体实现流程包括使用多任务学习曲线来建模和评估迁移效应，通过系统地比较不同任务组合下的性能变化来分析迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出新的多任务学习曲线方法来表征迁移效应，以及通过系统地比较统计和计算方法来分析迁移效应。相比之前的工作，这个方法更注重样本数量对模型性能的影响，并且能够更有效地捕捉迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出新的多任务学习曲线方法，有效地表征和分析了多任务学习中的迁移效应。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Transfer effects manifest themselves both during training using a fixed data set and in inductive inference using accumulating data. We hypothesize that perturbing the data set by including more samples, instead of perturbing the model by gradient updates, provides a complementary and more fundamental characterization of transfer effects. To capture this phenomenon, we quantitatively model transfer effects using multi-task learning curves approximating the inductive performance over varying sample sizes. We describe an efficient method to approximate multi-task learning curves analogous to the Task Affinity Grouping method applied during training. We compare the statistical and computational approaches to transfer, which indicates considerably higher compute costs for the previous but better power and broader applicability. Evaluations are performed using a benchmark drug-target interaction data set. Our results show that learning curves can better capture the effects of multi-task learning and their multi-task extensions can delineate pairwise and contextual transfer effects in foundation models.&lt;/p&gt;</description></item><item><guid>2512.24880v1</guid><title>mHC: Manifold-Constrained Hyper-Connections</title><link>http://arxiv.org/abs/2512.24880v1</link><author>Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 近期研究，如Hyper-Connections (HC)，通过扩展残差流宽度和多样化连接模式，扩展了过去十年建立的普遍残差连接范式。虽然这带来了显著的性能提升，但这种多样化从根本上破坏了残差连接的恒等映射特性，导致严重的训练不稳定性、有限的扩展性，并增加了显著的内存访问开销。为了解决这些挑战，我们提出了Manifold-Constrained Hyper-Connections (mHC)，一个将HC的残差连接空间投影到特定流形上的通用框架，以恢复恒等映射特性，同时结合严格的架构优化以确保效率。实证实验表明，mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。我们预计，作为HC的一种灵活且实用的扩展，mHC将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Hyper-Connections (HC)通过扩展残差流宽度和多样化连接模式提升了性能，但破坏了恒等映射特性，导致训练不稳定性、有限扩展性和内存访问开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决HC带来的训练不稳定性、有限扩展性和内存访问开销问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出Manifold-Constrained Hyper-Connections (mHC)，将残差连接空间投影到特定流形上，恢复恒等映射特性，并结合严格的架构优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; mHC作为HC的灵活且实用的扩展，将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近期研究，如Hyper-Connections (HC)，通过扩展残差流宽度和多样化连接模式，扩展了过去十年建立的普遍残差连接范式。虽然这带来了显著的性能提升，但这种多样化从根本上破坏了残差连接的恒等映射特性，导致严重的训练不稳定性、有限的扩展性，并增加了显著的内存访问开销。为了解决这些挑战，我们提出了Manifold-Constrained Hyper-Connections (mHC)，一个将HC的残差连接空间投影到特定流形上的通用框架，以恢复恒等映射特性，同时结合严格的架构优化以确保效率。实证实验表明，mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。我们预计，作为HC的一种灵活且实用的扩展，mHC将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Hyper-Connections（HC）在扩展连接复杂度时导致的训练不稳定和可扩展性问题。这个问题在研究中很重要，因为HC虽然能提升性能，但其无约束的连接方式破坏了残差连接的恒等映射特性，导致大规模训练时信号放大或衰减，影响训练效率和模型稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，特别是HC的设计思想，提出了Manifold-Constrained Hyper-Connections（mHC）方法。mHC借鉴了将残差连接空间投影到特定流形上的思想，利用Sinkhorn-Knopp算法将残差连接矩阵投影到Birkhoff多面体上，以恢复恒等映射特性，同时结合基础设施优化确保效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; mHC的核心思想是将HC的残差连接空间投影到由双随机矩阵构成的流形上，以恢复恒等映射特性。整体实现流程包括：1) 利用Sinkhorn-Knopp算法将残差连接矩阵投影到Birkhoff多面体上；2) 通过矩阵的行和列和为1的特性，确保信号在传播过程中的均值和范数得到有效控制；3) 结合基础设施优化，如内核融合、混合精度内核和通信重叠等，提高效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; mHC的关键创新点包括：1) 将残差连接空间投影到特定流形上，恢复恒等映射特性，提高训练稳定性；2) 结合基础设施优化，如内核融合、混合精度内核和通信重叠等，提高效率。相比之前的工作，mHC在保持HC拓扑复杂度的同时，解决了其训练不稳定和可扩展性问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; mHC通过将残差连接空间投影到特定流形上，恢复了恒等映射特性，同时结合基础设施优化，提高了大规模训练的稳定性和可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.&lt;/p&gt;</description></item><item><guid>2512.24896v1</guid><title>Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing</title><link>http://arxiv.org/abs/2512.24896v1</link><author>Andrii Gamalii, Daniel Górniak, Robert Nowak, Bartłomiej Olber, Krystian Radlak, Jakub Winter</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该报告介绍了DARTS项目内开发的一种半自动化数据标注流程，旨在创建大规模、多模态的波兰驾驶场景数据集。手动标注此类异构数据既昂贵又耗时。为应对这一挑战，提出的解决方案采用人机协作方法，结合人工智能与人类专业知识，以降低标注成本和时长。系统自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。核心工具依赖于3D目标检测算法生成初步标注。总体而言，开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 手动标注大规模、多模态的波兰驾驶场景数据集既昂贵又耗时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 创建大规模、多模态的波兰驾驶场景数据集，并降低标注成本和时长。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用人机协作方法，结合人工智能与人类专业知识，自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该报告介绍了DARTS项目内开发的一种半自动化数据标注流程，旨在创建大规模、多模态的波兰驾驶场景数据集。手动标注此类异构数据既昂贵又耗时。为应对这一挑战，提出的解决方案采用人机协作方法，结合人工智能与人类专业知识，以降低标注成本和时长。系统自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。核心工具依赖于3D目标检测算法生成初步标注。总体而言，开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶车辆测试中多传感器数据集的手动标注成本高、耗时长的问题。这个问题在现实或研究中很重要，因为高质量的标注数据集是训练和评估自动驾驶感知系统的关键，而手动标注难以满足大规模、多模态数据集的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过结合人工智能和人类专业知识，设计了一个半自动化的数据标注流程。这个方法借鉴了现有的3D目标检测算法和人类在环验证技术，并对这些技术进行了改进和整合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用人工智能自动生成初始标注，然后通过人类验证和修正来提高标注质量。整体实现流程包括数据预处理、自动标注生成、人类验证、质量控制和模型迭代训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括结合人工 intelligence 和人类专业知识进行半自动化标注，以及使用3D目标检测算法和领域适应技术来提高标注效率和准确性。相比之前的工作，这个方法更加注重效率和准确性，并且能够处理多模态数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种半自动化的数据标注方法，通过结合人工智能和人类专业知识，有效地提高了自动驾驶车辆测试中多传感器数据集的标注效率和准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project&amp;#x27;s standardized format, strengthening the technological base for autonomous vehicle research in Poland.&lt;/p&gt;</description></item><item><guid>2512.24922v1</guid><title>Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</title><link>http://arxiv.org/abs/2512.24922v1</link><author>Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak</author><pubDate>Fri, 02 Jan 2026 11:54:02 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于神经元激活模式的激光雷达领域适应方法，只需对目标域中少量精选且多样的样本进行标注，即可实现最先进的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D目标检测器是自动驾驶感知系统的核心，但在不同地区或数据分布之间的迁移能力不足，例如在美国训练的模型在亚洲或欧洲表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在开发一种能够在极小标注预算下实现跨域适应的技术，并通过后训练策略防止模型权重漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用神经元激活模式挑选目标域的代表性样本进行少量标注，随后结合受持续学习启发的后训练技术，对模型进行微调，以保持原始模型的权重稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该领域适应方法在精度上超过线性探测和现有最先进的领域适应技术，并且只需极少的标注成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过正确选择少量多样化的目标域样本并使用持续学习的后训练手段，可在保持原模型性能的同时实现高效的跨域适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D目标检测器是自动驾驶感知系统的基础组件。虽然这些检测器在标准的自动驾驶基准上取得了显著的性能，但它们常常难以在不同领域之间实现泛化——例如，在美国训练的模型在亚洲或欧洲等地区可能表现不佳。本文提出了一种基于神经元激活模式的全新激光雷达领域适应方法，证明只要正确选择目标域中少量具有代表性且多样性的样本进行标注，就能实现最先进的性能。该方法只需极少的标注预算，并且结合受持续学习启发的后训练技术，可防止模型权重从原始模型漂移。实证评估表明，所提出的领域适应方法在性能上优于线性探测和最先进的领域适应技术。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D目标检测模型在不同地区或传感器条件下的域适应问题，即如何用极少量标注的目标域数据使模型在新环境中保持高性能。这在自动驾驶实际部署中非常关键，因为不同地区的车辆尺寸、道路布局和LiDAR配置差异会导致模型性能大幅下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为低层特征对跨域检测至关重要，因而聚焦于细粒度的微调，并借鉴了持续学习防止灾难性遗忘的技术、L2‑SP 权重正则化以及已有的点云密度和边框尺寸对齐方法。同时，他们创新性地使用神经元激活模式来挑选最具代表性的目标域样本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过提取ROI头的激活向量，剪枝并二值化后衡量样本多样性，选出少量代表性帧进行微调。整体流程为：从预训练模型中获取激活模式 → 处理得到稀疏二进制特征 → 计算多样性并挑选样本 → 使用小学习率或L2‑SP 等正则化进行后训练，同时加入持续学习技巧以保持源域性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 基于神经激活模式的多样性样本选择算法；② 证明仅需少量目标域样本即可实现有效适应；③ 将持续学习正则化与微调相结合防止权重漂移；④ 在保持实现简洁的同时超越了线性探测和其他先进域适应方法。与以往依赖对抗学习、伪标签或大规模未标注数据的做法不同，本文侧重于少量高质量标注和激活驱动的选择。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于激活模式的多样性驱动微调加持续学习正则化的域适应方案，使3D LiDAR目标检测在仅使用少量精选目标域样本的情况下实现跨地区高效迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.&lt;/p&gt;</description></item><item><guid>2512.25008v1</guid><title>FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</title><link>http://arxiv.org/abs/2512.25008v1</link><author>Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</author><pubDate>Fri, 02 Jan 2026 11:54:02 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 FoundationSLAM，一种基于学习的单目稠密 SLAM 系统，通过将光流估计与几何推理相结合，解决了以往光流方法缺乏几何一致性的问题，实现了精准且鲁棒的跟踪与建图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往基于光流的 SLAM 方法在几何一致性方面存在不足，导致跟踪和映射的精度和鲁棒性受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在构建一个能够在几何一致性约束下进行准确、鲁棒且实时的单目稠密 SLAM 系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 核心思路是利用基础深度模型的引导，将光流估计与几何推理相结合。具体包括：1）设计混合光流网络，生成几何感知的对应关系，实现跨关键帧的一致深度和位姿推断；2）提出双一致性束束优化层，在多视图约束下联合优化关键帧位姿和深度，以保证全局一致性；3）引入可靠性感知的细化机制，通过区分可靠和不确定区域动态调整光流更新，形成匹配与优化的闭环反馈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 大量实验表明，FoundationSLAM 在多个具有挑战性的数据集上实现了更高的轨迹精度和稠密重建质量，且能够以 18 帧每秒的实时速度运行，展示了对各种场景的强泛化能力和实际应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FoundationSLAM 通过融合光流与深度的几何约束，成功提升了单目稠密 SLAM 的精度、鲁棒性和实时性，具备良好的通用性和实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 FoundationSLAM，这是一种基于学习的单目稠密 SLAM 系统，解决了以往基于光流的方法在几何一致性方面的缺失，从而实现了准确且鲁棒的跟踪和建图。我们的核心思想是通过利用基础深度模型的引导，将光流估计与几何推理相结合。为此，我们首先开发了一个混合光流网络，生成几何感知的对应关系，使得在不同关键帧之间能够进行一致的深度和位姿推断。为了强制全局一致性，我们提出了双一致性束束优化层，在多视图约束下联合优化关键帧位姿和深度。此外，我们引入了可靠性感知的细化机制，通过区分可靠和不确定区域动态调整光流更新过程，在匹配和优化之间形成闭环反馈。大量实验表明，FoundationSLAM 在多个具有挑战性的数据集上实现了卓越的轨迹精度和稠密重建质量，同时以 18 FPS 的实时速度运行，展示了对各种场景的强泛化能力和我们方法的实际适用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文针对现有单目密集 SLAM 系统仅依赖光流而缺乏几何一致性的问题，导致位姿估计和稠密重建出现结构错误和不完整。几何一致性是机器人导航和三维建模的核心，直接影响系统的鲁棒性和精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为需要将几何先验引入光流匹配，并在多视图优化中强化几何约束，于是结合了已有的流式 SLAM（如 DROID‑SLAM）和基础深度模型（如 FoundationStereo）的思路，设计了融合深度特征的混合流网络和双向束调整层。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让光流估计受深度先验指导，并在闭环中通过多视图几何约束共同优化深度和位姿。整体流程是：先用混合流网络利用基础深度特征预测光流；随后双向束调整层利用流和几何残差联合优化关键帧的深度与位姿；最后可靠性感知的细化模块根据优化残差更新流的可靠性掩码并再次迭代。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新包括：① 将基础深度模型的几何特征注入光流网络，实现几何感知的对应估计；② 提出双向一致性束调整层，显式加入正向和反向几何一致性约束；③ 可靠性感知的流细化机制，根据优化残差动态调整流更新。相比仅基于像素相关性的传统流式 SLAM，这些设计实现了更强的全局几何一致性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文通过融合基础深度先验、双向几何约束和残差驱动的流细化，构建了一个几何一致且实时的单目稠密 SLAM 系统。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.&lt;/p&gt;</description></item></channel></rss>