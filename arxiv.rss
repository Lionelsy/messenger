<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Arxiv论文推荐</title><link>https://arxiv.org/</link><description>Arxiv论文推荐（自动生成）</description><language>zh-CN</language><lastBuildDate>Fri, 02 Jan 2026 20:58:49 +0800</lastBuildDate><item><guid>2512.18954v1</guid><title>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</title><link>http://arxiv.org/abs/2512.18954v1</link><author>Zaidao Han, Risa Higashita, Jiang Liu</author><pubDate>Fri, 02 Jan 2026 20:58:49 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. SSC 通过单张图像推断完整的三维语义与几何体素。2. 现有方法忽略可见区域感知与遮挡区域推理之间的干扰，导致特征稀释与误差传播。3. 提出离线可见区域标签提取（VRLE）策略，分离可见体素的监督。4. 设计可见-遮挡交互完成网络（VOIC），双解码器框架分别处理可见语义感知与遮挡场景完成。5. 在 SemanticKITTI 与 SSCBench-KITTI360 基准上，VOIC 在几何完成与语义分割上均超越现有单目 SSC 方法，取得最先进性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; SSC 是自动驾驶与机器人场景理解的关键任务，目标是从单张图像生成完整的三维语义与几何表示。单图输入导致可见区域与被遮挡区域推理之间存在干扰，影响性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决单图输入导致的可见区域感知与遮挡区域推理之间的干扰，提高 SSC 的几何与语义完成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 离线可见区域标签提取（VRLE）策略，从稠密 3D 真实标签中提取可见体素的监督。2. 可见-遮挡交互完成网络（VOIC）双解码器框架：   - 基础 3D 体素表示通过融合图像特征与深度占据率构建。   - 可见解码器生成高保真几何与语义先验。   - 遮挡解码器利用先验与跨模态交互进行全局场景推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; VOIC 在 SemanticKITTI 与 SSCBench-KITTI360 基准上，在几何完成与语义分割准确率上均优于现有单目 SSC 方法，取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过显式分离可见与遮挡子任务的监督，并采用双解码器交互式完成框架，VOIC 有效提升了单目 SSC 的整体表现，树立了新的性能基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Camera-based 3D Semantic Scene Completion (SSC) 是自动驾驶和机器人场景理解中的关键任务。它旨在从单张图像推断完整的三维体素化语义与几何表示。现有方法通常聚焦于端到端的 2D 到 3D 特征提升与体素完成，但往往忽视单图输入导致的高置信度可见区域感知与低置信度被遮挡区域推理之间的干扰，进而产生特征稀释与误差传播。为解决这些挑战，我们提出离线可见区域标签提取（VRLE）策略，显式从稠密 3D 真实标签中分离并提取可见区域的体素级监督，净化可见感知与遮挡推理两项互补子任务的监督空间。在此基础上，我们提出可见-遮挡交互完成网络（VOIC），一种新颖的双解码器框架，将 SSC 明确拆分为可见区域语义感知与遮挡区域场景完成。VOIC 首先通过融合图像特征与深度衍生占据率构建基础 3D 体素表示；可见解码器聚焦生成高保真几何与语义先验；遮挡解码器利用这些先验与跨模态交互进行全局场景推理。对 SemanticKITTI 与 SSCBench-KITTI360 基准的广泛实验表明，VOIC 在几何完成与语义分割准确率上均优于现有单目 SSC 方法，达成了最先进的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决单目图像下的三维语义场景完成问题，即从一张RGB图像推断完整的三维几何和语义体素。该问题在自动驾驶、机器人导航和增强现实等领域至关重要，因为它提供了对环境的完整、结构化理解，而不依赖昂贵的LiDAR传感器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有单目SSC方法在可见与被遮挡区域之间的统一解码导致特征稀释和误差传播，因而提出将任务拆分为可见区域感知和被遮挡区域完成两步。设计中借鉴了MonoScene、VoxFormer等前沿工作，并在此基础上引入可见区域标签提取（VRLE）和双解码器架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用VRLE生成可见体素的监督标签，再通过可见解码器（VD）精确重建可见体素的几何和语义，随后用遮挡解码器（OD）利用VD产生的空间语义先验完成整个场景。实现流程为：输入图像 → 2D特征提取 → VEFC将特征投影到3D → VD预测可见体素 → OD结合VD先验完成全体素 → 输出完整的3D语义体素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) VRLE离线可见区域标签提取，提供明确的可见监督；2) 双解码器（可见-遮挡）架构，采用可见优先、遮挡后续的推理顺序；3) VEFC与多级位置编码提升体素几何辨识度；4) 自适应特征交互模块纠正可见先验误差。与以往统一解码或仅使用多帧/多视角的工作不同，VOIC在单帧单目条件下实现了更高精度的完整场景完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VOIC通过可见-遮挡解耦、可见标签提取和双解码器设计，在单目图像下实现了领先的三维语义场景完成。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.   To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.   Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.&lt;/p&gt;</description></item><item><guid>2512.22972v1</guid><title>Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection</title><link>http://arxiv.org/abs/2512.22972v1</link><author>Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daizong Liu, Tao Huang, Guoqiang Mao, Hui Xiong</author><pubDate>Fri, 02 Jan 2026 20:58:49 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 4D毫米波雷达在自动驾驶和机器人感知中被广泛使用，但其稀疏性和语义不足限制了感知能力。通过将相机数据与4D雷达融合，可利用两种模态的互补优势。本文提出WRCFormer框架，利用原始雷达立方体与相机输入的多视角表示进行融合。核心是小波注意力模块和基于小波的特征金字塔网络，用于增强稀疏雷达信号和图像的表示。进一步引入两阶段查询式、模态无关的几何引导渐进融合机制，以高效整合多视角特征。实验表明，在K-Radar基准上，WRCFormer在所有场景中比最佳模型提升约2.4%，在雨雾场景提升1.6%，显示出在恶劣天气下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 4D毫米波雷达因低成本和全天候性能被广泛采用，但其稀疏性和语义丰富度不足限制了感知能力。相机与雷达的融合被视为一种成本效益高的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决雷达点云信息损失和原始雷达数据计算成本高的问题，提出一种能够高效融合原始雷达立方体和相机数据的3D目标检测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计Wavelet Attention Module作为小波特征金字塔网络的基本模块，增强稀疏雷达信号和图像的表示；引入两阶段查询式、模态无关的Geometry-guided Progressive Fusion机制，利用几何引导高效融合多视角特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; WRCFormer在K-Radar基准上实现了最先进的性能，比现有最佳模型提升约2.4%（所有场景）和1.6%（雨雾场景），证明其在恶劣天气下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过小波注意力和几何引导渐进融合，WRCFormer显著提升了4D雷达与相机融合的3D目标检测性能，尤其在低语义和稀疏条件下表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 4D毫米波雷达因其低成本和全天候鲁棒性，已被广泛应用于自动驾驶和机器人感知。然而，其固有的稀疏性和有限的语义丰富度显著限制了感知能力。最近，通过将相机数据与4D雷达融合，利用两种模态的互补优势，已成为一种有前景的成本效益解决方案。然而，基于点云的雷达常常受到多阶段信号处理引入的信息损失，而直接使用原始4D雷达数据则会产生巨大的计算成本。为解决这些挑战，我们提出了WRCFormer，一种新颖的3D目标检测框架，通过对解耦雷达立方体的多视角表示，将原始雷达立方体与相机输入进行融合。具体而言，我们设计了Wavelet Attention Module，作为基于小波的特征金字塔网络（FPN）的基本模块，以增强稀疏雷达信号和图像数据的表示。我们进一步引入了一种两阶段查询式、模态无关的融合机制，称为Geometry-guided Progressive Fusion，以高效整合来自两种模态的多视角特征。大量实验表明，WRCFormer在K-Radar基准上实现了最先进的性能，在所有场景中超过最佳模型约2.4%，在雨雾场景中提升1.6%，凸显了其在恶劣天气条件下的鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升自动驾驶和机器人在各种天气条件下的三维目标检测准确性，解决毫米波雷达稀疏且语义信息不足的问题。雷达成本低、全天候可靠，但传统点云表示会丢失大量环境信息；相机提供丰富语义，但受光照影响。将两者融合，可在保持低成本的同时获得更鲁棒的感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到点云雷达信息损失严重、原始雷达张量计算量大，随后将雷达四维张量拆分为范围-方位和仰角-方位两视图，以保留完整属性并降低维度。借鉴了波形变换与注意力机制的研究，提出波形注意力混合专家（WA‑MoE）模块来提取雷达特征，并设计几何引导的渐进融合（GPF）来实现跨模态对齐，参考了EchoFusion、DPFT、ASF等融合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用原始雷达张量的多视图表示与相机图像进行多尺度特征融合，并通过波形注意力混合专家提升雷达特征表达。实现流程包括：①将雷达张量拆分为RA和EA两张图；②分别用ResNet编码并通过WA‑MoE FPN提取多尺度特征；③使用GPF先用交叉注意力将图像语义与EA雷达对齐，再用基于RA雷达的可变形注意力细化；④将融合后的特征送入检测头，迭代优化得到三维框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出WRCFormer框架，首次在同一BEV空间内联合编码雷达张量和相机特征；②设计WA‑MoE模块，将可学习的波形分解与稀疏专家门控结合，显著提升雷达特征质量；③引入几何引导的渐进融合，利用几何先验实现高效、细粒度的跨模态对齐；④在K‑Radar基准上实现显著性能提升，尤其在雨雪等恶劣天气下。与以往点云或原始张量融合方法相比，本文在保持信息完整性的同时大幅降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; WRCFormer通过波形注意力混合专家和几何引导的渐进融合，将原始4D雷达张量与相机图像高效融合，显著提升了在恶劣天气下的三维目标检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.&lt;/p&gt;</description></item><item><guid>2411.04865v4</guid><title>ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset</title><link>http://arxiv.org/abs/2411.04865v4</link><author>Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了ZAHA数据集和LoFG分类体系，提供了最大规模的3D立面语义分割数据，并评估了基线方法，讨论了未解决的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 立面语义分割在摄影测量和计算机视觉中长期存在挑战，现有方法缺乏全面的立面类别和覆盖建筑多样性的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出基于国际城市建模标准的分层立面通用级别（LoFG）分类体系，并构建最大规模的3D立面分割数据集，以促进方法比较和发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计LoFG2和LoFG3两级分层分类，收集并标注6.01亿点的3D立面数据，使用基线语义分割方法进行性能分析，并讨论挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 构建了601百万点、5类和15类的LoFG2/LoFG3数据集，成为迄今最大规模的3D立面分割数据；基线方法在该数据集上的表现被评估，揭示了仍需解决的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ZAHA数据集和LoFG分类将推动3D立面语义分割技术进步，为城市数字孪生的稳健分割奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 立面语义分割是摄影测量和计算机视觉中的长期挑战。尽管过去几十年涌现了大量立面分割方法，但仍缺乏覆盖建筑多样性的全面立面类别和数据。在ZAHA中，我们引入了立面通用级别（LoFG），一种基于国际城市建模标准设计的分层立面类别，确保与现实世界的挑战性类别兼容，并实现方法的统一比较。实现LoFG后，我们发布了迄今为止最大的语义3D立面分割数据集，提供了6.01亿个标注点，分别对应LoFG2的5类和LoFG3的15类。此外，我们分析了基线语义分割方法在我们引入的LoFG类别和数据上的表现，并补充了对立面分割未解决挑战的讨论。我们坚信ZAHA将促进3D立面语义分割方法的进一步发展，为创建城市数字孪生提供不可或缺的稳健分割。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Facade semantic segmentation is a long-standing challenge in photogrammetry and computer vision. Although the last decades have witnessed the influx of facade segmentation methods, there is a lack of comprehensive facade classes and data covering the architectural variability. In ZAHA, we introduce Level of Facade Generalization (LoFG), novel hierarchical facade classes designed based on international urban modeling standards, ensuring compatibility with real-world challenging classes and uniform methods&amp;#x27; comparison. Realizing the LoFG, we present to date the largest semantic 3D facade segmentation dataset, providing 601 million annotated points at five and 15 classes of LoFG2 and LoFG3, respectively. Moreover, we analyze the performance of baseline semantic segmentation methods on our introduced LoFG classes and data, complementing it with a discussion on the unresolved challenges for facade segmentation. We firmly believe that ZAHA shall facilitate further development of 3D facade semantic segmentation methods, enabling robust segmentation indispensable in creating urban digital twins.&lt;/p&gt;</description></item><item><guid>2412.17699v2</guid><title>Establishing Reality-Virtuality Interconnections in Urban Digital Twins for Superior Intelligent Road Inspection and Simulation</title><link>http://arxiv.org/abs/2412.17699v2</link><author>Yikang Zhang, Chuang-Wei Liu, Jiahang Li, Yingbing Chen, Jie Cheng, Rui Fan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种多模态传感平台与城市数字孪生系统相结合的智能道路检测方法，通过构建分层道路模型和数字孪生，生成高保真道路缺陷场景，显著提升感知与决策任务性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统人工评估道路缺陷耗时费力，数据驱动方法受限于缺陷数据稀缺与空间稀疏；现有仿真器缺乏道路缺陷模型，且缺陷区域的高级驾驶任务研究不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决道路缺陷数据不足、仿真缺陷模型缺失以及缺陷区域驾驶任务研究不足的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用车载多模态传感器采集真实驾驶数据，构建分层道路模型；生成数字道路孪生用于创建仿真环境；将场景导入仿真器进行数据采集与物理仿真；评估感知与决策算法性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 高保真道路缺陷场景显著提升了驾驶任务中的感知与决策性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多模态传感平台与城市数字孪生相结合的方案能够有效生成高质量道路缺陷数据，推动智能道路检测与高级驾驶任务研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 道路检查对于维护道路可用性和确保交通安全至关重要，因为道路缺陷会逐渐发展并削弱功能。传统的检查方法依赖人工评估，劳动强度大、成本高且耗时。虽然数据驱动方法正逐渐流行，但真实道路缺陷的稀缺和空间稀疏性给获取高质量数据集带来重大挑战。现有的用于生成详细合成驾驶场景的仿真器缺乏道路缺陷模型，且涉及道路表面交互的高级驾驶任务（如缺陷区域的规划与控制）仍未得到充分探索。为解决这些限制，我们提出了一个集成多模态传感平台与城市数字孪生（UDT）系统的智能道路检查方案。首先，利用车载传感器收集的真实驾驶数据构建分层道路模型，得到高度详细的道路缺陷结构和表面高程表示。随后生成数字道路孪生，用于创建仿真环境，以全面分析和评估算法性能。将这些场景导入仿真器，既可进行数据采集，也可进行物理仿真。实验结果表明，利用我们系统生成的高保真道路缺陷场景，驾驶任务（包括感知和决策）获得了显著收益。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决道路缺陷检测与仿真数据不足的问题。传统人工检查耗时且危险，现有数据驱动方法受限于缺乏高质量、稀疏的缺陷样本；而现有仿真器缺乏真实道路缺陷模型，导致驾驶算法难以在真实环境中验证。提供逼真的缺陷模型和仿真环境，可提升道路安全评估和自动驾驶系统的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别现有方法在数据获取、缺陷建模和仿真集成方面的不足，随后提出基于多模态传感器（LiDAR、立体摄像头、GNSS）的数据采集平台，并借鉴数字孪生、语义分割（Grounded‑SAM）和立体视差技术来重建道路表面与缺陷。随后利用Moller‑Trumbore算法和三角网格处理，将重建模型嵌入仿真器，形成完整的数字孪生系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次化建模将真实道路表面与缺陷分别重建为高精度三维网格，然后将这些模型无缝集成到城市数字孪生仿真环境中。实现流程包括：①使用多模态传感器采集道路数据；②粗粒度流利用语义掩码和LiDAR点云重建平整道路表面；③细粒度流利用立体视差提取缺陷区域并重建细节；④将两类模型存入库；⑤在仿真器中导出平面道路资产，随机投射缺陷模型并去除交叉三角面；⑥生成完整的仿真场景，用于数据合成和物理仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①可携带的多模态采集平台；②层次化道路模型生成器，既重建平整表面又捕获细节缺陷；③数字孪生生成器能够将缺陷模型准确投射到仿真道路并保持拓扑完整；④提供完整的感知与决策算法基准数据集；⑤采用轮胎级碰撞检测实现更灵活的缺陷避让策略。与以往仅使用平面道路或单独缺陷重建的工作不同，该方法实现了真实道路缺陷的三维再现与仿真集成，显著提升了数据真实性和算法迁移性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套基于多模态传感器的层次化道路缺陷建模与数字孪生集成流程，能够生成逼真的三维缺陷场景并用于感知与决策算法的训练与评估。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Road inspection is crucial for maintaining road serviceability and ensuring traffic safety, as road defects gradually develop and compromise functionality. Traditional inspection methods, which rely on manual evaluations, are labor-intensive, costly, and time-consuming. While data-driven approaches are gaining traction, the scarcity and spatial sparsity of real-world road defects present significant challenges in acquiring high-quality datasets. Existing simulators designed to generate detailed synthetic driving scenes, however, lack models for road defects. Moreover, advanced driving tasks that involve interactions with road surfaces, such as planning and control in defective areas, remain underexplored. To address these limitations, we propose a multi-modal sensor platform integrated with an urban digital twin (UDT) system for intelligent road inspection. First, hierarchical road models are constructed from real-world driving data collected using vehicle-mounted sensors, resulting in highly detailed representations of road defect structures and surface elevations. Next, digital road twins are generated to create simulation environments for comprehensive analysis and evaluation of algorithm performance. These scenarios are then imported into a simulator to facilitate both data acquisition and physical simulation. Experimental results demonstrate that driving tasks, including perception and decision-making, benefit significantly from the high-fidelity road defect scenes generated by our system.&lt;/p&gt;</description></item><item><guid>2501.08983v4</guid><title>Compositional Generative Model of Unbounded 4D Cities</title><link>http://arxiv.org/abs/2501.08983v4</link><author>Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 CityDreamer4D，一种专门用于生成无限扩展的 4D 城市的组合式生成模型。通过将动态对象与静态场景分离，并为建筑、车辆和背景等不同类型的对象使用不同的神经场，模型能够高效生成结构复杂、视觉多样的城市环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着 3D 场景生成技术的快速发展，生成 4D 城市面临更高的挑战，主要因为城市中存在结构复杂、视觉多样的建筑和车辆，以及人类对城市环境失真更敏感。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决 4D 城市生成中的挑战，提出一种能够生成无限扩展、真实感强的 4D 城市的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用组合式生成框架，将动态交通场景与静态城市布局分别由 Traffic Scenario Generator 和 Unbounded Layout Generator 生成，并使用紧凑的鸟瞰视图表示。城市中的对象通过结合面向背景的神经场和面向实例的神经场生成，神经场采用定制的哈希网格和周期性位置嵌入进行场景参数化。同时提供 OSM、Google Earth 和 CityTopia 三个数据集支持训练与评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CityDreamer4D 在生成真实感强的 4D 城市方面实现了最先进的性能，并支持实例编辑、城市风格化和城市仿真等下游应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 组合式设计使得 CityDreamer4D 能够高效、可扩展地生成结构复杂且视觉多样的 4D 城市，为城市生成与仿真研究提供了新的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 场景生成近年来受到越来越多关注，并取得了显著进展。与 3D 场景相比，生成 4D 城市更具挑战性，因为城市中存在结构复杂、视觉多样的建筑和车辆，并且人们对城市环境中的失真更为敏感。为了解决这些问题，我们提出了 CityDreamer4D，一种专门为生成无限扩展的 4D 城市而设计的组合式生成模型。我们的主要见解是：1）4D 城市生成应将动态对象（如车辆）与静态场景（如建筑和道路）分离；2）4D 场景中的所有对象都应由建筑、车辆和背景等不同类型的神经场组成。具体而言，我们提出了 Traffic Scenario Generator 和 Unbounded Layout Generator，利用高度紧凑的鸟瞰视图表示来生成动态交通场景和静态城市布局。4D 城市中的对象通过结合面向背景的神经场和面向实例的神经场生成，针对背景和实例的不同特性，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们提供了包含 OSM、Google Earth 和 CityTopia 的完整数据集套件。OSM 数据集提供了多种真实城市布局，而 Google Earth 和 CityTopia 数据集则提供了大规模、高质量的城市图像，并配有 3D 实例注释。凭借其组合式设计，CityDreamer4D 支持实例编辑、城市风格化和城市仿真等多种下游应用，并在生成逼真的 4D 城市方面实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在生成无限扩展的四维（空间+时间）城市场景，解决现有方法在处理复杂建筑、车辆等动态对象以及保持时间一致性方面的不足。此类生成技术对元宇宙、城市规划、环境模拟和游戏资产开发等领域具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到四维城市生成需要把静态建筑与道路与动态车辆分离，并借鉴 MaskGIT、VQVAE、NeRF 等已有技术来实现布局生成、BEV 表示和神经字段渲染。他们在此基础上设计了 Traffic Scenario Generator、Unbounded Layout Generator、以及针对背景、建筑和车辆的专用神经字段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将四维城市拆分为静态场景和动态交通两部分，分别用专门的生成器产生布局、背景、建筑实例和车辆实例，然后通过合成器将它们组合成完整的时间序列图像。流程包括：生成城市布局 → 生成背景图像 → 生成建筑实例 → 生成交通场景 → 生成车辆实例 → 合成最终四维图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）首次实现无限扩展的四维城市生成并分离动态与静态；2）使用专门的神经字段（背景用哈希网格，实例用周期性位置编码）来捕捉不同对象的多样性；3）引入高度紧凑的 BEV 表示和底部到顶部高度图；4）Traffic Scenario Generator 生成真实交通场景；5）Vehicle Instance Generator 基于规范化特征空间；6）提供新的 CityTopia 数据集。与以往仅生成 3D 场景或受限规模的 4D 场景不同，本文实现了大规模、可编辑的四维城市。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CityDreamer4D 提出了一个可拆分的生成框架，能够同时生成无限扩展的静态城市布局和动态交通场景，生成逼真的四维城市并支持实例级编辑。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.&lt;/p&gt;</description></item><item><guid>2502.05769v3</guid><title>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform</title><link>http://arxiv.org/abs/2502.05769v3</link><author>Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种针对单栋建筑的数字孪生框架，利用云地图平台、先进的大语言模型和高斯散点网格提取技术，实现建筑三维模型和视觉描述的自动获取，并与云端地图和数据分析无缝集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市数字孪生通过多源数据和数据分析来优化城市规划、基础设施管理和决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种聚焦单栋建筑规模的数字孪生框架，以实现建筑级别的三维建模和信息集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 框架通过连接云地图平台（如谷歌地图API），使用多智能体大型语言模型（ChatGPT(4o)和Deepseek-V3/R1）进行数据分析，并采用基于高斯散点的网格提取管道来生成建筑三维模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架能够根据建筑地址、邮政编码或地理坐标检索建筑的三维模型和视觉描述，并实现云地图与大型语言模型数据分析的集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 单栋建筑级别的数字孪生框架可实现自动化三维建模和信息集成，为城市规划与管理提供精准的数据支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 城市数字孪生是利用多源数据和数据分析来优化城市规划、基础设施管理和决策的城市虚拟复制品。为此，我们提出了一个聚焦单栋建筑规模的框架。通过连接云地图平台（如谷歌地图平台API），利用最先进的多智能体大型语言模型（ChatGPT(4o)和Deepseek-V3/R1）进行数据分析，并使用基于高斯散点的网格提取管道，我们的数字孪生建筑框架能够根据建筑地址、邮政编码或地理坐标检索建筑的三维模型、视觉描述，并实现云端地图与大型语言模型数据分析的集成。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现一种自动化的数字建筑分析系统，能够从云地图服务获取建筑信息、生成三维网格模型，并利用多代理大型语言模型提供可视化描述。该问题在城市规划、建筑设计和数字孪生等领域具有重要意义，因为它能快速、准确地把现实建筑转化为可分析的数字资产。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了 Google Earth Studio、Segment Anything Model‑2、GroundingDINO、Gaussian Splatting 等现有技术，构建了一个端到端的管道。通过 Google Maps Platform 的 API 进行地理编码、地形和影像检索，并在此基础上使用多代理 LLM 进行图像分析和文本生成，充分借鉴了前人关于三维重建、云地图集成和 LLM 视觉分析的工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将云地图数据、三维重建和多代理 LLM 结合起来，实现从建筑地址到三维模型和视觉描述的全流程。实现流程包括：①使用 Google Maps API 获取地址、坐标、建筑多边形和地形；②用 Google Earth Studio 采集多视角图像；③通过 Gaussian Splatting 提取三维网格并生成合成图像；④启动多代理 LLM，分别提取关键词、聚合关键词并生成最终描述；⑤评估生成文本与图像的匹配度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：①整合云地图服务与三维重建的完整管道；②使用 Gaussian Splatting 进行高质量建筑网格提取；③设计多代理 LLM 模块实现多视角图像的关键词聚合与自动生成描述；④在无标注数据的情况下采用 CLIP/BLIP/PAC 评估指标。与以往工作相比，该方法首次将云地图、三维重建和多代理 LLM 统一到同一框架，并在多视角图像上实现自动化视觉描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套端到端的数字建筑分析框架，能够自动从 Google Maps 获取建筑信息，利用 Gaussian Splatting 重建三维网格，并通过多代理大型语言模型生成多视角视觉描述。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building&amp;#x27;s 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building&amp;#x27;s address, postal code, or geographic coordinates.&lt;/p&gt;</description></item><item><guid>2502.12532v3</guid><title>CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space</title><link>http://arxiv.org/abs/2502.12532v3</link><author>Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, Jincai Huang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了在城市环境中进行具身问答的新任务CityEQA，并提供了相应的基准数据集CityEQA-EC和一种新型的具身智能体PMA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往的具身问答研究主要聚焦于室内环境，城市环境的复杂性尚未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个在动态城市空间中通过主动探索回答开放词汇问题的任务，并提供支持该任务的数据集与智能体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 创建了包含1412个人工标注任务的CityEQA-EC数据集，并设计了Planner-Manager-Actor（PMA）架构的智能体，PMA通过规划、管理和执行三个层次实现长时程规划与分层任务执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明PMA在回答准确率上达到60.7%，显著优于现有基线，但与人类水平仍有差距，提示需要进一步提升视觉推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该工作为城市空间智能的未来发展奠定了基础，并展示了具身问答在城市环境中的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在真实城市环境中，具身代理如何通过主动探索来回答开放式自然语言问题的挑战。该问题在自动驾驶、城市治理和公共服务等实际应用中至关重要，因为它要求代理在复杂、动态的城市空间中进行感知、规划和推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先发现现有的具身问答研究几乎全部聚焦室内场景，缺乏城市尺度的开放式任务。为此，他们构建了 CityEQA‑EC 数据集，并借鉴了 LLM 的链式思考、VLM 的视觉语言理解、Ground‑SAM 的目标分割以及 A* 路径规划等技术，设计了 Planner‑Manager‑Actor 三层结构的层次化代理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将长时程的城市问答任务拆解为规划、映射与执行三大模块。流程为：1）Planner 用 LLM 解析问题并生成导航、探索、收集子任务；2）Manager 维护二维对象中心认知图和记忆，控制子任务执行；3）Actor 根据 Manager 指令调用 Navigator、Explorer 或 Collector 产生具体动作；4）循环更新地图与记忆，直至完成所有子任务后由 Manager 生成答案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①首个面向城市空间的开放式具身问答基准 CityEQA‑EC；②基于 LLM 的 Planner‑Manager‑Actor 层次化代理；③二维对象中心认知图与跨尺度动作控制；④将 VLM 作为收集子任务的视觉语言动作模块。与以往室内或固定视角的 EQA 任务不同，本文处理更大尺度、开放词汇、动态感知的城市环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出首个城市尺度的开放式具身问答基准，并设计了通过 LLM 规划、认知图映射和多模态动作执行的层次化代理，为实现城市环境中的自主感知与推理奠定了重要基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.&lt;/p&gt;</description></item><item><guid>2502.13078v3</guid><title>L4P: Towards Unified Low-Level 4D Vision Perception</title><link>http://arxiv.org/abs/2502.13078v3</link><author>Abhishek Badki, Hang Su, Bowen Wen, Orazio Gallo</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; L4P 是一种前馈、通用的 4D 感知架构，能够在统一框架下解决多种低层次视频任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频像素的时空关系包含关键信息，现有方法多为任务专用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种通用模型，能够在单一框架中高效完成多种 4D 感知任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用预训练的 ViT 视频编码器，并为每个任务添加轻量级头部，保持前馈结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; L4P 在密集任务（深度、光流）和稀疏任务（2D/3D 跟踪）上与专用方法竞争，并且一次性完成所有任务的时间与单任务方法相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通用前馈架构可在多任务 4D 感知中实现高效且竞争性的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视频像素的时空关系携带着低层次 4D 感知任务的关键信息。一个能够推理它的单一模型应该能够很好地解决多种此类任务。然而，大多数最先进的方法依赖于专门为特定任务设计的架构。我们提出 L4P，一种前馈、通用架构，在统一框架下解决低层次 4D 感知任务。L4P 利用预训练的 ViT 视频编码器，并结合每个任务的轻量级头部，因此不需要大量训练。尽管其通用且前馈的形式，我们的方法在密集任务（如深度或光流估计）和稀疏任务（如 2D/3D 跟踪）上与现有专用方法具有竞争力，并且一次性解决所有任务的时间与单任务方法相当。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在统一低级4D视觉感知任务。其重要性未在文本中明确说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作，如VideoMAE、Omnimotion等。具体设计思路未在文本中详细说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是统一低级4D视觉感知。实现流程未在文本中说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点未在文本中说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The spatio-temporal relationship between the pixels of a video carries critical information for low-level 4D perception tasks. A single model that reasons about it should be able to solve several such tasks well. Yet, most state-of-the-art methods rely on architectures specialized for the task at hand. We present L4P, a feedforward, general-purpose architecture that solves low-level 4D perception tasks in a unified framework. L4P leverages a pre-trained ViT-based video encoder and combines it with per-task heads that are lightweight and therefore do not require extensive training. Despite its general and feedforward formulation, our method is competitive with existing specialized methods on both dense tasks, such as depth or optical flow estimation, and sparse tasks, such as 2D/3D tracking. Moreover, it solves all tasks at once in a time comparable to that of single-task methods.&lt;/p&gt;</description></item><item><guid>2503.14558v2</guid><title>SuperPC: A Single Diffusion Model for Point Cloud Completion, Upsampling, Denoising, and Colorization</title><link>http://arxiv.org/abs/2503.14558v2</link><author>Yi Du, Zhipeng Zhao, Shaoshu Su, Sharath Golluri, Haoze Zheng, Runmao Yao, Chen Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SuperPC 是一种统一的扩散模型，能够同时完成点云的补全、上采样、去噪和上色，利用三层条件扩散框架和空间混合融合策略，显著优于单独任务模型及其组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云处理任务在自动驾驶和三维重建等应用中至关重要，但现有方法往往分别针对每个缺陷开发独立模型，忽视了缺陷共存和相互影响的现实。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种统一模型，解决点云多重缺陷的同时处理问题，减少误差累积和计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用三层条件扩散框架，并引入空间混合融合策略，利用四种缺陷之间的相关性实现并行处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 SuperPC 在补全、上采样、去噪和上色四项任务上均优于现有最先进的专用模型及其组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 统一扩散模型能够有效处理多重点云缺陷，提供更高效、更准确的点云处理方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; SuperPC is the first unified diffusion model that can simultaneously handle point cloud completion, upsampling, denoising, and colorization. It uses a three-level conditioned diffusion framework and a novel spatial-mix-fusion strategy to leverage correlations among these defects for efficient, concurrent processing, outperforming state-of-the-art specialized models and their combinations on all four tasks.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在一次性解决点云的四大处理任务——补全、上采样、去噪和上色。现实中这些缺陷往往同时出现，单独使用专门模型会导致误差累积和计算成本高。统一模型可提高效率并减少错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了扩散模型在点云生成中的成功，并结合早期和深度融合技术，提出三层条件（原始、局部、全局）与空间混合融合（SMF）策略。该设计在保持局部细节、全局语义和原始信息的同时，兼顾图像与点云两种模态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是用条件扩散模型生成点云，并通过三层条件引导去噪过程。流程包括：①将输入点云和图像通过SMF得到原始、局部、全局条件；②在扩散的前向噪声过程中逐步加入噪声；③在逆向过程中，网络预测噪声并利用三层条件逐步恢复干净、完整、上采样且上色的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①首次将四项点云任务统一到单一扩散模型；②提出三层条件框架，避免单一条件导致的局部最优；③设计空间混合融合策略，兼顾早期与深度融合；④在多场景基准上实现SOTA性能。与以往只处理单一任务或单模态的工作不同，SuperPC同时利用图像与点云信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SuperPC通过在原始、局部和全局层面融合图像与点云信息，构建单一扩散模型，能够同时完成点云补全、上采样、去噪和上色，并在多种基准上超越专用模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud (PC) processing tasks-such as completion, upsampling, denoising, and colorization-are crucial in applications like autonomous driving and 3D reconstruction. Despite substantial advancements, prior approaches often address each of these tasks independently, with separate models focused on individual issues. However, this isolated approach fails to account for the fact that defects like incompleteness, low resolution, noise, and lack of color frequently coexist, with each defect influencing and correlating with the others. Simply applying these models sequentially can lead to error accumulation from each model, along with increased computational costs. To address these challenges, we introduce SuperPC, the first unified diffusion model capable of concurrently handling all four tasks. Our approach employs a three-level-conditioned diffusion framework, enhanced by a novel spatial-mix-fusion strategy, to leverage the correlations among these four defects for simultaneous, efficient processing. We show that SuperPC outperforms the state-of-the-art specialized models as well as their combination on all four individual tasks.&lt;/p&gt;</description></item><item><guid>2503.17316v1</guid><title>Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors</title><link>http://arxiv.org/abs/2503.17316v1</link><author>Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, Jerome Revaud</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Pow3r 是一种新型的大规模 3D 视觉回归模型，能够灵活接受多种输入模态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往的前馈模型在测试时无法利用已知的相机或场景先验信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在单一网络中结合相机内参、相对姿态、稠密或稀疏深度等辅助信息的模型，以提升预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在 DUSt3R 变压器架构基础上加入轻量化的条件化机制，并在训练时随机提供不同子集的模态，使模型能在测试时适应不同程度的先验信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 3D 重建、深度补全、多视角深度预测、多视角立体匹配和多视角姿态估计等任务上，Pow3r 达到或超过了现有最优结果，验证了其充分利用所有可用信息的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Pow3r 通过灵活的模态融合和预训练的变压器架构，实现了在多种 3D 视觉任务中的卓越性能，并开启了原始分辨率推理和点云补全等新功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 Pow3r，一种新型的大规模 3D 视觉回归模型，能够高度灵活地接受多种输入模态。与以往缺乏在测试时利用已知相机或场景先验信息机制的前馈模型不同，Pow3r 在单一网络中结合了相机内参、相对姿态、稠密或稀疏深度等任何组合的辅助信息。基于最近的 DUSt3R 变压器范式，该模型利用强大的预训练，并通过轻量化且多功能的条件化作为额外引导，使网络在有辅助信息时能够预测更准确的结果。在训练过程中，我们在每一次迭代中向模型提供随机子集的模态，从而使模型能够在测试时在不同程度的已知先验下工作。这进一步开启了新的功能，例如在原生图像分辨率下进行推理，或完成点云。我们在 3D 重建、深度补全、多视角深度预测、多视角立体匹配和多视角姿态估计任务上的实验取得了最先进的结果，并证实了 Pow3r 在充分利用所有可用信息方面的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present Pow3r, a novel large 3D vision regression model that is highly versatile in the input modalities it accepts. Unlike previous feed-forward models that lack any mechanism to exploit known camera or scene priors at test time, Pow3r incorporates any combination of auxiliary information such as intrinsics, relative pose, dense or sparse depth, alongside input images, within a single network. Building upon the recent DUSt3R paradigm, a transformer-based architecture that leverages powerful pre-training, our lightweight and versatile conditioning acts as additional guidance for the network to predict more accurate estimates when auxiliary information is available. During training we feed the model with random subsets of modalities at each iteration, which enables the model to operate under different levels of known priors at test time. This in turn opens up new capabilities, such as performing inference in native image resolution, or point-cloud completion. Our experiments on 3D reconstruction, depth completion, multi-view depth prediction, multi-view stereo, and multi-view pose estimation tasks yield state-of-the-art results and confirm the effectiveness of Pow3r at exploiting all available information. The project webpage is https://europe.naverlabs.com/pow3r.&lt;/p&gt;</description></item><item><guid>2503.18007v1</guid><title>SymmCompletion: High-Fidelity and High-Consistency Point Cloud Completion with Symmetry Guidance</title><link>http://arxiv.org/abs/2503.18007v1</link><author>Hongyu Yan, Zijun Li, Kunming Luo, Li Lu, Ping Tan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于对称引导的点云补全方法 SymmCompletion，利用局部对称变换网络和对称引导变压器两大组件，能够在保持几何细节的同时提升补全结果的几何一致性，并在多个基准数据集上优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有点云补全方法虽然能得到全局完整的点云，但往往会丢失原始几何细节，并出现补全部分与已有点云之间的几何不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入对称引导机制，解决几何细节丢失和几何不一致的问题，提高补全点云的质量与一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 局部对称变换网络（LSTNet）估计点级局部对称变换，将部分输入的关键几何映射到缺失区域，生成几何对齐的部分-缺失对和初始点云；2) 对称引导变压器（SGFormer）利用这些对齐对的几何特征作为显式对称引导，约束初始点云的细化过程，从而得到高保真且几何一致的最终点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上的定性和定量评估表明，SymmCompletion 在补全质量和几何一致性方面均优于现有最先进的补全网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于对称引导的补全方法能够有效保留几何细节并提升几何一致性，显著提升点云补全性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分点云恢复完整点形。虽然现有方法能够在全局完整性上形成令人满意的点云，但它们往往失去原始几何细节，并面临现有点云与重建缺失部分之间几何不一致的问题。为解决此问题，我们提出了基于对称引导的高效补全方法 SymmCompletion。该方法由两个主要组件组成：局部对称变换网络（LSTNet）和对称引导变压器（SGFormer）。首先，LSTNet 高效估计点级局部对称变换，将部分输入的关键几何映射到缺失区域，从而生成几何对齐的部分-缺失对和初始点云。其次，SGFormer 利用部分-缺失对的几何特征作为显式对称引导，约束初始点云的细化过程。结果，SGFormer 能够利用提供的先验形成高保真且几何一致的最终点云。在多个基准数据集上的定性和定量评估表明，我们的方法优于现有最先进的补全网络。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to recover a complete point shape from a partial point cloud. Although existing methods can form satisfactory point clouds in global completeness, they often lose the original geometry details and face the problem of geometric inconsistency between existing point clouds and reconstructed missing parts. To tackle this problem, we introduce SymmCompletion, a highly effective completion method based on symmetry guidance. Our method comprises two primary components: a Local Symmetry Transformation Network (LSTNet) and a Symmetry-Guidance Transformer (SGFormer). First, LSTNet efficiently estimates point-wise local symmetry transformation to transform key geometries of partial inputs into missing regions, thereby generating geometry-align partial-missing pairs and initial point clouds. Second, SGFormer leverages the geometric features of partial-missing pairs as the explicit symmetric guidance that can constrain the refinement process for initial point clouds. As a result, SGFormer can exploit provided priors to form high-fidelity and geometry-consistency final point clouds. Qualitative and quantitative evaluations on several benchmark datasets demonstrate that our method outperforms state-of-the-art completion networks.&lt;/p&gt;</description></item><item><guid>2503.21104v1</guid><title>StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency</title><link>http://arxiv.org/abs/2503.21104v1</link><author>Yuyin Chen, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Yifei Zhan, Xianpeng Lang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 StyledStreets 多风格街景模拟器，实现指令驱动的场景编辑，保证空间和时间一致性，支持多种环境条件的真实感风格迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市场景重建需要同时建模静态基础设施和动态元素，并支持多样化环境条件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在不同季节、天气和摄像机设置下进行指令驱动的风格迁移，同时保持空间和时间一致性的街景模拟器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 基于 Gaussian Splatting 框架，加入姿态优化和多视角训练，提出三项创新：混合嵌入分离几何与风格、基于不确定性的渲染降低监督噪声、统一参数模型防止几何漂移，保证多视角一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示在雪、沙尘暴、夜晚等多种条件下实现了可信的过渡，且在风格迁移下保持了最先进的几何精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; StyledStreets 为城市模拟提供了新的能力，可用于自动驾驶测试和增强现实系统，代码将公开。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The paper presents StyledStreets, a multi-style street simulator that enables instruction-driven scene editing with guaranteed spatial and temporal consistency, built on a Gaussian Splatting framework enhanced by pose optimization and multi-view training. It introduces three key innovations: a hybrid embedding scheme that separates persistent geometry from transient style attributes, uncertainty-aware rendering to reduce supervision noise from diffusion priors, and a unified parametric model to prevent geometric drift and maintain multi-view consistency across seven vehicle-mounted cameras. The framework preserves motion patterns and geometric relationships, demonstrating plausible transitions across diverse conditions such as snow, sandstorms, and night, with state-of-the-art geometric accuracy under style transfers. This approach establishes new capabilities for urban simulation, applicable to autonomous vehicle testing and augmented reality systems requiring reliable environmental consistency. Code will be publicly available upon publication.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban scene reconstruction requires modeling both static infrastructure and dynamic elements while supporting diverse environmental conditions. We present \textbf{StyledStreets}, a multi-style street simulator that achieves instruction-driven scene editing with guaranteed spatial and temporal consistency. Building on a state-of-the-art Gaussian Splatting framework for street scenarios enhanced by our proposed pose optimization and multi-view training, our method enables photorealistic style transfers across seasons, weather conditions, and camera setups through three key innovations: First, a hybrid embedding scheme disentangles persistent scene geometry from transient style attributes, allowing realistic environmental edits while preserving structural integrity. Second, uncertainty-aware rendering mitigates supervision noise from diffusion priors, enabling robust training across extreme style variations. Third, a unified parametric model prevents geometric drift through regularized updates, maintaining multi-view consistency across seven vehicle-mounted cameras.   Our framework preserves the original scene&amp;#x27;s motion patterns and geometric relationships. Qualitative results demonstrate plausible transitions between diverse conditions (snow, sandstorm, night), while quantitative evaluations show state-of-the-art geometric accuracy under style transfers. The approach establishes new capabilities for urban simulation, with applications in autonomous vehicle testing and augmented reality systems requiring reliable environmental consistency. Codes will be publicly available upon publication.&lt;/p&gt;</description></item><item><guid>2504.13580v4</guid><title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title><link>http://arxiv.org/abs/2504.13580v4</link><author>Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文展示了利用自动检索的合成CAD模型生成高质量3D标注，并证明在此基础上训练的深度学习模型在点云补全和单视角CAD检索与对齐任务上优于手工标注模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高层次3D场景理解在众多应用中至关重要，但准确的3D标注生成成本高、难度大，限制了深度学习模型的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证自动检索的合成CAD模型可作为高质量训练数据，并评估其在实际任务中的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用与ScanNet场景自动标注相似的管道，对缺乏标注的ScanNet++ v1数据集进行自动标注，生成CAD模型与9维姿态对应，并用这些标注训练监督式深度学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 自动获得的标注不仅可用于训练深度学习模型，而且训练出的模型在点云补全和单视角CAD检索与对齐任务上均优于使用手工标注的数据训练的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自动3D标注能够提升模型性能并显著降低标注成本，作者将发布标注集SCANnotate++及训练好的模型以支持后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 高层次3D场景理解在许多应用中至关重要。然而，生成准确的3D注释的挑战使得深度学习模型的开发变得困难。我们转向最近在自动检索合成CAD模型方面的进展，并展示了由此方法生成的数据可用作训练监督式深度学习模型的高质量真值。更确切地说，我们采用了与之前用于自动为ScanNet场景中的对象标注9维姿态和CAD模型相似的管道。这一次，我们将其应用于最近的ScanNet++ v1数据集，该数据集以前缺乏此类注释。我们的发现表明，不仅可以在这些自动获得的注释上训练深度学习模型，而且得到的模型在单视角CAD模型检索与对齐和点云补全等两个不同任务上优于使用手工注释的数据训练的模型。我们的结果强调了自动3D注释在提升模型性能的同时显著降低注释成本的潜力。为支持未来的3D场景理解研究，我们将发布我们的注释集，称为SCANnotate++，以及我们的训练模型。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决如何为室内 RGB‑D 场景自动生成高质量的 CAD 模型与姿态注释，从而降低人工标注成本并避免人工误差。准确的 3D 注释是训练深度学习模型进行点云补全、CAD 检索等任务的关键，而人工标注既耗时又不易保持一致；自动化方法可大幅提升数据规模与质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的 SCANnotate 自动标注框架基础上，引入更强大的 HOC‑Search 检索算法，并对阈值与姿态细化步骤做了改进。该方法借鉴了 ShapeNet、HOC‑Search、ShapeGF、ROCA 等现有工作，并结合 ScanNet++ 数据集进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 HOC‑Search 在大规模 CAD 数据库中联合搜索离散与连续参数，得到与目标物体最匹配的 CAD 模型与姿态，然后通过梯度优化细化姿态，并对同类物体进行聚类与克隆。实现流程为：① 读取 RGB‑D 场景与 3D 语义分割；② 计算每个物体的初始包围盒；③ 运行 HOC‑Search 进行 CAD 检索与姿态估计；④ 对候选模型做姿态细化；⑤ 对同类物体聚类、克隆并手工验证；⑥ 输出 SCANnotate++ 注释并用于后续训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 将 HOC‑Search 应用于 ScanNet++，生成规模达 5k 对象的自动注释；② 证明自动注释训练的模型在点云补全和单视图 CAD 检索任务上优于使用人工注释的模型；③ 公开 SCANnotate++ 数据集与预训练模型。与之前仅在 ScanNet 上使用手工或半自动注释的工作不同，本文实现了全自动、可扩展的注释流程，并展示了其在新数据集上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们证明了自动检索并对齐 CAD 模型的注释可以替代昂贵的人工标注，提升点云补全和单视图 CAD 检索的性能，并公开了 SCANnotate++ 数据集与训练模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.&lt;/p&gt;</description></item><item><guid>2504.13788v1</guid><title>RefComp: A Reference-guided Unified Framework for Unpaired Point Cloud Completion</title><link>http://arxiv.org/abs/2504.13788v1</link><author>Yixuan Yang, Jinyu Yang, Zixiang Zhao, Victor Sanchez, Feng Zheng</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种参考引导的点云补全框架RefComp，能够在有类别信息和无类别信息的训练环境下实现高质量的点云补全。该框架将无配对补全问题转化为形状翻译问题，并利用部分-完整点云对作为参考，引导补全过程。实验表明，RefComp在类别感知训练中达到最先进水平，在类别无关训练中也表现出竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的无配对点云补全方法多为类别感知，需要为每个物体类别训练单独模型，导致泛化能力有限，难以在真实场景中处理多样化的3D物体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种既能在类别感知又能在类别无关训练中表现优异的无配对点云补全框架，以提升在通用3D物体上的补全效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; RefComp框架通过在部分点云的潜在特征空间中进行形状翻译来完成补全。首先使用待补全的部分点云作为模板检索对应的完整点云，形成部分-完整点云对作为参考数据。框架包含共享参数的参考分支和目标分支，并通过潜在形状融合模块（LSFM）实现形状融合与翻译，强化补全过程中的结构特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在虚拟扫描和真实数据集上进行的大量实验表明，RefComp在类别感知训练设置下实现了最先进的性能，在类别无关训练设置下也取得了竞争性的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RefComp通过参考引导的形状翻译方法，显著提升了无配对点云补全的泛化能力，能够在多种训练场景下提供高质量的补全结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 无配对点云补全任务旨在使用未配对的模型完成部分点云。现有的无配对点云补全方法是类别感知的，即每个物体类别需要单独的模型。由于其泛化能力有限，这些方法在面对广泛的通用3D物体点云时在真实场景中表现不佳。本文提出了一种新颖的无配对点云补全框架——参考引导补全（RefComp）框架，在类别感知和类别无关的训练设置下均能取得强劲表现。RefComp框架将无配对补全问题转化为形状翻译问题，并在部分点云的潜在特征空间中解决。为此，我们引入了使用待补全的部分点云作为模板检索的部分-完整点云对，这些点云对被用作参考数据来引导补全过程。我们的RefComp框架使用共享参数的参考分支和目标分支，通过潜在形状融合模块（LSFM）实现形状融合和形状翻译，以增强补全管道中的结构特征。大量实验表明，RefComp框架在类别感知训练设置下不仅实现了最先进的性能，在类别无关训练设置下也取得了竞争性的结果，适用于虚拟扫描和真实世界数据集。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在没有配对完整点云的情况下完成部分点云的问题。传统方法需要为每个物体类别训练单独模型，难以在真实场景中泛化。能够在无监督、无类别限制的条件下完成点云，对自动驾驶、3D重建等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有无配对完成方法大多是类感知且依赖GAN或扩散模型。借鉴参考引导技术和形状翻译思路，他们提出用目标部分点云作为模板检索相似完整点云，形成参考对。随后在潜在空间中融合特征，完成点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是把无配对完成转化为形状翻译：为目标部分点云检索若干参考完整-部分对，使用共享编码器提取特征，Latent Shape Fusion Module 将目标特征与参考特征融合并映射到完整空间，最后解码得到完整点云。整个流程包括检索、编码、融合、解码和对齐损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首个统一的类无关无配对完成框架；2) 引入参考引导的形状翻译思路；3) 设计 Latent Shape Fusion Module 在潜在空间融合缺失结构；4) 通过检索得到的参考对实现无监督对齐。与以往依赖GAN或扩散、仅类感知的工作不同，RefComp 在类无关设置下也能取得竞争性甚至领先的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RefComp 通过参考引导的形状翻译和潜在特征融合，构建了一个同时支持类感知和类无关的无配对点云完成框架，并在多数据集上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The unpaired point cloud completion task aims to complete a partial point cloud by using models trained with no ground truth. Existing unpaired point cloud completion methods are class-aware, i.e., a separate model is needed for each object class. Since they have limited generalization capabilities, these methods perform poorly in real-world scenarios when confronted with a wide range of point clouds of generic 3D objects. In this paper, we propose a novel unpaired point cloud completion framework, namely the Reference-guided Completion (RefComp) framework, which attains strong performance in both the class-aware and class-agnostic training settings. The RefComp framework transforms the unpaired completion problem into a shape translation problem, which is solved in the latent feature space of the partial point clouds. To this end, we introduce the use of partial-complete point cloud pairs, which are retrieved by using the partial point cloud to be completed as a template. These point cloud pairs are used as reference data to guide the completion process. Our RefComp framework uses a reference branch and a target branch with shared parameters for shape fusion and shape translation via a Latent Shape Fusion Module (LSFM) to enhance the structural features along the completion pipeline. Extensive experiments demonstrate that the RefComp framework achieves not only state-of-the-art performance in the class-aware training setting but also competitive results in the class-agnostic training setting on both virtual scans and real-world datasets.&lt;/p&gt;</description></item><item><guid>2505.07396v2</guid><title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title><link>http://arxiv.org/abs/2505.07396v2</link><author>Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; TUM2TWIN 是首个面向城市数字孪生的多模态基准数据集，覆盖地面、移动、空中和卫星观测，包含 32 个子集、约 100,000 平方米面积和 767 GB 数据。该数据集提供地理参考、语义对齐的 3D 模型与网络，支持高精度室内外采集和多模态融合，帮助研究人员分析传感器性能并开发先进的重建方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市数字孪生在城市管理和多源异构数据集成中变得不可或缺，但现有数据集往往只覆盖处理链的一部分，限制了对完整数字孪生的验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个全面、可验证的多模态城市数字孪生基准数据集，以解决数据获取、模型重建、更新维护和互操作性等多阶段挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过地理参考、语义对齐的 3D 模型与网络，结合地面、移动、空中和卫星观测，创建 32 个子集，覆盖约 100,000 平方米面积，数据量约 767 GB。随后在该数据集上演示新视角合成、太阳能潜力分析、点云语义分割和 LoD3 建筑重建等下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; TUM2TWIN 能够支持传感器的稳健分析和先进重建方法的开发，并在新视角合成、太阳能分析、语义分割和建筑重建等任务中展示出显著潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该数据集为克服城市数字孪生创建中的现有限制奠定基础，推动新的研究方向和面向更智能、数据驱动城市环境的实用解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 城市数字孪生（UDTs）已成为管理城市和整合来自多样来源的复杂异构数据的关键。创建 UDTs 涉及多个过程阶段的挑战，包括获取准确的 3D 源数据、重建高保真 3D 模型、维护模型更新以及确保与下游任务的无缝互操作性。目前的数据集通常仅限于处理链的一部分，阻碍了对完整 UDTs 的验证。为了解决这些挑战，我们推出了首个全面的多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包含地理参考、语义对齐的 3D 模型和网络，以及各种地面、移动、空中和卫星观测，拥有 32 个数据子集，覆盖约 100,000 平方米面积，当前数据量约 767 GB。通过确保地理参考的室内外采集、高精度和多模态数据集成，基准支持对传感器的稳健分析和先进重建方法的发展。此外，我们还探索了下游任务，展示了 TUM2TWIN 的潜力，包括 NeRF 和高斯散点的新视角合成、太阳能潜力分析、点云语义分割和 LoD3 建筑重建。我们相信这项贡献为克服 UDT 创建中的当前局限性奠定了基础，促进了新的研究方向和面向更智能、数据驱动城市环境的实用解决方案。项目可在 https://tum2t.win 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models&amp;#x27; updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win&lt;/p&gt;</description></item><item><guid>2505.19026v3</guid><title>Staircase Recognition and Location Based on Polarization Vision</title><link>http://arxiv.org/abs/2505.19026v3</link><author>Weifeng Kong, Zhiying Tan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对楼梯场景感知技术的挑战，提出了融合偏振与光强的对比增强算法，并结合YOLOv11实现点云分割；通过融合偏振双目与TOF深度信息实现楼梯的三维重建；同时提出基于ICP配准和改进灰狼优化的单目相机与TOF相机联合标定方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 楼梯是人工场景中最常见的结构，机器人及下肢残疾或视力障碍者在无传感器与智能算法辅助下难以跨越；现有双目与TOF重建受光照与材质影响，偏振重建受纹理限制，且识别准确率低、噪声大、输出不稳定、计算量大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 实现楼梯检测与高质量三维重建，提升感知精度与鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 对比增强算法融合偏振与光强信息；2. 基于YOLOv11的点云分割；3. 融合偏振双目与TOF深度实现3D重建；4. 采用ICP配准结合改进灰狼优化算法进行单目相机与TOF相机联合标定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 楼梯是人工场景中最常见的结构。然而，缺乏传感器和智能算法的情况下，人形机器人以及下肢残疾或视力障碍者很难跨越楼梯。楼梯场景感知技术是识别与定位的前提，对机器人模式切换和脚印位置计算以适应不连续地形具有重要意义。然而，该技术仍存在许多限制，如识别准确率低、传感器初始噪声高、输出信号不稳定以及计算需求高。在场景重建方面，双目和飞行时间（TOF）重建容易受到环境光和目标表面材质的影响。相比之下，偏振器的特殊结构可以选择性地传输特定方向的偏振光，且该重建方法依赖于目标表面偏振信息，因而具有不易受环境光影响且不依赖纹理信息的优势。本文为实现楼梯检测，提出了一种融合偏振与光强信息的对比增强算法，并结合YOLOv11进行点云分割。为实现高质量重建，提出了融合偏振双目与TOF深度信息的三维重建方法。同时，还提出了基于ICP配准和改进灰狼优化算法的单目相机与TOF相机联合标定算法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Staircase is one of the most common structures in artificial scenes. However, it is difficult for humanoid robots and people with lower limb disabilities or visual impairment to cross the scene without the help of sensors and intelligent algorithms. Staircase scene perception technology is a prerequisite for recognition and localization. This technology is of great significance for the mode switching of the robot and the calculation of the footprint position to adapt to the discontinuous terrain. However, there are still many problems that constrain the application of this technology, such as low recognition accuracy, high initial noise from sensors, unstable output signals and high computational requirements. In terms of scene reconstruction, the binocular and time of flight (TOF) reconstruction of the scene can be easily affected by environmental light and the surface material of the target object. In contrast, due to the special structure of the polarizer, the polarization can selectively transmit polarized light in a specific direction and this reconstruction method relies on the polarization information of the object surface. So the advantages of polarization reconstruction are reflected, which are less affected by environmental light and not dependent on the texture information of the object surface. In this paper, in order to achieve the detection of staircase, this paper proposes a contrast enhancement algorithm that integrates polarization and light intensity information, and integrates point cloud segmentation based on YOLOv11. To realize the high-quality reconstruction, we proposed a method of fusing polarized binocular and TOF depth information to realize the three-dimensional (3D) reconstruction of the staircase. Besides, it also proposes a joint calibration algorithm of monocular camera and TOF camera based on ICP registration and improved gray wolf optimization algorithm.&lt;/p&gt;</description></item><item><guid>2505.19518v2</guid><title>Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions</title><link>http://arxiv.org/abs/2505.19518v2</link><author>Nakul Poudel, Zixin Yang, Kelly Merrell, Richard Simon, Cristian A. Linte</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出基于VN-OccNet的患者特异性点云补全方法，以解决术中点云部分可见导致的配准困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 术中图像引导手术中捕获的数据缺乏亚表面信息，关键区域如血管和肿瘤难以观测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过点云补全提升图像与物理配准的初始刚性配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用VN-OccNet在患者特异性训练下，从部分术中点云生成完整肝表面；随后将补全表面与Go-ICP配准算法结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; VN-OccNet的旋转等变特性有效恢复完整表面，补全后配准结果显著改善。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 患者特异性点云补全能缓解术中可见性不足问题，为鲁棒配准框架提供有力支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在图像引导手术中捕获的术中数据缺乏亚表面信息，关键感兴趣区域如血管和肿瘤位于表面之下。图像到物理的配准可以将术前信息与术中数据（通常表示为点云）融合。然而，由于术中点云的部分可见性，这一配准过程面临困难。在本研究中，我们提出了一种患者特异性的点云补全方法来辅助配准过程。具体而言，我们利用VN-OccNet从部分术中点云生成完整的肝表面。该网络以患者特异性方式训练，使用术前模型的模拟变形来训练模型。首先，我们深入分析了VN-OccNet的旋转等变属性及其在从部分术中表面恢复完整表面方面的有效性。接下来，我们将补全后的术中表面集成到Go-ICP配准算法中，以展示其在改善初始刚性配准结果方面的实用性。我们的结果突显了患者特异性补全方法在缓解部分术中可见性挑战方面的前景。VN-OccNet的旋转等变和表面生成能力为开发针对术中点云变化的鲁棒配准框架提供了强有力的前景。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在图像引导肝脏手术中，术中点云因视角受限而缺乏完整表面信息，导致图像与物理空间配准困难。完整的配准对于准确定位肿瘤和血管等亚表面目标至关重要，直接影响手术安全与效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了已有的点云补全网络和配准技术，提出使用向量神经元占据网络（VN‑OccNet）进行患者特定的点云补全，并通过模拟变形训练模型。该思路在前人工作（如Jia等和Foti等）的基础上，克服了需要手动对应或刚性初始化的缺点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用VN‑OccNet的旋转等变特性，从部分术中点云预测占据概率，构建完整表面网格，然后将网格顶点作为完整点云与预手术点云进行Go‑ICP刚性配准。实现流程包括：①将部分点云输入VN‑OccNet编码器得到潜在特征；②用解码器查询占据概率；③通过多分辨率等值面提取和Marching Cubes生成网格；④提取顶点得到完整点云；⑤使用Go‑ICP完成配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①患者特定的训练策略，利用预手术模型的模拟变形生成训练数据；②采用旋转等变的VN‑OccNet，能在任意姿态下完成补全；③生成完整网格实现均匀采样，提升配准质量；④直接与Go‑ICP配准，无需手动对应或刚性预估。与以往工作相比，它不需要手动对应或刚性初始化，且在旋转变化下表现更稳健。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种患者特定、旋转等变的点云补全管线，能够从部分术中肝脏表面重建完整网格，并显著提升图像与物理空间的配准精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Intra-operative data captured during image-guided surgery lacks sub-surface information, where key regions of interest, such as vessels and tumors, reside. Image-to-physical registration enables the fusion of pre-operative information and intra-operative data, typically represented as a point cloud. However, this registration process struggles due to partial visibility of the intra-operative point cloud. In this research, we propose a patient-specific point cloud completion approach to assist with the registration process. Specifically, we leverage VN-OccNet to generate a complete liver surface from a partial intra-operative point cloud. The network is trained in a patient-specific manner, where simulated deformations from the pre-operative model are used to train the model. First, we conduct an in-depth analysis of VN-OccNet&amp;#x27;s rotation-equivariant property and its effectiveness in recovering complete surfaces from partial intra-operative surfaces. Next, we integrate the completed intra-operative surface into the Go-ICP registration algorithm to demonstrate its utility in improving initial rigid registration outcomes. Our results highlight the promise of this patient-specific completion approach in mitigating the challenges posed by partial intra-operative visibility. The rotation equivariant and surface generation capabilities of VN-OccNet hold strong promise for developing robust registration frameworks for variations of the intra-operative point cloud.&lt;/p&gt;</description></item><item><guid>2505.19919v1</guid><title>Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time</title><link>http://arxiv.org/abs/2505.19919v1</link><author>Chen Sang, Yeqiang Qian, Jiale Zhang, Chunxiang Wang, Ming Yang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于高斯喷射的框架，用于快速重建真实场景并在合成的四维天气效果下进行渲染，解决了传统手工建模和渲染在复杂场景和天气效果上的高成本和低质量问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在城市数字孪生、VR/AR/游戏场景设计以及合成电影等任务中，传统工业方法依赖手工建模和多种渲染引擎，导致高劳动成本、硬件需求大且在复制复杂真实场景时质量不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有重建与渲染算法无法有效重建和渲染真实世界天气效果的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建基于高斯喷射的框架，利用捕获的真实场景数据进行重建，并通过高斯建模与渲染技术合成四维天气效果，实现连续动态天气变化并可细粒度控制效果细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架能够模拟多种常见天气效果，支持连续动态天气变化，细节可控，且硬件要求低，能够实现实时渲染。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于高斯喷射的框架在重建真实场景并渲染合成天气方面表现出色，具备低硬件需求和实时性能，适用于多种应用场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要：对于城市数字孪生、VR/AR/游戏场景设计或合成电影等任务，传统工业方法通常涉及手工建模场景并使用各种渲染引擎完成渲染过程。这种方法通常需要高劳动成本和硬件需求，并且在复制复杂真实场景时可能导致质量不佳。更高效的方法是使用捕获的真实世界场景数据，然后应用重建和渲染算法快速重现真实场景。然而，当前算法无法有效重建和渲染真实世界的天气效果。为了解决这个问题，我们提出了一个基于高斯喷射的框架，能够在合成的四维天气效果下重建真实场景并进行渲染。我们的工作可以通过应用高斯建模和渲染技术模拟各种常见天气效果。它支持连续动态天气变化，并且可以轻松控制效果的细节。此外，我们的工作具有低硬件要求并实现实时渲染性能。结果演示可在我们的项目主页访问：weathermagician.github.io&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现对真实场景的实时、高清重建与渲染，并在此基础上合成多种动态天气效果。此问题在数字孪生、VR/AR、游戏和影视制作等领域尤为重要，因为传统手工建模成本高、硬件需求大且难以真实再现复杂天气。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先构建无天气的清晰场景，再通过在3D高斯散射（3DGS）框架中添加天气高斯来实现天气合成。方法借鉴了3DGS、Rainy‑GS、ClimateNeRF等现有工作，但在此基础上引入深度与法线监督、显式天气高斯建模，并实现了实时渲染。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将天气元素视为可编辑的高斯分布，直接叠加到已重建的3DGS场景中。实现流程包括：①使用SfM生成点云；②在3DGS中训练并通过深度/法线监督优化几何；③根据天气类型生成相应的高斯（雨滴、雪花、雾等）并编辑其属性；④将天气高斯与原场景高斯一起渲染，得到实时天气合成图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①实时、低硬件需求的4D天气合成；②对动态天气（雨、雪）和静态天气（雾、雾霾）的可控显式建模；③利用深度与法线监督提升几何质量；④在3DGS框架中实现完整的天气编辑与渲染。与之前工作相比，它突破了仅能生成静态或非实时天气的局限，提供了更真实、更可控的动态天气效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Weather‑Magician 提供了一种基于 3D 高斯散射的实时 4D 天气合成框架，能够在消费级 GPU 上以高保真度渲染可控的动态与静态天气效果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;For tasks such as urban digital twins, VR/AR/game scene design, or creating synthetic films, the traditional industrial approach often involves manually modeling scenes and using various rendering engines to complete the rendering process. This approach typically requires high labor costs and hardware demands, and can result in poor quality when replicating complex real-world scenes. A more efficient approach is to use data from captured real-world scenes, then apply reconstruction and rendering algorithms to quickly recreate the authentic scene. However, current algorithms are unable to effectively reconstruct and render real-world weather effects. To address this, we propose a framework based on gaussian splatting, that can reconstruct real scenes and render them under synthesized 4D weather effects. Our work can simulate various common weather effects by applying Gaussians modeling and rendering techniques. It supports continuous dynamic weather changes and can easily control the details of the effects. Additionally, our work has low hardware requirements and achieves real-time rendering performance. The result demos can be accessed on our project homepage: weathermagician.github.io&lt;/p&gt;</description></item><item><guid>2505.24348v1</guid><title>A 3D Mobile Crowdsensing Framework for Sustainable Urban Digital Twins</title><link>http://arxiv.org/abs/2505.24348v1</link><author>Taku Yamazaki, Kaito Watanabe, Tatsuya Kase, Kenta Hasegawa, Koki Saida, Takumi Miyoshi</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种面向可持续城市数字孪生的三维移动众包感知框架，包含四个关键机制：①三维移动众包机制（主动与被动模型）；②基于Geohash的空间信息管理机制；③动态点云集成机制；④基于网页的实时可视化工具。主动模型通过增强现实领土上色游戏收集点云数据；被动模型让参与者佩戴手机进行不干扰日常的感知。空间管理通过Geohash划分区域，点云集成通过全局与局部配准实现。通过真实世界实验验证了模型的有效性，并分析了点云集成性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市数字孪生需要大量高质量三维数据，传统采集方式成本高、效率低，移动众包提供了低成本、可扩展的数据获取途径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种可持续、低成本的三维移动众包框架，以支持城市数字孪生的持续更新与可视化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计主动与被动两种感知模型，使用Geohash进行空间划分，开发动态点云集成与网页可视化工具，并在真实环境中进行实验评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 主动游戏化模型和被动佩戴模型均能有效收集点云数据；主观评价与数据分析表明两模型均具备可行性；动态点云集成在实验数据集上表现良好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的三维移动众包框架能够有效支持可持续城市数字孪生的构建与更新，主动与被动模型相结合可满足不同场景需求，集成与可视化机制实现了数据的高效利用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed at sustainable urban digital twins (UDTs). The framework comprises four key mechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models; (2) the Geohash-based spatial information management mechanism; (3) the dynamic point cloud integration mechanism for UDTs; and (4) the web-based real-time visualizer for 3D-MCS and UDTs. The active sensing model features a gamified 3D-MCS approach, where participants collect point cloud data through an augmented reality territory coloring game. In contrast, the passive sensing model employs a wearable 3D-MCS approach, where participants wear smartphones around their necks without disrupting daily activities. The spatial information management mechanism efficiently partitions the space into regions using Geohash. The dynamic point cloud integration mechanism incorporates point clouds collected by 3D-MCS into UDTs through global and local point cloud registration. Finally, we evaluated the proposed framework through real-world experiments. We verified the effectiveness of the proposed 3D-MCS models from the perspectives of subjective evaluation and data collection and analysis. Furthermore, we analyzed the performance of the dynamic point cloud integration using a dataset.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决如何持续、准确地收集大规模三维空间信息，以支持可持续的城市数字孪生（UDT）的构建与维护。该问题重要，因为城市数字孪生需要实时、精确的三维数据来实现智能城市服务，而传统的测绘方法成本高、更新慢。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于移动众测（MCS）与LiDAR技术的进步，提出了主动（AR游戏）和被动（可穿戴）两种感知模型，并结合Geohash空间管理和动态点云注册。设计借鉴了先前的AR游戏、MCS、Geohash编码以及点云配准等工作（如文献[7]–[10]）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过主动游戏和被动可穿戴两种方式收集LiDAR点云，使用Geohash对数据进行分区管理，并在服务器端通过全局+局部配准将新点云动态合并到城市级点云（UDT）中，最后通过Web可视化展示。实现流程包括：用户加入游戏/佩戴设备 → 采集点云并上传 → 服务器预处理、特征匹配、RANSAC+ICP配准 → 合并到UDT → 实时可视化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 结合主动AR游戏与被动可穿戴的双模式MCS框架；② 基于Geohash的可扩展空间信息管理；③ 采用全局+局部配准实现动态点云集成；④ Web实时可视化与机器学习辅助信息。与以往工作相比，该框架将感知、管理、集成和可视化四个环节统一在一个可扩展系统中，并通过游戏化激励提升参与度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 提出了一套完整的3D移动众测框架，融合AR游戏与可穿戴LiDAR采集、Geohash空间管理和动态点云集成，能够实时构建和维护可持续的城市数字孪生。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed at sustainable urban digital twins (UDTs). The framework comprises four key mechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models; (2) the Geohash-based spatial information management mechanism; (3) the dynamic point cloud integration mechanism for UDTs; and (4) the web-based real-time visualizer for 3D-MCS and UDTs. The active sensing model features a gamified 3D-MCS approach, where participants collect point cloud data through an augmented reality territory coloring game. In contrast, the passive sensing model employs a wearable 3D-MCS approach, where participants wear smartphones around their necks without disrupting daily activities. The spatial information management mechanism efficiently partitions the space into regions using Geohash. The dynamic point cloud integration mechanism incorporates point clouds collected by 3D-MCS into UDTs through global and local point cloud registration. Finally, we evaluated the proposed framework through real-world experiments. We verified the effectiveness of the proposed 3D-MCS models from the perspectives of subjective evaluation and data collection and analysis. Furthermore, we analyzed the performance of the dynamic point cloud integration using a dataset.&lt;/p&gt;</description></item><item><guid>2506.00600v2</guid><title>SatDreamer360: Multiview-Consistent Generation of Ground-Level Scenes from Satellite Imagery</title><link>http://arxiv.org/abs/2506.00600v2</link><author>Xianghui Ze, Beiyi Zhu, Zhenbo Song, Jianfeng Lu, Yujiao Shi</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了 SatDreamer360 框架，能够从单张卫星图像和预定义的姿态轨迹生成几何一致的多视角 360 度地面全景。通过三平面表示、基于射线的像素注意机制以及全景极线约束注意模块，解决了视角差异和多帧一致性问题，并在新构建的 VIGOR++ 数据集上验证了其优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成多视角一致的 360 度地面场景是模拟、自动驾驶和数字孪生城市等领域的重要任务，但现有方法多聚焦单一全景，依赖高度图或手工投影，难以实现多视角一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够从单张卫星图像生成几何一致、多视角地面全景的方法，并提供相应的数据集用于评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用三平面表示编码场景特征，使用射线基像素注意机制提取视角特定特征；引入全景极线约束注意模块根据已知相对姿态对齐多帧特征；构建 VIGOR++ 数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SatDreamer360 在卫星到地面对齐和多视角一致性方面优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架有效解决了卫星图像到多视角地面全景的生成问题，并通过新数据集验证了其性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成多视角一致的 360 度地面场景从卫星图像是一个具有广泛应用的挑战性任务，涉及模拟、自动驾驶和数字孪生城市等领域。现有方法主要关注合成单个地面视图全景，往往依赖高度图或手工投影，并难以生成多视角一致的序列。本文提出 SatDreamer360 框架，能够从单张卫星图像和预定义的姿态轨迹生成几何一致的多视角地面全景。为解决地面与卫星图像之间的巨大视角差异，我们采用三平面表示编码场景特征，并设计基于射线的像素注意机制，从三平面中检索视角特定特征。为保持多帧一致性，我们引入基于全景极线约束的注意模块，根据已知相对姿态对齐不同帧的特征。为支持评估，我们构建了 VIGOR++ 数据集，在原始 VIGOR 数据集的基础上增加了更多地面视图图像及其姿态标注。实验表明，SatDreamer360 在卫星到地面对齐和多视角一致性方面优于现有方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在从单张卫星图像和预定义轨迹生成多视角一致的 360° 地面全景图。此问题重要，因为卫星图像获取成本低、覆盖广泛，能够为仿真、自动驾驶和数字孪生城市等应用提供可视化支持，但现有方法多聚焦单帧生成，缺乏连续性和几何一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出单视角生成方法在多帧一致性和几何对齐上的不足，随后借鉴了潜在扩散模型、三平面（triplane）表示、光线引导注意力以及视差约束等技术。通过将这些技术整合到一个统一框架中，作者设计了 SatDreamer360。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 triplane 表示捕捉场景几何，并通过光线引导像素注意力提取视角特定特征，再用潜在扩散模型生成地面全景。实现流程为：① 用卫星图像构建 triplane；② 对每个轨迹姿态，计算光线并在 triplane 上采样特征；③ 将聚合特征作为条件输入到扩散模型；④ 通过视差约束注意力在连续帧间对齐特征，保证多视角一致性；⑤ 输出连续的 360° 全景序列。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 统一框架实现从单张卫星图像到连续地面全景的生成；② 光线引导的跨视角特征条件化，利用 triplane 进行几何感知；③ 适用于全景图的视差约束注意力模块，提升多帧一致性；④ 新的 VIGOR++ 数据集提供大规模评估。与以往单帧或依赖高度图、手工投影的方法不同，SatDreamer360 在保持几何一致性的同时实现了多视角连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SatDreamer360 通过 triplane 几何表示、光线引导注意力和视差约束扩散模型，实现了从单张卫星图像生成连续、几何一致的 360° 地面全景。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generating multiview-consistent $360^\circ$ ground-level scenes from satellite imagery is a challenging task with broad applications in simulation, autonomous navigation, and digital twin cities. Existing approaches primarily focus on synthesizing individual ground-view panoramas, often relying on auxiliary inputs like height maps or handcrafted projections, and struggle to produce multiview consistent sequences. In this paper, we propose SatDreamer360, a framework that generates geometrically consistent multi-view ground-level panoramas from a single satellite image, given a predefined pose trajectory. To address the large viewpoint discrepancy between ground and satellite images, we adopt a triplane representation to encode scene features and design a ray-based pixel attention mechanism that retrieves view-specific features from the triplane. To maintain multi-frame consistency, we introduce a panoramic epipolar-constrained attention module that aligns features across frames based on known relative poses. To support the evaluation, we introduce {VIGOR++}, a large-scale dataset for generating multi-view ground panoramas from a satellite image, by augmenting the original VIGOR dataset with more ground-view images and their pose annotations. Experiments show that SatDreamer360 outperforms existing methods in both satellite-to-ground alignment and multiview consistency.&lt;/p&gt;</description></item><item><guid>2506.04837v1</guid><title>OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model</title><link>http://arxiv.org/abs/2506.04837v1</link><author>Kunshen Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 OpenMaskDINO3D，一种基于大语言模型的 3D 理解与分割框架，能够通过点云数据和自然语言提示直接生成高精度实例分割掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然 2D 视觉分割技术已取得显著进展，但现有系统仍需明确的人类指令或预定义类别来识别目标物体，且缺乏对应的 3D 分割框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在 3D 场景中实现与 2D 系统相当的隐式意图理解与分割的完整框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用大语言模型处理点云与文本提示，加入 SEG 令牌和对象标识符，实现从自然语言指令到点云分割掩码的直接映射。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，OpenMaskDINO3D 在 ScanNet 大规模数据集上实现了高精度 3D 分割，并在多种任务中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenMaskDINO3D 成功填补了 3D 语义分割的空白，为基于语言的 3D 视觉理解提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管感知系统近年来取得了显著进展，尤其是在二维推理分割方面，但这些系统仍然依赖明确的人类指令或预定义类别来识别目标对象，然后再执行视觉识别任务。这些系统已显著成熟，能够在二维环境中推理并理解隐式用户意图，基于复杂且隐式的查询文本生成准确的分割掩码。然而，缺乏与之相当的三维推理分割框架和结构。本文提出了 OpenMaskDINO3D，一种为全面三维理解与分割而设计的大语言模型。OpenMaskDINO3D 处理点云数据和文本提示，生成实例分割掩码，在多种三维任务中表现出色。通过引入 SEG 令牌和对象标识符，我们实现了高精度的三维分割掩码生成，使模型能够直接根据自然语言指令生成准确的点云分割结果。对大规模 ScanNet 数据集的实验结果验证了我们 OpenMaskDINO3D 在各类任务中的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从3D点云和自然语言指令生成3D分割的任务，即3D推理分割。该问题重要，因为它使系统能够在没有预定义类别或人工指令的情况下理解并定位三维场景中的对象，广泛应用于机器人导航、增强现实和智能环境等领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了2D推理分割中的SEG token和位置标记技术，并结合大型语言模型（LLM）来引导分割。通过将3D、2D特征与文本融合到LLM中，利用&amp;lt;SEG&amp;gt; token生成分割查询，最终实现3D分割。该设计参考了OpenMask3D、DINO、CLIP等相关工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让LLM在融合3D、2D和文本信息后，通过&amp;lt;SEG&amp;gt; token产生的嵌入作为查询，驱动Transformer解码器生成3D分割掩码。实现流程包括：①点云通过backbone提取超体素特征；②使用Uni3D和DINOv2分别提取3D和2D特征；③将这些特征与文本编码后输入LLM；④LLM输出&amp;lt;SEG&amp;gt; token嵌入，经过MLP得到查询；⑤查询与backbone的键值对输入解码器，得到最终分割结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①引入SEG token和对象标识符以实现语言驱动的分割；②采用embedding-as-mask范式，将LLM输出直接映射为分割查询；③在3D场景中实现零样本推理并通过少量微调提升性能；④构建了包含1000+样本的3D推理分割基准。与以往工作相比，OpenMaskDINO3D首次将LLM与3D分割深度融合，支持复杂语义指令和空间关系推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenMaskDINO3D通过将大型语言模型与3D/2D特征融合，并利用专门的&amp;lt;SEG&amp;gt; token，实现了从自然语言指令直接生成高精度3D分割掩码的开源框架，并在ScanNet上展示了强大的零样本和微调性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although perception systems have made remarkable advancements in recent years, particularly in 2D reasoning segmentation, these systems still rely on explicit human instruction or pre-defined categories to identify target objects before executing visual recognition tasks. Such systems have matured significantly, demonstrating the ability to reason and comprehend implicit user intentions in two-dimensional contexts, producing accurate segmentation masks based on complex and implicit query text. However, a comparable framework and structure for 3D reasoning segmentation remain absent. This paper introduces OpenMaskDINO3D, a LLM designed for comprehensive 3D understanding and segmentation. OpenMaskDINO3D processes point cloud data and text prompts to produce instance segmentation masks, excelling in many 3D tasks. By introducing a SEG token and object identifier, we achieve high-precision 3D segmentation mask generation, enabling the model to directly produce accurate point cloud segmentation results from natural language instructions. Experimental results on large-scale ScanNet datasets validate the effectiveness of our OpenMaskDINO3D across various tasks.&lt;/p&gt;</description></item><item><guid>2506.05009v1</guid><title>Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2506.05009v1</link><author>Alfred T. Christiansen, Andreas H. Højrup, Morten K. Stephansen, Md Ibtihaj A. Sakib, Taman S. Poojary, Filip Slezak, Morten S. Laursen, Thomas B. Moeslund, Joakim B. Haurum</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用3D高斯喷射和高斯不透明度场生成多种农业车辆3D资产的合成数据管线，并通过模拟激光雷达生成点云。实验表明，仅使用合成数据训练的语义分割模型可达到高精度，并在某些情况下优于使用真实数据训练的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 训练3D点云语义分割模型需要大量标注数据，但获取和标注真实点云成本高且劳动强度大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够生成逼真合成点云数据的新管线，以降低数据获取成本并提升模型训练效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先使用3D高斯喷射和高斯不透明度场生成多种农业车辆的3D模型，然后将这些模型放置在模拟环境中，通过模拟激光雷达生成点云。该方法支持随意更改激光雷达参数，无需额外成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在仅使用合成数据训练的情况下，Point Transformer V3模型取得91.35%的mIoU；合成数据训练的模型在某些场景下表现优于使用真实数据训练的模型；模型能够跨语义类别泛化，在从未训练过的网格模型上也能准确预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 合成数据管线能够生成高质量点云，训练出的模型不仅精度高，还能在不同语义类别和未见模型上保持良好泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 训练用于3D点云语义分割等任务的神经网络需要大量数据集，但获取和标注真实世界点云成本高且劳动强度大。本研究旨在通过利用3D高斯喷射（3DGS）和高斯不透明度场（GOF）生成多种不同农业车辆的3D资产，替代使用通用模型，从而引入一种新的生成逼真合成数据的管线。这些资产被放置在模拟环境中，使用模拟激光雷达生成点云。这是一种灵活的方法，允许在不产生额外成本的情况下更改激光雷达规格。我们通过仅使用合成数据训练和验证模型，评估了合成数据对PointNet++、Point Transformer V3和OACNN等分割模型的影响。值得注意的是，PTv3模型在未使用任何真实数据训练或验证的情况下，取得了91.35%的mIoU。进一步研究甚至表明，在某些场景下，仅使用合成生成的数据训练的模型表现优于使用真实世界数据训练的模型。最后，实验表明模型能够跨语义类别泛化，在从未训练过的网格模型上实现准确预测。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决农业车辆点云语义分割训练所需的大规模标注数据缺乏问题。由于真实点云采集和标注成本高且耗时，缺少足够的数据会限制深度学习模型的性能和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了 3D Gaussian Splatting（3DGS）和 Gaussian Opacity Fields（GOF）技术，从无人机拍摄的图像中快速生成高质量车辆网格，并在 Gazebo 中模拟 Ouster LiDAR 采样。该思路借鉴了 3DGS、GOF、以及已有的点云合成与语义分割模型（如 PTv3、PointNet++）等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过 3DGS+GOF 从实景图像生成真实感车辆网格，然后在仿真环境中以真实 LiDAR 采样方式生成带标签的点云。实现流程包括：无人机拍摄 → SfM 生成稀疏点云 → GOF 提取网格 → 裁剪单个车辆 → 导入 Gazebo 并模拟 LiDAR → 生成合成点云并训练分割模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 用 3DGS+GOF 自动生成高质量车辆网格；② 在仿真中使用真实 LiDAR 采样模式生成点云；③ 证明仅用合成数据训练的模型可在真实数据上取得竞争性能，并能泛化到未见车辆。与以往使用通用模型或均匀采样的合成数据不同，该方法提供更逼真、更可调的训练资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一套完整的从无人机图像到仿真 LiDAR 点云的流水线，利用 3D Gaussian splatting 与 Gaussian opacity fields 生成逼真车辆网格，使得仅用合成数据即可训练出高性能的农业车辆点云语义分割模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Training neural networks for tasks such as 3D point cloud semantic segmentation demands extensive datasets, yet obtaining and annotating real-world point clouds is costly and labor-intensive. This work aims to introduce a novel pipeline for generating realistic synthetic data, by leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to generate 3D assets of multiple different agricultural vehicles instead of using generic models. These assets are placed in a simulated environment, where the point clouds are generated using a simulated LiDAR. This is a flexible approach that allows changing the LiDAR specifications without incurring additional costs. We evaluated the impact of synthetic data on segmentation models such as PointNet++, Point Transformer V3, and OACNN, by training and validating the models only on synthetic data. Remarkably, the PTv3 model had an mIoU of 91.35\%, a noteworthy result given that the model had neither been trained nor validated on any real data. Further studies even suggested that in certain scenarios the models trained only on synthetically generated data performed better than models trained on real-world data. Finally, experiments demonstrated that the models can generalize across semantic classes, enabling accurate predictions on mesh models they were never trained on.&lt;/p&gt;</description></item><item><guid>2506.09552v1</guid><title>Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments</title><link>http://arxiv.org/abs/2506.09552v1</link><author>Fatemeh Mohammadi Amin, Darwin G. Caldwell, Hans Wernher van de Venn</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种双流网络架构（FUSION），用于将3D点云语义分割从模拟环境迁移到真实工业环境，从而提升人机协作的安全性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在工业人机协作中，3D环境的鲁棒解释至关重要，语义分割能够提供精确的环境理解，但需要大量真实标注数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够从模拟环境稳健迁移到真实世界的网络，以增强工业人机协作的实用性和安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用双流网络FUSION，融合动态图卷积神经网络（DGCNN）和带残差层的卷积神经网络（CNN），实现Sim2Real领域适配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实人机协作设置和工业模拟点云上评估，模型实现了97.76%的语义分割准确率，并表现出优于现有方法的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型通过高精度语义分割和强鲁棒性，显著提升了工业人机协作的安全性和操作效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 对3D环境的鲁棒解释对于人机协作至关重要，安全和操作效率是首要考虑。语义分割在此背景下发挥关键作用，使环境得到精确细致的理解。由于现实工业环境需要大量标注数据来实现有效的语义分割，本文提出了一种在Sim2Real领域适配3D点云语义分割的创新方法，专为人机协作设计。我们致力于开发一种能够从模拟环境稳健迁移到真实世界的网络，从而提升其实用性和对安全人机协作的影响。本文提出了一个双流网络架构（FUSION），结合动态图卷积神经网络（DGCNN）和带残差层的卷积神经网络（CNN），作为工业环境的Sim2Real领域适配算法。该模型在真实人机协作设置和工业模拟点云上进行评估，表现出更高的最先进性能，语义分割准确率达到97.76%，并且相较于现有方法具有更好的鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决工业环境中人机协作（HRC）场景下3D点云语义分割的准确性问题，尤其是模拟与真实数据之间的差距。准确的感知对于保障人机交互的安全和效率至关重要，但真实标注数据稀缺，导致模型在真实环境中的表现受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将现有的点云网络DGCNN与CNN（含残差层）结合，构建了双流网络FUSION，并利用NVIDIA IsaacSim生成的合成数据进行预训练。随后采用域适应技术将模型迁移到真实数据，借鉴了Sim2Real适应、COVERED数据集以及相关点云分割研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双流网络融合局部几何特征和全局语义特征，并通过域适应将模型从合成环境迁移到真实环境。实现流程包括：①使用IsaacSim生成合成点云数据；②在合成数据上训练FUSION；③使用真实数据进行域适应或微调；④在在线和离线场景下评估分割精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①双流FUSION架构用于点云分割；②针对工业HRC的Sim2Real域适应流程；③公开了包含真实与合成数据的完整数据集。与以往工作不同的是，该方法实现了实时高精度（97.6%）的分割，并在真实工业环境中表现出更强的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 论文提出一种基于双流网络的Sim2Real域适应框架，利用合成3D点云训练并在工业人机协作环境中实现高精度实时语义分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The robust interpretation of 3D environments is crucial for human-robot collaboration (HRC) applications, where safety and operational efficiency are paramount. Semantic segmentation plays a key role in this context by enabling a precise and detailed understanding of the environment. Considering the intense data hunger for real-world industrial annotated data essential for effective semantic segmentation, this paper introduces a pioneering approach in the Sim2Real domain adaptation for semantic segmentation of 3D point cloud data, specifically tailored for HRC. Our focus is on developing a network that robustly transitions from simulated environments to real-world applications, thereby enhancing its practical utility and impact on a safe HRC.   In this work, we propose a dual-stream network architecture (FUSION) combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional Neural Networks (CNN) augmented with residual layers as a Sim2Real domain adaptation algorithm for an industrial environment. The proposed model was evaluated on real-world HRC setups and simulation industrial point clouds, it showed increased state-of-the-art performance, achieving a segmentation accuracy of 97.76%, and superior robustness compared to existing methods.&lt;/p&gt;</description></item><item><guid>2506.14066v1</guid><title>A Point Cloud Completion Approach for the Grasping of Partially Occluded Objects and Its Applications in Robotic Strawberry Harvesting</title><link>http://arxiv.org/abs/2506.14066v1</link><author>Ali Abouzeid, Malak Mansour, Chengsong Hu, Dezhen Song</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种端到端的框架，用于解决机器人果实采摘中因部分遮挡导致的物体检测、分割和抓取规划问题，以草莓采摘为例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在无结构环境下，物体遮挡是设计抓取算法的主要挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过改进点云处理和抓取规划，提高机器人在草莓采摘中的抓取成功率和安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先对点云进行去噪和分割，定位果实；使用点云补全模型生成完整的三维重建；挑选成熟草莓为目标，其他视为障碍物；将处理后的点云转换为占据图，用于碰撞感知的运动规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，形状重建误差最低，抓取成功率提升至79.17%，整体成功率为89.58%；障碍物碰撞率从43.33%降至13.95%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该管线显著提升了自主草莓采摘的效率和可靠性，为更高效的机器人果实采摘系统奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在机器人果实采摘应用中，管理无结构环境下的物体遮挡对抓取算法的设计提出了重大挑战。以草莓采摘为案例，我们提出了一套端到端的框架，用于有效的物体检测、分割和抓取规划，以解决部分遮挡导致的问题。我们的策略首先对点云进行去噪和分割，以准确定位果实。为补偿因遮挡导致的不完整扫描，我们应用点云补全模型生成草莓的密集三维重建。目标选择聚焦成熟草莓，将其他果实归类为障碍物，然后将精炼后的点云转换为占据图，用于碰撞感知的运动规划。实验结果表明，形状重建误差最低，抓取成功率显著提升至79.17%，在真实世界草莓采摘中的整体成功率为89.58%。此外，我们的方法将障碍物碰撞率从43.33%降低至13.95%，凸显了其在提升抓取质量和安全性方面相较于先前方法的有效性。这一管线大幅提升了自主草莓采摘，推动了更高效、更可靠的机器人果实采摘系统的发展。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决机器人在不规则、遮挡严重的环境中对部分可见水果（如草莓）的识别、重建和抓取问题。该问题重要，因为传统的抓取方法依赖完整视角，而在真实农田中叶片、茎和其他果实会遮挡目标，导致抓取失败，影响自动化采摘的效率和质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先对深度图进行去噪和体素下采样，再利用实例分割提取单个水果的点云，随后采用基于 PointAttn 的深度学习完成网络重建完整形状。该思路借鉴了现有的点云完成方法、Sim‑to‑Real 适配技术（如 R2SGrasp）以及前期的机器人采摘研究，结合了完整形状重建与碰撞感知规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点云去噪、分割、深度学习完成和目标选择，构建完整的三维水果模型并在此基础上进行碰撞感知的抓取规划。实现流程为：RGB‑D 输入 → 深度去噪与体素下采样 → 实例分割与点云提取 → 余量去噪 → 通过完成网络生成完整点云 → 识别熟果并选取最近目标 → 将其余果体视为障碍物构建占据图 → 进行碰撞感知的逆运动学与路径规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 通过轻量级去噪技术有效弥合 Sim‑to‑Real 差距；2) 对部分可见草莓进行完整 3D 重建，显著降低 Chamfer 距离；3) 将非目标草莓视为障碍物实现安全抓取；4) 在真实采摘实验中实现更高的抓取成功率和更低的碰撞率。与以往仅关注单视角或仅完成形状的研究不同，该工作同时解决了遮挡、完整重建和安全规划三大难点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套端到端的点云去噪、分割、完整重建与碰撞感知抓取管线，在真实草莓采摘中显著提升了抓取成功率并降低了碰撞风险。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In robotic fruit picking applications, managing object occlusion in unstructured settings poses a substantial challenge for designing grasping algorithms. Using strawberry harvesting as a case study, we present an end-to-end framework for effective object detection, segmentation, and grasp planning to tackle this issue caused by partially occluded objects. Our strategy begins with point cloud denoising and segmentation to accurately locate fruits. To compensate for incomplete scans due to occlusion, we apply a point cloud completion model to create a dense 3D reconstruction of the strawberries. The target selection focuses on ripe strawberries while categorizing others as obstacles, followed by converting the refined point cloud into an occupancy map for collision-aware motion planning. Our experimental results demonstrate high shape reconstruction accuracy, with the lowest Chamfer Distance compared to state-of-the-art methods with 1.10 mm, and significantly improved grasp success rates of 79.17%, yielding an overall success-to-attempt ratio of 89.58\% in real-world strawberry harvesting. Additionally, our method reduces the obstacle hit rate from 43.33% to 13.95%, highlighting its effectiveness in improving both grasp quality and safety compared to prior approaches. This pipeline substantially improves autonomous strawberry harvesting, advancing more efficient and reliable robotic fruit picking systems.&lt;/p&gt;</description></item><item><guid>2506.15747v2</guid><title>A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion</title><link>http://arxiv.org/abs/2506.15747v2</link><author>Fangzhou Lin, Zilin Dai, Rigved Sanku, Songlin Hou, Kazunori D Yamada, Haichong K. Zhang, Ziming Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种仅使用部分点云而不依赖单视角图像的点云补全方法，并通过实验验证其优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单视角图像引导点云补全（SVIPC）已被证明有效，但图像引导的必要性尚未得到充分探讨。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究图像引导在SVIPC中的核心作用，并提出一种无视角的基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建基于注意力的多分支编码-解码网络，采用层次自融合机制，利用交叉注意力和自注意力层融合多条信息流，从而增强特征表达并提升几何结构捕捉能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ShapeNet-ViPC数据集上，所提出的无视角框架在实验和消融研究中均优于现有最先进的SVIPC方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 研究结果为SVIPC多模态学习的发展提供了新的见解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 单视角图像引导点云补全（SVIPC）任务旨在利用单视角图像帮助从部分输入重建完整点云。虽然以前的工作已证明这种多模态方法的有效性，但图像引导的基本必要性仍未得到充分检验。为此，我们提出了一种基于注意力的多分支编码-解码网络的强基线方法，该方法仅以部分点云为输入，完全不依赖视角。我们的层次自融合机制由交叉注意力和自注意力层驱动，有效整合多条流的信息，丰富特征表示并增强网络捕捉几何结构的能力。对ShapeNet-ViPC数据集进行的大量实验和消融研究表明，我们的无视角框架在性能上优于现有最先进的SVIPC方法。我们希望我们的发现为SVIPC多模态学习的发展提供新的洞见。我们的演示代码将发布在 https://github.com/Zhang-VISLab。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; The paper investigates whether single‑view images are necessary for completing partial point clouds and proposes a strong baseline that relies only on the point cloud. Completing 3D shapes from incomplete scans is vital for robotics, autonomous driving, and AR, where sensor data is often occluded or limited.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; The authors noted the drawbacks of image‑guided pipelines—image quality, calibration, and extra computation—and therefore designed a view‑free architecture. They built on encoder‑decoder frameworks such as PointNet++ and SnowflakeNet, and incorporated attention mechanisms inspired by XMFNet and transformer‑based point methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; The core idea is to extract multi‑scale features from the partial point cloud using several independent encoder branches, fuse these features with cross‑attention and self‑attention (self‑fusion), and then feed the enriched representation into a decoder that predicts the missing points. The pipeline is: partial point cloud → multi‑branch encoder → hierarchical feature extraction → self‑fusion network → concatenated fused features → decoder → complete point cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; Key innovations include a view‑free baseline that outperforms image‑guided methods, a multi‑branch encoder that captures diverse geometric cues, an attention‑based self‑fusion module that adaptively merges features across branches and levels, and a flexible architecture that scales to more branches. Unlike prior work that fuses image and point‑cloud modalities with simple concatenation, this approach relies solely on point‑cloud data and uses cross‑ and self‑attention for richer fusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; We demonstrate that a multi‑branch attention‑enhanced encoder‑decoder using only partial point clouds can surpass state‑of‑the‑art image‑guided completion methods, proving that image guidance is not essential for high‑quality point‑cloud reconstruction.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The single-view image guided point cloud completion (SVIPC) task aims to reconstruct a complete point cloud from a partial input with the help of a single-view image. While previous works have demonstrated the effectiveness of this multimodal approach, the fundamental necessity of image guidance remains largely unexamined. To explore this, we propose a strong baseline approach for SVIPC based on an attention-based multi-branch encoder-decoder network that only takes partial point clouds as input, view-free. Our hierarchical self-fusion mechanism, driven by cross-attention and self-attention layers, effectively integrates information across multiple streams, enriching feature representations and strengthening the networks ability to capture geometric structures. Extensive experiments and ablation studies on the ShapeNet-ViPC dataset demonstrate that our view-free framework performs superiorly to state-of-the-art SVIPC methods. We hope our findings provide new insights into the development of multimodal learning in SVIPC. Our demo code will be available at https://github.com/Zhang-VISLab.&lt;/p&gt;</description></item><item><guid>2506.15849v2</guid><title>PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps</title><link>http://arxiv.org/abs/2506.15849v2</link><author>Kirill Muravyev, Artem Kobozev, Vasily Yuryev, Alexander Melekhin, Oleg Bulichev, Dmitry Yudin, Konstantin Yakovlev</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; PRISM-Loc 是一种轻量且鲁棒的定位方法，适用于大型户外环境，结合紧凑拓扑表示与基于原始 LiDAR 扫描的扫描匹配与路缘检测模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统定位方法在资源受限平台上难以实时运行，且易受城市环境传感挑战影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种资源友好、实时且对城市感知挑战具有弹性的定位方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用紧凑拓扑地图进行全局位置识别，并使用原创扫描匹配技术直接处理原始 LiDAR 点云，实现路缘检测与匹配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ITLP-Campus 大规模数据集上实现 99% 成功率，定位时间 150 毫秒，地图大小仅 20 MB。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PRISM-Loc 在资源受限平台上实现高效、准确的城市规模定位，证明其轻量化与鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; PRISM-Loc 是一种轻量且鲁棒的定位方法，适用于大型户外环境，结合紧凑拓扑表示与基于原始 LiDAR 扫描的扫描匹配与路缘检测模块。该方法针对资源受限平台设计，强调实时性能和对常见城市感知挑战的弹性。它通过全局位置识别和原创扫描匹配技术，在紧凑拓扑地图上实现准确定位。实验在标准基准和嵌入式平台上验证了其有效性，在大型 ITLP-Campus 数据集上实现 99% 的成功率，定位时间为 150 毫秒，使用 20 MB 的地图。主要贡献包括：① 城市规模定位的紧凑表示；② 直接在原始 LiDAR 点云上运行的路缘检测与扫描匹配管道；③ 对方法进行全面评估与性能分析。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现一种轻量级、实时的长距离 LiDAR 定位方法，适用于城市环境中的资源受限平台。该问题重要，因为传统的高精度定位需要密集的全局 LiDAR 地图，既占用大量内存又计算量大，难以在嵌入式设备上实时运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将已有的 PRISM‑TopoMap 图结构、MinkLoc3D 轻量级点云识别以及传统的扫描匹配技术结合起来，并在此基础上加入了基于原始 LiDAR 点云的路缘检测。设计思路是先用全局识别快速定位到候选节点，再用精细的扫描匹配校正姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用图结构的拓扑地图和原始 LiDAR 的路缘信息实现快速、精确的定位。流程包括：1）使用里程计更新当前节点和相对姿态；2）检查扫描与当前节点的重叠；3）若重叠不足，先尝试沿图边移动；4）若仍失败，使用 MinkLoc3D 进行全局识别得到候选节点；5）对候选节点执行基于路缘的扫描匹配，得到精确变换；6）更新当前节点和全局姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 采用极小的拓扑地图，每个节点仅占几 KB，支持城市级规模；2) 开发了基于原始 LiDAR 的路缘检测与扫描匹配算法，提升了在低重叠场景下的匹配精度；3) 在嵌入式平台上实现 150 ms 的实时定位，地图体积仅 20 MB。与以往依赖密集度量地图或昂贵学习模型的定位方法不同，PRISM‑Loc 在保持高精度的同时大幅降低了计算和存储成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PRISM‑Loc 通过将轻量级拓扑地图、MinkLoc3D 识别和路缘感知扫描匹配相结合，提供了一种在城市大范围内实时、资源友好的 LiDAR 定位方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose PRISM-Loc - a lightweight and robust approach for localization in large outdoor environments that combines a compact topological representation with a novel scan-matching and curb-detection module operating on raw LiDAR scans. The method is designed for resource-constrained platforms and emphasizes real-time performance and resilience to common urban sensing challenges. It provides accurate localization in compact topological maps using global place recognition and an original scan matching technique. Experiments on standard benchmarks and on an embedded platform demonstrate the effectiveness of our approach. Our method achieves a 99\% success rate on the large-scale ITLP-Campus dataset while running at 150 ms per localization and using a 20 MB map for localization. We highlight three main contributions: (1) a compact representation for city-scale localization; (2) a novel curb detection and scan matching pipeline operating directly on raw LiDAR points; (3) a thorough evaluation of our method with performance analysis.&lt;/p&gt;</description></item><item><guid>2506.17290v1</guid><title>SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation</title><link>http://arxiv.org/abs/2506.17290v1</link><author>Yuqi Li, Junhao Dong, Zeyu Dong, Chuanguang Yang, Zhulin An, Yongjun Xu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种结构与关系感知的知识蒸馏框架SRKD，能够将大型冻结教师模型的几何与语义知识迁移到轻量级学生模型，从而在保持高精度的同时显著降低模型复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云分割常用的大规模变压器模型计算复杂度高，部署受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过知识蒸馏将大模型的丰富信息转移到小模型，实现高效分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SRKD包括基于亲和矩阵的关系对齐模块、跨样本小批量构造策略、KL散度对齐语义分布以及真实标签监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明SRKD在大幅减小模型规模的同时，取得了最先进的分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在实际部署场景中既高效又有效，证明了知识蒸馏在点云分割中的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云分割面临实际挑战，因为大型基于变压器的模型计算复杂度高且部署受限。为了解决这一问题，我们提出了一种新颖的结构与关系感知知识蒸馏框架，称为SRKD，它将来自大型冻结教师模型（&amp;gt;100M）的丰富几何和语义知识迁移到轻量级学生模型（&amp;lt;15M）。具体而言，我们提出了基于亲和矩阵的关系对齐模块，通过点对点相似度匹配将教师的结构依赖关系蒸馏给学生，从而增强学生学习上下文交互的能力。同时，我们引入了跨样本小批量构造策略，使学生能够感知稳定且通用的几何结构，这种对齐发生在教师的不同点云实例之间，而不是单个样本内部。此外，使用KL散度对齐语义分布，并通过真实标签监督进一步强化准确分割。我们的方法在显著降低模型复杂度的同时实现了最先进的性能，证明了其在真实部署场景中的有效性和效率。代码可在 https://github.com/itsnotacie/SRKD 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云语义分割中模型体量大、计算复杂度高的问题，使得在自动驾驶、机器人和AR/VR等需要实时、边缘部署的场景中能够使用轻量化模型，同时保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于知识蒸馏的思路，结合Transformer在点云分割中的优势，借鉴了2D视觉中的KD技术、关系蒸馏和体素采样方法，提出了跨样本几何对齐和基于亲和矩阵的关系对齐两大模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多种蒸馏目标让轻量化学生模型学习教师模型的语义分布、通道特征、点间与体素间的关系以及跨样本的几何相似性。实现流程包括：1）用冻结的教师和学生网络分别提取特征和预测；2）计算亲和矩阵并对齐点/体素关系；3）构造跨样本相似矩阵并对齐；4）联合KL、交叉熵和亲和损失进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①跨样本mini-batch几何蒸馏，捕获全局几何一致性；②基于亲和矩阵的关系蒸馏，传递点间和体素间的结构信息；③多目标蒸馏框架，将语义、通道、关系和几何信息统一融合。与以往仅在单样本内进行语义或特征蒸馏的工作不同，SRKD同时利用跨样本几何和关系信息显著提升学生模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SRKD通过联合跨样本几何对齐和关系亲和矩阵蒸馏，将大型Transformer点云分割模型压缩为轻量化网络，同时保持甚至超越原模型的精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D point cloud segmentation faces practical challenges due to the computational complexity and deployment limitations of large-scale transformer-based models. To address this, we propose a novel Structure- and Relation-aware Knowledge Distillation framework, named SRKD, that transfers rich geometric and semantic knowledge from a large frozen teacher model (&amp;gt;100M) to a lightweight student model (&amp;lt;15M). Specifically, we propose an affinity matrix-based relation alignment module, which distills structural dependencies from the teacher to the student through point-wise similarity matching, enhancing the student&amp;#x27;s capability to learn contextual interactions. Meanwhile, we introduce a cross-sample mini-batch construction strategy that enables the student to perceive stable and generalized geometric structure. This aligns across diverse point cloud instances of the teacher, rather than within a single sample. Additionally, KL divergence is applied to align semantic distributions, and ground-truth supervision further reinforces accurate segmentation. Our method achieves state of the art performance with significantly reduced model complexity, demonstrating its effectiveness and efficiency in real-world deployment scenarios. Our Code is available at https://github.com/itsnotacie/SRKD.&lt;/p&gt;</description></item><item><guid>2506.18292v2</guid><title>Three-dimentional reconstruction of complex, dynamic population canopy architecture for crops with a novel point cloud completion model: A case study in Brassica napus rapeseed</title><link>http://arxiv.org/abs/2506.18292v2</link><author>Ziyue Guo, Xin Yang, Yutao Shen, Yang Zhu, Lixi Jiang, Haiyan Cen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对油菜种群的复杂动态冠层结构的三维重建方法，利用点云补全模型实现完整冠层的重建，并通过自动化标注框架生成训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确描述完整冠层结构对于评估作物光合作用和产量、指导理想型设计至关重要，但现有三维重建技术因冠层遮挡问题难以获得精确结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种有效的点云补全方法，克服遮挡限制，实现油菜种群冠层的完整三维重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建完整点云生成框架进行自动标注；设计CP-PCN网络，包含多分辨率动态图卷积编码器(MRDG)、点金字塔解码器(PPD)和动态图卷积特征提取模块(DGCFE)以预测被遮挡点并捕捉生长期间结构变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CP-PCN在四个生长阶段的Chamfer距离为3.35–4.51厘米，优于基于Transformer的PoinTr；消融实验验证了MRDG和DGCFE模块的有效性；基于CP-PCN的果荚效率指数将油菜产量预测精度提升了11.2%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CP-PCN管线可推广至其他作物，显著提升田间种群冠层结构的定量分析能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 定量描述完整冠层结构对于准确评估作物光合作用和产量表现至关重要，可指导理想型设计。虽然已经开发了多种用于单株植物和冠层三维重建的传感技术，但由于复杂冠层结构中的严重遮挡，它们未能获得准确的冠层结构描述。我们提出了一种针对油菜作物的复杂动态种群冠层结构三维重建的有效方法，采用新颖的点云补全模型。通过区分冠层内表面点与被遮挡点，开发了完整点云生成框架以实现训练数据的自动标注。随后设计了作物种群点云补全网络（CP-PCN），该网络包含多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD），用于预测被遮挡点。为进一步增强特征提取，提出了动态图卷积特征提取模块（DGCFE），以捕捉整个油菜生长期间的结构变化。结果表明，CP-PCN在四个生长阶段的Chamfer距离（CD）值为3.35厘米–4.51厘米，优于基于Transformer的方法（PoinTr）。消融研究确认了MRDG和DGCFE模块的有效性。此外，验证实验表明，基于CP-PCN的果荚效率指数将油菜产量预测的整体准确性提高了11.2%，相较于使用不完整点云。CP-PCN管线有望扩展到其他作物，显著推进田间种群冠层结构的定量分析。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文试图解决在密集、动态的油菜田中，如何准确重建完整的冠层三维结构，克服遮挡导致的点云缺失问题。完整的冠层描述对评估光合效率、产量预测和理想品种设计至关重要，能够帮助农学家和育种者做出更科学的决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先收集单株油菜的高质量三维数据，然后通过虚实融合（VRI）方法模拟整个田间种植模式，生成包含遮挡信息的完整点云。为训练数据自动标注，他们设计了遮挡点检测算法。网络设计借鉴了图卷积、Transformer和GAN等现有技术，提出了多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD），并加入动态图卷积特征提取器（DGCFE）以捕捉生长阶段的结构变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用模拟数据构建完整的冠层点云，再训练一个点云补全网络，使其能够在真实田间采集的缺失点云中恢复被遮挡的部分。实现流程包括：① UAV和NeRF获取单株三维模型；② VRI生成种植模式下的完整种群点云；③ 遮挡点检测实现自动标注；④ 用这些数据训练CP‑PCN网络；⑤ 在田间采集的缺失点云上应用CP‑PCN完成冠层；⑥ 通过完成的点云提取结构特征并计算种子效率指数以提升产量预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① VRI方法生成与真实田间极为相似的种群点云；② 自动遮挡点检测实现大规模训练集标注；③ CP‑PCN网络结合多分辨率动态图卷积和点金字塔解码器，显著提升补全精度；④ 引入动态图卷积特征提取器捕捉生长阶段的结构变化；⑤ 通过完整点云改进产量预测，提升准确率。与以往仅针对单株或简单结构的补全方法不同，该工作针对密集、动态的田间冠层实现了高精度补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套从模拟生成完整冠层点云到训练专用补全网络，再到在真实田间数据上实现高精度冠层重建和产量预测的完整流程，显著提升了油菜田间冠层三维建模的准确性和实用价值。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Quantitative descriptions of the complete canopy architecture are essential for accurately evaluating crop photosynthesis and yield performance to guide ideotype design. Although various sensing technologies have been developed for three-dimensional (3D) reconstruction of individual plants and canopies, they failed to obtain an accurate description of canopy architectures due to severe occlusion among complex canopy architectures. We proposed an effective method for 3D reconstruction of complex, dynamic population canopy architecture for rapeseed crops with a novel point cloud completion model. A complete point cloud generation framework was developed for automated annotation of the training dataset by distinguishing surface points from occluded points within canopies. The crop population point cloud completion network (CP-PCN) was then designed with a multi-resolution dynamic graph convolutional encoder (MRDG) and a point pyramid decoder (PPD) to predict occluded points. To further enhance feature extraction, a dynamic graph convolutional feature extractor (DGCFE) module was proposed to capture structural variations over the whole rapeseed growth period. The results demonstrated that CP-PCN achieved chamfer distance (CD) values of 3.35 cm -4.51 cm over four growth stages, outperforming the state-of-the-art transformer-based method (PoinTr). Ablation studies confirmed the effectiveness of the MRDG and DGCFE modules. Moreover, the validation experiment demonstrated that the silique efficiency index developed from CP-PCN improved the overall accuracy of rapeseed yield prediction by 11.2% compared to that of using incomplete point clouds. The CP-PCN pipeline has the potential to be extended to other crops, significantly advancing the quantitatively analysis of in-field population canopy architectures.&lt;/p&gt;</description></item><item><guid>2506.23227v1</guid><title>High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation</title><link>http://arxiv.org/abs/2506.23227v1</link><author>Lunhao Duan, Shanshan Zhao, Xingxing Weng, Jing Zhang, Gui-Song Xia</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究在仅有场景级注释的室内点云语义分割问题，提出一种高质量伪标签生成框架，通过多模态信息和区域-点语义一致性提升分割精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统方法依赖稀疏点级标签，场景级注释的点云分割研究相对较少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在缺乏精确点级标签的情况下，生成准确的伪标签以训练分割模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用跨模态特征引导模块将2D图像像素与3D点云特征对齐；引入区域-点语义一致性模块，通过区域投票产生区域语义来指导点级预测，从而纠正训练过程中的错误伪标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ScanNet v2和S3DIS数据集上，方法相较于之前工作取得显著提升；消融实验验证了各模块的贡献。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架能够有效生成高质量伪标签，显著提升场景级注释下的点云语义分割性能，代码已公开。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究在仅有场景级注释的室内点云语义分割问题。缺乏精确点级标签时，现有方法先生成点级伪标签，再训练分割模型。然而，仅凭场景级注释生成准确伪标签具有挑战性，严重影响分割效果。为提高精度，本文提出高质量伪标签生成框架，利用多模态信息和区域-点语义一致性。具体而言，跨模态特征引导模块利用2D-3D对应关系，将点云特征与对应的2D图像像素对齐，帮助点云特征学习。为进一步缓解场景级注释带来的挑战，引入区域-点语义一致性模块。该模块通过基于点级语义的区域投票策略产生区域语义，用于指导点级语义预测。利用上述模块，方法可在训练期间纠正不准确的点级语义预测，获得高质量伪标签。在ScanNet v2和S3DIS数据集上，本文方法在场景级注释下显著优于以往工作，验证了其有效性。进一步的消融研究验证了各组件的贡献。代码已公开。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决仅使用场景级标签进行室内点云语义分割的问题。由于点级标注成本高，场景级标注更易获取，提升其下的分割性能对实际应用和大规模数据集的研究都具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为伪标签质量是关键，借鉴了跨模态知识蒸馏、区域投票和无监督分割等现有技术（如PCAM、WyPR、MIT、PPKT）。他们结合2D-3D对应关系和区域-点一致性，设计了跨模态特征引导和区域-点语义一致性模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点对像素的对比蒸馏将2D图像特征引导3D点云特征，并利用教师网络生成的区域语义来监督学生网络的点级预测，从而迭代提升伪标签质量。实现流程包括：① 2D/3D网络提取特征和预测；② 跨模态对比蒸馏对齐特征；③ 教师网络产生区域语义；④ 区域-点一致性模块监督学生网络；⑤ 生成高质量伪标签并训练分割模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 采用点对像素的对比蒸馏在场景级标注下实现跨模态特征对齐；② 引入动态阈值的区域-点语义一致性模块，缓解无监督分割噪声；③ 通过教师-学生迭代提升伪标签质量。与之前的工作相比，本文不再依赖粗糙的CAM或单一区域标签，而是结合2D信息和一致性约束显著提升伪标签精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种结合跨模态对比蒸馏和区域-点一致性的框架，在仅有场景级标签的条件下生成高质量伪标签，显著提升室内点云语义分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper investigates indoor point cloud semantic segmentation under scene-level annotation, which is less explored compared to methods relying on sparse point-level labels. In the absence of precise point-level labels, current methods first generate point-level pseudo-labels, which are then used to train segmentation models. However, generating accurate pseudo-labels for each point solely based on scene-level annotations poses a considerable challenge, substantially affecting segmentation performance. Consequently, to enhance accuracy, this paper proposes a high-quality pseudo-label generation framework by exploring contemporary multi-modal information and region-point semantic consistency. Specifically, with a cross-modal feature guidance module, our method utilizes 2D-3D correspondences to align point cloud features with corresponding 2D image pixels, thereby assisting point cloud feature learning. To further alleviate the challenge presented by the scene-level annotation, we introduce a region-point semantic consistency module. It produces regional semantics through a region-voting strategy derived from point-level semantics, which are subsequently employed to guide the point-level semantic predictions. Leveraging the aforementioned modules, our method can rectify inaccurate point-level semantic predictions during training and obtain high-quality pseudo-labels. Significant improvements over previous works on ScanNet v2 and S3DIS datasets under scene-level annotation can demonstrate the effectiveness. Additionally, comprehensive ablation studies validate the contributions of our approach&amp;#x27;s individual components. The code is available at https://github.com/LHDuan/WSegPC .&lt;/p&gt;</description></item><item><guid>2507.05211v2</guid><title>All in One: Visual-Description-Guided Unified Point Cloud Segmentation</title><link>http://arxiv.org/abs/2507.05211v2</link><author>Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为VDG-Uni3DSeg的新框架，通过融合预训练的视觉-语言模型和大型语言模型，利用多模态信息提升3D点云的统一分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云的统一分割对场景理解至关重要，但由于点云稀疏、标注有限以及在复杂环境中区分细粒度物体类别的难度，现有方法难以充分捕获语义与上下文信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述挑战，提升3D点云在语义、实例和全景分割任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用LLM生成文本描述和网络参考图像作为多模态查询，设计语义-视觉对比损失将点特征与查询对齐，并引入空间增强模块高效建模全局关系；整个过程在离线生成的多模态知识下进行闭集推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在语义、实例和全景分割任务上，VDG-Uni3DSeg实现了最先进的性能，验证了多模态知识融合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架提供了一种可扩展、实用的3D理解方案，为未来点云分割研究提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云的统一分割对于场景理解至关重要，但由于其稀疏结构、标注有限以及在复杂环境中区分细粒度物体类别的挑战，仍面临困难。现有方法往往难以捕获丰富的语义和上下文信息，主要原因是监督不足和缺乏多样化的多模态线索，导致类别和实例的区分效果不佳。为解决这些问题，我们提出了VDG-Uni3DSeg，一种新颖的框架，将预训练的视觉-语言模型（如CLIP）和大型语言模型（LLM）相结合，以提升3D分割性能。通过利用LLM生成的文本描述和来自互联网的参考图像，我们的方法引入了丰富的多模态线索，促进了细粒度类别和实例的分离。我们进一步设计了语义-视觉对比损失，将点特征与多模态查询对齐，并引入空间增强模块，以高效建模全局场景关系。在离线生成的多模态知识支持下，VDG-Uni3DSeg在语义、实例和全景分割任务中实现了最先进的结果，提供了一种可扩展且实用的3D理解方案。代码已公开在 https://github.com/Hanzy1996/VDG-Uni3DSeg。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现统一的3D点云分割，包括语义、实例和全景分割。由于点云稀疏、注释稀缺且难以区分细粒度类别，现有方法在复杂场景中表现不佳。统一分割对自动驾驶、机器人和AR/VR等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了OneFormer3D等统一分割框架，发现缺乏多模态信息。为此，他们利用预训练的CLIP和LLM生成文本描述与参考图像，构建离线多模态查询。通过语义-视觉对比损失和空间增强模块，提升点云特征与多模态查询的对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将LLM生成的类描述和互联网图像作为多模态查询，使用CLIP编码后与点云特征融合，生成统一的分割掩码。实现流程包括：① 3D骨干提取点特征；② 空间增强模块捕获全局关系；③ 生成描述和图像查询并编码；④ 通过多模态融合层和掩码解码器得到语义、实例和全景掩码；⑤ 对语义结果进行投票融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 离线生成的LLM描述和互联网图像作为多模态查询；② 语义-视觉对比损失对齐点云与多模态特征；③ 空间增强模块高效建模全局关系；④ 统一掩码解码器与投票融合实现三种分割任务。与以往需要实时图像-文本配对或额外模块的开放词汇方法不同，本文在闭集范式下实现高效、稳健的分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VDG-Uni3DSeg通过离线的LLM文本描述和互联网图像查询，结合CLIP编码和空间增强，统一实现语义、实例和全景3D点云分割，并在保持高效推理的同时取得了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.&lt;/p&gt;</description></item><item><guid>2507.06564v1</guid><title>SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments</title><link>http://arxiv.org/abs/2507.06564v1</link><author>Tianshun Li, Tianyi Huai, Zhen Li, Yichun Gao, Haoang Li, Xinhu Zheng</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SkyVLN 是一种将视觉与语言导航与非线性模型预测控制相结合的框架，旨在提升无人机在复杂城市环境中的自主导航能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 无人机因其机动性和适应性在各行业得到广泛应用，但传统导航方法在动态三维城市空间中面临精度和鲁棒性挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过利用大型语言模型解释自然语言指令和视觉观测，改进无人机在动态环境中的导航准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建多模态导航代理，配备细粒度空间口头化器和历史路径记忆机制，并集成非线性模型预测控制模块实现动态障碍物避让和轨迹跟踪。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在使用 AirSim 的高保真三维城市仿真环境中，SkyVLN 在新颖和未见环境中显著提升了导航成功率和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SkyVLN 通过结合语言理解与预测控制，能够在复杂城市环境中实现更准确、更鲁棒的无人机导航。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 无人机因其机动性和适应性在各行业得到广泛应用。本文提出了 SkyVLN，一种将视觉与语言导航（VLN）与非线性模型预测控制（NMPC）相结合的新框架，以提升无人机在复杂城市环境中的自主性。与传统导航方法不同，SkyVLN 利用大型语言模型（LLM）解释自然语言指令和视觉观测，使无人机能够在动态三维空间中以更高的准确性和鲁棒性导航。我们提出了一个多模态导航代理，配备细粒度空间口头化器和历史路径记忆机制。这些组件使无人机能够消除空间语境歧义、处理模糊指令，并在必要时回溯。该框架还集成了 NMPC 模块，用于动态障碍物避让，确保精确的轨迹跟踪和碰撞预防。为验证我们的方案，我们使用 AirSim 开发了一个高保真三维城市仿真环境，具有逼真的图像和动态城市元素。大量实验表明，SkyVLN 在新颖和未见环境中显著提高了导航成功率和效率。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在让无人机在复杂城市环境中通过视觉和自然语言指令实现自主导航，解决传统导航方法在动态障碍、GNSS失效和多维动作空间下的准确性与鲁棒性不足的问题。此类能力对于无人机在监视、物流、救援等实际应用中的安全、高效执行至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了大型语言模型（LLM）对指令的语义理解、视觉语言模型（VLM）对景物的检测以及非线性模型预测控制（NMPC）的轨迹规划，形成端到端的多模态决策框架。设计过程中借鉴了GroundingDINO、LMAR、WPO等现有视觉语言导航与控制技术，并在此基础上加入了高分辨率空间描述器和历史路径记忆。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视觉感知、语言推理与控制优化三者无缝集成。流程为：①前视摄像头捕获图像并用VLM检测地标；②LLM提取子目标并生成空间描述；③Wayfinding Prompt Optimization 将视觉信息转化为高分辨率文本并利用历史路径记忆补充上下文；④LLM运动生成器根据提示输出动作；⑤NMPC根据动作规划并执行轨迹，同时避障。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① SkyVLN框架将VLN与NMPC结合，实现自然语言驱动的三维导航；②细粒度空间描述器提供精确的地标定位；③历史路径记忆支持模糊指令下的回溯与重规划；④在AirSim中构建高保真城市模拟并验证性能。与以往仅关注视觉或语言导航、或仅使用传统控制的工作不同，SkyVLN在动态障碍、三维动作空间和指令歧义处理上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SkyVLN通过将大型语言模型的语义推理与视觉感知和非线性模型预测控制相结合，使无人机能够以自然语言指令在复杂城市环境中实现安全、高效的自主三维导航。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments.&lt;/p&gt;</description></item><item><guid>2507.06592v1</guid><title>Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning</title><link>http://arxiv.org/abs/2507.06592v1</link><author>Yang Chen, Yueqi Duan, Haowen Sun, Jiwen Lu, Yap-Peng Tan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种适用于点云3D语义分割的自适应边缘对比学习方法，能够根据每个点的模糊程度动态调整学习目标，从而在保持低模糊点准确性的同时容忍高模糊点的错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法采用统一惩罚目标，忽略了点级模糊性和过渡区域的特征差异，导致对高度模糊点的硬约束产生次优模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种将对比学习与模糊估计框架结合的自适应方法，以更合理地处理不同模糊程度的点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先提出 AMContrast3D，将对比学习嵌入模糊估计框架，根据点的模糊度设定自适应目标；随后改进为 AMContrast3D++，采用并行双分支训练，并加入模糊预测模块和掩码细化机制，使模糊嵌入更可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 S3DIS 和 ScanNet 两个室内场景数据集上实验表明，该方法显著提升了分割精度，并增强了模型鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应边缘对比学习结合模糊估计能够有效解决点云语义分割中的模糊问题，提供更稳健的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种用于点云3D语义分割的自适应边缘对比学习方法。大多数现有方法使用等惩罚目标，忽略了每个点的模糊性以及来自过渡区域的特征差异。然而，极度模糊的点即使对人类也难以区分，其人工标注标签也不够可靠，对这些点施加硬约束会导致次优模型。为了解决这一问题，我们首先设计了 AMContrast3D，它将对比学习嵌入模糊估计框架，并根据模糊程度为每个点制定自适应目标。结果，该方法促进了模型训练，确保低模糊点的正确性，同时允许高模糊点出现错误。模糊性是基于标签间位置差异来定义的，推理过程中的优化受限于假设所有未标注点均为无模糊，缺乏模糊意识。受到联合训练洞察的启发，我们进一步提出了 AMContrast3D++，将其与两个并行训练的分支集成，其中一个新颖的模糊预测模块同时学习来自生成嵌入的点模糊性。为此，我们设计了一种掩码细化机制，利用预测的模糊性使模糊嵌入更可靠，从而提升分割性能并增强鲁棒性。在 3D 室内场景数据集 S3DIS 和 ScanNet 上的实验结果证明了所提出方法的有效性。代码可在 https://github.com/YangChenApril/AMContrast3D 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注点云分割中点的模糊性，尤其是过渡区域的点难以区分。传统方法对所有点使用相同的训练难度，导致高模糊点被过度关注，低模糊点被忽视，从而影响整体分割精度。解决这一问题能提升室内场景等复杂环境下的分割性能和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有对比学习使用固定边界，忽略点级模糊性。借鉴 2D 任务中通过决策边距调节训练难度的做法，提出根据点的邻域标签分布估计模糊度，并将其映射为可调边距。随后将该边距嵌入对比损失中，并在分割网络中加入模糊预测分支，形成联合训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让每个点的对比学习边距随其模糊度自适应变化，从而在训练时对高模糊点放宽约束、对低模糊点加大约束。实现流程包括：①在编码阶段提取位置嵌入；②通过邻域标签分布计算每点的模糊度；③将模糊度映射为边距；④在解码阶段使用自适应边距对比损失正负样本；⑤在 AMContrast3D++ 中加入模糊预测分支和掩码细化机制，联合训练分割与模糊预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出基于邻域标签分布的模糊度估计与自适应边距生成；②将自适应边距嵌入对比学习，形成 AMContrast3D；③在 AMContrast3D++ 中加入轻量化模糊预测分支和掩码细化机制，实现推理时的模糊感知；④通过联合训练提升分割鲁棒性。与以往使用固定边距的对比学习或仅关注全局特征的分割方法不同，本文在点级别引入模糊感知并动态调节训练难度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种自适应边距对比学习框架，结合点级模糊度估计和联合模糊预测分支，显著提升 3D 点云分割的准确性与鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper proposes an adaptive margin contrastive learning method for 3D semantic segmentation on point clouds. Most existing methods use equally penalized objectives, which ignore the per-point ambiguities and less discriminated features stemming from transition regions. However, as highly ambiguous points may be indistinguishable even for humans, their manually annotated labels are less reliable, and hard constraints over these points would lead to sub-optimal models. To address this, we first design AMContrast3D, a method comprising contrastive learning into an ambiguity estimation framework, tailored to adaptive objectives for individual points based on ambiguity levels. As a result, our method promotes model training, which ensures the correctness of low-ambiguity points while allowing mistakes for high-ambiguity points. As ambiguities are formulated based on position discrepancies across labels, optimization during inference is constrained by the assumption that all unlabeled points are uniformly unambiguous, lacking ambiguity awareness. Inspired by the insight of joint training, we further propose AMContrast3D++ integrating with two branches trained in parallel, where a novel ambiguity prediction module concurrently learns point ambiguities from generated embeddings. To this end, we design a masked refinement mechanism that leverages predicted ambiguities to enable the ambiguous embeddings to be more reliable, thereby boosting segmentation performance and enhancing robustness. Experimental results on 3D indoor scene datasets, S3DIS and ScanNet, demonstrate the effectiveness of the proposed method. Code is available at https://github.com/YangChenApril/AMContrast3D.&lt;/p&gt;</description></item><item><guid>2507.11037v1</guid><title>A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion</title><link>http://arxiv.org/abs/2507.11037v1</link><author>Jie-Wen Li, Zi-Han Ye, Qingyuan Zhou, Jiayi Song, Ying He, Ben Fei, Wen-Ming Chen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了FootGait3D数据集，专注于步态过程中足踝部位的三维点云捕捉，为足踝运动学分析和临床评估提供高分辨率数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 足踝运动学分析对生物力学研究和临床评估至关重要，但在步态动态条件下获取准确的足踝表面几何数据因足部遮挡和视角限制而具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个多视角、高分辨率的足踝点云数据集，以支持足踝形状补全方法的评估和比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用自定义的五摄像头深度传感系统采集46名受试者的8403帧点云，每帧包含完整的5视角重建（真值）以及仅使用4、3、2视角得到的部分点云，形成不同遮挡级别的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该数据集为单模态和多模态形状补全网络提供了基准，验证了多视角信息对恢复完整足踝几何的重要性，并为多段足模型研究提供了实验平台。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FootGait3D为足踝运动学、生物力学、假肢设计和机器人等领域提供了高质量的三维模型数据，能够推动相关技术的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; FootGait3D数据集是一个多视角、高分辨率的足踝表面点云数据集，专门用于步态分析。与现有的全身或下肢运动数据集不同，FootGait3D聚焦于足踝区域的细致建模，提供更高粒度的运动数据。该数据集包含8403帧点云，来自46名受试者，使用自定义的五摄像头深度传感系统采集。每帧包括完整的5视角重建（作为真值）以及仅使用四、三、两视角得到的部分点云。此结构化的变化使得在不同遮挡水平和视角下对3D点云补全方法进行严格评估成为可能。该数据集旨在用于形状补全任务，方便对单模态（如PointTr、SnowflakeNet、Anchorformer）和多模态（如SVDFormer、PointSea、CSDN）补全网络进行基准测试，以从遮挡输入中恢复完整足部几何。FootGait3D具有显著潜力，可推动生物力学和多段足模型研究的发展，为临床步态分析、假肢设计和需要足部三维模型的机器人应用提供有价值的测试平台。数据集已在 https://huggingface.co/datasets/ljw285/FootGait3D 上公开。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在步态过程中获取完整、准确的足踝三维点云数据的难题。由于步态时足部被另一只脚遮挡以及摄像角度受限，传统方法难以得到完整表面信息，而足踝运动学分析对临床评估、假肢设计和运动科学至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出现有数据集缺乏真实的部分-完整点云配对，随后借鉴了先前的Markerless PFA系统和多摄像头深度传感技术，设计了一个五摄像头的多视角捕捉装置，并结合现有的深度相机与点云配准方法实现数据采集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多视角深度相机获取足踝完整表面，并通过选择不同摄像头子集生成对应的部分点云，从而得到完整-部分配对数据。实现流程包括：① 采集五摄像头深度图；② 校准并将各视角点云转换到全局坐标系；③ 合并并裁剪得到完整点云；④ 通过检测足部接触事件提取步态站立阶段；⑤ 通过去除另一只脚和背景噪声得到干净的完整点云；⑥ 选取不同摄像头组合生成部分点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 提供了大规模、真实动态的足踝点云数据集；② 生成了完整-部分配对样本，满足监督式点云补全训练需求；③ 采用多视角高分辨率深度相机，覆盖足踝全表面；④ 为足踝运动学和假肢设计提供了细粒度数据。与以往仅有合成数据或全身/下肢数据集不同，FootGait3D专注于足踝细节并提供真实的缺失模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FootGait3D提供了首个大规模、真实动态的多视角足踝点云数据集，包含完整-部分配对样本，为足踝运动学分析和三维补全算法的评估与发展奠定了基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The kinematics analysis of foot-ankle complex during gait is essential for advancing biomechanical research and clinical assessment. Collecting accurate surface geometry data from the foot and ankle during dynamic gait conditions is inherently challenging due to swing foot occlusions and viewing limitations. Thus, this paper introduces FootGait3D, a novel multi-view dataset of high-resolution ankle-foot surface point clouds captured during natural gait. Different from existing gait datasets that typically target whole-body or lower-limb motion, FootGait3D focuses specifically on the detailed modeling of the ankle-foot region, offering a finer granularity of motion data. To address this, FootGait3D consists of 8,403 point cloud frames collected from 46 subjects using a custom five-camera depth sensing system. Each frame includes a complete 5-view reconstruction of the foot and ankle (serving as ground truth) along with partial point clouds obtained from only four, three, or two views. This structured variation enables rigorous evaluation of 3D point cloud completion methods under varying occlusion levels and viewpoints. Our dataset is designed for shape completion tasks, facilitating the benchmarking of state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the challenge of recovering the full foot geometry from occluded inputs. FootGait3D has significant potential to advance research in biomechanics and multi-segment foot modeling, offering a valuable testbed for clinical gait analysis, prosthetic design, and robotics applications requiring detailed 3D models of the foot during motion. The dataset is now available at https://huggingface.co/datasets/ljw285/FootGait3D.&lt;/p&gt;</description></item><item><guid>2507.14485v1</guid><title>Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion</title><link>http://arxiv.org/abs/2507.14485v1</link><author>Hongye Hou, Liu Zhan, Yang Yang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种检索增强的点云补全框架，通过跨模态检索学习结构先验信息，利用结构共享特征编码器和层次特征融合生成器，实现了对稀疏点云和未见类别的高质量补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全是挑战性任务，尤其在残留点云缺乏典型结构特征时。现有跨模态学习方法引入实例图像帮助结构特征学习，但仍局限于特定输入类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将跨模态检索融入补全任务，学习相似参考样本的结构先验信息，以提升补全效果并增强生成能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计结构共享特征编码器（SSFE）提取跨模态特征并重建参考特征作为先验；使用双通道控制门增强相关结构特征并抑制无关信息；提出渐进检索增强生成器（PRAG）采用层次特征融合机制，从全局到局部整合参考先验与输入特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多数据集和真实场景评估中，方法在生成细粒度点云方面表现出色，并在处理稀疏数据和未见类别时具有良好的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 检索增强的点云补全框架通过跨模态检索和层次特征融合，有效提升了点云补全质量，并展现出对稀疏数据和新类别的强大适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于不完整点云完成整个三维结构是一项具有挑战性的任务，尤其当残余点云缺乏典型结构特征时。最近基于跨模态学习的方法尝试引入实例图像来辅助结构特征学习。然而，它们仍然聚焦于每个特定输入类别，限制了其生成能力。在本研究中，我们提出了一种新颖的检索增强点云补全框架。核心思路是将跨模态检索融入补全任务，以从相似参考样本中学习结构先验信息。具体而言，我们设计了一个结构共享特征编码器（SSFE），用于联合提取跨模态特征并重建参考特征作为先验。借助编码器中的双通道控制门，增强参考样本中的相关结构特征并抑制无关信息干扰。此外，我们提出了一个渐进检索增强生成器（PRAG），采用层次特征融合机制，从全局到局部整合参考先验信息与输入特征。通过在多个数据集和真实场景上的广泛评估，我们的方法在生成细粒度点云方面表现出其有效性，并在处理稀疏数据和未见类别时展现出良好的泛化能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决基于不完整点云恢复完整三维结构的问题，尤其是在残留点云缺乏典型结构特征时。该问题在自动驾驶、机器人感知和三维场景理解等实际应用中至关重要，因为完整的三维模型能显著提升后续任务的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人类大脑在修复未知结构时先参考已知相似结构的启发，提出将跨模态检索与点云完成结合。方法借鉴了跨模态学习、检索增强生成（RAG）以及现有点云完成框架（如Encoder‑Decoder、Transformer等），并结合CLIP进行检索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过跨模态检索得到相似的完整点云作为参考，并在编码阶段使用SSFE与SACG门控增强相关结构特征、抑制噪声；随后在解码阶段使用PRAG从全局到局部逐层融合参考先验与输入特征，最终生成细粒度完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）检索增强的点云完成框架；2）SSFE编码器配合SACG双门控机制，能够自适应提取参考中的相关结构；3）PRAG解码器采用层次化全局‑局部融合；4）去掉检索点云的绝对位置信息，提升对姿态变化的鲁棒性。相比以往仅利用单模态或简单融合的完成方法，该方案在细节恢复和对稀疏/未知类别的泛化上表现更优。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种检索增强的跨模态点云完成框架，利用双门控编码器和层次化解码器从相似参考中提取结构先验，实现了更高质量、更具泛化能力的三维重建。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Completing the whole 3D structure based on an incomplete point cloud is a challenging task, particularly when the residual point cloud lacks typical structural characteristics. Recent methods based on cross-modal learning attempt to introduce instance images to aid the structure feature learning. However, they still focus on each particular input class, limiting their generation abilities. In this work, we propose a novel retrieval-augmented point cloud completion framework. The core idea is to incorporate cross-modal retrieval into completion task to learn structural prior information from similar reference samples. Specifically, we design a Structural Shared Feature Encoder (SSFE) to jointly extract cross-modal features and reconstruct reference features as priors. Benefiting from a dual-channel control gate in the encoder, relevant structural features in the reference sample are enhanced and irrelevant information interference is suppressed. In addition, we propose a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical feature fusion mechanism to integrate reference prior information with input features from global to local. Through extensive evaluations on multiple datasets and real-world scenes, our method shows its effectiveness in generating fine-grained point clouds, as well as its generalization capability in handling sparse data and unseen categories.&lt;/p&gt;</description></item><item><guid>2507.16743v1</guid><title>Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption</title><link>http://arxiv.org/abs/2507.16743v1</link><author>Keneni W. Tesema, Lyndon Hill, Mark W. Jones, Gary K. L. Tam</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究点云补全与去噪，提出一种新框架DWCNet，能够在噪声和遮挡严重的部分点云上实现高质量补全，并在多种数据集上取得最优表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全在自动驾驶、增强现实和机器人等领域至关重要，但真实环境中的噪声和遮挡使得获取干净完整的点云变得困难。现有网络多在合成数据上训练，难以处理真实世界的退化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在多重退化影响下的高度损坏部分点云的补全与去噪问题，并评估现有方法在此场景下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先构建了Corrupted Point Cloud Completion Dataset (CPCCD) 用于评估鲁棒性；随后提出DWCNet框架，包含噪声管理模块NMM，利用对比学习和自注意力机制抑制噪声并建模结构关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DWCNet在干净与受损、合成与真实数据集上均实现了最先进的性能，证明了噪声管理模块的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入专门的噪声管理模块，点云补全网络可以在复杂退化环境中保持高精度，DWCNet为鲁棒点云补全提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全对于自动驾驶、增强现实和机器人等3D计算机视觉任务至关重要。然而，在真实环境中获取干净完整的点云因噪声和遮挡而具有挑战性。因此，大多数现有的补全网络在合成数据上训练后，难以处理真实世界的退化。在本研究中，我们解决了在多重退化影响下高度损坏的部分点云的补全和去噪问题。为评估鲁棒性，我们引入了Corrupted Point Cloud Completion Dataset (CPCCD)，该数据集突出了当前方法在多样化退化下的局限性。在此基础上，我们提出了DWCNet（Denoising-While-Completing Network），该补全框架通过噪声管理模块（NMM）利用对比学习和自注意力来抑制噪声并建模结构关系。DWCNet在干净和受损、合成和真实数据集上均实现了最先进的性能。数据集和代码将公开发布。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在完成并去噪受多种噪声和遮挡严重破坏的部分点云。现实中，激光雷达和深度相机捕获的点云往往包含噪声、外部干扰和遮挡，导致现有仅在干净合成数据上训练的完成网络性能大幅下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先发现缺乏针对点云完成的鲁棒性基准，随后构建了 CPCCD 数据集，模拟真实环境中的多种噪声。方法借鉴了现有的点云完成网络、对比学习、以及多头自注意力和多尺度卷积等技术，整合进一个噪声管理模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将去噪与完成统一到同一网络中。流程为：输入受损点云 → 提取特征 → 噪声管理模块将特征分为干净与噪声两类 → 对干净特征进行对比学习过滤噪声 → 多头自注意力捕捉结构关系并通过多尺度卷积处理不同尺度噪声 → 输出去噪且完成的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) CPCCD 作为首个针对点云完成的鲁棒性基准；2) DWCNet 通过噪声管理模块实现去噪与完成的协同；3) 采用对比学习和自注意力来抑制多种噪声。与以往仅处理高斯噪声或在完成后单独去噪的工作不同，DWCNet 能同时应对多种真实噪声并在合成与真实数据上取得最优表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一个同时去噪和完成受多种噪声破坏的点云的鲁棒框架，并通过新构建的 CPCCD 基准验证其优越性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is crucial for 3D computer vision tasks in autonomous driving, augmented reality, and robotics. However, obtaining clean and complete point clouds from real-world environments is challenging due to noise and occlusions. Consequently, most existing completion networks -- trained on synthetic data -- struggle with real-world degradations. In this work, we tackle the problem of completing and denoising highly corrupted partial point clouds affected by multiple simultaneous degradations. To benchmark robustness, we introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which highlights the limitations of current methods under diverse corruptions. Building on these insights, we propose DWCNet (Denoising-While-Completing Network), a completion framework enhanced with a Noise Management Module (NMM) that leverages contrastive learning and self-attention to suppress noise and model structural relationships. DWCNet achieves state-of-the-art performance on both clean and corrupted, synthetic and real-world datasets. The dataset and code will be publicly available at https://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions&lt;/p&gt;</description></item><item><guid>2507.22020v1</guid><title>XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation</title><link>http://arxiv.org/abs/2507.22020v1</link><author>Raju Ningappa Mulawade, Christoph Garth, Alexander Wiebel</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于分割的可解释人工智能方法，用于点云分类网络。核心是新颖的点位移机制，用于在点云中引入扰动，从而生成易于人类理解的显著性图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着人工智能在关键领域的广泛应用，理解其决策过程变得尤为重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 生成可被人类直观解释的显著性图，以帮助分析点云分类算法的工作原理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用点云分割模型生成分段，利用点位移机制对输入点云进行扰动，进而产生显著性图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 所提出的分段更具可解释性，生成的显著性图比传统聚类方法更有意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法能够提供更易解释的显著性图，提升对点云分类模型的理解与决策支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种新颖的基于分割的可解释人工智能（XAI）方法，用于处理点云分类的神经网络。作为该方法的一个构建块，我们提出了一种新颖的点位移机制，以在点云数据中引入扰动。近年来，人工智能呈指数级增长。因此，在将人工智能算法应用于关键领域时，了解其决策过程变得重要。我们的工作聚焦于解释分类点云数据的人工智能算法。解释人工智能算法的方法的一个重要方面是它们能够生成易于人类理解的解释。这使得人们能够更好地分析人工智能算法，并根据该分析做出适当的决策。因此，在本工作中，我们打算生成易于人类解释的有意义的解释。我们考虑的点云数据代表了汽车、吉他和笔记本电脑等三维物体。我们利用点云分割模型为分类模型的工作生成解释。分段用于在输入点云数据中引入扰动并生成显著性图。扰动是使用本文提出的新颖点位移机制引入的，该机制确保位移后的点不再影响分类算法的输出。与以前的方法相比，我们的方法使用的分段是有意义的，即人类可以轻易解释分段的含义。因此，我们的方法相对于其他方法的优点在于能够生成更有意义的显著性图。我们将我们的方法与使用经典聚类算法生成解释进行比较。我们还分析了使用我们的方法为示例输入生成的显著性图，以展示该方法在生成有意义解释方面的有用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在为点云分类模型提供可解释性，解决其黑盒特性导致的决策不透明问题。点云数据在自动驾驶、机器人和工业检测等高风险领域被广泛使用，缺乏可解释性会阻碍其安全可靠部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的基于扰动的XAI方法（如SHAP、LIME、PointMask等）的基础上，发现这些方法往往使用无意义的聚类或单点扰动，难以产生易于人类理解的解释。于是他们引入了语义分割模型和新的点位移机制，构建了一个模型无关、可解释性更强的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用语义分割将点云划分为人类可识别的部件，再通过点位移扰动这些部件，观察分类器输出的变化来计算重要性。实现流程包括：① 用分类器预测原始点云类别；② 选取对应的分割模型对点云进行语义分割；③ 对分割得到的每个片段使用点位移机制产生扰动；④ 通过比较扰动前后分类器输出，生成每个片段的显著性映射。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 采用语义分割得到的有意义片段作为扰动单元；② 提出了新的点位移机制，使扰动后的点不再影响分类结果；③ 设计了两种扰动方式（保留/移除）以提供不同视角的解释；④ 对比聚类方法，证明语义分割能生成更易解释的显著性图。与以往使用无意义聚类或单点扰动的方法不同，本文的方案更贴合人类认知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种基于语义分割和点位移的扰动式XAI框架，能够为点云分类模型生成易于人类理解的显著性图，显著提升了解释的可读性和实用性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose a novel segmentation-based explainable artificial intelligence (XAI) method for neural networks working on point cloud classification. As one building block of this method, we propose a novel point-shifting mechanism to introduce perturbations in point cloud data. Recently, AI has seen an exponential growth. Hence, it is important to understand the decision-making process of AI algorithms when they are applied in critical areas. Our work focuses on explaining AI algorithms that classify point cloud data. An important aspect of the methods used for explaining AI algorithms is their ability to produce explanations that are easy for humans to understand. This allows them to analyze the AI algorithms better and make appropriate decisions based on that analysis. Therefore, in this work, we intend to generate meaningful explanations that can be easily interpreted by humans. The point cloud data we consider represents 3D objects such as cars, guitars, and laptops. We make use of point cloud segmentation models to generate explanations for the working of classification models. The segments are used to introduce perturbations into the input point cloud data and generate saliency maps. The perturbations are introduced using the novel point-shifting mechanism proposed in this work which ensures that the shifted points no longer influence the output of the classification algorithm. In contrast to previous methods, the segments used by our method are meaningful, i.e. humans can easily interpret the meaning of the segments. Thus, the benefit of our method over other methods is its ability to produce more meaningful saliency maps. We compare our method with the use of classical clustering algorithms to generate explanations. We also analyze the saliency maps generated for example inputs using our method to demonstrate the usefulness of the method in generating meaningful explanations.&lt;/p&gt;</description></item><item><guid>2507.22668v1</guid><title>Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation</title><link>http://arxiv.org/abs/2507.22668v1</link><author>Hongbin Lin, Yifan Jiang, Juangui Xu, Jesse Jiaxi Xu, Yi Lu, Zhengyu Hu, Ying-Cong Chen, Hao Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于图引导的双层约束数据增强框架，用于生成逼真的3D场景，从而提升点云分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法主要采用局部变换或语义重组的数据增强，缺乏对场景全局结构依赖的考虑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决缺乏全局结构约束的问题，提升增强数据的真实性和多样性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 从真实数据学习对象关系统计，构建引导图；局部约束保证几何可行性和语义一致性；全局约束通过与引导图对齐保持场景拓扑结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在室内外数据集上实验表明，该框架生成多样且高质量的增强场景，显著提升多种模型的点云分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 双层约束图引导的数据增强能有效提升3D点云分割效果，具有广泛应用前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云分割旨在为场景中的每个点分配语义标签，以实现细粒度空间理解。现有方法通常采用数据增强来减轻大规模标注的负担。然而，大多数增强策略仅关注局部变换或语义重组，缺乏对场景内全局结构依赖的考虑。为解决这一限制，我们提出了一个基于图引导的双层约束数据增强框架，用于生成逼真的3D场景。我们的方法从真实数据中学习对象关系统计，构建用于场景生成的引导图。局部层约束强制几何可行性和对象间语义一致性，而全局层约束通过将生成的布局与引导图对齐来保持场景的拓扑结构。在室内和室外数据集上进行的大量实验表明，我们的框架生成多样且高质量的增强场景，在各种模型上均实现了持续的点云分割性能提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 3D 点云分割中缺乏真实、结构化数据增强的问题。由于标注成本高，现有方法往往只能通过局部几何变换或简单语义插入来扩充数据，导致生成的场景缺乏全局结构一致性，影响模型在真实环境中的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统增强仅关注局部变换，忽略了场景中的全局拓扑关系，随后借鉴场景图、图神经网络以及基于统计的关系建模等已有研究，提出通过构造对象关系图（ORG）并使用 Jensen‑Shannon 散度匹配真实分布，来引导场景生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将场景拆分为背景与前景对象，构建描述对象共现与空间关系的图，然后在图的引导下生成新场景，同时通过局部几何约束（碰撞、功能关系）和全局图一致性损失（GGCL）保证生成结果既几何合理又结构一致。实现流程包括：场景分解 → 构造 ORG → 采样节点并生成条件图 → 位置与方向调整（局部约束） → 通过 GNN 对齐生成图与指导图（全局约束） → 输出增强点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 采用双层约束（局部几何 + 全局拓扑）实现更真实的场景合成；2) 使用对象关系图和 Jensen‑Shannon 散度保证采样分布与真实数据一致；3) 引入 Graph Global Constraint Loss 通过 GNN 对齐生成图与指导图，确保全局结构。与以往仅做局部变换或简单语义插入的增强方法不同，该框架同时考虑了全局拓扑和局部语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种基于场景图的双层约束增强框架，通过学习对象共现与空间关系，生成既几何合理又结构一致的 3D 点云场景，从而显著提升点云分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D point cloud segmentation aims to assign semantic labels to individual points in a scene for fine-grained spatial understanding. Existing methods typically adopt data augmentation to alleviate the burden of large-scale annotation. However, most augmentation strategies only focus on local transformations or semantic recomposition, lacking the consideration of global structural dependencies within scenes. To address this limitation, we propose a graph-guided data augmentation framework with dual-level constraints for realistic 3D scene synthesis. Our method learns object relationship statistics from real-world data to construct guiding graphs for scene generation. Local-level constraints enforce geometric plausibility and semantic consistency between objects, while global-level constraints maintain the topological structure of the scene by aligning the generated layout with the guiding graph. Extensive experiments on indoor and outdoor datasets demonstrate that our framework generates diverse and high-quality augmented scenes, leading to consistent improvements in point cloud segmentation performance across various models.&lt;/p&gt;</description></item><item><guid>2508.00259v1</guid><title>PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting</title><link>http://arxiv.org/abs/2508.00259v1</link><author>Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 提出 PointGauss 框架，实现实时多目标分割；2. 通过点云驱动的高效解码器在一分钟内生成 3D 实例掩码；3. GPU 加速的 2D 掩码渲染保证多视角一致性；4. 在多视角 mIoU 上比现有方法提升 1.89% 至 31.78%；5. 同时保持更高的计算效率；6. 引入 DesktopObjects-360 数据集，解决单目标、评估不一致、规模小等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法在 Gaussian Splatting 表示中进行多目标分割时，往往需要较长的初始化时间，且多视角一致性不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 实现一种高效、实时的多目标分割框架，兼顾速度与准确性，并提供更完善的评估数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 基于点云的 Gaussian 原语解码器，可在一分钟内生成 3D 实例掩码；2) GPU 加速的 2D 掩码渲染系统，确保多视角一致性；3) 直接通过点云分割驱动的管线解析 Gaussian 原语，实现高效 3D 分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，PointGauss 在多视角 mIoU 上比之前的最先进方法提升 1.89% 至 31.78%，同时保持更优的计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PointGauss 成功实现了实时、多目标的 Gaussian Splatting 分割，并通过 DesktopObjects-360 数据集为未来研究提供了更全面的评估平台。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 PointGauss，一种新颖的点云引导框架，用于在高斯喷射表示中实现实时多目标分割。与现有方法相比，后者在初始化时间长且多视角一致性有限的情况下，PointGauss 通过点云分割驱动的管线直接解析高斯原语，实现了高效的 3D 分割。其核心创新体现在两个方面：一是基于点云的高斯原语解码器，能够在一分钟内生成 3D 实例掩码；二是 GPU 加速的 2D 掩码渲染系统，确保多视角一致性。大量实验表明，与之前的最先进方法相比，PointGauss 在多视角 mIoU 上提升了 1.89% 至 31.78%，同时保持了卓越的计算效率。为了解决当前基准测试的局限性（单目标聚焦、3D 评估不一致、小规模、部分覆盖），我们提出了 DesktopObjects-360，一个用于高斯辐射场 3D 分割的新型综合数据集，包含：复杂的多目标场景、全局一致的 2D 注释、超过 27000 张 2D 掩码的大规模训练数据、完整的 360° 覆盖以及 3D 评估掩码。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现对 Gaussian Splatting 场景的实时多物体分割，解决现有方法在初始化慢、跨视角一致性差以及对 3D 结构利用不足等问题。该问题在增强现实、机器人导航等需要快速、准确 3D 语义理解的应用中至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有基于 2D Mask 或 SAM 的 3DGS 分割方法的局限，随后借鉴点云分割技术，提出直接在 Gaussian 原语上进行点云分割的思路。方法中融合了 Prompt Encoder、Gaussian Decoder 和 GPU 加速的 Splatting Projection，构成了端到端的分割流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 Gaussian 原语视为点云，利用点云分割网络生成实例标签，再通过可微分渲染将标签投影到任意视角。实现流程包括：①构建 Gaussian 模型；②将用户点击转化为空间特征并与 Gaussian 属性融合；③使用 PointTransformerV3 对融合后的点云进行语义/实例分割；④通过 GPU 加速的 splatting 渲染得到一致的 2D 掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①点云引导的 Gaussian 原语解码器，可在一分钟内生成 3D 实例掩码；②GPU 加速的 2D 掩码渲染实现多视角一致性；③提出 DesktopObjects‑360 大规模、360° 覆盖的评测基准。与以往依赖 2D 迁移或多阶段对齐的工作不同，PointGauss 直接利用 3D 结构，简化了网络架构并显著提升了速度和一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointGauss 通过在 Gaussian 原语上直接进行点云分割，实现了实时、多视角一致的 3D 多物体分割，并在新基准 DesktopObjects‑360 上显著优于现有方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce PointGauss, a novel point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations. Unlike existing methods that suffer from prolonged initialization and limited multi-view consistency, our approach achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline. The key innovation lies in two aspects: (1) a point cloud-based Gaussian primitive decoder that generates 3D instance masks within 1 minute, and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view consistency. Extensive experiments demonstrate significant improvements over previous state-of-the-art methods, achieving performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency. To address the limitations of current benchmarks (single-object focus, inconsistent 3D evaluation, small scale, and partial coverage), we present DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in radiance fields, featuring: (1) complex multi-object scenes, (2) globally consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D masks), (4) full 360° coverage, and (5) 3D evaluation masks.&lt;/p&gt;</description></item><item><guid>2508.04705v1</guid><title>Occupancy Learning with Spatiotemporal Memory</title><link>http://arxiv.org/abs/2508.04705v1</link><author>Ziyang Leng, Jiawei Yang, Wenlong Yi, Bolei Zhou</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为ST-Occ的场景级占位符表示学习框架，旨在高效聚合多帧输入的3D占位符信息，提升时间一致性和预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D占位符在自动驾驶中被视为细粒度环境建模的有前景的感知表示，但在多帧时间上聚合其信息面临高处理成本、体素不确定性和动态变化等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决多帧3D占位符聚合效率低、时间不一致性高的问题，构建能够捕捉时空特征并保持时间一致性的表示学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ST-Occ包含两大核心设计：1）时空记忆模块，用场景级表示高效存储历史信息；2）记忆注意力模块，利用不确定性和动态感知模型将当前占位符表示与时空记忆关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明ST-Occ在3D占位符预测任务中显著提升时空表示，mIoU提升3分，时间不一致性降低29%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过利用多帧输入的时间依赖性，ST-Occ在占位符预测上优于现有方法，证明了时空记忆与注意力机制的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在高效地将多帧输入的 3D 占据信息进行时空融合，解决占据体素在时间维度上的高计算成本、噪声不确定性和动态位移问题。3D 占据是自动驾驶感知的细粒度环境表示，能够更好地捕捉周围物体的几何与语义信息，对安全与决策至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有基于队列、递归或堆叠的时空融合方法在 3D 占据中的内存与计算瓶颈，并指出它们未充分考虑不确定性与动态位移。随后借鉴 BEV 时空建模（如 BEVFormer、BEVDet）和占据预测的相关工作（OccNet、FB-OCC、FlashOcc 等），提出在场景中心坐标系下构建统一的时空记忆，并通过记忆注意力结合不确定性与流信息实现融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用场景级的时空记忆来存储历史占据特征及其不确定性和流信息，并通过记忆注意力将当前帧的占据表示与记忆中的历史信息进行条件融合。实现流程为：多视角图像 → 视角编码器得到 ego‑centered 占据 V_t → 记忆注意力利用记忆 M_t 采样得到 H_t 并与 V_t 融合得到 V~_t → 用 V~_t 及其属性更新记忆 M_{t+1} → 输出占据预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 统一时空建模范式：在场景中心坐标系下仅使用一个记忆而非 k 个帧的队列，显著降低内存与计算开销；2) 时空记忆包含历史类别激活、方差与占据流，提供不确定性与动态信息；3) 记忆注意力将当前占据与记忆条件化，提升时空一致性。与以往仅在 ego‑centered 维度堆叠或递归融合的做法不同，ST‑Occ 在效率、鲁棒性和精度上均取得显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ST‑Occ 通过场景级时空记忆与记忆注意力，实现高效、鲁棒的多帧 3D 占据融合，显著提升精度与时空一致性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.&lt;/p&gt;</description></item><item><guid>2508.05531v1</guid><title>Point cloud segmentation for 3D Clothed Human Layering</title><link>http://arxiv.org/abs/2508.05531v1</link><author>Davide Garavaso, Federico Masi, Pietro Musoni, Umberto Castellani</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的3D点云分割范式——穿着人类分层，允许同一点同时属于多层，从而实现对底层身体和被遮挡服装区域的估计，并通过合成数据集和多种神经网络设置验证了该方法在合成与真实扫描数据上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D服装建模与仿真在时尚、娱乐和动画等领域至关重要，但实现高质量结果因穿着体形的巨大变异和逼真皱纹的生成而具有挑战性；3D扫描采集能提供更高精度，却缺乏可推断的语义信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建可靠的语义重建管线，解决穿着体形建模中形状分割的不足，提出能够处理多层重叠的分割方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出每个3D点可同时关联多层的分割范式；创建合成数据集提供服装层的真值；设计并评估多种神经网络设置，涵盖粗粒度和细粒度的每层服装识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，在合成和真实扫描数据集上引入适当的服装域分割策略能够显著提升分割与重建效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 穿着人类分层范式及其配套数据集为3D服装建模提供了有效的语义分割与重建方案，神经网络设置在该领域具有显著优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 服装建模与仿真对于角色创建在时尚、娱乐和动画等领域至关重要。实现高质量结果具有挑战性，主要因为穿着体形的巨大变异，尤其是逼真皱纹的生成。3D 扫描采集能更准确地表示现实世界对象，但缺乏可通过可靠语义重建管线推断的语义信息。为此，形状分割在识别语义形状部件方面起着关键作用。然而，目前的 3D 形状分割方法主要用于场景理解和解释，只有少数工作关注建模。在穿着体形建模的背景下，分割是完全语义形状部件重建的前置步骤，即底层身体和涉及的服装。这些部件代表多层并具有强重叠，与标准分割方法提供的互斥集合形成对比。本研究提出一种新的 3D 点云分割范式，每个 3D 点可同时关联到不同层。通过这种方式，我们可以估计底层身体部件和未被上层服装遮挡的服装区域。我们将此分割范式命名为“穿着人类分层”。我们创建了一个新的合成数据集，模拟非常逼真的 3D 扫描，并提供涉及服装层的真值。我们提出并评估了不同的神经网络设置，以处理 3D 服装分层。我们考虑了粗粒度和细粒度的每层服装识别。实验表明，在合成和真实扫描数据集上引入适当的服装域分割策略具有优势。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从3D扫描得到的穿着人体点云中同时识别可见和被遮挡的多层服装以及底层身体的分割问题。该问题对服装建模、虚拟试衣和动画等领域至关重要，因为它直接影响到服装的真实感和后续的几何重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的点云分割网络（如PointNet++、DGCNN、Point Transformer）基础上，提出将每个点的标签从单一类别扩展为多维向量，以表示多层服装。为实现这一思路，他们构建了一个包含真实扫描噪声的合成数据集，并参考了之前的服装分割与点云分割研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是“clothed human layering”，即为每个点预测一个包含身体和所有穿着服装层的多维标签。实现流程包括：①使用结构光扫描仿真器生成带噪声的点云；②利用原始服装网格投影得到多层标注；③训练改进的点云分割网络，使其输出多维标签；④在合成和真实扫描数据上评估粗细分割效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 提出了多层分割范式，允许单点同时属于多个服装层；2) 公开了一个包含多层标注的高质量合成扫描数据集；3) 将现有点云分割网络改造为向量化输出。与以往只产生互斥分割的工作不同，该方法能够恢复被遮挡的服装部分和底层身体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种多层点云分割框架和对应的合成数据集，使得从3D扫描中同时识别可见与被遮挡的服装层及底层身体成为可能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Cloth modeling and simulation is essential for avatars creation in several fields, such as fashion, entertainment, and animation. Achieving high-quality results is challenging due to the large variability of clothed body especially in the generation of realistic wrinkles. 3D scan acquisitions provide more accuracy in the representation of real-world objects but lack semantic information that can be inferred with a reliable semantic reconstruction pipeline. To this aim, shape segmentation plays a crucial role in identifying the semantic shape parts. However, current 3D shape segmentation methods are designed for scene understanding and interpretation and only few work is devoted to modeling. In the context of clothed body modeling the segmentation is a preliminary step for fully semantic shape parts reconstruction namely the underlying body and the involved garments. These parts represent several layers with strong overlap in contrast with standard segmentation methods that provide disjoint sets. In this work we propose a new 3D point cloud segmentation paradigm where each 3D point can be simultaneously associated to different layers. In this fashion we can estimate the underlying body parts and the unseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer above. We name this segmentation paradigm clothed human layering. We create a new synthetic dataset that simulates very realistic 3D scans with the ground truth of the involved clothing layers. We propose and evaluate different neural network settings to deal with 3D clothing layering. We considered both coarse and fine grained per-layer garment identification. Our experiments demonstrates the benefit in introducing proper strategies for the segmentation on the garment domain on both the synthetic and real-world scan datasets.&lt;/p&gt;</description></item><item><guid>2508.19909v1</guid><title>Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</title><link>http://arxiv.org/abs/2508.19909v1</link><author>Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用二维基础模型生成的分割掩码来最大化稀疏三维标注的利用率，并通过几何对应将二维掩码传播到三维空间，从而显著扩充可用标签并提升弱监督三维语义分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 三维点云数据难以大规模标注，现有方法多仅关注三维域，且对稀疏标注或伪标签的利用不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在有限的三维标注条件下，充分利用二维基础模型的强大分割能力，提升三维弱监督分割效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①使用二维基础模型生成分割掩码；②通过建立三维场景与二维视图的几何对应，将二维掩码投射到三维空间；③将稀疏三维标注扩展到掩码覆盖区域；④对三维点云做增强后应用置信度与不确定性一致性正则化，筛选可靠伪标签并进一步扩散到三维掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该策略显著增加了可用标签数量，弥补了三维标注不足，并通过一致性正则化提升了模型的鲁棒性，最终提升了三维弱监督分割的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将二维基础模型与几何对应相结合，可有效解决三维标注稀缺问题，显著提升弱监督三维语义分割效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 当前针对三维语义分割的方法提出使用有限标注训练模型，以解决标注大型、形状不规则且无序的三维点云数据的难题。它们通常仅关注三维域，未能利用二维与三维数据的互补性。此外，一些方法扩展原始标签或生成伪标签来指导训练，但往往未能充分利用这些标签或解决其中的噪声。与此同时，全面且可适应的基础模型的出现为二维数据分割提供了有效解决方案。借助这一进展，我们提出了一种新方法，通过结合二维基础模型生成的分割掩码，最大化稀疏可用三维标注的利用率。我们进一步通过建立三维场景与二维视图之间的几何对应，将二维分割掩码传播到三维空间。我们将高度稀疏的标注扩展到三维掩码所划定的区域，从而大幅增加可用标签池。此外，我们在三维点云的增强上应用基于置信度和不确定性的“一致性正则化”，并选择可靠的伪标签，再将其进一步传播到三维掩码以生成更多标签。这一创新策略弥合了有限三维标注与二维基础模型强大能力之间的差距，最终提升了三维弱监督分割的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在3D点云分割中，如何在仅有稀疏标注的情况下获得高质量分割结果。由于3D点云数据难以大规模标注，弱监督方法可以显著降低人工成本。通过有效利用有限标注并结合2D图像信息，提升分割性能具有重要的实际和科研意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了2D基础模型（如Semantic‑SAM）在图像分割上的强大能力，并结合已有的弱监督3D方法（如PointMatch、RAC‑Net、OTOC、ActiveST）和噪声鲁棒学习技术。思路是先用2D模型生成分割掩码，再将其投影到3D空间，扩展稀疏标注，并通过一致性正则化筛选可靠伪标签，最终使用噪声鲁棒损失进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型生成的分割掩码来丰富3D点云的标签信息。实现流程包括：1）用Semantic‑SAM生成每个视角的2D掩码；2）将掩码投影到3D空间并融合多视角掩码；3）将稀疏3D标注扩展到掩码区域；4）通过一致性正则化得到可靠伪标签并传播到3D掩码；5）使用噪声鲁棒的归一化损失训练模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①将2D基础模型掩码投影到3D并完整融合多视角信息；②利用一致性正则化筛选可靠伪标签并在3D掩码上扩展；③采用噪声鲁棒归一化损失处理投影掩码中的噪声。与之前工作相比，它充分利用了2D掩码在3D空间的潜力，而非仅在2D平面使用，并通过伪标签扩展显著提升了可用标签量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种弱监督3D点云分割框架，利用2D基础模型掩码投影到3D、扩展稀疏标注并通过一致性正则化筛选伪标签，最终使用噪声鲁棒损失实现了状态‑最优的分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.&lt;/p&gt;</description></item><item><guid>2508.20135v1</guid><title>Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads</title><link>http://arxiv.org/abs/2508.20135v1</link><author>Andrew Yarovoi, Christopher R. Valenta</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了一种数据高效的点云分割管线和训练框架，能够在仅有少量标注数据的情况下，对未改造道路及七类其他目标实现鲁棒分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在低数据场景下，3D语义分割的泛化能力差，传统方法需要大量标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过多数据集预训练和轻量化微调，提升在目标域的分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用两阶段训练：先用投影卷积网络在公共城市数据集与少量域内数据上预训练；随后仅在域内数据上微调轻量预测头；同时引入点提示训练、Manifold Mixup 正则化以及直方图归一化环境增强。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仅使用50个标注点云，所提方法将平均交并比从33.5%提升至51.8%，整体准确率从85.5%提升至90.8%；表明跨数据集预训练是提升泛化的关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在低数据、挑战性环境下提供了实用且鲁棒的3D语义分割方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在本案例研究中，我们提出了一种数据高效的点云分割管线和训练框架，用于对未改造道路和另外七个类别进行鲁棒分割。我们的方法采用两阶段训练框架：首先，使用基于投影的卷积神经网络在公共城市数据集和少量精心策划的域内数据上进行预训练；然后，使用轻量级预测头仅在域内数据上进行微调。在此过程中，我们探讨了将点提示训练应用于批归一化层以及在管线中使用流形混合作为正则化的效果。我们还探讨了引入直方图归一化环境以进一步提升性能的影响。仅使用目标域的50个标注点云，我们展示了所提出的训练方法将平均交并比从33.5%提升至51.8%，整体准确率从85.5%提升至90.8%，与在域内数据上进行简单训练相比。关键的是，我们的结果表明，在多个数据集上进行预训练是提升泛化并在有限域内监督下实现鲁棒分割的关键。本研究总体上展示了在挑战性、低数据场景下实现鲁棒3D语义分割的实用框架。我们的代码可在 https://github.com/andrewyarovoi/MD-FRNet 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在稀缺标注数据下，对乡村、碎石、森林小径等非改良道路环境进行高质量点云语义分割的问题。由于手工标注耗时且昂贵，传统方法需要成千上万的扫描，而实际应用往往只能获得几十个扫描，缺乏数据的情况下仍需实现可靠的道路识别，对自动驾驶、基础设施检测等领域具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将任务视为少样本学习（Few‑Shot Learning）问题，借鉴了图像领域的迁移学习和度量学习思想，并结合点云专属技术如Point Prompt Training（PPT）和Manifold Mixup（MM）。他们先在大规模公共数据集上预训练特征提取器，再用少量目标域数据微调轻量化分类头，并通过数据增强、环境归一化等手段提升泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是两阶段训练：①预训练阶段，用FRNet等投影卷积网络在Semantic KITTI、Waymo等多域数据上学习通用特征；②微调阶段，冻结特征提取器，仅训练一个小型多层感知机（MLP）作为预测头，仅使用目标域的50个标注扫描。整个流程包括数据统一格式、增强（强度、环境、直方图归一化、旋转）、PPT应用于批归一化、MM正则化以及最终评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出针对低数据点云分割的实用两阶段管线；②首次将PPT迁移到卷积网络而非仅限Transformer；③在分割头中使用MM正则化；④利用直方图归一化的环境信息提升性能；⑤在仅50扫描的条件下实现显著IoU提升。与以往仅关注图像或单物体分类的少样本方法不同，该工作针对大规模场景分割，并在投影CNN框架中实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 论文提出一种两阶段、数据高效的点云语义分割管线，结合多域预训练、PPT、MM和环境归一化，在仅50个标注扫描的情况下显著提升乡村道路环境的分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this case study, we present a data-efficient point cloud segmentation pipeline and training framework for robust segmentation of unimproved roads and seven other classes. Our method employs a two-stage training framework: first, a projection-based convolutional neural network is pre-trained on a mixture of public urban datasets and a small, curated in-domain dataset; then, a lightweight prediction head is fine-tuned exclusively on in-domain data. Along the way, we explore the application of Point Prompt Training to batch normalization layers and the effects of Manifold Mixup as a regularizer within our pipeline. We also explore the effects of incorporating histogram-normalized ambients to further boost performance. Using only 50 labeled point clouds from our target domain, we show that our proposed training approach improves mean Intersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5% to 90.8%, when compared to naive training on the in-domain data. Crucially, our results demonstrate that pre-training across multiple datasets is key to improving generalization and enabling robust segmentation under limited in-domain supervision. Overall, this study demonstrates a practical framework for robust 3D semantic segmentation in challenging, low-data scenarios. Our code is available at: https://github.com/andrewyarovoi/MD-FRNet.&lt;/p&gt;</description></item><item><guid>2509.08280v1</guid><title>Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration</title><link>http://arxiv.org/abs/2509.08280v1</link><author>Hyeonseok Kim, Byeongkeun Kang, Yeejin Lee</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种新的方法E3DPC-GZSL，用于3D点云的泛化零样本语义分割，解决了模型对已见类别过度自信的偏差问题，并在ScanNet v2和S3DIS数据集上取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在3D点云语义分割中，模型往往倾向于已训练过的类别，导致对未见类别的预测不准确，尤其在训练数据规模较小的3D任务中更为突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过改进不确定性估计和训练策略，降低对已见类别的过度自信，提高对未见类别的识别准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 将基于证据的不确定性估计器嵌入分类器；2) 使用动态校准堆叠因子根据点级不确定性调整预测概率；3) 通过将可学习参数与文本特征融合来细化语义空间，从而提升不确定性估计和模型优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明E3DPC-GZSL在ScanNet v2和S3DIS等数据集上实现了最优的泛化零样本语义分割效果，显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过整合证据不确定性估计和语义空间细化，E3DPC-GZSL有效缓解了3D点云分割中已见类别的过度自信问题，为泛化零样本语义分割提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该摘要的中文翻译：泛化零样本3D点云语义分割旨在将每个点分类为已见和未见类别。现有模型往往偏向训练时出现的类别，尤其在3D任务中更为明显。我们提出了E3DPC-GZSL方法，通过将基于证据的不确定性估计器嵌入分类器，并使用动态校准堆叠因子调整预测概率，来降低对已见类别的过度自信。同时，采用新的训练策略，将可学习参数与文本特征融合，细化语义空间，从而提升对未见数据的优化。实验表明，该方法在ScanNet v2和S3DIS数据集上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云的通用零样本语义分割中出现的过度自信偏向已见类别的问题。该问题在自动驾驶、医疗成像等需要对未知物体进行精确分割的实际场景中尤为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已知的二分类与校准堆叠方法，借鉴了证据理论的置信度估计和先前的零样本生成技术，提出在分类器中嵌入基于证据的不确定性估计器，并通过动态校准因子自适应调整概率。该设计在保持无超参数的同时，兼顾了对已见与未见类别的统一处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用证据‑基础的不确定性估计来动态校准每个点的预测概率，从而抑制对已见类别的过度自信。实现流程分为三阶段：①训练编码器提取特征；②训练解码器在文本嵌入和场景语义的条件下合成特征；③训练分类器与不确定性估计器，使用动态校准因子调整最终分割结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①无超参数的动态校准因子；②将证据理论与分类器耦合的置信度估计；③通过可学习的场景语义向量细化文本嵌入，提升合成特征质量；④在训练阶段使用解码器生成未见类别特征，弥补数据稀缺。与以往需要固定校准因子或分离已见/未见分类器的方法不同，E3DPC‑GZSL实现了统一、可自适应的推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; E3DPC‑GZSL通过证据‑基础的动态校准和语义细化的特征合成，消除了通用零样本3D点云分割中的过度自信偏差，实现了无超参数、统一的高性能分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generalized zero-shot semantic segmentation of 3D point clouds aims to classify each point into both seen and unseen classes. A significant challenge with these models is their tendency to make biased predictions, often favoring the classes encountered during training. This problem is more pronounced in 3D applications, where the scale of the training data is typically smaller than in image-based tasks. To address this problem, we propose a novel method called E3DPC-GZSL, which reduces overconfident predictions towards seen classes without relying on separate classifiers for seen and unseen data. E3DPC-GZSL tackles the overconfidence problem by integrating an evidence-based uncertainty estimator into a classifier. This estimator is then used to adjust prediction probabilities using a dynamic calibrated stacking factor that accounts for pointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel training strategy that improves uncertainty estimation by refining the semantic space. This is achieved by merging learnable parameters with text-derived features, thereby improving model optimization for unseen data. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on generalized zero-shot semantic segmentation datasets, including ScanNet v2 and S3DIS.&lt;/p&gt;</description></item><item><guid>2509.08982v1</guid><title>iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning</title><link>http://arxiv.org/abs/2509.08982v1</link><author>Karim Slimani, Catherine Achard, Brahim Tamadazte</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; iMatcher 是一种完全可微分的点云特征匹配框架，利用学习到的特征预测几何一致的置信矩阵，结合局部和全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准需要高精度的特征匹配，传统方法受限于局部匹配误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种同时考虑局部图嵌入和全局几何一致性的匹配方法，以提升刚性配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先使用局部图嵌入模块初始化分数矩阵；随后通过双向最近邻搜索重新定位并细化矩阵；最后将配对特征堆叠并通过全局几何一致性学习得到点级匹配概率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 KITTI、KITTI-360、3DMatch、TUD-L、MVP-RG 等数据集上，iMatcher 在入模率和配准精度上均超过现有方法，最高入模率达 97%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; iMatcher 在多种室外、室内和姿态估计任务中表现出色，证明了局部与全局一致性结合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper presents iMatcher, a fully differentiable framework for feature matching in point cloud registration. The proposed method leverages learned features to predict a geometrically consistent confidence matrix, incorporating both local and global consistency. First, a local graph embedding module leads to an initialization of the score matrix. A subsequent repositioning step refines this matrix by considering bilateral source-to-target and target-to-source matching via nearest neighbor search in 3D space. The paired point features are then stacked together to be refined through global geometric consistency learning to predict a point-wise matching probability. Extensive experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch) datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial matching (MVP-RG), demonstrate that iMatcher significantly improves rigid registration performance. The method achieves state-of-the-art inlier ratios, scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch, highlighting its robustness across diverse settings.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决了在部分重叠点云中准确匹配点对以提升配准精度的问题，这对自动驾驶、机器人导航和三维重建等领域至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了图卷积网络、加权SVD预对齐、双向最近邻匹配以及受 LightGlue 启发的全局一致性学习，借鉴了 GeoTransformer、Diffusion 模型和 Sinkhorn 等现有技术，构建了一个全微分的匹配框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用图卷积提取局部几何特征生成初始匹配分数，再通过加权SVD预对齐源点云并进行双向最近邻匹配，随后堆叠匹配对特征并学习全局一致性以得到匹配概率，最后将局部分数与全局匹配概率融合得到最终软分配矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 全微分的局部到全局一致性学习框架；2) 通过加权SVD和双向最近邻实现的重定位步骤；3) 受 LightGlue 启发的全局一致性模块；4) 通过融合局部分数与全局匹配概率生成置信矩阵，避免了迭代 Sinkhorn。与以往方法相比，它不依赖昂贵的 transformer 或迭代优化，且在多数据集上实现了更高的内点比例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; iMatcher 提出了一种全微分的局部到全局几何一致性学习框架，显著提升点云配准的匹配准确性，并在多种真实数据集上实现了领先的内点比例。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents iMatcher, a fully differentiable framework for feature matching in point cloud registration. The proposed method leverages learned features to predict a geometrically consistent confidence matrix, incorporating both local and global consistency. First, a local graph embedding module leads to an initialization of the score matrix. A subsequent repositioning step refines this matrix by considering bilateral source-to-target and target-to-source matching via nearest neighbor search in 3D space. The paired point features are then stacked together to be refined through global geometric consistency learning to predict a point-wise matching probability. Extensive experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch) datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial matching (MVP-RG), demonstrate that iMatcher significantly improves rigid registration performance. The method achieves state-of-the-art inlier ratios, scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch, highlighting its robustness across diverse settings.&lt;/p&gt;</description></item><item><guid>2509.10842v1</guid><title>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</title><link>http://arxiv.org/abs/2509.10842v1</link><author>Chongyu Wang, Kunlei Jing, Jihua Zhu, Di Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了OpenUrban3D，一种针对大规模城市点云的开词汇语义分割框架，能够在没有多视角图像、预训练网络或人工标注的情况下，实现对任意文本查询的零样本分割，并在城市场景中表现出更高的分割精度和更好的跨场景泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 开词汇语义分割使模型能够识别并分割来自任意自然语言描述的对象，适用于数字孪生、智慧城市管理和城市分析等应用。然而，在大规模城市点云中缺乏高质量、多视角图像且现有3D分割方法在不同城市环境中的泛化能力差，导致该技术在此领域尚未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决缺乏多视角图像和预训练网络的限制，构建一种能够在大规模城市点云中实现零样本、开词汇语义分割的框架，并提升分割精度与跨场景泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过多视角、多粒度渲染生成鲁棒语义特征，使用基于掩码的视觉-语言特征提取和样本平衡融合，再将结果蒸馏到3D骨干网络，实现对原始点云的直接特征学习，并支持任意文本查询的零样本分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在SensatUrban和SUM等大规模城市基准上，OpenUrban3D在分割准确率和跨场景泛化方面显著优于现有方法，验证了其在3D城市场景理解中的可行性与优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenUrban3D为大规模城市点云提供了一种灵活、可扩展的开词汇语义分割解决方案，能够在缺乏多视角图像和人工标注的情况下实现高精度、零样本分割，为城市数字化和智慧城市建设提供了重要技术支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Open-vocabulary semantic segmentation enables models to recognize and segment objects from arbitrary natural language descriptions, offering the flexibility to handle novel, fine-grained, or functionally defined categories beyond fixed label sets. While this capability is crucial for large-scale urban point clouds that support applications such as digital twins, smart city management, and urban analytics, it remains largely unexplored in this domain. The main obstacles are the frequent absence of high-quality, well-aligned multi-view imagery in large-scale urban point cloud datasets and the poor generalization of existing three-dimensional (3D) segmentation pipelines across diverse urban environments with substantial variation in geometry, scale, and appearance. To address these challenges, we present OpenUrban3D, the first 3D open-vocabulary semantic segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud segmentation networks, or manual annotations. Our approach generates robust semantic features directly from raw point clouds through multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model. This design enables zero-shot segmentation for arbitrary text queries while capturing both semantic richness and geometric priors. Extensive experiments on large-scale urban benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves significant improvements in both segmentation accuracy and cross-scene generalization over existing methods, demonstrating its potential as a flexible and scalable solution for 3D urban scene understanding.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现对大规模城市点云的开放词汇语义分割，而不需要对齐的多视角图像、预训练的点云分割网络或人工标注。该问题重要，因为城市数字孪生、智慧城市管理和城市分析等应用需要能够识别和分割任意自然语言描述的对象，尤其是新颖或细粒度的类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了2D开放词汇分割、Mask-Clip、SAM等技术，提出了多视角多粒度投影来生成信息丰富的渲染图像，并使用预训练的视觉-语言模型提取掩码级特征。随后通过将这些2D特征投影回点云并进行样本平衡融合，利用知识蒸馏训练3D骨干网络，使其与2D特征对齐。该思路在保持开放词汇能力的同时，克服了缺乏高质量图像和大规模标注的限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先从原始点云生成多视角、多粒度的渲染图像，利用视觉-语言模型提取掩码特征，再将这些特征映射回点云并融合，最后通过知识蒸馏让3D骨干学习与2D特征一致的表示。实现流程包括：1）多视角多粒度投影生成图像；2）使用VLM提取掩码特征；3）投影回点云并进行样本平衡融合得到2D特征库；4）用该特征库蒸馏训练3D骨干；5）推理时融合2D和3D特征，并与文本嵌入计算相似度得到开放词汇分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①完全无标注、无对齐图像的开放词汇点云分割框架；②多视角多粒度投影生成高质量渲染图像；③掩码级视觉-语言特征提取与投影回点云；④样本平衡融合与知识蒸馏实现3D特征对齐；⑤实现零样本分割并在大规模城市数据上表现出色。与之前的工作相比，OpenUrban3D不依赖预训练的点云分割网络或高质量图像，能够处理尺度差异大、遮挡严重的城市场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenUrban3D提供了一种完全无标注、无对齐图像的开放词汇点云分割方法，通过多视角投影、掩码特征提取和知识蒸馏，实现了在大规模城市点云上的零样本语义分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Open-vocabulary semantic segmentation enables models to recognize and segment objects from arbitrary natural language descriptions, offering the flexibility to handle novel, fine-grained, or functionally defined categories beyond fixed label sets. While this capability is crucial for large-scale urban point clouds that support applications such as digital twins, smart city management, and urban analytics, it remains largely unexplored in this domain. The main obstacles are the frequent absence of high-quality, well-aligned multi-view imagery in large-scale urban point cloud datasets and the poor generalization of existing three-dimensional (3D) segmentation pipelines across diverse urban environments with substantial variation in geometry, scale, and appearance. To address these challenges, we present OpenUrban3D, the first 3D open-vocabulary semantic segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud segmentation networks, or manual annotations. Our approach generates robust semantic features directly from raw point clouds through multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model. This design enables zero-shot segmentation for arbitrary text queries while capturing both semantic richness and geometric priors. Extensive experiments on large-scale urban benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves significant improvements in both segmentation accuracy and cross-scene generalization over existing methods, demonstrating its potential as a flexible and scalable solution for 3D urban scene understanding.&lt;/p&gt;</description></item><item><guid>2509.12595v1</guid><title>DisorientLiDAR: Physical Attacks on LiDAR-based Localization</title><link>http://arxiv.org/abs/2509.12595v1</link><author>Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, Wanpeng Shao</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对基于 LiDAR 的定位系统的对抗攻击框架 DisorientLiDAR，利用逆向工程定位模型识别关键点并有针对性地移除，从而破坏点云配准和车辆定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度学习模型易受视觉上不可察觉的扰动攻击，尤其对自动驾驶车辆的定位安全构成威胁，但针对 LiDAR 的攻击研究较少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索并验证对 LiDAR 基础定位的有效对抗攻击方法，并评估其在真实系统中的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过逆向工程特征提取网络，定位关键点，删除包含 Top-K 关键点的区域；在 KITTI 数据集上对 HRegNet、D3Feat、GeoTransformer 进行实验；在 Autoware 平台上验证定位漂移；在物理世界中使用近红外吸收材料遮挡关键区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 删除包含 Top-K 关键点的区域显著降低点云配准精度；在 Autoware 中仅遮挡少数关键区域即可导致明显定位漂移；物理遮挡实验成功复制了数据集中的攻击效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DisorientLiDAR 能有效破坏 LiDAR 定位，攻击在仿真和物理环境均可实现，表明该方法具有真实性和普适性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Deep learning models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Even this poses a serious security challenge for the localization of self-driving cars, there has been very little exploration of attack on it, as most of adversarial attacks have been applied to 3D perception. In this work, we propose a novel adversarial attack framework called DisorientLiDAR targeting LiDAR-based localization. By reverse-engineering localization models (e.g., feature extraction networks), adversaries can identify critical keypoints and strategically remove them, thereby disrupting LiDAR-based localization. Our proposal is first evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and GeoTransformer) using the KITTI dataset. Experimental results demonstrate that removing regions containing Top-K keypoints significantly degrades their registration accuracy. We further validate the attack&amp;#x27;s impact on the Autoware autonomous driving platform, where hiding merely a few critical regions induces noticeable localization drift. Finally, we extended our attacks to the physical world by hiding critical regions with near-infrared absorptive materials, thereby successfully replicate the attack effects observed in KITTI data. This step has been closer toward the realistic physical-world attack that demonstrate the veracity and generality of our proposal.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在揭示并评估一种可在物理世界中实施的攻击方式，该攻击通过隐藏关键的 LiDAR 可见区域来破坏基于深度学习的定位系统。定位是自动驾驶车辆安全的核心功能，若被攻击会导致车辆误定位、路径规划错误，进而产生严重安全风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到现有的点云配准网络高度依赖于局部几何特征（关键点）的匹配。基于此，他们逆向工程目标模型，识别出对配准贡献最大的关键点，并提出用近红外吸收材料遮挡这些区域来实现攻击。该思路借鉴了对感知模块的对抗攻击研究，但在定位领域提出了新的物理实现方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过物理遮挡关键点区域，使 LiDAR 无法获取这些重要几何信息，从而迫使配准网络只能使用次优匹配导致定位误差。实现流程包括：① 用与目标车辆相同的网络复制模型提取两帧点云中的高置信度关键点；② 对关键点按置信度排序，挑选前 K 个；③ 在物理环境中用近红外吸收材料遮挡对应区域；④ 评估遮挡后对三种主流配准模型和 Autoware 平台的定位误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①首次提出无需物理接触 LiDAR 的关键点隐藏攻击；②在三种先进的点云配准网络和真实 AV 平台上验证攻击效果；③实现了基于近红外吸收材料的物理遮挡方案；④提出了对抗训练和异常检测两种防御思路。与以往需要硬件注入或直接操纵激光的攻击不同，DisorientLiDAR 通过简单的遮挡即可实现高效破坏。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DisorientLiDAR 证明了通过物理遮挡少量关键 LiDAR 区域即可可靠破坏深度学习定位系统，揭示了自动驾驶车辆安全的新漏洞。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Deep learning models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Even this poses a serious security challenge for the localization of self-driving cars, there has been very little exploration of attack on it, as most of adversarial attacks have been applied to 3D perception. In this work, we propose a novel adversarial attack framework called DisorientLiDAR targeting LiDAR-based localization. By reverse-engineering localization models (e.g., feature extraction networks), adversaries can identify critical keypoints and strategically remove them, thereby disrupting LiDAR-based localization. Our proposal is first evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and GeoTransformer) using the KITTI dataset. Experimental results demonstrate that removing regions containing Top-K keypoints significantly degrades their registration accuracy. We further validate the attack&amp;#x27;s impact on the Autoware autonomous driving platform, where hiding merely a few critical regions induces noticeable localization drift. Finally, we extended our attacks to the physical world by hiding critical regions with near-infrared absorptive materials, thereby successfully replicate the attack effects observed in KITTI data. This step has been closer toward the realistic physical-world attack that demonstrate the veracity and generality of our proposal.&lt;/p&gt;</description></item><item><guid>2509.12924v2</guid><title>MATTER: Multiscale Attention for Registration Error Regression</title><link>http://arxiv.org/abs/2509.12924v2</link><author>Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forssén</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于回归的点云配准质量验证方法，利用多尺度特征提取和注意力聚合，能够更细粒度地量化配准误差，并在多样化数据集上实现准确、鲁棒的误差估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是许多下游任务（如SLAM和目标跟踪）的关键步骤，检测和量化配准误差（即配准质量验证）因此成为重要任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将配准质量验证从传统的分类任务转为回归任务，以实现更细粒度的误差量化，并通过改进特征提取提升估计精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用回归模型对配准误差进行预测，扩展了以往的误差相关特征，使用多尺度提取和基于注意力的聚合方式来构建更丰富的特征表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在多种数据集上，尤其是空间密度不均匀的点云中，能够实现准确且稳健的误差估计；并且在指导后续映射任务时，显著提升了在相同重配准帧数下的映射质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于回归的配准质量验证方法在精度和鲁棒性上优于现有的分类方法，并能有效提升下游映射任务的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准（PCR）对于许多下游任务至关重要，例如同步定位与地图构建（SLAM）和目标跟踪。这使得检测和量化配准失配，即PCR质量验证，成为一项重要任务。现有的所有方法都将验证视为分类任务，旨在将PCR质量分配到少数几个类别。本文我们改用回归进行PCR验证，允许对配准质量进行更细粒度的量化。我们还通过多尺度提取和基于注意力的聚合扩展了先前使用的失配相关特征。这在多样化数据集上实现了准确且稳健的配准误差估计，尤其适用于空间密度不均匀的点云。此外，当用于指导下游映射任务时，我们的方法在给定数量的重配准帧下显著提升了映射质量，优于最先进的基于分类的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在精确估计点云配准后的对齐误差，即检测配准误差。该问题重要，因为配准误差会在 SLAM、地图构建和机器人导航等后续任务中累积，导致地图失真和定位失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有方法将误差检测视为分类任务，粗略划分误差等级，缺乏细粒度评估。于是他们将任务转化为回归问题，并借鉴了 FACT、CorAl 等工作中的特征提取和网络结构，进一步引入多尺度特征和注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多尺度几何特征并通过注意力自适应加权，随后利用点变换器和 MLP 回归对齐误差。实现流程包括：对齐后采样锚点，计算不同半径下的熵、Sinkhorn 散度和覆盖率等特征；用注意力网络为每个尺度分配权重；将加权特征送入点变换器和回归头，输出误差估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 将误差检测从分类改为回归，提供连续误差估计；2) 引入多尺度注意力机制，自动选择合适的邻域尺度；3) 在多样化数据集上实现更高的精度和鲁棒性；4) 在地图构建任务中显著提升重定位质量。与之前的工作相比，MATTER 通过注意力融合多尺度特征，避免了单尺度或粗分类的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATTER 提出一种多尺度注意力回归框架，能够精确估计点云配准误差，并在多种场景下优于传统分类方法，显著提升后续地图构建质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e., PCR quality validation, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.&lt;/p&gt;</description></item><item><guid>2509.13692v1</guid><title>HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion</title><link>http://arxiv.org/abs/2509.13692v1</link><author>Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; HGACNet是一种通过层次化图注意力编码和单视RGB图像引导的先验融合，实现完整点云重建的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云不完整会影响机器人感知、物体重建以及抓取、避障等下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出HGACNet以解决因自遮挡和传感器限制导致的几何缺失问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用层次化图注意力编码器自适应选择关键局部点并逐层细化几何特征；通过多尺度跨模态融合模块对几何特征与视觉表示进行注意力对齐；采用对比损失对齐跨模态特征分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ShapeNet-ViPC和YCB-Complete数据集上实验表明，HGACNet达到或超过现有最优性能，并在真实机器人操作中表现出良好适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; HGACNet通过层次化编码和跨模态融合显著提升点云完成质量，具有实际应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云完成对于机器人感知、物体重建以及抓取规划、避障和操作等下游任务至关重要。然而，由于自遮挡和传感器限制导致的几何不完整会显著降低下游推理和交互的效果。为解决这些挑战，我们提出了HGACNet，一种通过层次化编码3D几何特征并与单视RGB图像的图像引导先验融合来重建单个物体完整点云的新框架。我们的核心方法是层次化图注意力（HGA）编码器，它通过基于图注意力的下采样自适应选择关键局部点，并逐步细化层次化几何特征，以更好地捕捉结构连续性和空间关系。为加强跨模态交互，我们进一步设计了多尺度跨模态融合（MSCF）模块，该模块通过注意力对齐层次化几何特征与结构化视觉表示，实现细粒度语义指导完成。除此之外，我们提出了对比损失（C-Loss）来显式对齐跨模态特征分布，在模态差异下提升完成精度。最后，在ShapeNet-ViPC基准和YCB-Complete数据集上进行的大量实验验证了HGACNet的有效性，展示了其在实时机器人操作任务中的先进性能和强大适用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从不完整的点云中恢复完整三维形状的问题。由于传感器遮挡、噪声和视角限制，实际点云往往缺失重要几何信息，导致机器人抓取、路径规划等任务的性能下降。完整的点云能显著提升这些应用的可靠性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到单模态方法难以捕捉细节，随后结合图注意力和视觉先验，设计了分层的图注意力编码器和多尺度跨模态融合模块。该思路借鉴了现有的图网络、Swin Transformer、跨模态注意力以及对比学习等技术，并在此基础上提出了新的层次化融合与对比损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次化的图注意力编码器提取点云的全局与局部特征，并利用多尺度跨模态注意力将这些几何特征与来自单视RGB图像的视觉特征对齐，最后用对比损失进一步缩小模态差距。实现流程为：输入部分点云和RGB图像 → HGA编码器得到全局/局部点云特征；Swin Transformer提取图像特征 → MSCF模块进行自注意力、跨注意力和跨模态注意力融合 → 对齐特征后通过解码器生成完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) HGA编码器采用图注意力下采样，能够自适应选择关键点并保留结构连续性；2) MSCF模块实现多尺度的跨模态注意力融合，既保留全局语义又细化局部细节；3) 引入对比损失显式对齐模态特征，减少模态差异。与以往早期融合或全局注意力密集的方案不同，HGACNet在层次化层面高效对齐模态，显著提升重建精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HGACNet通过层次化图注意力编码和多尺度跨模态注意力融合，并结合对比损失，实现了基于RGB引导的高精度点云完成，取得了跨模态点云完成的最新性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is essential for robotic perception, object reconstruction and supporting downstream tasks like grasp planning, obstacle avoidance, and manipulation. However, incomplete geometry caused by self-occlusion and sensor limitations can significantly degrade downstream reasoning and interaction. To address these challenges, we propose HGACNet, a novel framework that reconstructs complete point clouds of individual objects by hierarchically encoding 3D geometric features and fusing them with image-guided priors from a single-view RGB image. At the core of our approach, the Hierarchical Graph Attention (HGA) encoder adaptively selects critical local points through graph attention-based downsampling and progressively refines hierarchical geometric features to better capture structural continuity and spatial relationships. To strengthen cross-modal interaction, we further design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs attention-based feature alignment between hierarchical geometric features and structured visual representations, enabling fine-grained semantic guidance for completion. In addition, we proposed the contrastive loss (C-Loss) to explicitly align the feature distributions across modalities, improving completion fidelity under modality discrepancy. Finally, extensive experiments conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset confirm the effectiveness of HGACNet, demonstrating state-of-the-art performance as well as strong applicability in real-world robotic manipulation tasks.&lt;/p&gt;</description></item><item><guid>2509.15882v1</guid><title>Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration</title><link>http://arxiv.org/abs/2509.15882v1</link><author>Xingmei Wang, Xiaoyu Hu, Chengkai Huang, Ziyan Zeng, Guohao Nie, Quan Z. Sheng, Lina Yao</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个名为CrossI2P的自监督框架，用于将二维图像与三维点云进行配准。该框架通过双路径对比学习构建几何-语义融合嵌入空间，并采用粗到细的两阶段配准策略，最终在KITTI和nuScenes数据集上显著提升了配准精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶等自主系统中，融合二维图像与三维点云是实现稳健感知的关键，但由于图像纹理丰富但深度不确定，点云稀疏但精确，传统的图像到点云配准面临语义几何差距和局部最优问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服现有方法在语义几何差距和局部最优收敛方面的局限，提出一种统一的跨模态学习与两阶段配准的端到端自监督框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 通过双路径对比学习学习几何-语义融合嵌入空间，实现无标注的双向对齐；2) 采用粗到细配准：全局阶段通过联合模态内上下文与跨模态交互建立超点-超像素对应；细化阶段在几何约束下进行点级精细配准；3) 使用动态训练机制和梯度归一化平衡特征对齐、对应细化和位姿估计的损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CrossI2P在KITTI Odometry基准上比最先进方法提升23.7%，在nuScenes提升37.9%，在准确性和鲁棒性方面均有显著改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CrossI2P通过跨模态学习与两阶段配准的结合，显著解决了图像-点云配准中的语义几何差距和局部最优问题，为自动驾驶等领域提供了更可靠的感知方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 跨模态的二维图像与三维点云的桥接对于自动驾驶系统的稳健感知至关重要。然而，由于纹理丰富但深度不确定的图像与稀疏但精确的点云之间存在语义几何差距，以及现有方法易陷入局部最优的倾向，图像到点云（I2P）配准仍然具有挑战性。为克服这些限制，我们提出了CrossI2P，一个自监督框架，将跨模态学习和两阶段配准统一到一个端到端管道中。首先，我们通过双路径对比学习学习几何-语义融合嵌入空间，实现无标注的双向对齐二维纹理与三维结构。其次，我们采用粗到细的配准范式：全局阶段通过联合模态内上下文和跨模态交互建模，建立超点-超像素对应；随后进行几何约束的点级细化，以实现精确配准。第三，我们采用动态训练机制和梯度归一化，平衡特征对齐、对应细化和位姿估计的损失。大量实验表明，CrossI2P在KITTI Odometry基准上比最先进方法提升23.7%，在nuScenes提升37.9%，显著提高了准确性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决图像与点云之间的配准问题，即在没有预先标定的情况下，将二维图像与三维点云对齐。该问题在自动驾驶、机器人感知等场景中至关重要，因为多模态传感器的精确配准是实现语义一致性、环境建模和决策的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别了语义-几何鸿沟、局部最优陷阱和非可微 PnP 的三大挑战，随后借鉴了对比学习（如 CLIP、Contrastive Learning）和 Transformer 的全局上下文建模技术，设计了自监督跨模态对比学习、两阶段粗细配准以及可微 PnP 的端到端框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督对比学习将图像和点云映射到共享特征空间，然后使用 Transformer 进行粗略的超点-超像素匹配，接着在细粒度上进行点级精细化，最后利用可微 PnP 估计相机位姿，并通过动态协同训练平衡各损失。整体流程包括特征提取、跨模态对齐、粗配准、细配准、位姿回归和动态损失调节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 双路径自监督对比学习实现语义与几何的融合嵌入；2) 两阶段粗细配准结合超点-超像素与点级匹配；3) 可微 PnP 使整个系统可端到端训练；4) 动态协同训练平衡多任务损失。与以往方法相比，CrossI2P 解决了语义鸿沟、局部最优和非可微问题，显著提升了配准精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CrossI2P 提出了一种自监督、端到端的图像-点云配准框架，通过跨模态对比学习、两阶段粗细匹配和可微 PnP，实现了无标定、精确且鲁棒的 2D‑3D 对齐。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Bridging 2D and 3D sensor modalities is critical for robust perception in autonomous systems. However, image-to-point cloud (I2P) registration remains challenging due to the semantic-geometric gap between texture-rich but depth-ambiguous images and sparse yet metrically precise point clouds, as well as the tendency of existing methods to converge to local optima. To overcome these limitations, we introduce CrossI2P, a self-supervised framework that unifies cross-modal learning and two-stage registration in a single end-to-end pipeline. First, we learn a geometric-semantic fused embedding space via dual-path contrastive learning, enabling annotation-free, bidirectional alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine registration paradigm: a global stage establishes superpoint-superpixel correspondences through joint intra-modal context and cross-modal interaction modeling, followed by a geometry-constrained point-level refinement for precise registration. Third, we employ a dynamic training mechanism with gradient normalization to balance losses for feature alignment, correspondence refinement, and pose estimation. Extensive experiments demonstrate that CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry benchmark and by 37.9% on nuScenes, significantly improving both accuracy and robustness.&lt;/p&gt;</description></item><item><guid>2509.15886v3</guid><title>RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation</title><link>http://arxiv.org/abs/2509.15886v3</link><author>Paul Julius Kühn, Duc Anh Nguyen, Arjan Kuijper, Holger Graf, Saptarshi Neil Sinha</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了将最新视觉基础模型 SAM2 适配为激光雷达点云在视角投影下的分割框架，利用 2D 语义分割技术实现高效、可扩展的 3D 分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云分割是自动驾驶和 3D 场景理解的核心，传统的体素和点基方法虽然能捕捉细粒度几何，但计算成本高、内存访问不规则，实时性受限；视角投影方法尚未被充分探索，可利用成熟的 2D 分割技术实现快速准确的预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估 SAM2 作为 3D 分割的强大骨干网络，探究其在视角投影下对激光雷达点云分割的适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建首个将 SAM2 适配为 3D 分割的视角投影框架，结合 2D 特征提取与投影/反投影操作；对编码器做三项改进：①强调水平空间依赖的模块；②针对球面投影的几何属性定制配置；③专门捕捉视角伪图像中独特空间模式和不连续性的机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 SemanticKITTI 数据集上实现了与现有方法相当的性能，同时获得了 2D 方案的速度、可扩展性和部署简易性；验证了视觉基础模型可作为 3D 感知的通用骨干网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 使用视觉基础模型的视角投影分割方法表现出良好效果，为统一、基于基础模型的激光雷达分割奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云分割是自动驾驶和 3D 场景理解的核心。体素和点基方法因与深度架构兼容且能捕捉细粒度几何而占据主导地位，但往往导致高计算成本、不规则内存访问和有限的实时效率。相比之下，视角投影方法虽然相对未被充分探索，但可利用成熟的 2D 语义分割技术实现快速准确的预测。受视觉基础模型在图像字幕、零样本识别和多模态任务方面快速进展的激励，我们研究了 SAM2——当前最先进的视觉基础模型——是否能作为激光雷达点云在视角投影下的强大骨干网络。我们提出了首个将 SAM2 适配为 3D 分割的视角投影框架，将高效的 2D 特征提取与标准投影/反投影相结合，以在点云上操作。为优化 SAM2 在视角投影表示上的表现，我们对编码器做了多项架构改进：①一个新模块强调激光雷达视角图像中固有的水平空间依赖；②针对球面投影几何属性的定制配置；③在编码器骨干中专门设计的机制，用以捕捉视角伪图像中独特的空间模式和不连续性。我们的方法在 SemanticKITTI 上实现了竞争性性能，同时受益于 2D 中心化管线的速度、可扩展性和部署简易性。此工作凸显了视觉基础模型作为 3D 感知通用骨干网络的可行性，并为统一、基于基础模型的激光雷达分割开辟了道路。结果表明，使用视觉基础模型的视角投影分割方法具有有前景的效果。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在实现高效、准确的 LiDAR 点云语义分割，满足自动驾驶和三维场景理解的需求。传统的体素或点云方法计算量大、内存访问不规则，导致实时性差；而基于范围视图的方法可以利用成熟的二维分割技术，但尚未得到充分探索。解决这一问题可显著提升车辆感知系统的性能与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从 SAM2 这一先进的视觉基础模型出发，设计了一个能够处理范围视图图像的编码器。通过引入 Stem 模块将 LiDAR 数据转换为 96 通道张量，并在 Hiera 块中使用水平窗口注意力来适配球面投影。解码器采用 Receptive Field Blocks 进行多尺度特征融合。该设计借鉴了 SAM2‑UNet、RangeFormer 等先前工作，并结合了范围视图投影与反投影技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 SAM2 的二维分割能力迁移到 LiDAR 点云，通过范围视图投影实现。流程包括：① 将点云投影为密集的二维范围图；② 通过 Stem 模块和改进的 SAM2 编码器提取特征；③ 使用 Receptive Field Blocks 解码得到二维分割图；④ 将二维分割结果反投影回原始点云得到三维标签；⑤ 可选地使用 k‑NN 传播进行后处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; RangeSAM 首次将视觉基础模型 SAM2 应用于范围视图 LiDAR 分割；提出了强调水平空间依赖的 Stem 模块；定制了适用于球面投影的 Hiera 块和窗口注意力；解码器采用 Receptive Field Blocks 提升分割质量。与以往仅使用 CNN 或 Transformer 的范围视图方法不同，RangeSAM 利用预训练的基础模型，实现了竞争性的准确率，同时保持更快的推理速度和更简洁的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RangeSAM 展示了视觉基础模型可以被有效改造用于范围视图 LiDAR 分割，提供了一种快速、准确且易于部署的替代方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.&lt;/p&gt;</description></item><item><guid>2509.16832v2</guid><title>L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models</title><link>http://arxiv.org/abs/2509.16832v2</link><author>Ziyang Xu, Benedikt Schwab, Yihui Yang, Thomas H. Kolbe, Christoph Holst</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出L2M-Reg方法，利用平面对应、伪平面约束的Gauss-Helmert模型和自适应垂直平移估计，实现了在LoD2模型不确定性下的建筑级LiDAR与3D城市模型精确配准，实验表明其精度和效率均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在城市数字孪生中，LiDAR点云与语义3D城市模型的精确配准是基础，也是数字建造、变化检测和模型精细化等下游任务的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在LoD2级别存在模型不确定性时，单栋建筑级LiDAR与模型配准的准确性挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; L2M-Reg方法包括三步：1) 建立可靠的平面对应；2) 构建伪平面约束的Gauss-Helmert模型；3) 自适应估计垂直平移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明L2M-Reg在三组真实数据集上比现有ICP和其他平面基方法更精确且计算更高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; L2M-Reg为存在模型不确定性的建筑级LiDAR-模型配准提供了新方案，提升了配准精度和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Accurate registration between LiDAR point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection and model refinement. However, achieving accurate LiDAR-to-Model registration at individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model, and adaptively estimating vertical translation. Experiments on three real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现单栋建筑级别的 LiDAR 点云与 LoD2 城市模型之间的精确配准，并且显式考虑模型的不确定性。该问题在数字孪生、建筑施工、变更检测等高精度应用中至关重要，因为 LoD2 模型往往基于地籍边界生成，导致地基与立面之间存在水平偏移，若忽略会导致配准误差累积。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出现有配准方法普遍假设模型无误差的缺陷，随后结合平面特征匹配、ICP 变体和 Gauss–Helmert 模型的优势，设计了 L2M‑Reg。方法借鉴了已有的平面基配准（如 Scantra、PLADE）和 ICP 变体，但在此基础上加入了对模型不确定性的显式建模和垂直平移自适应估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可靠的平面对应、伪平面约束的 Gauss–Helmert 模型以及自适应垂直平移估计，来补偿 LoD2 模型的几何偏差。实现流程包括：①预处理 LiDAR 点云和 LoD2 模型；②利用语义信息提取并匹配平面；③构建伪平面约束的 Gauss–Helmert 模型求解 6-DoF 变换；④将垂直和平面分离，进行自适应垂直平移估计，最终得到精细配准结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①显式考虑 LoD2 模型的不确定性并通过伪平面约束进行补偿；②采用 2D‑3D 分离的变换估计策略，降低地面模型误差对水平配准的影响；③利用语义信息实现轻量级平面对应，无需将模型转为点云；④在保持高精度的同时显著提升计算效率。与以往仅假设模型无误差或依赖点云转换的 ICP/平面基方法不同，L2M‑Reg 在不确定性建模和效率上实现了突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L2M‑Reg 提供了一种快速、精确的建筑级 LiDAR‑to‑LoD2 配准方案，显式建模并补偿城市模型的几何不确定性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate registration between LiDAR (Light Detection and Ranging) point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection and model refinement. However, achieving accurate LiDAR-to-Model registration at individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model, and adaptively estimating vertical translation. Experiments on three real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present.&lt;/p&gt;</description></item><item><guid>2509.20705v1</guid><title>Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework</title><link>http://arxiv.org/abs/2509.20705v1</link><author>Reza Akhavian, Mani Amani, Johannes Mootz, Robert Ashe, Behrad Beheshti</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了BIM2RDT框架，将静态建筑信息模型转化为可供机器人使用的动态数字孪生，并通过实时感知与机器人操作提升施工现场数字管理与安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着网络物理系统和工地智能化的发展，连接设计模型、实时现场感知与自主操作的技术正在显著提升建筑行业的数字化管理水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够将已有BIM数据与现场实时信息融合，并生成安全优先的机器人可用数字孪生的智能框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 框架通过整合BIM几何语义、IoT传感器活动数据和机器人采集的视觉空间数据，采用基于大型语言模型推理的SG-ICP点云配准算法、YOLOE目标检测、Shi‑Tomasi角点检测以及实时手臂振动监测，实现数字孪生的持续更新与路径优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SG-ICP在遮挡特征场景下的配准误差比传统ICP低64.3%–88.3%，并能提供合理的物体朝向；手臂振动监测在超过ISO 5349‑1阈值时能及时发出警报，提升安全合规性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BIM2RDT框架通过高精度配准、实时感知与安全监测，显著提升了施工现场的数字孪生质量与机器人作业安全，为建筑行业的数字化管理提供了可行的技术路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The adoption of cyber‑physical systems and jobsite intelligence that connects design models, real‑time site sensing, and autonomous field operations can dramatically enhance digital management in the construction industry. This paper introduces BIM2RDT (Building Information Models to Robot‑Ready Site Digital Twins), an agentic artificial intelligence (AI) framework designed to transform static Building Information Modeling (BIM) into dynamic, robot‑ready digital twins (DTs) that prioritize safety during execution. The framework bridges the gap between pre‑existing BIM data and real‑time site conditions by integrating three key data streams: geometric and semantic information from BIM models, activity data from IoT sensor networks, and visual‑spatial data collected by robots during site traversal. The methodology introduces Semantic‑Gravity ICP (SG‑ICP), a point cloud registration algorithm that leverages large language model (LLM) reasoning. Unlike traditional methods, SG‑ICP utilizes an LLM to infer object‑specific, plausible orientation priors based on BIM semantics, improving alignment accuracy by avoiding convergence on local minima. This creates a feedback loop where robot‑collected data updates the DT, which in turn optimizes paths for missions. The framework employs YOLOE object detection and Shi‑Tomasi corner detection to identify and track construction elements while using BIM geometry as a priori maps. The framework also integrates real‑time Hand‑Arm Vibration (HAV) monitoring, mapping sensor‑detected safety events to the digital twin using IFC standards for intervention. Experiments demonstrate SG‑ICP&amp;#x27;s superiority over standard ICP, achieving RMSE reductions of 64.3%–88.3% in alignment across scenarios with occluded features, ensuring plausible orientations. HAV integration triggers warnings upon exceeding exposure limits, enhancing compliance with ISO 5349‑1.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在把静态的建筑信息模型（BIM）转化为能够实时更新、支持机器人导航并优先考虑安全的数字孪生（DT），从而解决施工现场信息滞后、工人安全风险高以及机器人在动态环境中定位困难等问题。该问题在现实中重要，因为施工现场复杂且安全事故频发，缺乏实时、可操作的数字化工具会导致效率低下和安全隐患。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已有的SLAM、ICP配准技术、YOLOE目标检测、Shi‑Tomasi角点检测以及工业物联网传感器，并在此基础上引入了大型语言模型（LLM）推理来提供语义重力先验，形成SG‑ICP算法。框架还借鉴了数字孪生与机器人协同、工人振动监测等前沿研究，形成了一个以安全为核心的代理式AI系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用BIM作为先验地图，让机器人通过视觉与深度传感器采集现场数据，并用语义驱动的SG‑ICP进行点云配准，实时更新数字孪生；随后代理AI根据更新后的DT规划路径、识别危险并触发HAV警报。整体流程包括：BIM加载 → 机器人巡检 → 目标检测与角点提取 → SG‑ICP配准 → DT更新 → 安全监测与路径规划 → 下一轮巡检。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) SG‑ICP利用LLM推理提供物体特定的重力先验，显著提升配准精度；2) 将工人手臂振动监测与IFC标准集成到DT中，实现实时安全干预；3) 采用YOLOE开放词汇检测与BIM语义提示相结合，增强目标识别；4) 构建完整的代理式AI安全优先框架，实现机器人与工人协同。与以往仅关注SLAM或单一传感器融合的工作不同，BIM2RDT实现了多模态数据融合、语义驱动配准和安全监测的统一系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BIM2RDT通过将BIM语义与机器人感知、LLM驱动的配准和实时安全监测融合，构建了一个安全优先的代理式AI框架，使施工现场能够实时生成机器人可用的数字孪生并主动预警风险。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The adoption of cyber-physical systems and jobsite intelligence that connects design models, real-time site sensing, and autonomous field operations can dramatically enhance digital management in the construction industry. This paper introduces BIM2RDT (Building Information Models to Robot-Ready Site Digital Twins), an agentic artificial intelligence (AI) framework designed to transform static Building Information Modeling (BIM) into dynamic, robot-ready digital twins (DTs) that prioritize safety during execution. The framework bridges the gap between pre-existing BIM data and real-time site conditions by integrating three key data streams: geometric and semantic information from BIM models, activity data from IoT sensor networks, and visual-spatial data collected by robots during site traversal. The methodology introduces Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that leverages large language model (LLM) reasoning. Unlike traditional methods, SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors based on BIM semantics, improving alignment accuracy by avoiding convergence on local minima. This creates a feedback loop where robot-collected data updates the DT, which in turn optimizes paths for missions. The framework employs YOLOE object detection and Shi-Tomasi corner detection to identify and track construction elements while using BIM geometry as a priori maps. The framework also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping sensor-detected safety events to the digital twin using IFC standards for intervention. Experiments demonstrate SG-ICP&amp;#x27;s superiority over standard ICP, achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with occluded features, ensuring plausible orientations. HAV integration triggers warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.&lt;/p&gt;</description></item><item><guid>2509.22132v1</guid><title>Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud</title><link>http://arxiv.org/abs/2509.22132v1</link><author>Jingjing Lu, Huilong Pi, Yunchuan Qin, Zhuo Tang, Ruihui Li</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督点云补全方法，利用多视角增强生成自监督信号，并引入 Mamba 模型提升生成质量，实验表明该方法在合成与真实数据集上均达到最先进水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有监督方法依赖真实标签，难以泛化到真实数据；无监督方法需要完整点云；弱监督方法需要多视角观测；现有自监督方法因信号有限导致预测不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服上述局限，提出一种能够在仅有单一部分点云的情况下进行自监督学习的补全方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计基于单个部分点云的多视角增强的自监督信号，并将 Mamba 模型引入自监督补全任务，以提升学习效果和生成点云质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成和真实数据集上的实验表明，该方法在点云补全任务中取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该自监督方法通过多视角增强和 Mamba 模型实现了高质量点云补全，并显著提升了对真实世界数据的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分观测中重建完整形状。虽然当前方法已取得显著性能，但仍存在一些局限：监督方法高度依赖真实标签，因合成到真实的域差导致在真实数据集上的泛化受限；无监督方法需要完整点云来构成无配对训练数据；弱监督方法需要对象的多视角观测。现有自监督方法由于自监督信号能力有限，往往产生不令人满意的预测。为克服这些挑战，我们提出了一种新颖的自监督点云补全方法。我们基于单个部分点云的多视角增强设计了一组新颖的自监督信号。此外，为了增强模型的学习能力，我们首先将 Mamba 引入自监督点云补全任务，鼓励模型生成质量更好的点云。对合成和真实数据集的实验表明，我们的方法实现了最先进的结果。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在完成从单一不完整点云中恢复完整三维形状的问题。现实中传感器往往产生缺失或遮挡的点云，缺失信息会影响机器人导航、三维重建等应用。研究中，如何在没有完整标注的情况下实现高质量补全是一个关键挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了监督、无监督和弱监督方法的局限，发现自监督方法缺乏多样化的自监督信号。为此他们借鉴了多视角数据增强、Hilbert曲线序列化、DGCNN补丁嵌入以及Mamba的选择性状态空间模型等已有技术，构建了基于多视角增强的自监督框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用单一部分点云生成多视角的合成部分点云，并让模型在所有视角下产生相同的完整点云，从而形成强大的自监督信号。实现流程包括：FPS采样、Hilbert曲线序列化、KNN补丁构造、DGCNN嵌入、八层Mamba编码器提取全局特征、特征拼接与池化得到512维全局向量，最后通过三层全连接生成8192点的完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①基于单一部分点云的多视角合成增强，提供多样化的自监督信号；②首次将Mamba编码器应用于点云补全，提升全局与局部特征提取能力；③结合加权Chamfer距离与一致性损失，强化模型对不同视角的鲁棒性。与以往自监督方法仅使用原始部分点云或中间表示不同，本方法通过视角多样化显著提升了补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种利用单一部分点云的多视角增强并结合Mamba编码器的自监督点云补全框架，在无需完整标注的情况下实现了更高质量、更具鲁棒性的三维形状恢复。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model&amp;#x27;s learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.&lt;/p&gt;</description></item><item><guid>2509.23375v1</guid><title>CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation</title><link>http://arxiv.org/abs/2509.23375v1</link><author>Yifan Yang, Yuxiang Yan, Boda Liu, Jian Pu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 CasPoinTr 框架，通过级联网络和知识蒸馏实现点云补全，显著提升形状恢复和细节保留。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 真实环境中采集的点云常因传感器分辨率、单视角、遮挡和噪声等因素不完整，点云补全成为关键技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决从高度不完整的点云中预测整体形状并重建缺失区域的难题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CasPoinTr 将补全任务拆分为形状重建和融合补全两阶段，利用形状重建生成辅助信息，融合补全结合知识蒸馏将稠密点云的完整-不完整关联知识迁移给学生模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ShapeNet-55 的不同难度设置下，CasPoinTr 在形状恢复和细节保留方面优于现有方法，验证了级联结构和蒸馏策略的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 级联网络与知识蒸馏的结合能更好捕捉全局形状上下文并细化局部细节，显著提升点云补全性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从真实环境中采集的点云往往因传感器分辨率有限、单一视角、遮挡和噪声等因素而不完整。这些挑战使得点云补全成为各类应用的必要技术。该任务的关键难点在于从高度不完整的点云中预测整体形状并重建缺失区域。为此，我们提出了 CasPoinTr，一种利用级联网络和知识蒸馏的全新点云补全框架。CasPoinTr 将补全任务拆分为两个协同阶段：形状重建阶段生成辅助信息，融合补全阶段则利用这些信息以及知识蒸馏来生成最终结果。通过知识蒸馏，训练于稠密点云的教师模型将不完整-完整关联知识迁移给学生模型，提升其估计整体形状和预测缺失区域的能力。级联网络与知识蒸馏共同增强模型捕捉全局形状上下文并细化局部细节的能力，有效弥合不完整输入与完整目标之间的差距。在 ShapeNet-55 的不同难度设置下的实验表明，CasPoinTr 在形状恢复和细节保留方面优于现有方法，凸显了我们级联结构和蒸馏策略的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成问题，即从不完整的点云中恢复完整的三维形状。现实中，传感器受限、单视角、遮挡和噪声导致点云缺失，影响机器人导航、AR/VR 等应用。研究中，准确完成点云能提升后续任务的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴人类先推断整体形状再细化细节的思路，结合现有的 PoinTr、AdaPoinTr、级联网络和知识蒸馏技术。通过分析现有方法在错误累积和教师-学生差距上的不足，提出了辅助完成和适当特权输入的策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云完成拆分为形状重建和融合完成两阶段，并用知识蒸馏让学生模型学习教师模型的完整-不完整关联。流程为：①形状重建阶段对不完整点云进行 4 倍上采样，生成稠密点云并提取辅助特征；②融合完成阶段以原始不完整点云为输入，结合辅助特征进行最终重建；③教师模型使用 2N 解析度的特权输入训练，学生通过 KL 散度蒸馏学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出辅助完成的级联结构，避免传统两步上采样导致的误差累积；②设计适当分辨率的特权输入（2N）来平衡教师-学生差距并防止捷径学习；③在特征层使用 KL 散度进行蒸馏，提升整体形状感知。与以往直接使用粗糙上采样或全分辨率特权输入的方法不同，CasPoinTr 在保持细节的同时显著提升了重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CasPoinTr 通过辅助完成的级联网络和基于适当特权输入的知识蒸馏，显著提升点云完成的整体形状恢复和细节保留效果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point clouds collected from real-world environments are often incomplete due to factors such as limited sensor resolution, single viewpoints, occlusions, and noise. These challenges make point cloud completion essential for various applications. A key difficulty in this task is predicting the overall shape and reconstructing missing regions from highly incomplete point clouds. To address this, we introduce CasPoinTr, a novel point cloud completion framework using cascaded networks and knowledge distillation. CasPoinTr decomposes the completion task into two synergistic stages: Shape Reconstruction, which generates auxiliary information, and Fused Completion, which leverages this information alongside knowledge distillation to generate the final output. Through knowledge distillation, a teacher model trained on denser point clouds transfers incomplete-complete associative knowledge to the student model, enhancing its ability to estimate the overall shape and predict missing regions. Together, the cascaded networks and knowledge distillation enhance the model&amp;#x27;s ability to capture global shape context while refining local details, effectively bridging the gap between incomplete inputs and complete targets. Experiments on ShapeNet-55 under different difficulty settings demonstrate that CasPoinTr outperforms existing methods in shape recovery and detail preservation, highlighting the effectiveness of our cascaded structure and distillation strategy.&lt;/p&gt;</description></item><item><guid>2509.23703v1</guid><title>DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph</title><link>http://arxiv.org/abs/2509.23703v1</link><author>Zhenyu Shu, Jian Yao, Shiqing Xin</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于可变节点度的点云完成网络，利用细节感知度量和几何感知图集成模块，显著提升点云重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云完成旨在补全因遮挡和传感器分辨率限制导致的不完整点云。传统方法使用固定局部划分，无法处理形状不同区域几何复杂度不均匀的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 改进点云完成方法，使其在细节丰富或结构不连续的区域实现更高效的表示和更优的重建效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出DFG-PCN框架，采用细节感知度量自适应分配节点度，并引入基于曼哈顿距离的几何感知图集成模块，将局部与全局特征进行细节引导融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上实验表明，该方法在重建精度上持续优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应节点度和几何感知图集成显著提升点云完成性能，证明了细节感知与结构重要性考虑的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云完成是一项重要任务，旨在重建完整的点云，并解决因遮挡和传感器分辨率有限导致的不完整性。传统方法依赖固定的局部区域划分，例如k近邻，无法考虑形状不同区域几何复杂度高度不均匀的情况。这一限制导致表示效率低下，重建效果不佳，尤其在细粒度细节或结构不连续的区域。本文提出一种名为Degree-Flexible Point Graph Completion Network（DFG-PCN）的点云完成框架。它使用结合特征变化和曲率的细节感知度量自适应地分配节点度，聚焦于结构重要区域。我们进一步引入几何感知图集成模块，使用曼哈顿距离进行边聚合，并通过细节引导融合局部和全局特征以增强表示。对多个基准数据集进行的大量实验表明，我们的方法始终优于最先进的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进点云补全技术，解决因遮挡和传感器分辨率有限导致的点云不完整问题。传统方法使用固定的k近邻划分，无法适应几何复杂度不均匀的区域，导致细节缺失和重建质量下降。点云补全在三维感知、机器人导航和虚拟现实等领域具有重要应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有补全方法在图结构上的“等度”缺陷，提出需要自适应节点度的图网络。设计中借鉴了PointNet、Point Transformer、Upsample Transformer等成熟模块，并在此基础上引入了度可调图构造、细节感知度分配和几何感知融合等新组件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个度可调的点图网络（DFG），根据点的细节丰富度（特征变化+曲率）动态分配邻接度。实现流程包括：①特征提取器提取全局与局部特征；②种子生成器利用Upsample Transformer生成粗略完整点云；③点生成模块由三层DFG块组成，每层使用PointNet、图构造、图聚合、图融合、MLP和去卷积逐步上采样，最终得到高分辨率完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①度可调点图网络，动态分配节点度；②细节感知度分配策略，结合特征变化和曲率；③几何感知图集成模块，使用曼哈顿距离聚合并细节引导的局部-全局特征融合。与以往固定k近邻或统一度的图网络不同，DFG-PCN能够在细节丰富区域提供更高的连接度，从而提升重建精度和细节保留。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DFG-PCN通过自适应度分配的点图网络，结合细节感知和几何融合，实现了比传统固定度方法更精细、更准确的点云补全。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is a vital task focused on reconstructing complete point clouds and addressing the incompleteness caused by occlusion and limited sensor resolution. Traditional methods relying on fixed local region partitioning, such as k-nearest neighbors, which fail to account for the highly uneven distribution of geometric complexity across different regions of a shape. This limitation leads to inefficient representation and suboptimal reconstruction, especially in areas with fine-grained details or structural discontinuities. This paper proposes a point cloud completion framework called Degree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns node degrees using a detail-aware metric that combines feature variation and curvature, focusing on structurally important regions. We further introduce a geometry-aware graph integration module that uses Manhattan distance for edge aggregation and detail-guided fusion of local and global features to enhance representation. Extensive experiments on multiple benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches.&lt;/p&gt;</description></item><item><guid>2509.23723v1</guid><title>DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion</title><link>http://arxiv.org/abs/2509.23723v1</link><author>Zijun Li, Hongyu Yan, Shijie Li, Kunming Luo, Li Lu, Xulei Yang, Weisi Lin</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 DiffPCN，一种基于扩散模型的粗细两阶段点云补全框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 扩散模型在低级视觉任务中表现出色，但由于点云的无序和不规则特性，点云补全尚未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用扩散模型的生成与理解能力，构建高质量、高完整度的点云补全方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先将不完整点云投影为结构化深度图，使用 DepthLDM 生成多视角完整深度图并构成粗点云；随后通过点去噪网络去除噪声并预测距离分数；最后使用关联感知点上采样器利用局部关联特征实现稠密高保真补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 DiffPCN 在几何精度和形状完整度上达到最先进水平，显著提升了点云补全的鲁棒性和一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DiffPCN 通过粗细两阶段的扩散与后处理，成功克服点云无序性，提供了高质量的点云补全方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 潜在扩散模型（LDMs）在各种低级视觉任务中展现了卓越的生成能力。然而，由于点云的无序和不规则特性，其在点云补全方面的潜力尚未得到充分探索。在本研究中，我们提出了 DiffPCN，一种新颖的基于扩散的粗细两阶段点云补全框架。我们的方法包括两个阶段：初始阶段用于生成粗略点云，精细阶段通过点去噪和上采样来提升其质量。具体而言，我们首先将无序且不规则的部分点云投影为结构化深度图，作为精心设计的 DepthLDM 的条件，用以合成完整的多视角深度图，随后将其用于构建粗点云。通过这种方式，DiffPCN 能够利用 LDM 强大的生成和理解能力，生成高质量且完整度高的粗点云。随后，由于 LDM 在生成深度图时不可避免地会引入离群点，我们设计了点去噪网络，通过预测每个点的距离分数来去除粗点云中的伪影。最后，我们提出了关联感知点上采样器，利用输入点云与对应粗点之间的局部关联特征来指导上采样过程，进一步得到稠密且高保真度的输出。实验结果表明，DiffPCN 在几何精度和形状完整度方面达到了最先进的性能，显著提升了点云补全的鲁棒性和一致性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成问题，即从部分、稀疏的点云中恢复完整、密集且结构连贯的三维点云。该问题在现实中十分重要，因为实际传感器（如 LiDAR、深度相机）往往受到遮挡、噪声和分辨率限制，导致采集到的点云不完整，影响后续的三维感知、导航和重建任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者意识到传统点云完成方法多依赖全局特征，导致细节缺失且效率低下，借鉴了图像领域的潜在扩散模型（Latent Diffusion Model）以及多视角深度图生成技术（如 MVDD、Wonder3D）。他们将点云投影为多视角深度图，使用 VAE 编码后通过 DepthLDM（结合跨视角注意力和点对齐注意力）生成完整深度图，再反投影得到粗点云，并在此基础上设计点去噪网络和关联感知上采样器进行细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云完成拆分为粗略生成和细化两个阶段，并利用潜在扩散模型在结构化的深度图空间中生成高质量粗点云。实现流程包括：① 将部分点云投影为六个视角深度图；② 用 VAE 编码并训练 DepthLDM，利用跨视角和点对齐注意力生成完整深度图；③ 将生成的深度图反投影得到粗点云；④ 用点去噪网络预测并剔除异常点；⑤ 用关联感知上采样器通过关联变换和自注意力进一步细化并上采样，得到最终完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 在点云完成中首次引入潜在扩散模型 DepthLDM，利用多视角深度图实现高效一致的粗点云生成；② 结合跨视角注意力和点对齐注意力提升三维结构感知；③ 设计点去噪网络通过距离分数剔除噪声点；④ 设计关联感知上采样器利用部分点云与粗点云的关联关系实现细粒度上采样。与以往直接在三维空间或仅使用全局特征的扩散/点云方法不同，DiffPCN 在结构化二维空间中生成并细化点云，显著提升了几何精度和完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DiffPCN 提出一种两阶段潜在扩散框架，将部分点云投影为多视角深度图，通过 DepthLDM 生成高质量粗点云，并通过点去噪与关联感知上采样实现细化，达成点云完成领域的最新性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Latent diffusion models (LDMs) have demonstrated remarkable generative capabilities across various low-level vision tasks. However, their potential for point cloud completion remains underexplored due to the unstructured and irregular nature of point clouds. In this work, we propose DiffPCN, a novel diffusion-based coarse-to-fine framework for point cloud completion. Our approach comprises two stages: an initial stage for generating coarse point clouds, and a refinement stage that improves their quality through point denoising and upsampling. Specifically, we first project the unordered and irregular partial point cloud into structured depth images, which serve as conditions for a well-designed DepthLDM to synthesize completed multi-view depth images that are used to form coarse point clouds. In this way, our DiffPCN can yield high-quality and high-completeness coarse point clouds by leveraging LDM&amp;#x27; s powerful generation and comprehension capabilities. Then, since LDMs inevitably introduce outliers into the generated depth maps, we design a Point Denoising Network to remove artifacts from the coarse point cloud by predicting a per-point distance score. Finally, we devise an Association-Aware Point Upsampler, which guides the upsampling process by leveraging local association features between the input point cloud and the corresponding coarse points, further yielding a dense and high-fidelity output. Experimental results demonstrate that our DiffPCN achieves state-of-the-art performance in geometric accuracy and shape completeness, significantly improving the robustness and consistency of point cloud completion.&lt;/p&gt;</description></item><item><guid>2509.24273v1</guid><title>Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds</title><link>http://arxiv.org/abs/2509.24273v1</link><author>Yongqiang Wang, Weigang Li, Wenping Liu, Zhiqiang Tian, Jinling Li</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于骨架的鲁棒点云配准框架，利用抗干扰的骨架表示提升配准的稳健性和精度，并通过分布距离损失函数加强源目标骨架的一致性。实验表明该方法在多种噪声、密度失真和几何畸变场景下均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准在三维视觉应用中至关重要，但真实点云常受传感器限制、环境噪声和预处理误差影响，导致密度失真、噪声污染和几何畸变，传统直接匹配或表面特征提取方法易受干扰，精度下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决受损点云配准的挑战，提升配准的鲁棒性和准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入抗干扰骨架表示，将骨架结构融入配准流程；同时结合受损点云与骨架配准得到的变换以获得最优配准；设计分布距离损失函数以强制源目标骨架一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多种受损数据集上，所提框架SRRF在密度失真、噪声污染和几何畸变等场景中均持续优于最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SRRF在处理受损点云时表现出高度鲁棒性，可作为真实场景三维感知任务的潜在方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准是3D视觉应用中的基础，包括自动驾驶、机器人和医学成像等领域，精确对齐多点云对于准确的环境重建至关重要。然而，真实点云往往受到传感器限制、环境噪声和预处理错误的影响，导致密度失真、噪声污染和几何畸变，使配准变得具有挑战性。现有配准方法依赖直接点匹配或表面特征提取，易受这些破坏的影响，导致对齐精度下降。为解决这些挑战，本文提出了一个基于骨架的鲁棒配准框架，引入抗干扰骨架表示以提升配准的稳健性和准确性。该框架将骨架结构整合到配准过程中，并结合受损点云配准和其骨架配准得到的变换以实现最佳配准。此外，设计了分布距离损失函数以强制源目标骨架之间的一致性，显著提升配准性能。该框架确保对齐同时考虑原始局部几何特征和骨架结构的全局稳定性，得到鲁棒且准确的配准结果。对多种受损数据集的实验评估表明，SRRF在密度失真、噪声污染和几何畸变等多种破坏场景下始终优于最先进的配准方法。结果证实了SRRF在处理受损点云方面的鲁棒性，使其成为真实场景中3D感知任务的潜在方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在现实环境中受噪声、密度失衡和几何畸变影响的三维点云配准问题。点云配准是自动驾驶、机器人导航和医学成像等领域的核心技术，准确对齐点云直接决定了后续建图、定位和识别的质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到骨架（Skeleton）作为点云的全局结构在噪声和稀疏情况下更为稳健，决定将骨架与传统点云配准相结合。方法借鉴了现有的骨架提取技术（如 Point2Skeleton）和基于深度学习的配准框架（如 DCP、PRNet），并在此基础上提出了分布距离损失来强化骨架一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过骨架表示提供全局稳定性，并将骨架配准与点云配准得到的变换融合，得到更准确的整体变换。实现流程包括：①提取源点云和目标点云的骨架；②使用深度学习模型对原始点云进行配准；③对骨架进行配准并计算骨架变换；④将两种变换加权融合；⑤使用分布距离损失约束骨架一致性，最终得到鲁棒的配准结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①提出骨架驱动的配准框架，利用骨架的全局稳定性提升鲁棒性；②设计分布距离损失，强制源骨架与目标骨架在分布上保持一致；③构建了包含密度、噪声和几何畸变的综合腐败点云基准，系统评估方法鲁棒性。与以往仅依赖原始点匹配或表面特征的配准方法不同，SRRF 在腐败场景下显著提升了配准精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SRRF 通过骨架表示和一致性损失实现了对受噪声、稀疏和畸变影响的三维点云的鲁棒且高精度配准，优于现有最先进方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration is fundamental in 3D vision applications, including autonomous driving, robotics, and medical imaging, where precise alignment of multiple point clouds is essential for accurate environment reconstruction. However, real-world point clouds are often affected by sensor limitations, environmental noise, and preprocessing errors, making registration challenging due to density distortions, noise contamination, and geometric deformations. Existing registration methods rely on direct point matching or surface feature extraction, which are highly susceptible to these corruptions and lead to reduced alignment accuracy. To address these challenges, a skeleton-based robust registration framework is presented, which introduces a corruption-resilient skeletal representation to improve registration robustness and accuracy. The framework integrates skeletal structures into the registration process and combines the transformations obtained from both the corrupted point cloud alignment and its skeleton alignment to achieve optimal registration. In addition, a distribution distance loss function is designed to enforce the consistency between the source and target skeletons, which significantly improves the registration performance. This framework ensures that the alignment considers both the original local geometric features and the global stability of the skeleton structure, resulting in robust and accurate registration results. Experimental evaluations on diverse corrupted datasets demonstrate that SRRF consistently outperforms state-of-the-art registration methods across various corruption scenarios, including density distortions, noise contamination, and geometric deformations. The results confirm the robustness of SRRF in handling corrupted point clouds, making it a potential approach for 3D perception tasks in real-world scenarios.&lt;/p&gt;</description></item><item><guid>2509.24275v1</guid><title>Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context</title><link>http://arxiv.org/abs/2509.24275v1</link><author>Yongqiang Wang, Weigang Li, Wenping Liu, Zhe Xu, Zhiqiang Tian</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于全局上下文的置信度估计框架CEGC，用于解决部分点云配准中的结构歧义、部分可见性和噪声问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 部分点云配准在自动驾驶感知和三维场景理解中至关重要，但由于结构歧义、部分可见性和噪声，仍然具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种统一的、置信度驱动的框架，以实现对复杂场景中部分点云的准确对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CEGC通过共享全局上下文同时建模重叠置信度和对应可靠性。混合重叠置信度估计模块结合语义描述符和几何相似性，早期检测重叠区域并抑制离群点；上下文感知匹配策略利用全局注意力为对应关系分配软置信度，从而降低歧义；这些置信度引导可微分加权奇异值分解求解器计算精确变换，整个流程自适应地降低不确定区域的权重并强调可靠匹配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ModelNet40、ScanObjectNN和7Scenes等三大数据集上，CEGC在准确性、鲁棒性和泛化能力方面均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CEGC提供了一种可解释且可扩展的解决方案，能够在挑战性条件下实现高质量的部分点云配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 部分点云配准对于自动驾驶感知和三维场景理解至关重要，但由于结构歧义、部分可见性和噪声，仍然具有挑战性。我们通过提出基于全局上下文的置信度估计（CEGC）来解决这些问题，CEGC是一种统一的、以置信度为驱动的框架，用于实现鲁棒的部分三维配准。CEGC通过在共享的全局上下文中联合建模重叠置信度和对应可靠性，实现了在复杂场景中的精确对齐。具体而言，混合重叠置信度估计模块结合语义描述符和几何相似性，检测重叠区域并在早期抑制离群点；上下文感知匹配策略通过全局注意力为对应关系分配软置信度，降低歧义并提升鲁棒性。这些置信度引导可微分加权奇异值分解求解器计算精确变换。该紧耦合的流程自适应地降低不确定区域的权重，并强调上下文可靠的匹配。对ModelNet40、ScanObjectNN和7Scenes三大三维视觉数据集的实验表明，CEGC在准确性、鲁棒性和泛化能力方面优于现有最先进方法。总体而言，CEGC为在挑战性条件下的部分点云配准提供了一种可解释且可扩展的解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决部分重叠、噪声和结构歧义下的三维点云配准问题。该问题在自动驾驶、机器人感知和三维重建等实际应用中至关重要，因为真实场景往往只能获得不完整的点云，准确配准是后续理解和操作的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了传统ICP、FGR等几何方法以及DCP、REGTR等学习方法在部分配准中的局限，发现它们缺乏对重叠区域和对应关系的联合置信度建模。基于此，他们借鉴了OMNet、RPMNet等置信度估计思路，设计了HOCE和CAMS模块，并将其与全局注意力机制和可微SVD求解器结合，形成端到端的CEGC框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过全局上下文同时估计重叠置信度和对应置信度，并用这些置信度加权求解变换。实现流程包括：①使用AGNN提取局部几何特征并增强全局上下文；②HOCE模块融合语义和几何信息预测每个点的重叠概率；③CAMS模块利用全局注意力生成软对应关系并给出置信度；④将置信度加权后输入可微SVD求解器得到最终刚性变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①HOCE模块首次将语义与几何特征融合来预测重叠区域；②CAMS通过全局注意力为对应关系赋予软置信度，缓解结构歧义；③置信度与求解器在同一端到端体系中耦合，保证不确定性在整个流程中被一致处理；④相较于以往分阶段或仅关注对应关系的学习方法，CEGC实现了更高的鲁棒性、准确性和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CEGC提出了一种统一的置信度驱动框架，能够在全局上下文中同时估计重叠与对应置信度，并通过加权SVD实现对部分点云的高精度、鲁棒配准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Partial point cloud registration is essential for autonomous perception and 3D scene understanding, yet it remains challenging owing to structural ambiguity, partial visibility, and noise. We address these issues by proposing Confidence Estimation under Global Context (CEGC), a unified, confidence-driven framework for robust partial 3D registration. CEGC enables accurate alignment in complex scenes by jointly modeling overlap confidence and correspondence reliability within a shared global context. Specifically, the hybrid overlap confidence estimation module integrates semantic descriptors and geometric similarity to detect overlapping regions and suppress outliers early. The context-aware matching strategy smitigates ambiguity by employing global attention to assign soft confidence scores to correspondences, improving robustness. These scores guide a differentiable weighted singular value decomposition solver to compute precise transformations. This tightly coupled pipeline adaptively down-weights uncertain regions and emphasizes contextually reliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D vision datasets demonstrate that CEGC outperforms state-of-the-art methods in accuracy, robustness, and generalization. Overall, CEGC offers an interpretable and scalable solution to partial point cloud registration under challenging conditions.&lt;/p&gt;</description></item><item><guid>2509.24370v1</guid><title>DINOReg: Strong Point Cloud Registration with Vision Foundation Model</title><link>http://arxiv.org/abs/2509.24370v1</link><author>Congjia Chen, Yufu Qu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出 DINOReg 网络，利用视觉与几何信息进行点云配准，显著提升配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是 3D 计算机视觉的基础任务，传统方法主要依赖几何特征，最近的研究尝试加入 RGB‑D 颜色信息，但仍未充分利用图像纹理与语义信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种充分利用视觉与几何信息的配准网络，以提高点云配准的精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用 DINOv2 提取图像视觉特征，在补丁级别与几何特征融合，并引入混合位置编码以捕捉图像与点云空间的空间关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 RGBD‑3DMatch 与 RGBD‑3DLoMatch 数据集上，DINOReg 相比现有几何或多模态方法提升了 14.2% 的补丁内点比例和 15.7% 的配准召回率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 融合视觉与几何特征并采用混合位置编码的 DINOReg 能显著提升点云配准性能，为多模态配准提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准是 3D 计算机视觉中的一项基础任务。大多数现有方法仅依赖几何信息进行特征提取和匹配。最近，一些研究将 RGB‑D 数据中的颜色信息纳入特征提取。虽然这些方法取得了显著改进，但它们并未充分利用图像中丰富的纹理和语义信息，且特征融合以图像失真方式进行，限制了性能。在本文中，我们提出 DINOReg，一种充分利用视觉和几何信息解决点云配准问题的网络。受视觉基础模型进展的启发，我们使用 DINOv2 从图像中提取信息丰富的视觉特征，并在补丁级别融合视觉和几何特征。该设计有效地将 DINOv2 提取的丰富纹理和全局语义信息与几何骨干捕获的细节几何结构信息相结合。此外，提出了一种混合位置嵌入，用于编码图像空间和点云空间的位置信息，增强了模型感知补丁间空间关系的能力。在 RGBD‑3DMatch 和 RGBD‑3DLoMatch 数据集上进行的大量实验表明，我们的方法相较于最先进的仅几何和多模态配准方法取得了显著提升，补丁内点比例提高了 14.2%，配准召回率提高了 15.7%。代码已公开发布在 https://github.com/ccjccjccj/DINOReg。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进点云配准，尤其在重叠度低或几何结构模糊时几何信息不足的场景。点云配准是三维重建、姿态估计等关键任务的基础，提升其鲁棒性和精度对实际应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法仅利用几何或仅用颜色信息，未充分利用图像的纹理与语义。借鉴了Transformer‑based注册网络（如GeoTransformer、Predator）和多模态融合思路，结合大规模视觉基础模型DINOv2，设计了在补丁级别进行视觉与几何特征融合的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是将图像补丁特征与点云补丁特征对齐并融合，随后通过混合位置编码的Transformer进行全局上下文聚合。实现流程包括：①用DINOv2提取图像补丁特征；②用KPConv‑FPN提取点云补丁特征；③通过相机标定将几何补丁投影到图像平面并聚合邻域视觉特征；④在补丁级别融合视觉与几何特征；⑤使用自注意力和交叉注意力的Transformer进行特征匹配；⑥根据匹配结果估计变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①使用视觉基础模型DINOv2获取丰富的语义与纹理信息；②在补丁级别进行视觉与几何特征的窗口聚合与融合；③提出混合位置编码同时编码图像像素位置和几何相对位置；④构建RGBD‑3DMatch与RGBD‑3DLoMatch数据集。与以往仅使用颜色或点级融合的方法不同，DINOReg充分利用图像全局语义并在补丁层面实现更精细的多模态融合，显著提升配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DINOReg通过将视觉基础模型与补丁级多模态融合及混合位置编码相结合，提供了更具辨别力的特征，显著提升了点云配准的准确性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration is a fundamental task in 3D computer vision. Most existing methods rely solely on geometric information for feature extraction and matching. Recently, several studies have incorporated color information from RGB-D data into feature extraction. Although these methods achieve remarkable improvements, they have not fully exploited the abundant texture and semantic information in images, and the feature fusion is performed in an image-lossy manner, which limit their performance. In this paper, we propose DINOReg, a registration network that sufficiently utilizes both visual and geometric information to solve the point cloud registration problem. Inspired by advances in vision foundation models, we employ DINOv2 to extract informative visual features from images, and fuse visual and geometric features at the patch level. This design effectively combines the rich texture and global semantic information extracted by DINOv2 with the detailed geometric structure information captured by the geometric backbone. Additionally, a mixed positional embedding is proposed to encode positional information from both image space and point cloud space, which enhances the model&amp;#x27;s ability to perceive spatial relationships between patches. Extensive experiments on the RGBD-3DMatch and RGBD-3DLoMatch datasets demonstrate that our method achieves significant improvements over state-of-the-art geometry-only and multi-modal registration methods, with a 14.2% increase in patch inlier ratio and a 15.7% increase in registration recall. The code is publicly available at https://github.com/ccjccjccj/DINOReg.&lt;/p&gt;</description></item><item><guid>2510.06582v2</guid><title>Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation</title><link>http://arxiv.org/abs/2510.06582v2</link><author>Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种半自动、基于不确定性感知的地面激光扫描点云语义分割流程，结合球面投影、特征增强、集成学习和目标标注，显著降低人工标注成本并保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统TLS点云语义分割需要大量人工标注，成本高昂。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种高效、低成本的半自动标注管线，并评估所需标注数据量与特征重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将3D点投影到2D球面网格，使用多源特征增强像素，训练集成分割网络生成伪标签和不确定性图，利用不确定性图指导标注；随后将2D结果反投影回3D，配合三层可视化工具进行快速审核。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 性能在约12个标注扫描后趋于饱和；几何特征贡献最大；九通道特征堆叠已能捕获大部分判别力，平均交并比约为0.76；方法在其他数据集上也能泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该管线可实现可扩展、高质量的TLS点云语义分割，适用于生态监测等场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确的地面激光扫描（TLS）点云语义分割受限于昂贵的人工标注。我们提出一种半自动、基于不确定性的管线，集成球面投影、特征增强、集成学习和目标标注，以降低标注工作量，同时保持高精度。我们的方案将3D点投影到2D球面网格，用多源特征增强像素，并训练一组分割网络生成伪标签和不确定性图，后者指导对模糊区域的标注。2D输出再反投影回3D，得到密集标注的点云，并配备三层可视化工具（2D特征图、3D彩色点云、紧凑虚拟球体），用于快速筛选和审阅。利用该管线，我们构建了Mangrove3D，针对红树林的TLS语义分割数据集。我们进一步评估数据效率和特征重要性，回答两个关键问题：（1）需要多少标注数据；（2）哪些特征最重要。结果显示，性能在约12个标注扫描后饱和，几何特征贡献最大，紧凑的九通道堆叠几乎捕获了所有判别力，平均交并比在0.76左右稳定。最后，我们通过在ForestSemantic和Semantic3D数据集上的交叉测试确认了特征增强策略的泛化能力。我们的贡献包括：（i）一个稳健、基于不确定性的TLS标注管线及可视化工具；（ii）Mangrove3D数据集；（iii）关于数据效率和特征重要性的经验指导，从而实现可扩展、高质量的TLS点云分割，适用于生态监测及其他领域。数据集和处理脚本公开可在 https://fz-rit.github.io/through-the-lidars-eye/ 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在降低地面激光扫描（TLS）点云语义分割的人工标注成本，并提升在复杂生态环境（如红树林）中的分割精度。准确的分割是森林生物量估算、碳汇评估等生态监测任务的基础，而现有数据集稀缺且标注工作耗时，限制了深度学习方法的广泛应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已有的球面投影、特征融合、集成学习和不确定性分析技术，提出了一个半自动、基于不确定性的标注流程。该方法借鉴了自动驾驶领域的 RangeNet、SalsaNext 等投影网络，以及主动学习和自训练的策略，并将其迁移到生态 TLS 场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 3D 点云投影到 2D 球面图像，使用多源特征构建九通道输入，训练集成网络生成伪标签和不确定性图，利用不确定性引导人工标注，最后将标注结果反投影回 3D。整个流程包括：① 球面投影与特征堆叠；② 训练集成网络得到伪标签与不确定性；③ 通过不确定性聚焦人工纠正；④ 反投影生成完整 3D 标注，并提供三层可视化工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 提供了一个完整的不确定性驱动的半自动标注管线；② 在 TLS 上实现了多通道特征堆叠的球面投影，显著提升分割性能；③ 通过实验给出了数据量与特征重要性的经验指导；④ 通过跨数据集测试验证了方法的通用性；⑤ 发布了首个红树林 TLS 分割数据集 Mangrove3D。与以往主要针对城市或室内数据、缺乏不确定性引导的工作不同，本文针对生态复杂场景提供了可扩展的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于球面投影、特征融合和不确定性引导的半自动标注管线，显著降低了红树林等生态 TLS 点云分割的人工成本，并通过 Mangrove3D 数据集验证了其高效性与通用性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D.   Our contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.&lt;/p&gt;</description></item><item><guid>2510.10365v1</guid><title>PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion</title><link>http://arxiv.org/abs/2510.10365v1</link><author>Linlian Jiang, Rui Ma, Li Gu, Ziqiang Wang, Xinxin Zuo, Yang Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为 PointMAC 的元学习框架，用于点云完成任务的测试时自适应。通过自监督辅助目标和基于 MAML 的元学习，模型能够在推理时对每个样本进行特定细化，从而显著提升完成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云完成在机器人和增强现实等安全关键应用中至关重要，但现有模型在推理时是静态的，过度依赖训练时学习到的归纳偏置，难以适应测试时的新结构模式和传感器失真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在测试时自适应的点云完成方法，使模型能够针对每个样本进行细化，而无需额外监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PointMAC 在推理时通过优化两种自监督辅助目标（模拟结构和传感器缺失）来调整共享编码器，解码器保持不变。采用基于 MAML 的元辅助学习策略保证辅助目标的优化与主任务一致，并引入 Adaptive λ-Calibration 机制平衡主任务与辅助目标的梯度，提升自适应稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成、模拟和真实数据集上的实验表明，PointMAC 能够逐样本细化点云，取得领先的完成效果，显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文首次将元辅助测试时自适应应用于点云完成，证明了该策略在提升模型鲁棒性和完成质量方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云完成对于机器人和增强现实等安全关键应用中的鲁棒三维感知至关重要。然而，现有模型在推理时是静态的，并且过度依赖训练期间学习到的归纳偏置，限制了它们在测试时适应新结构模式和传感器引起的失真。为了解决这一限制，我们提出了 PointMAC，一种用于点云完成的元学习框架，可实现稳健的测试时自适应。它能够在不需要额外监督的情况下对每个样本进行特定细化。我们的方法在两个自监督辅助目标下优化完成模型，这些目标模拟结构和传感器级的不完整性。基于 Model-Agnostic Meta-Learning（MAML）的元辅助学习策略确保由辅助目标驱动的自适应始终与主要完成任务保持一致。在推理过程中，我们通过优化辅助损失实时调整共享编码器，而解码器保持固定。为进一步稳定自适应，我们引入 Adaptive λ-Calibration，一种元学习机制，用于平衡主任务与辅助目标之间的梯度。对合成、模拟和真实数据集的广泛实验表明，PointMAC 通过对每个样本进行细化，取得了最先进的结果，生成高质量的完成点云。据我们所知，这是首次将元辅助测试时自适应应用于点云完成。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成模型在测试时缺乏自适应能力的问题，使其无法针对不同结构缺失和传感器噪声进行动态调整，从而产生泛化不足的通用补全结果。该问题在机器人、自动驾驶和增强现实等安全关键场景中尤为重要，因为准确的三维感知直接影响决策与操作安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过将测试时自适应（TTA）与自监督辅助任务相结合，提出了Bi‑Aux Units来模拟结构缺失和噪声扰动，并采用MAML框架使辅助任务的梯度与主任务保持一致。该设计借鉴了Transformer编码器、双向自监督重建、噪声去除以及梯度平衡技术等已有工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用元学习驱动的自监督辅助任务，在测试时对共享编码器进行样本特定的微调，从而提升完成质量。实现流程包括：①训练阶段，使用MAML内循环对编码器进行辅助任务微调，外循环优化主完成损失；②推理阶段，冻结解码器，仅通过Bi‑Aux产生的自监督损失更新编码器；③得到适配后的编码器后生成最终补全点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①首次将元学习驱动的测试时自适应应用于点云完成；②设计Bi‑Aux Units提供结构遮挡与噪声去除两种自监督信号；③引入Adaptive λ‑Calibration平衡主任务与辅助任务梯度；④通过MAML确保辅助任务与主任务一致性。与以往静态推理或未对齐辅助任务的工作相比，PointMAC实现了更稳健、样本特定的补全效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointMAC通过元学习驱动的自监督辅助任务和自适应梯度平衡，实现了点云完成的测试时自适应，显著提升了补全质量并达到最先进水平。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is essential for robust 3D perception in safety-critical applications such as robotics and augmented reality. However, existing models perform static inference and rely heavily on inductive biases learned during training, limiting their ability to adapt to novel structural patterns and sensor-induced distortions at test time. To address this limitation, we propose PointMAC, a meta-learned framework for robust test-time adaptation in point cloud completion. It enables sample-specific refinement without requiring additional supervision. Our method optimizes the completion model under two self-supervised auxiliary objectives that simulate structural and sensor-level incompleteness. A meta-auxiliary learning strategy based on Model-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary objectives is consistently aligned with the primary completion task. During inference, we adapt the shared encoder on-the-fly by optimizing auxiliary losses, with the decoder kept fixed. To further stabilize adaptation, we introduce Adaptive $λ$-Calibration, a meta-learned mechanism for balancing gradients between primary and auxiliary objectives. Extensive experiments on synthetic, simulated, and real-world datasets demonstrate that PointMAC achieves state-of-the-art results by refining each sample individually to produce high-quality completions. To the best of our knowledge, this is the first work to apply meta-auxiliary test-time adaptation to point cloud completion.&lt;/p&gt;</description></item><item><guid>2510.10471v2</guid><title>DAGLFNet: Deep Feature Attention Guided Global and Local Feature Fusion for Pseudo-Image Point Cloud Segmentation</title><link>http://arxiv.org/abs/2510.10471v2</link><author>Chuang Chen, Yi Lin, Bo Wang, Jing Hu, Xi Wu, Wenyi Ge</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于伪图像的语义分割框架 DAGLFNet，旨在解决 LiDAR 点云与二维网格融合时的特征不一致问题。通过全局-局部特征融合编码、多分支特征提取和深度特征引导注意力机制，显著提升了点云语义分割的特征辨别能力，并在两个公开数据集上取得了优异的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 环境感知系统在高精度地图构建和自动驾驶中起着关键作用，LiDAR 作为核心传感器提供精确的三维点云数据。然而，如何高效处理无结构点云并提取结构化语义信息仍是一个重要挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够在保持效率的同时，提升伪图像与原始三维信息融合质量的语义分割方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DAGLFNet 由三大模块组成：全局-局部特征融合编码增强局部相关性并捕获全局上下文；多分支特征提取网络获取更丰富的邻域信息，提升轮廓特征辨别；深度特征引导注意力机制细化跨通道特征融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 SemanticKITTI 验证集上获得 69.9% 的平均交并比，在 nuScenes 验证集上获得 78.7%，显示出在准确率与效率之间取得了良好平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DAGLFNet 通过改进特征融合策略，有效解决了伪图像与三维点云不一致导致的特征辨别不足问题，证明了其在高精度环境感知中的可行性和优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 环境感知系统对于高精度地图构建和自动驾驶至关重要，LiDAR 作为核心传感器提供准确的三维点云数据。高效处理无结构点云并提取结构化语义信息仍是一个重大挑战。近年来，出现了许多基于伪图像的表示方法，通过将三维点云与二维网格融合来平衡效率和性能。然而，伪图像表示与原始三维信息之间的根本不一致严重削弱了二维-三维特征融合，成为统一信息融合的主要障碍，并导致特征辨别能力差。本文提出 DAGLFNet，一种基于伪图像的语义分割框架，旨在提取具有辨别力的特征。它包含三个关键组件：首先，Global-Local Feature Fusion Encoding（GL-FFE）模块增强同一集合内的局部特征相关性并捕获全局上下文信息；其次，多分支特征提取（MB-FE）网络捕获更丰富的邻域信息，提升轮廓特征的辨别力；第三，Feature Fusion via Deep Feature-guided Attention（FFDFA）机制细化跨通道特征融合精度。实验评估表明，DAGLFNet 在 SemanticKITTI 和 nuScenes 验证集上分别取得 69.9% 和 78.7% 的平均交并比，方法在准确率与效率之间实现了优异的平衡。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文想解决在将 LiDAR 点云投影成伪图像时，深度冲突和边界模糊导致的特征信息丢失问题。把三维点云转换成二维图像可以提高计算效率，但会破坏空间结构，影响分割精度。准确地保留点云的几何和语义信息对自动驾驶、地图构建等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有伪图像方法的缺陷，发现它们忽略了子集内部的特征关联和距离信息。随后借鉴了 FRNet、FARVNet、RangeNet++ 等工作中的投影与特征融合思路，提出了 GL-FFE、MB-FE 和 FFDFA 三个模块来补足这些不足。设计时结合了多分支卷积、注意力机制和距离加权融合等技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过全局-局部特征融合、分支特征提取和深度引导注意力，构建更具判别力的伪图像特征。实现流程包括：①将点云按激光束分组并编码为多维特征；②将组特征映射到二维图像；③使用 GL-FFE 捕获全局上下文和局部几何；④用 MB-FE 扩大感受野并强化边界特征；⑤通过 FFDFA 用距离信息加权跨通道融合；⑥最后的融合头输出每点语义标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①GL-FFE 模块同时建模全局依赖和局部几何；②MB-FE 网络通过多分支扩展感受野，提升边界识别；③FFDFA 采用距离加权的深度引导注意力，精细化特征融合。与之前的工作相比，DAGLFNet 更好地保留了三维结构信息，解决了深度冲突和边界模糊问题，并在 SemanticKITTI 和 nuScenes 上取得更高的 mIoU。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAGLFNet 通过全局-局部特征融合、多分支提取和距离引导注意力，显著提升了伪图像 LiDAR 点云分割的精度，实现了 SemanticKITTI 和 nuScenes 上的领先 mIoU。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Environmental perception systems are crucial for high-precision mapping and autonomous navigation, with LiDAR serving as a core sensor providing accurate 3D point cloud data. Efficiently processing unstructured point clouds while extracting structured semantic information remains a significant challenge. In recent years, numerous pseudo-image-based representation methods have emerged to balance efficiency and performance by fusing 3D point clouds with 2D grids. However, the fundamental inconsistency between the pseudo-image representation and the original 3D information critically undermines 2D-3D feature fusion, posing a primary obstacle for coherent information fusion and leading to poor feature discriminability. This work proposes DAGLFNet, a pseudo-image-based semantic segmentation framework designed to extract discriminative features. It incorporates three key components: first, a Global-Local Feature Fusion Encoding (GL-FFE) module to enhance intra-set local feature correlation and capture global contextual information; second, a Multi-Branch Feature Extraction (MB-FE) network to capture richer neighborhood information and improve the discriminability of contour features; and third, a Feature Fusion via Deep Feature-guided Attention (FFDFA) mechanism to refine cross-channel feature fusion precision. Experimental evaluations demonstrate that DAGLFNet achieves mean Intersection-over-Union (mIoU) scores of 69.9% and 78.7% on the validation sets of SemanticKITTI and nuScenes, respectively. The method achieves an excellent balance between accuracy and efficiency.&lt;/p&gt;</description></item><item><guid>2510.10876v1</guid><title>rareboost3d: a synthetic lidar dataset with enhanced rare classes</title><link>http://arxiv.org/abs/2510.10876v1</link><author>Shutong Lin, Zhengkang Xiang, Jianzhong Qi, Kourosh Khoshelham</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一个名为RareBoost3D的合成点云数据集，并提出了CSC loss跨域语义对齐方法，以解决真实点云数据中稀有类别的长尾问题，并显著提升了基于LiDAR的点云分割模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 真实点云数据集在LiDAR感知技术发展中起重要作用，但稀有类别样本不足导致长尾问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过合成数据补充稀有类别样本，并通过跨域对齐提升模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 创建RareBoost3D合成点云数据集；提出CSC loss跨域语义对齐方法，将不同域中同一类别的特征表示对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CSC loss对齐显著提升了LiDAR点云分割模型在真实数据上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 合成数据与跨域对齐方法有效缓解长尾问题，提升了点云分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 真实世界的点云数据集为基于LiDAR的感知技术（如自动驾驶中的目标分割）做出了重要贡献。然而，由于某些稀有类别实例数量有限，长尾问题仍是现有数据集面临的主要挑战。为解决此问题，我们提出了一个名为RareBoost3D的合成点云数据集，该数据集通过提供大量稀有类别实例来补充现有真实数据集。为有效利用合成与真实数据，我们进一步提出了名为CSC loss的跨域语义对齐方法，该方法将不同域中同一类别的特征表示对齐。实验结果表明，该对齐方法显著提升了基于LiDAR的点云分割模型在真实数据上的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 LiDAR 点云语义分割中稀有类别样本不足导致的长尾问题，这一问题在自动驾驶等实际应用中会削弱模型对少见物体的识别能力，影响安全与性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先利用 CARLA 仿真器生成大量带有稀有类别的合成点云，随后采用对比学习中的跨域语义一致性（CSC）损失，将合成与真实数据的特征对齐；这一思路借鉴了 SynLiDAR、PointAug 等合成数据增强方法以及 PointDR 的对比学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是通过合成数据补充稀有类别样本，并用对比学习把不同域同类别的特征聚集在一起。实现流程包括：1）使用 CARLA 生成 RareBoost3D 数据；2）将合成与真实数据统一标签；3）构建每个类别的特征原型记忆库；4）在训练时计算 CSC 损失与分割损失的总和；5）在 MinkUNet 或 Point Transformer 等骨干网络上训练并评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）RareBoost3D 数据集专门提升稀有类别样本量；2）基于原型的 CSC 损失实现跨域对比学习，无需复杂的对抗训练；3）在多种骨干网络上验证了对稀有类别分割的显著提升。与以往仅做数据增强或使用对抗域适配的工作不同，本文提供了更易实现且效果更好的对比学习方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一个稀有类别样本丰富的合成 LiDAR 数据集和一种基于对比学习的跨域语义一致性损失，显著提升了真实场景下稀有类别的点云分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Real-world point cloud datasets have made significant contributions to the development of LiDAR-based perception technologies, such as object segmentation for autonomous driving. However, due to the limited number of instances in some rare classes, the long-tail problem remains a major challenge in existing datasets. To address this issue, we introduce a novel, synthetic point cloud dataset named RareBoost3D, which complements existing real-world datasets by providing significantly more instances for object classes that are rare in real-world datasets. To effectively leverage both synthetic and real-world data, we further propose a cross-domain semantic alignment method named CSC loss that aligns feature representations of the same class across different domains. Experimental results demonstrate that this alignment significantly enhances the performance of LiDAR point cloud segmentation models over real-world data.&lt;/p&gt;</description></item><item><guid>2510.11565v1</guid><title>SNAP: Towards Segmenting Anything in Any Point Cloud</title><link>http://arxiv.org/abs/2510.11565v1</link><author>Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SNAP 是一种统一的交互式 3D 点云分割模型，支持点和文本提示，跨室内、室外和空中等多域训练，利用域自适应归一化避免负迁移，自动生成掩码并与 CLIP 嵌入匹配，实现全景和开放词汇分割，实验表明在多项零样本基准上取得领先或竞争性表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法往往仅限于单一域（室内或室外）和单一交互方式（空间点击或文本提示），多数据集训练会导致负迁移，缺乏通用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够在多域、多交互方式下统一工作的 3D 分割模型，克服现有局限并提升跨域泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在七个不同域的数据集上训练 SNAP，使用域自适应归一化防止负迁移；对文本提示自动生成掩码候选并与 CLIP 文本嵌入匹配，实现全景和开放词汇分割；支持点提示和文本提示两种交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SNAP 在 9 个零样本空间提示基准中 8 个获得最优成绩，在 5 个文本提示基准中表现竞争性；整体分割质量高，证明统一模型可匹配或超越专用域模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 统一模型能够在多域、多交互方式下实现高质量分割，为可扩展的 3D 注释提供实用工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 交互式 3D 点云分割通过用户引导提示实现对复杂 3D 场景的高效标注。然而，当前方法通常仅限于单一域（室内或室外）和单一交互形式（空间点击或文本提示）。此外，在多个数据集上训练往往导致负迁移，产生缺乏通用性的域特定工具。为解决这些局限，我们提出 SNAP（Segment Anything in Any Point cloud），一种支持点基和文本基提示、跨多域的统一交互式 3D 分割模型。我们的方案通过在覆盖室内、室外和空中环境的七个数据集上训练，并采用域自适应归一化来防止负迁移，从而实现跨域泛化。对于文本提示分割，我们自动生成掩码候选并与 CLIP 文本嵌入匹配，支持全景和开放词汇分割。大量实验表明，SNAP 一直提供高质量分割结果，在 9 个零样本空间提示基准中 8 个实现了最先进性能，在 5 个文本提示基准中表现竞争。结果表明，统一模型可以匹配或超过专用域特定方法，为可扩展 3D 注释提供实用工具。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在构建一个能够在室内、室外和空中三种不同点云域中统一工作、同时支持点和文本两种提示方式的交互式点云分割模型。该问题重要，因为手工标注三维点云既耗时又昂贵，缺乏通用工具会限制大规模数据集的构建和研究进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 2D 领域的 SAM 模型和 CLIP 的跨模态嵌入，结合 3D 点云编码器 PTv3，并在此基础上引入域归一化、自动提示点生成和文本匹配等技术。通过对七个不同域的数据集进行联合训练，作者解决了跨域负迁移问题，并实现了多模态交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用统一的点云编码器和掩码解码器，同时处理空间提示和文本提示。流程为：①用 PTv3 + 域归一化编码点云；②对空间提示点进行编码并与点云特征融合；③解码器生成掩码、置信度和 CLIP 嵌入；④若为文本提示，先用自动提示点生成掩码候选，再将候选的 CLIP 嵌入与文本嵌入匹配，得到最终分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①域归一化（而非数据集归一化）以缓解多域负迁移；②统一模型支持室内、室外、空中三大域；③同时支持点提示和文本提示；④自动提示点生成实现无人工交互的全景分割；⑤预测掩码的 CLIP 嵌入实现开放词汇分割。与以往仅针对单一域或单一提示方式的模型相比，SNAP 在通用性和多模态交互上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SNAP 提供了一个跨域、支持点与文本双模态交互的统一点云分割框架，在室内、室外和空中场景中实现了领先的零样本性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Interactive 3D point cloud segmentation enables efficient annotation of complex 3D scenes through user-guided prompts. However, current approaches are typically restricted in scope to a single domain (indoor or outdoor), and to a single form of user interaction (either spatial clicks or textual prompts). Moreover, training on multiple datasets often leads to negative transfer, resulting in domain-specific tools that lack generalizability. To address these limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in \textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D segmentation that supports both point-based and text-based prompts across diverse domains. Our approach achieves cross-domain generalizability by training on 7 datasets spanning indoor, outdoor, and aerial environments, while employing domain-adaptive normalization to prevent negative transfer. For text-prompted segmentation, we automatically generate mask proposals without human intervention and match them against CLIP embeddings of textual queries, enabling both panoptic and open-vocabulary segmentation. Extensive experiments demonstrate that SNAP consistently delivers high-quality segmentation results. We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for spatial-prompted segmentation and demonstrate competitive results on all 5 text-prompted benchmarks. These results show that a unified model can match or exceed specialized domain-specific approaches, providing a practical tool for scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/&lt;/p&gt;</description></item><item><guid>2510.13245v2</guid><title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title><link>http://arxiv.org/abs/2510.13245v2</link><author>Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; • 研究目标是生成逼真且语义丰富的户外 3D 场景，用于城市仿真和自动驾驶等应用。  • 现有进展受限于缺乏公开、标注完善的数据集。  • 提出 SketchSem3D，首个大规模基于手绘草图和卫星图像伪标签的户外 3D 语义场景生成基准。  • SketchSem3D 包含 Sketch-based SemanticKITTI 与 Sketch-based KITTI-360 两个子集，提供 LiDAR 体素、草图及标注卫星图像。  • 设计 Cylinder Mamba Diffusion（CymbaDiff）模型，强化空间连贯性，捕捉圆柱连续性与垂直层级，保持物理邻域关系与全局上下文。  • 在 SketchSem3D 上的大量实验表明，CymbaDiff 在语义一致性、空间逼真度和跨数据集泛化方面均优于现有方法。  • 代码与数据集将公开发布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 户外 3D 语义场景生成需要逼真且语义丰富的环境，但缺乏公开且标注完善的数据集限制了研究进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个基于手绘草图和卫星图像伪标签的大规模户外 3D 语义场景生成基准 SketchSem3D，并提出 Cylinder Mamba Diffusion（CymbaDiff）模型以提升空间连贯性和生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SketchSem3D 包含两个子集：Sketch-based SemanticKITTI 与 Sketch-based KITTI-360，分别提供 LiDAR 体素、对应草图和伪标注卫星图像；CymbaDiff 在扩散模型中加入圆柱结构化空间排序，显式捕捉圆柱连续性与垂直层级，保持物理邻域关系与全局上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CymbaDiff 在 SketchSem3D 上实现了更高的语义一致性、更逼真的空间表现以及更好的跨数据集泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SketchSem3D 与 CymbaDiff 为户外 3D 语义场景生成提供了新的数据基准和方法，显著提升了生成质量，并将代码与数据集公开，促进后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 户外 3D 语义场景生成能够为城市仿真和自动驾驶等应用提供逼真且语义丰富的环境。然而，在该方向上的进展受到缺乏公开、标注完善的数据集的限制。我们提出 SketchSem3D，这是首个基于抽象手绘草图和卫星图像伪标签的大规模户外 3D 语义场景生成基准。SketchSem3D 包含两个子集：Sketch-based SemanticKITTI 和 Sketch-based KITTI-360（包含 LiDAR 体素以及对应的草图和标注卫星图像），以实现标准化、严格且多样化的评估。我们还提出 Cylinder Mamba Diffusion（CymbaDiff），显著提升户外 3D 场景生成的空间连贯性。CymbaDiff 强制执行结构化空间排序，显式捕捉圆柱连续性和垂直层级，并在生成的场景中保持物理邻域关系和全局上下文。对 SketchSem3D 的广泛实验表明，CymbaDiff 在语义一致性、空间逼真度和跨数据集泛化方面表现优异。代码和数据集将公开发布在 https://github.com/Lillian-research-hub/CymbaDiff。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在从自由手绘草图和伪标注的卫星图像中生成逼真且语义丰富的三维户外城市场景。此类生成在城市仿真、自动驾驶等应用中至关重要，但受限于缺乏公开、规模大且标注完善的数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先提出“基于草图的三维户外场景生成”这一新任务，并构建了SketchSem3D数据集。方法借鉴了状态空间模型（SSM）在长距离依赖建模中的优势，以及扩散模型在三维生成中的稳定性，结合了先前的BEV条件生成和多尺度策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将草图和伪标注的卫星图像作为条件，利用场景结构估计网络（SSEN）得到粗略结构，再通过潜在映射网络（LMN）压缩为潜在表示，随后使用带有圆柱形Mamba块（CylMa）的扩散去噪器（CymbaDiff）逐步生成完整的三维体素网格。整体流程为：输入草图+卫星图 → SSEN → LMN → CymbaDiff去噪 → 3D体素输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①提出基于草图的三维户外场景生成任务；②首个公开的SketchSem3D大规模基准数据集；③设计圆柱形Mamba块以强化空间连贯性和垂直层次；④在扩散框架中引入结构化空间排序。与以往仅使用BEV或多尺度室内场景的工作不同，本文实现了更自然的用户交互和更高的语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一个基于草图和卫星图像的三维户外场景生成框架，并通过圆柱形Mamba扩散模型实现了更真实、更语义一致的三维城市场景，同时发布了首个大规模SketchSem3D数据集。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Outdoor 3D semantic scene generation produces realistic and semantically rich environments for applications such as urban simulation and autonomous driving. However, advances in this direction are constrained by the absence of publicly available, well-annotated datasets. We introduce SketchSem3D, the first large-scale benchmark for generating 3D outdoor semantic scenes from abstract freehand sketches and pseudo-labeled annotations of satellite images. SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based KITTI-360 (containing LiDAR voxels along with their corresponding sketches and annotated satellite images), to enable standardized, rigorous, and diverse evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that significantly enhances spatial coherence in outdoor 3D scene generation. CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical continuity and vertical hierarchy, and preserves both physical neighborhood relationships and global context within the generated scenes. Extensive experiments on SketchSem3D demonstrate that CymbaDiff achieves superior semantic consistency, spatial realism, and cross-dataset generalization. The code and dataset will be available at https://github.com/Lillian-research-hub/CymbaDiff&lt;/p&gt;</description></item><item><guid>2510.13307v2</guid><title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title><link>http://arxiv.org/abs/2510.13307v2</link><author>Yang Li, Aming Wu, Zihao Zhang, Yahong Han</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种基于因果关系的点云分割新类别发现方法，利用已标注的基础类别信息来学习对未标注新类别的分割。通过结构因果模型和因果表示原型，消除隐藏混杂因素，实现从基础类别到新类别的因果推理，实验表明方法优于现有技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的新类别发现方法在点云分割中往往依赖粗糙或统计相关性，容易导致新类别推断混淆。需要更精确的点表示与类别标签之间的关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入因果关系约束，学习能够准确对应类别的点云表示，从而实现仅凭基础类别监督即可对新类别进行分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用结构因果模型分析基础类别表示中的隐藏混杂因素，构建消除混杂的因果表示原型；利用图结构建模基础类别与新类别原型之间的因果关系，实现因果推理；整体方法称为联合学习因果表示与推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验和可视化结果表明，该方法在3D和2D新类别发现语义分割任务中表现出更优的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过因果关系建模和因果表示学习，可以显著提升点云分割中新类别发现的效果，为相关任务提供了有效的技术方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文聚焦于点云分割的新类别发现（3D-NCD），旨在学习一种仅利用已标注基础类别监督即可对未标注（新）3D类别进行分割的模型。该任务的关键在于建立点表示与其基础类别标签之间的精确关联，以及基础类别与新类别点之间的表示关联。粗糙或统计相关性学习可能导致新类别推断混淆。如果我们在学习过程中强制施加因果关系作为一种强关联约束，则应能揭示准确对应类别的本质点云表示。为此，我们引入结构因果模型（SCM）重新表述3D-NCD问题，并提出一种新方法，即联合学习因果表示与推理。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别与新类别之间的因果关系。随后设计了消除混杂的因果表示原型，以捕捉基础类别的因果表示。随后使用图结构来建模基础类别因果表示原型与新类别原型之间的因果关系，从而实现从基础到新类别的因果推理。对3D和2D NCD语义分割的广泛实验与可视化结果表明，我们的方法具有优越性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在点云语义分割中，如何仅利用已标注的基类信息来发现并分割未标注的新类。该问题在自动驾驶、机器人等实际场景中至关重要，因为真实环境中会出现未知物体，传统的闭域假设无法满足需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先发现基类分类器往往学习到快捷特征（混淆因子），并认为新类是基类通过因果机制产生的变体。基于此，他们借鉴了结构因果模型、因果对抗去混淆以及 2D NCD 的聚类思路，设计了基于因果表示原型学习和图结构因果推理的完整框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先通过因果对抗去除基类特征中的混淆因子，得到干净的因果表示；随后为基类和新类分别学习原型，并将这些原型作为图节点构建因果图；通过因果剪枝和方向一致性约束优化图结构；最后将优化后的图输入图卷积网络，得到基类标签和新类伪标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次将因果学习引入 3D 新类发现；2) 提出因果表示原型学习和图结构因果推理；3) 通过因果对抗去混淆、因果剪枝和方向一致性约束实现更可靠的推理。与以往仅基于统计相似度的 3D NCD 方法不同，该方法显式建模因果关系并消除快捷特征，提升了新类分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种因果驱动的框架，学习无混淆因子表示并建模基类与新类之间的因果关系，从而实现对 3D 点云中未知类别的准确发现与分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation (3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes using only the supervision from labeled (base) 3D classes. The key to this task is to setup the exact correlations between the point representations and their base class labels, as well as the representation correlations between the points from base and novel classes. A coarse or statistical correlation learning may lead to the confusion in novel class inference. lf we impose a causal relationship as a strong correlated constraint upon the learning process, the essential point cloud representations that accurately correspond to the classes should be uncovered. To this end, we introduce a structural causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method, i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we first analyze hidden confounders in the base class representations and the causal relationships between the base and novel classes through SCM. We devise a causal representation prototype that eliminates confounders to capture the causal representations of base classes. A graph structure is then used to model the causal relationships between the base classes&amp;#x27; causal representation prototypes and the novel class prototypes, enabling causal reasoning from base to novel classes. Extensive experiments and visualization results on 3D and 2D NCD semantic segmentation demonstrate the superiorities of our method.&lt;/p&gt;</description></item><item><guid>2510.15018v1</guid><title>UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</title><link>http://arxiv.org/abs/2510.15018v1</link><author>Mingxuan Liu, Honglin He, Elisa Ricci, Wayne Wu, Bolei Zhou</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; UrbanVerse 是一种基于真实城市视频的仿真系统，能够生成高质量、物理感知的交互式城市场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市中出现了许多具身 AI 代理，如送货机器人和四足机器人，它们需要在多样且真实的城市环境中训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有人工或程序生成的仿真场景缺乏可扩展性或无法捕捉真实复杂性的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; UrbanVerse 包含 100k+ 注释的 3D 资产库 UrbanVerse-100K，以及自动化管道 UrbanVerse-Gen，能够从城市旅游视频中提取布局并实例化度量级 3D 仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 UrbanVerse 场景在语义和布局上与真实世界保持一致，且在仿真和零样本仿真到真实迁移中，训练的导航策略比之前方法提升了 6.3% 和 30.1%，并在真实 300 米任务中仅需两次干预。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UrbanVerse 提供了可扩展、真实感强的城市仿真环境，显著提升了具身 AI 代理的训练效果和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本论文旨在解决如何为城市导航机器人提供规模化、真实感强的训练环境。传统的手工或程序生成的模拟场景缺乏真实世界的复杂性或无法大规模扩展，而真实城市视频却包含丰富多样的布局与细节。通过将这些视频转化为可交互的物理模拟，可大幅提升机器人在真实城市中的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先构建了一个包含10万+物理属性标注的3D资产库UrbanVerse‑100K，并结合开源的数字孪生技术与现有的城市模拟平台。随后设计了UrbanVerse‑Gen流水线，利用开源的视觉基础模型（如YoloWorld、SAM 2）和SfM技术从未标注的视频中提取语义与几何信息。该方法借鉴了数字孪生、3D Gaussian Splatting以及之前的UrbanSim、MetaUrban等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是把城市旅游视频“数字孪生”为物理可交互的模拟场景。实现流程分为三步：①从视频中通过深度估计和语义检测生成包含物体、地面和天空的场景图；②在UrbanVerse‑100K中检索与场景图匹配的资产并进行多样化；③将匹配的资产按空间信息拼装到IsaacSim中，得到完整的交互式模拟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①构建了规模化、物理属性丰富的UrbanVerse‑100K资产库；②提出了自动化的UrbanVerse‑Gen流水线，可从未标注视频中生成完整的交互式场景；③生成了160个跨24国的真实分布场景，并在导航任务中展示了规模化学习的功效和显著的零样本转移。与以往手工或程序化生成的模拟器不同，本工作直接利用真实视频并保持物理真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanVerse通过将城市旅游视频转化为物理真实、可交互的模拟场景，并提供大规模资产库，显著提升了城市机器人训练的真实性与泛化能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.&lt;/p&gt;</description></item><item><guid>2510.16865v1</guid><title>Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection</title><link>http://arxiv.org/abs/2510.16865v1</link><author>Yuyang Yu, Zhengwei Chen, Xuemiao Xu, Lei Zhang, Haoxin Yang, Yongwei Nie, Shengfeng He</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D点云异常检测关键，现有记忆库方法在特征变换一致性、局部几何细节捕捉和旋转不变性方面存在局限。本文提出将点云配准与记忆库异常检测结合的注册诱导旋转不变特征提取框架，通过在配准学习中嵌入特征提取，实现对齐与表示学习的联合优化，从而获得既旋转不变又局部判别力强的特征。实验表明该方法在两个数据集上均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云异常检测在工业质量控制中至关重要，但现有基于记忆库的方法在特征转换一致性和判别能力方面存在局限，尤其在捕捉局部几何细节和实现旋转不变性方面表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将点云配准与异常检测任务结合，提出一种能够同时实现几何对齐和旋转不变、局部判别特征提取的框架，以提升异常检测的可靠性和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在配准学习过程中嵌入特征提取模块，联合优化配准目标和记忆库异常检测目标，利用局部几何结构和样本间特征相似性来学习旋转不变且判别力强的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Anomaly-ShapeNet和Real3D-AD数据集上，所提方法在效果和泛化性上均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将配准与特征学习联合优化能够显著提升3D点云异常检测的性能，尤其在旋转不变性和局部细节捕捉方面表现突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云数据中的异常检测对于工业质量控制至关重要，目标是以高可靠性识别结构缺陷。然而，当前基于记忆库的方法往往存在特征转换不一致和判别能力有限的问题，尤其在捕捉局部几何细节和实现旋转不变性方面。若配准失败，这些局限性会更加明显，导致检测结果不可靠。我们认为点云配准不仅在对齐几何结构方面起关键作用，还能引导特征提取朝向旋转不变且局部判别的表示。为此，我们提出了一种由配准诱导的旋转不变特征提取框架，将点云配准和基于记忆库的异常检测目标整合在一起。我们的核心见解是，这两项任务都依赖于对局部几何结构的建模以及利用样本间的特征相似性。通过将特征提取嵌入配准学习过程，我们的框架实现了对齐与表示学习的联合优化。这种整合使网络能够获得既对旋转鲁棒又高度有效的异常检测特征。在Anomaly-ShapeNet和Real3D-AD数据集上进行的大量实验表明，我们的方法在效果和泛化性方面始终优于现有方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提高3D点云异常检测的准确性，尤其是在点云旋转和局部几何细节变化下的鲁棒性。该问题在工业质量控制中至关重要，因为缺陷往往表现为细微的几何偏差，且点云采集时可能存在任意旋转。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到点云配准与基于记忆库的异常检测都需要局部几何建模和特征相似性，决定将配准任务嵌入特征学习中。方法借鉴了FPFH、RANSAC、粗细配准、Geometric Transformer以及现有的记忆库异常检测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过配准引导的特征学习获得旋转不变且局部判别力强的表示。训练阶段生成随机变换的点云对，进行多尺度采样，构造匹配关系，使用RIConv++、KPConv‑FPN和Geometric Transformer提取特征并优化配准损失；推理阶段计算测试点云与原型的配准矩阵，提取旋转不变特征，与记忆库进行归一化比较得到异常分数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 将配准任务与异常检测联合训练，形成Reg2Inv框架；2) 设计旋转不变的特征提取器，兼顾局部细节与全局结构；3) 在推理时使用配准矩阵对齐后再进行记忆库比较。与以往将配准作为预处理或使用不具备旋转不变性的编码器不同，本文通过配准学习直接提升特征鲁棒性和检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Reg2Inv通过配准引导的特征学习，生成旋转不变且局部判别力强的表示，从而显著提升3D点云异常检测的准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D anomaly detection in point-cloud data is critical for industrial quality control, aiming to identify structural defects with high reliability. However, current memory bank-based methods often suffer from inconsistent feature transformations and limited discriminative capacity, particularly in capturing local geometric details and achieving rotation invariance. These limitations become more pronounced when registration fails, leading to unreliable detection results. We argue that point-cloud registration plays an essential role not only in aligning geometric structures but also in guiding feature extraction toward rotation-invariant and locally discriminative representations. To this end, we propose a registration-induced, rotation-invariant feature extraction framework that integrates the objectives of point-cloud registration and memory-based anomaly detection. Our key insight is that both tasks rely on modeling local geometric structures and leveraging feature similarity across samples. By embedding feature extraction into the registration learning process, our framework jointly optimizes alignment and representation learning. This integration enables the network to acquire features that are both robust to rotations and highly effective for anomaly detection. Extensive experiments on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method consistently outperforms existing approaches in effectiveness and generalizability.&lt;/p&gt;</description></item><item><guid>2510.17568v3</guid><title>PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception</title><link>http://arxiv.org/abs/2510.17568v3</link><author>Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; PAGE-4D 是一种前馈模型，扩展了 VGGT 以处理动态场景，能够在不需要后处理的情况下完成相机姿态估计、深度预测和点云重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的 3D 前馈模型（如 VGGT）在静态数据集上训练，擅长推断静态场景的 3D 属性，但在包含移动人类或可变形物体（如伞）的真实动态场景中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决静态训练导致的动态场景推断困难，使模型能够在动态环境中准确估计相机姿态、深度和几何结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在 VGGT 的基础上引入 PAGE-4D，并设计了一个动态感知聚合器，该聚合器通过预测动态感知掩码来分离静态与动态信息，从而在相机姿态估计时抑制运动线索，在几何重建时放大运动线索，实现多任务 4D 重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，PAGE-4D 在动态场景中始终优于原始 VGGT，在相机姿态估计、单目和视频深度估计以及稠密点图重建方面取得了更好的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PAGE-4D 通过动态感知掩码有效解决了多任务 4D 重建中的冲突，显著提升了动态场景下的 3D 预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近的 3D 前馈模型（如 Visual Geometry Grounded Transformer，VGGT）在推断静态场景的 3D 属性方面表现出强大的能力。然而，由于它们通常在静态数据集上训练，这些模型在涉及复杂动态元素（如移动的人类或可变形物体如伞）的真实世界场景中往往表现不佳。为了解决这一限制，我们提出了 PAGE-4D，这是一种前馈模型，扩展了 VGGT 以适应动态场景，能够在不需要后处理的情况下完成相机姿态估计、深度预测和点云重建。多任务 4D 重建的核心挑战在于任务之间的固有冲突：准确的相机姿态估计需要抑制动态区域，而几何重建则需要对其建模。为了解决这一张力，我们提出了一个动态感知聚合器，通过预测动态感知掩码来分离静态和动态信息——在相机姿态估计中抑制运动线索，在几何重建中放大它们。大量实验表明，PAGE-4D 在动态场景中始终优于原始 VGGT，在相机姿态估计、单目和视频深度估计以及稠密点图重建方面取得了更好的结果。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从RGB图像序列中同时估计相机姿态、深度和点云的任务，尤其是在包含移动或变形物体的动态场景中。传统的静态场景模型在动态环境下表现不佳，导致姿态估计误差增大、几何重建不完整，这在自动驾驶、机器人导航等实际应用中是关键瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了VGGT在动态场景下的失败机制，发现姿态估计需要抑制动态区域，而几何重建则需要利用动态信息。基于此，他们借鉴VGGT的Transformer架构，加入动态感知聚合器和动态掩码预测模块，并采用针对性微调仅更新跨帧注意力层的中间层，保持模型轻量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过动态掩码将动态与静态信息解耦：对姿态估计抑制动态区域，对几何重建强调动态区域。实现流程包括：1）使用预训练的DINO编码器提取图像特征；2）三阶段聚合器（帧注意力、全局注意力、动态感知全局注意力）融合空间与时间信息；3）动态掩码预测模块生成动态掩码；4）分别通过轻量解码器得到深度、点云和姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①动态感知聚合器与动态掩码实现任务间动态信息解耦；②仅微调VGGT中间跨帧注意力层，保持模型高效；③在单一前向推理中同时输出姿态、深度和点云，避免模块化流水线。与以往需要多阶段或对静态假设的模型不同，PAGE‑4D在动态环境下保持高精度且计算开销极低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAGE‑4D通过动态掩码解耦动态与静态信息，扩展VGGT为统一的前向模型，在动态场景中实现高效准确的相机姿态、深度和点云估计。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.&lt;/p&gt;</description></item><item><guid>2510.22313v1</guid><title>Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis</title><link>http://arxiv.org/abs/2510.22313v1</link><author>Chen Zhiqiang, Le Gentil Cedric, Lin Fuling, Lu Minghao, Qiao Qiyuan, Xu Bowen, Qi Yuhua, Lu Peng</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对动态环境下的激光雷达-惯性里程计（LIO）挑战，提出一种将动态感知直接融入点云配准的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统LIO算法基于静态世界假设，在动态物体占主导且几何稀疏的场景中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决动态LIO中需要可靠静态特征识别与精确位姿估计相互依赖的循环问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入动态感知的迭代最近点算法，利用时空法线分析和高效的空间一致性验证来构建静态地图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该方法在几何结构有限的动态环境中显著优于现有最先进的LIO系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的动态感知配准方法有效提升了动态场景下的定位精度，为LIO在复杂环境中的应用奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文解决了动态环境下激光雷达-惯性里程计（LIO）的挑战，传统方法因静态世界假设而在动态物体占主导的场景中失效，尤其在几何稀疏环境中表现不佳。当前动态LIO方法面临的根本难题是：准确定位需要可靠的静态特征识别，而区分动态物体又需要精确的位姿估计。我们的方案通过将动态感知直接集成到点云配准过程中，打破了这一循环依赖。我们提出了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并辅以高效的空间一致性验证方法来提升静态地图构建。实验评估显示，在几何结构有限的动态环境中，该方法相较于最先进的LIO系统取得了显著的性能提升。代码和数据集可在 https://github.com/thisparticle/btsa 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决激光雷达-惯性里程计（LIO）在动态环境中的定位与建图问题。传统LIO方法假设环境静止，移动物体会导致姿态估计和地图构建误差，尤其在动态物体占比高或几何结构稀疏的场景中更为严重。准确的动态环境定位对机器人导航、自动驾驶等实际应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现姿态估计与动态物体检测之间存在循环依赖，传统方法往往先检测再定位或先定位再检测。为打破这一循环，他们将动态感知直接嵌入点云配准过程，并利用时空法向量进行动态点判别。该思路借鉴了ICP、时空法向量分析（[28],[29]）以及现有的动态检测与LIO框架（如FAST‑LIO2、LOAM）等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过时空法向量对每个点进行动态/静态分类，并在ICP配准时仅使用静态点进行姿态优化。实现流程包括：①预处理并去畸变的点云；②利用时间滑动窗口地图计算时空法向量；③根据法向量的时间分量筛选稳定点；④在全局静态地图上进行点到平面ICP优化姿态；⑤更新时间与全局地图并完成下一帧处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①将动态感知与ICP配准耦合，使用时空法向量实现动态点即时剔除；②通过空间一致性验证提升静态地图质量；③在几何稀疏、动态占比高的场景中实现鲁棒定位；④公开了新的动态环境数据集和代码。与以往先预处理或后处理动态检测的方法不同，该框架在配准循环内实时完成动态点判别，消除了姿态估计与动态检测的相互依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种将时空法向量嵌入ICP的动态感知LIO框架，在高度动态、几何稀疏的环境中实现稳健的姿态估计与静态地图构建。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper addresses the challenge of Lidar-Inertial Odometry (LIO) in dynamic environments, where conventional methods often fail due to their static-world assumptions. Traditional LIO algorithms perform poorly when dynamic objects dominate the scenes, particularly in geometrically sparse environments. Current approaches to dynamic LIO face a fundamental challenge: accurate localization requires a reliable identification of static features, yet distinguishing dynamic objects necessitates precise pose estimation. Our solution breaks this circular dependency by integrating dynamic awareness directly into the point cloud registration process. We introduce a novel dynamic-aware iterative closest point algorithm that leverages spatio-temporal normal analysis, complemented by an efficient spatial consistency verification method to enhance static map construction. Experimental evaluations demonstrate significant performance improvements over state-of-the-art LIO systems in challenging dynamic environments with limited geometric structure. The code and dataset are available at https://github.com/thisparticle/btsa.&lt;/p&gt;</description></item><item><guid>2510.23416v1</guid><title>Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation</title><link>http://arxiv.org/abs/2510.23416v1</link><author>Marco Antonio Ortiz Rincon, Yihui Yang, Christoph Holst</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出一种新型工作流程，用于在城市街道场景中高效准确地将大规模移动激光扫描点云与目标模型点云进行配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市环境复杂，点云密度、噪声和遮挡差异大，传统配准方法难以满足需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够克服上述挑战、提高配准精度并降低计算时间的工作流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入半球检查预处理技术SSC，将MLS轨迹按互相垂直的平面分割；随后使用基于平面体素的通用ICP PV-GICP，仅利用体素内的平面特征进行细化配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该工作流程在慕尼黑市中心数据集上平均配准误差低于0.01米，计算时间比传统点到平面ICP减少超过50%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法显著提升了城市三维建模与更新的自动化水平，可应用于城市规划、基础设施管理和动态城市监测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究提出一种新颖的工作流程，旨在高效且准确地将大规模移动激光扫描（MLS）点云与目标模型点云在城市街道场景中进行配准。该流程专门针对城市环境的复杂性，并巧妙解决了在繁忙城市中心常见的点云密度、噪声特征和遮挡场景差异所带来的挑战。我们引入了两项方法创新。首先，提出的半球检查（SSC）预处理技术通过识别互相垂直的平面表面，最优地将MLS轨迹数据分割，从而降低MLS漂移对整个点云配准精度的影响，并确保每个碎片内具有足够的几何特征以避免局部最小值。其次，我们提出基于平面体素的通用迭代最近点（PV-GICP）细化配准方法，该方法在体素划分内选择性利用平面表面。该预处理策略不仅提升了配准精度，还将计算时间相比传统点到平面ICP方法降低了50%以上。对慕尼黑市中心真实数据集的实验表明，我们的工作流程实现了平均配准精度低于0.01米，并显著缩短了处理时间。结果强调了所提出方法在推进自动化三维城市建模与更新方面的潜力，并直接适用于城市规划、基础设施管理和动态城市监测。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在城市街道场景中将大规模移动激光扫描（MLS）点云与参考模型点云精确、快速配准的问题。该问题重要，因为城市3D模型需要频繁更新以支持规划、基础设施管理和动态监测，而传统手工或现有配准方法在大规模、噪声多、重叠少的点云中效率低、精度差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有碎片化、粗配准和细配准技术的局限，借鉴了等距/等时间碎片化、ICP、GICP、K-means聚类、图可靠性去噪等方法。基于这些工作，他们提出了半球检查（SSC）来自适应碎片化，并设计了只使用平面体素的PV-GICP来加速细配准，形成完整的工作流。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先通过SSC确保每个碎片包含足够的正交平面以减小漂移，再用ISS+FPFH+GROR做粗配准，最后用PV-GICP在平面体素上做细配准。整体流程包括预处理（重采样、去噪、语义过滤）、初始时间/空间碎片化、SSC验证、粗配准、细配准、结果合并。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) SSC自适应碎片化，保证碎片中有三正交平面，显著降低漂移；2) PV-GICP只在平面体素上执行GICP，既提高精度又将计算时间缩短超过50%；3) 通过漂移分析评估并改进MLS源点云。与以往使用等距碎片或全局ICP的做法不同，本文在碎片化和细配准上引入了几何特征筛选和体素平面选择，取得了亚厘米精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种自适应碎片化与平面体素GICP相结合的配准流程，能够在大规模城市MLS点云中实现亚厘米级精度，同时将处理时间缩短一半以上。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This study presents a novel workflow designed to efficiently and accurately register large-scale mobile laser scanning (MLS) point clouds to a target model point cloud in urban street scenarios. This workflow specifically targets the complexities inherent in urban environments and adeptly addresses the challenges of integrating point clouds that vary in density, noise characteristics, and occlusion scenarios, which are common in bustling city centers. Two methodological advancements are introduced. First, the proposed Semi-sphere Check (SSC) preprocessing technique optimally fragments MLS trajectory data by identifying mutually orthogonal planar surfaces. This step reduces the impact of MLS drift on the accuracy of the entire point cloud registration, while ensuring sufficient geometric features within each fragment to avoid local minima. Second, we propose Planar Voxel-based Generalized Iterative Closest Point (PV-GICP), a fine registration method that selectively utilizes planar surfaces within voxel partitions. This pre-process strategy not only improves registration accuracy but also reduces computation time by more than 50% compared to conventional point-to-plane ICP methods. Experiments on real-world datasets from Munich&amp;#x27;s inner city demonstrate that our workflow achieves sub-0.01 m average registration accuracy while significantly shortening processing times. The results underscore the potential of the proposed methods to advance automated 3D urban modeling and updating, with direct applications in urban planning, infrastructure management, and dynamic city monitoring.&lt;/p&gt;</description></item><item><guid>2511.00260v1</guid><title>MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba</title><link>http://arxiv.org/abs/2511.00260v1</link><author>Linzhe Jiang, Jiayuan Huang, Sophia Bano, Matthew J. Clarkson, Zhehua Mao, Mobarak I. Hoque</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对内镜导航的3D点云配准方法，并构建了大规模临床数据集，用以评估和提升配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确的3D点云配准是可靠的影像引导结肠镜检查的基础，直接影响病变定位、切缘评估和导航安全。然而，生物组织的重复纹理和局部几何同质性导致特征退化，术前解剖与术中观测之间的显著域差进一步削弱配准稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述临床关键挑战，提出一种新型的无对应关系3D配准框架，并提供高质量、临床基准的数据集，以实现严格可重复的评测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建C3VD-Raycasting-10k数据集，包含一万零十四对几何对齐的点云；提出MambaNetLK框架，在PointNetLK基础上加入Mamba状态空间模型作为跨模态特征提取器，利用线性时间复杂度捕获长程依赖，并通过Lucas‑Kanade迭代实现配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在C3VD-Raycasting-10k数据集上，MambaNetLK相较于现有最优方法，旋转误差中位数降低56.04%，平移误差RMSE降低26.19%；在ModelNet40上表现出良好泛化能力，并对初始姿态扰动具有更强鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MambaNetLK为外科导航中的3D配准提供了稳健的基础，其基于SSM的全局特征提取器与大规模临床数据集的结合，可显著提升微创手术如结肠镜检查的引导系统的准确性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在结肠镜手术中将实时内镜点云与预手术CT模型对齐的难题。准确的三维配准直接影响病变定位、切除边缘评估和导航安全，因而在临床和研究中至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在PointNetLK的基础上引入Mamba状态空间模型，以捕捉全局几何依赖，并将其嵌入逆式Lucas‑Kanade迭代求解器。该设计借鉴了PointNetLK、Mamba、IC‑LK以及现有的医学点云配准方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用无对应关系的全局特征提取器（MambaNet）与IC‑LK求解器配合，迭代最小化源点云与目标点云特征向量的差异。流程包括：1）用共享权重的MambaNet分别编码源点云和目标点云；2）在目标点云上预计算雅可比矩阵；3）在特征空间中迭代求解增量变换并更新源点云姿态，直至收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）MambaNetLK框架，将Mamba状态空间模型用于点云编码；2）无对应关系的IC‑LK配准实现；3）新建C3VD‑Raycasting‑10k临床基准数据集；4）在临床数据上实现显著的旋转误差和位移误差下降，并在大初始旋转下保持鲁棒性。与以往方法相比，它在特征提取上突破了MLP局部感受野的限制，并提供了更高质量的评估基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MambaNetLK通过将状态空间模型编码器与Lucas‑Kanade迭代求解器相结合，实现了无对应关系的高精度结肠镜点云配准，并提供了新的临床基准数据集。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.&lt;/p&gt;</description></item><item><guid>2511.00635v1</guid><title>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</title><link>http://arxiv.org/abs/2511.00635v1</link><author>Hyungtae Lim, Daebeom Kim, Hyun Myung</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的多会话同步定位与地图构建框架 Multi-Mapcher，利用大规模地图对地图配准实现跨会话初始对齐，并通过锚点姿态图优化构建一致的全局地图，显著提升不同 LiDAR 传感器下的性能并加快速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着多种 3D LiDAR 传感器的出现，针对异构 LiDAR 的多会话同步定位与地图构建（MSS）研究日益活跃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 挑战传统依赖循环闭合检测的 MSS 方法，提出一种不依赖循环检测模块的框架，以提高跨会话对齐的鲁棒性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先使用鲁棒的 3D 点云配准实现大规模地图对地图的初始对齐；随后在初始对齐足够精确的前提下，通过半径搜索寻找跨会话循环；最后采用基于锚点的鲁棒姿态图优化构建一致的全局地图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该方法在使用不同 LiDAR 传感器捕获的会话中，MSS 性能显著优于现有方法，并且速度更快。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 利用大规模地图对地图配准和锚点姿态图优化，可有效提升异构 LiDAR 多会话同步定位与地图构建的性能与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着多种 3D 激光雷达（LiDAR）传感器的上市，针对异构 LiDAR 传感器的多会话同步定位与地图构建（MSS）研究已得到积极开展。现有的 MSS 方法大多依赖循环闭合检测来实现跨会话对齐；然而，由于不同会话中使用的传感器在点云密度和视场（FoV）上的差异，循环闭合检测的性能可能会受到影响。本文挑战了过度依赖循环检测模块的传统范式，提出了一种名为 Multi-Mapcher 的新型 MSS 框架，该框架利用大规模地图对地图配准来完成跨会话的初始对齐，尽管这通常被认为不可行，但通过使用鲁棒的 3D 点云配准实现。随后，在假设跨会话初始对齐足够精确的前提下，采用半径搜索寻找跨会话循环；随后采用基于锚点的鲁棒姿态图优化构建一致的全局地图。实验表明，我们的方法在使用不同 LiDAR 传感器捕获的会话中，MSS 性能显著优于现有方法，并且速度更快。我们的代码可在 https://github.com/url-kaist/multi-mapcher 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了多会话SLAM中不同类型LiDAR传感器之间的地图对齐问题。传统方法依赖循环闭合检测，而不同传感器的点云密度和视场差异会导致检测失效，影响地图一致性。对于需要长期更新地图的自动驾驶系统，这一问题尤为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为循环闭合检测不适用于异构LiDAR，提出利用鲁棒点云配准实现大规模地图对齐。方法借鉴了anchor node姿态图优化、outlier-robust registration以及之前的多会话框架（如Kim &amp;amp; Kim、Hydra-Multi等）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过鲁棒配准在地图层面完成初始对齐，然后在扫描层面检测并筛选跨会话循环。实现流程包括：单会话SLAM、地图对齐、半径搜索循环、误循环剔除、anchor node姿态图优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①完全去除循环闭合检测依赖；②在地图层面使用鲁棒配准实现异构LiDAR的初始对齐；③anchor node姿态图优化提升全局一致性；④在大规模、动态环境下保持鲁棒性并加速计算。与以往方法相比，它更适用于异构传感器且更高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Multi-Mapcher 提供了一种无循环闭合检测、鲁棒配准驱动的多会话SLAM框架，能够高效合并异构LiDAR地图并生成一致的全局地图。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted. Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions. In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration. Next, after finding inter-session loops by radius search based on the assumption that the inter-session initial alignment is sufficiently precise, anchor node-based robust pose graph optimization is employed to build a consistent global map. As demonstrated in our experiments, our approach shows substantially better MSS performance for various LiDAR sensors used to capture the sessions and is faster than state-of-the-art approaches. Our code is available at https://github.com/url-kaist/multi-mapcher.&lt;/p&gt;</description></item><item><guid>2511.05853v1</guid><title>Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology</title><link>http://arxiv.org/abs/2511.05853v1</link><author>Bingyang Guo, Qiang Zuo, Ruiyun Yu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 3D数据分割对工业应用至关重要，尤其在集成电路缺陷检测中。 2. 陶瓷封装基板（CPS）因其优异的物理化学特性在IC封装中占据重要位置，但其复杂结构和细微缺陷以及缺乏公开数据集阻碍了缺陷检测研究。 3. 本研究构建了高质量点云数据集CPS3D‑Seg，包含1300个样本、20个产品类别，并提供精确的点级标注。 4. 对现有最先进点云分割算法进行了全面基准测试。 5. 提出了基于因果推断的3D分割方法CINet，利用结构细化（SR）和质量评估（QA）模块量化点云中的潜在混杂因素。 6. 大量实验表明CINet在mIoU和准确率上显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D分割在工业领域，尤其是IC缺陷检测中具有关键作用；陶瓷封装基板因其优异特性被广泛使用，但其复杂结构和细微缺陷以及缺乏公开数据集限制了缺陷检测技术的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建高质量的CPS点云分割数据集CPS3D‑Seg，并通过基准测试验证其有效性，同时提出一种新的分割方法CINet以提升分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①收集并标注1300个点云样本，覆盖20个产品类别，形成CPS3D‑Seg数据集；②对现有最先进点云分割算法进行全面基准评估；③设计CINet模型，结合因果推断思想，使用结构细化（SR）和质量评估（QA）模块来量化并消除点云中的潜在混杂因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CINet在mIoU和准确率指标上显著优于现有算法，验证了因果推断方法在点云分割中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CPS3D‑Seg数据集为CPS表面缺陷检测提供了高质量的基准资源，CINet方法通过因果推断显著提升了3D分割性能，为工业缺陷检测提供了新的技术路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 有效的3D数据分割对于广泛的工业应用至关重要，尤其是在集成电路领域检测细微缺陷。陶瓷封装基板（CPS）作为重要的电子材料，因其优异的物理和化学特性在IC封装中必不可少。然而，CPS的复杂结构和细微缺陷以及缺乏公开数据集，严重阻碍了CPS表面缺陷检测的发展。本研究构建了高质量的点云数据集CPS3D‑Seg，用于CPS表面缺陷的3D分割，该数据集在点分辨率和精度方面优于现有的3D工业数据集。CPS3D‑Seg包含1300个点云样本，涵盖20个产品类别，每个样本都提供精确的点级标注。同时，我们基于最先进的点云分割算法进行了全面基准测试，以验证CPS3D‑Seg的有效性。此外，我们提出了一种基于因果推断的3D分割方法CINet，通过结构细化（SR）和质量评估（QA）模块量化点云中的潜在混杂因素。大量实验表明，CINet在mIoU和准确率上显著优于现有算法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决集成电路陶瓷封装基板（CPS）表面缺陷的三维点云分割问题。缺陷会导致器件失效，且传统二维方法缺乏深度信息，难以准确捕捉微小缺陷；因此需要高分辨率的三维数据和有效的分割算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先构建了高精度点云数据集 CPS3D‑Seg，采用多头激光扫描和精细标注；随后对现有最先进的点云分割算法进行基准测试。为克服点云中的潜在混杂因素，作者借鉴因果推断理论，提出基于结构因果模型的 CINet，利用结构细化（SR）和质量评估（QA）模块实现后门调整。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将因果推断引入点云分割，通过学习 SR 和 QA 模块来量化并消除潜在混杂因素，从而提升分割精度。实现流程包括：数据采集与配准 → 网格滤波去冗余 → 点云标注 → 训练 CINet（包含 SR、QA 与分割头） → 在 CPS3D‑Seg 上评估并与其他算法比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 提供了最高分辨率、最高精度的 CPS 点云分割数据集 CPS3D‑Seg；2) 建立了完整的基准评估框架；3) 首次将结构因果模型应用于三维点云分割，提出 CINet 并显著提升 mIoU 与准确率。与以往仅关注二维或通用工业数据集的工作不同，本文聚焦于陶瓷基板的细微缺陷，并通过因果推断解决点云噪声与结构误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提供了高精度陶瓷基板点云缺陷分割数据集，并提出基于因果推断的 CINet，显著提升了三维缺陷分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The effective segmentation of 3D data is crucial for a wide range of industrial applications, especially for detecting subtle defects in the field of integrated circuits (IC). Ceramic package substrates (CPS), as an important electronic material, are essential in IC packaging owing to their superior physical and chemical properties. However, the complex structure and minor defects of CPS, along with the absence of a publically available dataset, significantly hinder the development of CPS surface defect detection. In this study, we construct a high-quality point cloud dataset for 3D segmentation of surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution and precision compared to existing 3D industrial datasets. CPS3D-Seg consists of 1300 point cloud samples under 20 product categories, and each sample provides accurate point-level annotations. Meanwhile, we conduct a comprehensive benchmark based on SOTA point cloud segmentation algorithms to validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D segmentation method based on causal inference (CINet), which quantifies potential confounders in point clouds through Structural Refine (SR) and Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet significantly outperforms existing algorithms in both mIoU and accuracy.&lt;/p&gt;</description></item><item><guid>2511.05965v1</guid><title>Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration</title><link>http://arxiv.org/abs/2511.05965v1</link><author>Zhixin Cheng, Xiaotian Yin, Jiacheng Deng, Bohao Liao, Yujia Chen, Xu Zhou, Baoqun Yin, Tianzhu Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新的跨模态配准框架，旨在解决传统基于Transformer的检测自由方法在噪声环境下的匹配不准问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法在噪声干扰下难以准确计算相似度，且缺乏有效的跨模态信息选择机制，导致配准鲁棒性和精度受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过设计迭代代理选择和可靠代理交互两大模块，提升结构特征感知和跨模态交互的可靠性，从而提高配准的鲁棒性和准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 迭代代理选择（IAS）模块利用相位图增强结构特征感知，并采用强化学习原理高效挑选可靠代理。2) 可靠代理交互（RAI）模块利用已选代理引导跨模态交互，减少匹配错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在RGB-D Scenes v2和7-Scenes基准上，所提方法在多种挑战条件下均实现了持续领先的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架通过有效的代理选择与交互显著提升了检测自由图像到点云配准的鲁棒性和精度，达到了当前最先进水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Typical detection-free methods for image-to-point cloud registration leverage transformer-based architectures to aggregate cross-modal features and establish correspondences. However, they often struggle under challenging conditions, where noise disrupts similarity computation and leads to incorrect correspondences. Moreover, without dedicated designs, it remains difficult to effectively select informative and correlated representations across modalities, thereby limiting the robustness and accuracy of registration. To address these challenges, we propose a novel cross-modal registration framework composed of two key modules: the Iterative Agents Selection (IAS) module and the Reliable Agents Interaction (RAI) module. IAS enhances structural feature awareness with phase maps and employs reinforcement learning principles to efficiently select reliable agents. RAI then leverages these selected agents to guide cross-modal interactions, effectively reducing mismatches and improving overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes benchmarks demonstrate that our method consistently achieves state-of-the-art performance.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注图像与点云之间的配准，解决在噪声、重复结构和光照变化等挑战下的特征匹配错误。该问题在三维重建、SLAM和视觉定位等应用中至关重要，因为精确的配准是获取可靠三维信息的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在检测自由方法的基础上，借鉴了Transformer跨模态特征聚合、2D3D-MATR的粗细匹配思路，并引入傅里叶相位增强、强化学习奖励机制和三阶段代理优化，以更好地选择可靠特征并降低噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用相位图提升图像的结构感知，再通过三阶段代理优化（预热、奖励引导、软掩码）挑选可靠代理，随后用这些代理引导跨模态特征交互，逐步生成稠密对应并用PnP+RANSAC估计位姿。实现流程包括特征提取、相位增强、代理初始化与训练、代理选择、代理交互、细化匹配与位姿求解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 相位图增强图像结构特征；2) 三阶段代理优化策略，结合局部/全局奖励动态平衡；3) 代理引导的交互替代传统Transformer，降低噪声并提升效率。与以往固定代理或纯Transformer融合的做法不同，A2SI实现了自适应代理选择和更稳健的特征聚合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; A2SI通过相位增强与强化学习驱动的自适应代理选择，提出了一种高效、鲁棒的图像-点云配准框架，刷新了相关基准的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Typical detection-free methods for image-to-point cloud registration leverage transformer-based architectures to aggregate cross-modal features and establish correspondences. However, they often struggle under challenging conditions, where noise disrupts similarity computation and leads to incorrect correspondences. Moreover, without dedicated designs, it remains difficult to effectively select informative and correlated representations across modalities, thereby limiting the robustness and accuracy of registration. To address these challenges, we propose a novel cross-modal registration framework composed of two key modules: the Iterative Agents Selection (IAS) module and the Reliable Agents Interaction (RAI) module. IAS enhances structural feature awareness with phase maps and employs reinforcement learning principles to efficiently select reliable agents. RAI then leverages these selected agents to guide cross-modal interactions, effectively reducing mismatches and improving overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes benchmarks demonstrate that our method consistently achieves state-of-the-art performance.&lt;/p&gt;</description></item><item><guid>2511.07978v2</guid><title>DANCE: Density-agnostic and Class-aware Network for Point Cloud Completion</title><link>http://arxiv.org/abs/2511.07978v2</link><author>Da-Yeong Kim, Yeong-Jun Cho</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; DANCE是一种密度无关、类别感知的点云补全框架，利用多视角射线采样生成候选点，变压器解码器细化位置并预测不透明度，轻量级分类头提供语义指导，最终在PCN和MVP基准上实现了更高的准确性和结构一致性，并对输入密度和噪声具有鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全需要从不完整的3D扫描中恢复缺失几何结构，现有方法往往假设固定密度或依赖图像表示，难以适应真实场景中的可变稀疏度和有限监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种既能补全缺失区域又能保留已观测几何的框架，兼顾密度无关性和类别一致性，并在无外部图像监督的条件下实现高质量补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DANCE通过多视角射线采样生成候选点，变压器解码器细化点位并预测不透明度分数，轻量级分类头在几何特征上训练以提供语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在PCN和MVP基准上，DANCE在准确性和结构一致性方面优于现有最先进方法，并对不同输入密度和噪声水平保持鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DANCE提供了一种密度无关、类别感知的点云补全解决方案，能够在多变的真实场景中实现高质量、鲁棒的补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从不完整的3D扫描中恢复缺失的几何结构，这些扫描往往受到遮挡或有限的传感器视角影响。现有方法通常假设输入/输出密度固定或依赖基于图像的表示，使其在真实场景中对可变稀疏度和有限监督不够适用。本文提出了密度无关且类别感知的网络DANCE，一种新框架，只补全缺失区域，同时保留已观测的几何。DANCE通过从多个视角进行射线采样生成候选点。随后，变压器解码器细化它们的位置并预测不透明度分数，决定每个点是否包含在最终表面中。为引入语义指导，轻量级分类头直接在几何特征上训练，实现类别一致的补全，而不需要外部图像监督。在PCN和MVP基准上进行的大量实验表明，DANCE在准确性和结构一致性方面优于最先进的方法，并且对不同输入密度和噪声水平保持鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成中输入密度可变且缺乏语义指导的问题。现实场景中的扫描往往稀疏且视角受限，固定密度或缺乏类别信息的完成方法会导致几何失真或不一致，影响后续任务如机器人导航和三维重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 NeRF 的射线采样思想来生成候选点，并结合 Transformer 的跨视角注意力与自注意力来细化几何。方法中还引入了分类头和融合网络，以纯几何方式学习类别先验，避免了对图像监督的依赖。整体设计参考了 PCN、GenPC、PF‑Net 等现有完成框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用多视角射线采样产生候选点，然后通过共享编码器提取特征，利用面向视角的 Transformer 进行跨视角与局部自注意力，最后融合分类信息预测每个候选点的偏移量和不透明度，筛选出有效点并与原始点云合并得到完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 采用射线采样实现密度无关的候选点生成；② 通过分类头学习类别先验，实现类感知完成；③ 使用 Transformer 进行视角分组的跨视角与自注意力，提升局部一致性；④ 通过不透明度阈值动态控制输出密度，避免固定点数。与以往固定密度或依赖图像监督的方法不同，DANCE 能在多样化输入密度下保持高质量、语义一致的完成结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DANCE 提出了一种密度无关、类感知的点云完成框架，利用射线采样与 Transformer 细化，能够在不依赖图像监督的情况下生成高质量、语义一致的完整点云。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to recover missing geometric structures from incomplete 3D scans, which often suffer from occlusions or limited sensor viewpoints. Existing methods typically assume fixed input/output densities or rely on image-based representations, making them less suitable for real-world scenarios with variable sparsity and limited supervision. In this paper, we introduce Density-agnostic and Class-aware Network (DANCE), a novel framework that completes only the missing regions while preserving the observed geometry. DANCE generates candidate points via ray-based sampling from multiple viewpoints. A transformer decoder then refines their positions and predicts opacity scores, which determine the validity of each point for inclusion in the final surface. To incorporate semantic guidance, a lightweight classification head is trained directly on geometric features, enabling category-consistent completion without external image supervision. Extensive experiments on the PCN and MVP benchmarks show that DANCE outperforms state-of-the-art methods in accuracy and structural consistency, while remaining robust to varying input densities and noise levels.&lt;/p&gt;</description></item><item><guid>2511.09866v1</guid><title>IPCD: Intrinsic Point-Cloud Decomposition</title><link>http://arxiv.org/abs/2511.09866v1</link><author>Shogo Sato, Takuhiro Kaneko, Shoichiro Takeda, Tomoyasu Shimada, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida, Akisato Kimura</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种针对点云的内在分解方法，能够直接将彩色点云分解为反照率和阴影两部分，并通过点级特征聚合和投影光照分布技术解决传统图像分解模型在非网格结构和全局光方向考虑不足的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云在增强现实和机器人等领域被广泛使用，逼真可视化需要对点云进行重新照明和纹理编辑，而这需要准确分离反照率与阴影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在点云上实现高质量反照率与阴影分离的模型，并验证其在多种光照条件下的实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出IPCD-Net，利用点级特征聚合扩展图像分解模型；引入投影光照分布（PLD）并进行层次特征细化，通过多视角投影捕捉全局光信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明IPCD-Net能显著减少反照率中的投射阴影，并提升阴影部分的色彩准确度；在纹理编辑、重新照明和点云配准等应用中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; IPCD-Net在合成与真实场景中均能实现可靠的点云内在分解，为点云的可视化与配准提供了有效工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云在增强现实（AR）和机器人等多个领域得到广泛应用，其中重新照明和纹理编辑对于实现逼真可视化至关重要。完成这些任务需要准确地将反照率与阴影分离。然而，在点云上进行此类分离面临两个主要挑战：（1）点云的非网格结构使得传统基于图像的分解模型失效；（2）为其他任务设计的点云模型并未显式考虑全局光照方向，导致阴影不准确。本文提出了内在点云分解（IPCD），将图像分解扩展到彩色点云的直接分解为反照率和阴影。为克服挑战（1），我们提出了IPCD-Net，该网络通过点级特征聚合扩展了基于图像的模型，以处理非网格数据；为解决挑战（2），我们引入了基于投影的亮度分布（PLD），并通过多视角投影捕获全局光照线索，采用层次特征细化。为全面评估，我们创建了一个合成的户外场景数据集。实验结果表明，IPCD-Net能够减少反照率中的投射阴影，并提升阴影部分的色彩准确度。我们还展示了其在纹理编辑、重新照明和不同光照条件下的点云配准中的应用。最后，我们验证了IPCD-Net在真实世界中的适用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在将彩色点云直接分解为表面反射率（色度）和光照阴影，以便在增强现实、机器人视觉和点云配准等应用中实现更真实的纹理编辑、重光照和跨光照条件的配准。该问题重要，因为传统方法需要先渲染成图像再分解，容易产生伪影，且无法在非网格结构的点云上直接操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有的二维图像内在分解方法无法直接应用于点云，随后借鉴了 PoInt‑Net 的点云表示和光照估计思路，并将其改造成在三维点云空间直接监督的 IPCD‑NetBase。为补偿缺失的全局光照信息，他们设计了 Projection‑based Luminance Distribution (PLD) 并使用 SphereNet 处理球面数据，进一步提升了阴影与色度的分离效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点云特征聚合与多视角投影得到的全局光照统计（PLD）共同驱动的共享编码器，先粗略估计色度和阴影，再通过层次化特征细化得到最终结果。实现流程包括：①输入点云（位置+颜色）→ ②使用 PTv2 进行点级特征提取；③从多视角投影生成 PLD 并通过 SphereNet 提取光照特征；④将编码器输出与 PLD 特征融合，分别预测预色度和预阴影；⑤通过层次化细化网络迭代更新，得到最终的色度和阴影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①直接在点云空间进行内在分解，避免渲染导致的失真；②点级特征聚合（PTv2）替代传统 MLP，提升非网格数据处理能力；③提出 PLD 通过多视角投影捕获全局光照信息，解决光照方向缺失问题；④层次化细化实现色度与阴影的相互约束；⑤构建合成户外点云数据集用于评估。与 PoInt‑Net、NeRF 等方法相比，IPCD‑Net 不需要图像渲染或场景特定训练，且显著降低了阴影泄漏和色度不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出 IPCD‑Net，一种利用点级特征聚合和投影光照分布的点云网络，能够直接将彩色点云分解为色度和阴影，实现无场景微调的高质量重光照与纹理编辑。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point clouds are widely used in various fields, including augmented reality (AR) and robotics, where relighting and texture editing are crucial for realistic visualization. Achieving these tasks requires accurately separating albedo from shade. However, performing this separation on point clouds presents two key challenges: (1) the non-grid structure of point clouds makes conventional image-based decomposition models ineffective, and (2) point-cloud models designed for other tasks do not explicitly consider global-light direction, resulting in inaccurate shade. In this paper, we introduce \textbf{Intrinsic Point-Cloud Decomposition (IPCD)}, which extends image decomposition to the direct decomposition of colored point clouds into albedo and shade. To overcome challenge (1), we propose \textbf{IPCD-Net} that extends image-based model with point-wise feature aggregation for non-grid data processing. For challenge (2), we introduce \textbf{Projection-based Luminance Distribution (PLD)} with a hierarchical feature refinement, capturing global-light ques via multi-view projection. For comprehensive evaluation, we create a synthetic outdoor-scene dataset. Experimental results demonstrate that IPCD-Net reduces cast shadows in albedo and enhances color accuracy in shade. Furthermore, we showcase its applications in texture editing, relighting, and point-cloud registration under varying illumination. Finally, we verify the real-world applicability of IPCD-Net.&lt;/p&gt;</description></item><item><guid>2511.10209v2</guid><title>LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures</title><link>http://arxiv.org/abs/2511.10209v2</link><author>Wenzhe He, Xiaojun Chen, Ruiqi Wang, Ruihui Li, Huilong Pi, Jiapeng Zhang, Zhuo Tang, Kenli Li</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LiNeXt 是一种轻量级、非扩散网络，旨在快速准确地完成 LiDAR 点云场景补全，显著提升实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶感知系统中，3D LiDAR 场景补全是关键任务。传统方法多使用扩散模型实现高质量重建，但多步采样导致计算开销大，难以满足实时需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种不使用扩散模型、单步去噪和精细化的网络，以降低计算成本并保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; LiNeXt 由 Noise-to-Coarse 模块和 Refine 模块组成。Noise-to-Coarse 在单次前向传播中去噪，消除多步采样；Refine 模块利用粗点云及其中间特征进行精细化，提升结构完整性。同时引入 Distance-aware Selected Repeat 策略，生成更均匀的噪声点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 SemanticKITTI 数据集上，LiNeXt 推理速度提升 199.8 倍，Chamfer 距离降低 50.7%，参数量仅为 LiDiff 的 6.1%，验证了其高效性和有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LiNeXt 在实时场景补全方面表现出优越的效率和效果，适用于自动驾驶等需要快速点云处理的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D LiDAR 场景补全是自动驾驶感知系统的基本组成部分。以往方法主要使用扩散模型实现高保真重建，但其多步迭代采样导致显著的计算开销，限制了实时应用。为了解决这一问题，我们提出了 LiNeXt——一种轻量级、非扩散网络，优化用于快速准确的点云补全。具体而言，LiNeXt 首先使用 Noise-to-Coarse（N2C）模块在单次前向传播中去噪输入的噪声点云，从而消除了扩散方法的多步迭代采样。Refine 模块随后利用粗点云及其来自 N2C 模块的中间特征进行更精确的细化，进一步提升结构完整性。此外，我们观察到 LiDAR 点云呈现距离相关的空间分布，近距离采样密集，远距离稀疏。为此，我们提出了 Distance-aware Selected Repeat 策略，以生成更均匀分布的噪声点云。在 SemanticKITTI 数据集上，LiNeXt 的推理速度提升 199.8 倍，Chamfer 距离降低 50.7%，参数量仅为 LiDiff 的 6.1%，这些结果证明了 LiNeXt 在实时场景补全方面的卓越效率和有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 LiDAR 点云场景补全中的稀疏与遮挡问题，使得自动驾驶感知系统能够获得完整的三维环境信息，从而提升目标检测、姿态估计和地图构建等下游任务的鲁棒性与安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现传统扩散模型在补全时需要多步采样，导致计算开销大，难以满足实时需求。于是他们借鉴扩散框架的思路，但改为单步去噪，设计了 Noise‑to‑Coarse（N2C）和 Refine 模块，并引入距离感知重复、交叉点注意力（CPA）和多尺度稀疏卷积（MSSC）等技术，以提升效率与精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过距离感知的点重复与加噪得到均匀分布的噪声点云，然后用 N2C 模块一次性生成粗略补全，再用 Refine 模块细化细节。整体流程为：输入点云 → 距离感知重复 → 加噪 → N2C → 粗补全 → Refine → 完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 距离感知重复策略平衡近远点采样；② 交叉点注意力模块实现全局与局部特征的动态融合；③ 多尺度稀疏卷积高效提取多分辨率特征；④ 完全去除扩散采样，采用单步去噪。与以往扩散模型相比，LiNeXt 在推理速度提升约200倍、参数量仅占6%且 Chamfer 距离下降50%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiNeXt 提出一种轻量级、非扩散的 LiDAR 场景补全框架，通过距离感知采样、交叉点注意力和多尺度稀疏卷积，实现实时高精度补全，显著优于现有扩散方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D LiDAR scene completion from point clouds is a fundamental component of perception systems in autonomous vehicles. Previous methods have predominantly employed diffusion models for high-fidelity reconstruction. However, their multi-step iterative sampling incurs significant computational overhead, limiting its real-time applicability. To address this, we propose LiNeXt-a lightweight, non-diffusion network optimized for rapid and accurate point cloud completion. Specifically, LiNeXt first applies the Noise-to-Coarse (N2C) Module to denoise the input noisy point cloud in a single pass, thereby obviating the multi-step iterative sampling of diffusion-based methods. The Refine Module then takes the coarse point cloud and its intermediate features from the N2C Module to perform more precise refinement, further enhancing structural completeness. Furthermore, we observe that LiDAR point clouds exhibit a distance-dependent spatial distribution, being densely sampled at proximal ranges and sparsely sampled at distal ranges. Accordingly, we propose the Distance-aware Selected Repeat strategy to generate a more uniformly distributed noisy point cloud. On the SemanticKITTI dataset, LiNeXt achieves a 199.8x speedup in inference, reduces Chamfer Distance by 50.7%, and uses only 6.1% of the parameters compared with LiDiff. These results demonstrate the superior efficiency and effectiveness of LiNeXt for real-time scene completion.&lt;/p&gt;</description></item><item><guid>2511.12170v2</guid><title>Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective</title><link>http://arxiv.org/abs/2511.12170v2</link><author>Wang Luo, Di Wu, Hengyuan Na, Yinlin Zhu, Miao Hu, Guocong Quan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了点云补全的新范式——基于校正的补全，并实现了PGNet框架，显著提升了补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全面临遮挡和几何缺失问题，现有方法多采用基于填充的范式，易产生结构不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 重新思考点云补全任务，提出更稳健的Completion-by-Correction范式，并构建PGNet实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用预训练的图像到3D模型生成拓扑完整的形状先验，随后在特征空间进行校正；PGNet通过双特征编码、粗略结构生成和分层校正实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; PGNet在ShapeNetViPC数据集上相较于现有基线平均Chamfer距离下降23.5%，F-score提升7.1%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Completion-by-Correction范式和PGNet框架能实现结构一致且与观测对齐的点云补全，优于传统填充方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分观测中重建完整的三维形状，这一任务因严重遮挡和几何缺失而具有挑战性。尽管最近在多模态技术方面取得进展，利用互补的RGB图像来补偿缺失几何，但大多数方法仍遵循基于填充的补全范式，从融合的潜在特征中合成缺失结构。我们通过实验表明，该范式往往因几何和语义约束有限而导致结构不一致和拓扑伪影。为了解决这一问题，我们重新思考任务并提出了更稳健的范式——基于校正的补全（Completion-by-Correction），该方法以预训练的图像到3D模型生成的拓扑完整形状先验为起点，并在特征空间进行校正以与部分观测对齐。该范式将补全从无约束的合成转向有指导的细化，实现结构一致且与观测对齐的重建。在此基础上，我们提出了PGNet，一种多阶段框架，进行双特征编码以锚定生成先验，合成粗略但结构对齐的支架，并通过分层校正逐步细化几何细节。对ShapeNetViPC数据集的实验表明，PGNet在平均Chamfer距离下降23.5%和F-score提升7.1%方面优于最先进的基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决多模态点云补全问题，即从部分点云和单视角RGB图像中恢复完整的三维形状。点云在LiDAR或RGB‑D传感器中常因遮挡、反射或分辨率限制而稀疏不完整，影响自动驾驶、增强现实和机器人等应用的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统的Completion‑by‑Inpainting方法在严重缺失时易产生结构不一致和拓扑伪影，随后提出Completion‑by‑Correction思路，利用预训练的图像‑&amp;gt;3D模型生成完整形状先验并在特征空间进行校正。设计中借鉴了PoinTr、DGCNN、Salient Transformer、Grounding Transformer等现有技术，并在此基础上构建了三阶段PGNet框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先得到一个完整但可能不精确的形状先验，然后通过特征空间的对齐与校正，使其与部分观测对齐，最终得到完整点云。实现流程分为三阶段：1）Corrective Dual‑Feature Encoding，对先验和部分点云进行并行编码并通过交叉注意力进行特征对齐；2）Grounded Seed Generation，利用全局融合生成粗略但结构完整的种子点云并进行几何校正；3）Hierarchical Grounded Refinement，递归细化粗点云，融合双源特征以恢复高精度几何。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① Completion‑by‑Correction范式，将补全任务从无约束生成转为先验校正；② 双特征编码与显著门控机制，实现先验与观测的自适应融合；③ 基于种子生成的结构化粗点云与层级细化策略，显著降低拓扑伪影。与以往直接在融合特征上进行补全的工作不同，PGNet通过先验校正和分阶段细化实现了更高的结构一致性和更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种Completion‑by‑Correction框架，利用图像生成的完整形状先验并通过特征校正与分阶段细化，实现了更准确、更结构一致的多模态点云补全。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).&lt;/p&gt;</description></item><item><guid>2511.16161v1</guid><title>Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion</title><link>http://arxiv.org/abs/2511.16161v1</link><author>Lirui Zhang, Zhengkai Zhao, Zhi Zuo, Pan Gao, Jie Qin</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为Simba的新框架，用于点云补全，旨在同时保留细节并保持整体结构完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全任务需要在保持输入细节的同时保证完成形状的全局结构完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统基于回归的局部对称变换方法易过拟合且对噪声敏感的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将点级变换回归改为分布学习，结合对称先验与扩散模型的生成能力，并采用分层Mamba架构实现高质量上采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在PCN、ShapeNet和KITTI基准上，Simba实现了领先的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过分布学习和生成模型，Simba克服了过拟合和噪声敏感，显著提升了点云补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全是3D视觉中的一项基础任务。该领域的一个持续挑战是同时保留输入中存在的细粒度细节，并确保完成形状的全局结构完整性。最近利用局部对称变换通过直接回归的方法显著提升了几何结构细节的保留，但这些方法存在两个主要局限：（1）基于回归的方法易过拟合，倾向于记忆瞬时特定的变换，而不是学习可泛化的几何先验；（2）它们依赖点级变换回归，对输入噪声高度敏感，严重削弱了鲁棒性和泛化能力。为了解决这些挑战，我们提出了Simba，一种将点级变换回归重新表述为分布学习问题的新框架。我们的方法将对称先验与扩散模型的强大生成能力相结合，避免了实例特定的记忆，同时捕捉到稳健的几何结构。此外，我们引入了分层Mamba架构以实现高保真上采样。在PCN、ShapeNet和KITTI基准上的广泛实验验证了我们方法的领先性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成中同时保持细节与全局结构的问题，尤其是避免传统基于对称变换回归方法的过拟合和噪声敏感性。该问题在自动驾驶、机器人和增强现实等实际应用中至关重要，因为缺失的点云会导致感知错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为扩散模型能够生成多样化的几何先验，因而将点对点变换回归改为分布学习。借鉴了SymmCompletion的局部对称变换思想、Diffusion模型以及Mamba架构，设计了两阶段框架：先用SymmGT预训练目标变换，再用Sym-Diffuser学习条件分布，最后用MBA-Refiner细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型学习点云关键点的仿射变换场，而不是直接回归点坐标；先用SymmGT生成目标变换；Sym-Diffuser在条件下从噪声中恢复完整变换场；将变换应用于关键点得到粗完整点云；MBA-Refiner采用层级Mamba网络逐步细化并上采样，得到高精度完成结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 将点云完成视为条件生成变换场的任务；② 使用扩散模型学习仿射变换分布，避免过拟合；③ 设计MBA-Refiner的级联Mamba结构实现高效细化；④ 在合成到真实数据迁移上表现出色。与以往直接回归变换或点坐标的方法不同，Simba通过分布学习和Mamba加速实现更鲁棒、更细致的完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Simba提出一种基于扩散模型的对称变换场生成与Mamba细化的点云完成框架，兼顾细节保留与全局一致性，并在合成与真实数据上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method&amp;#x27;s state-of-the-art (SOTA) performance.&lt;/p&gt;</description></item><item><guid>2511.16807v1</guid><title>Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation</title><link>http://arxiv.org/abs/2511.16807v1</link><author>Xiatao Sun, Chen Liang, Qian Wang, Daniel Rakita</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文提出一种名为 Mesh RAG 的无训练、即插即用框架，用于改进自回归 3D 网格生成模型。通过检索点云分割、空间变换和配准技术，解耦了严格的顺序依赖，使生成过程更快、更高质量，并支持增量编辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D 网格在工业设计、游戏、仿真和机器人等领域至关重要，但传统手工制作耗时且难以扩展。自回归模型已成为自动生成网格的有效方法，但其顺序生成导致速度慢且难以编辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决自回归网格生成中质量与速度的权衡以及增量编辑困难的问题，提供一种更高效、可并行化且无需重新训练的改进方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用检索增强生成（RAG）思路，利用点云分割、空间变换和点云配准来检索、生成并整合网格组件，从而打破顺序限制，实现并行推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Mesh RAG 在多种基础自回归网格生成模型上均表现出显著提升网格质量、加速生成速度，并实现增量编辑，且不需要模型重新训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 检索增强的 Mesh RAG 为自回归网格生成提供了一种高效、灵活且易于集成的解决方案，可在保持或提升质量的同时显著提高生成速度，并支持增量编辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 网格是工业设计、游戏、仿真和机器人等应用的关键构件。传统上，网格由艺术家手工制作，耗时且难以扩展。为自动化和加速资产创建，已出现自回归模型作为强大的艺术网格生成范式。然而，提升质量的现有方法通常依赖更大的模型或更长的序列，导致生成时间更长，其固有的顺序特性也带来严重的质量-速度权衡。顺序依赖还显著复杂化增量编辑。为克服这些限制，我们提出 Mesh RAG，一种新颖的、无训练、即插即用框架，用于自回归网格生成模型。受语言模型 RAG 的启发，我们的方法通过利用点云分割、空间变换和点云配准来检索、生成并整合网格组件，从而增强生成过程。该检索式方法将生成与严格的顺序依赖解耦，促进高效且可并行化的推理。我们展示了 Mesh RAG 在多种基础自回归网格生成模型上的广泛适用性，证明它显著提升网格质量、加速生成速度，并实现增量编辑，且无需模型重新训练。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在解决自回归网格生成模型的顺序依赖导致的生成速度慢、质量与效率难以平衡以及增量编辑困难的问题。3D网格是游戏、工业设计、仿真等领域的核心资产，手工建模耗时且难以扩展，提升自动化生成的效率和可编辑性具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了语言模型中的检索增强生成（RAG）思路，将其迁移到3D网格域。通过使用P3‑SAM和Sonata进行点云分割，并结合粗略对齐与ICP注册来检索空间变换，形成一个训练‑free、可插拔的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将网格生成拆分为“分割-生成-检索-合并”四步：先把输入点云分割成若干部件；用现有自回归模型为每个部件生成归一化网格；通过AABB粗对齐和ICP细化检索每个部件的尺度、位置和姿态；最后将检索到的变换应用于生成的网格并拼接成完整模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新在于：①提出训练‑free、可插拔的检索增强框架；②通过空间变换检索解耦顺序依赖，实现并行生成和局部增量编辑；③在不改动原模型的前提下显著提升质量和速度。与以往关注标记化或模型训练的工作不同，Mesh RAG侧重于推理过程的改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Mesh RAG通过检索空间变换，将自回归网格生成拆分为并行的部件生成与合成，实现在不重新训练模型的情况下显著提升生成质量、速度和可编辑性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D meshes are a critical building block for applications ranging from industrial design and gaming to simulation and robotics. Traditionally, meshes are crafted manually by artists, a process that is time-intensive and difficult to scale. To automate and accelerate this asset creation, autoregressive models have emerged as a powerful paradigm for artistic mesh generation. However, current methods to enhance quality typically rely on larger models or longer sequences that result in longer generation time, and their inherent sequential nature imposes a severe quality-speed trade-off. This sequential dependency also significantly complicates incremental editing. To overcome these limitations, we propose Mesh RAG, a novel, training-free, plug-and-play framework for autoregressive mesh generation models. Inspired by RAG for language models, our approach augments the generation process by leveraging point cloud segmentation, spatial transformation, and point cloud registration to retrieve, generate, and integrate mesh components. This retrieval-based approach decouples generation from its strict sequential dependency, facilitating efficient and parallelizable inference. We demonstrate the wide applicability of Mesh RAG across various foundational autoregressive mesh generation models, showing it significantly enhances mesh quality, accelerates generation speed compared to sequential part prediction, and enables incremental editing, all without model retraining.&lt;/p&gt;</description></item><item><guid>2511.17054v1</guid><title>RL-AD-Net: Reinforcement Learning Guided Adaptive Displacement in Latent Space for Refined Point Cloud Completion</title><link>http://arxiv.org/abs/2511.17054v1</link><author>Bhanu Pratap Paregi, Vaibhav Kumar</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于强化学习的点云补全细化框架 RL-AD-Net，能够在预训练自编码器的潜在空间中对补全结果进行局部几何细化，并通过轻量级选择器挑选最优重建。实验表明该方法在不同裁剪场景下均能显著提升补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有点云补全模型能生成整体合理的形状，但常出现局部几何不一致的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过强化学习实现对补全结果的局部几何细化，提升整体几何精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用自编码器将补全结果编码为全局特征向量；随后强化学习代理在潜在空间中对特征进行选择性调整；使用轻量级非参数选择器比较原始与细化结果的几何一致性，保留更优的重建；若有真实标签，则用距离和一致性指标引导细化；训练按类别独立完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ShapeNetCore-2048 数据集上，RL-AD-Net 在随机裁剪和训练裁剪两种场景下均能持续提升补全效果，优于基线网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RL-AD-Net 轻量、模块化且与模型无关，可直接应用于多种补全网络，无需重新训练，且未来可扩展到多类别细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文介绍了最近的点云补全模型，包括基于变压器、去噪以及其他先进方法，它们能够从部分输入生成整体合理的形状，但往往留下局部几何不一致。我们提出了 RL-AD-Net，一种基于强化学习的细化框架，工作于预训练点云自编码器的潜在空间。自编码器将补全结果编码为紧凑的全局特征向量，随后强化学习代理在该空间中对特征进行选择性调整，以提升几何精度。为保证鲁棒性，轻量级非参数 PointNN 选择器会评估原始补全与 RL 细化结果的几何一致性，保留更优的重建。当有真实标签时，Chamfer 距离和几何一致性指标共同指导细化。由于强化学习的无监督和动态特性，训练在每个类别上单独进行，跨类别收敛较难，但未来可扩展到多类别细化。实验在 ShapeNetCore-2048 上表明，虽然基线补全网络在其训练裁剪方式下表现合理，但在随机裁剪场景下表现不佳；相比之下，RL-AD-Net 在两种设置下均能持续提升效果，凸显了基于强化学习的集成细化的有效性。该方法轻量、模块化且与模型无关，可应用于多种补全网络，无需重新训练。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进点云完成模型产生的局部几何不一致问题。准确的三维几何对于机器人抓取、自动驾驶感知和虚拟现实等应用至关重要。现有的强大模型往往在细节上出现模糊或缺失，影响后续任务的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到基线完成网络在局部细节上易出错，于是提出在预训练的自编码器潜在空间中进行细化。该思路借鉴了 RL-GAN-Net 的 RL 引导生成以及 Point-Patch RL 的局部补全经验，但区别在于不需要重新训练生成器，而是仅在潜在空间中调整。作者还参考了类别特定训练的优势，采用每个类别单独的 RL 代理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将完成结果编码为 128 维全局特征向量，使用强化学习代理预测一个微调向量，再将调整后的特征解码为改进的点云。实现流程为：①使用任意完成网络得到基线点云；②用预训练自编码器编码得到 GFV；③RL 代理输出增量并更新 GFV；④冻结解码器生成细化点云；⑤用 PointNN 评估基线与细化结果，选取质量更高的作为最终输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①在潜在空间中进行 RL 细化，保持全局结构不变；②采用 128 维轻量级潜在表示，便于高维连续控制；③每个类别单独训练 RL 代理，提升对类别几何先验的利用；④使用非参数 PointNN 进行无监督质量选择，保证细化不会退化；⑤整个框架模型无关、无需重新训练基线网络。与以往需要重新训练生成器或仅使用确定性滤波的工作不同，RL-AD-Net 通过学习的策略实现实例化、可迁移的细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RL-AD-Net 提供了一种轻量级、类别特定的强化学习细化模块，可在不改动任何完成网络的前提下，显著提升点云完成结果的局部几何质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent point cloud completion models, including transformer-based, denoising-based, and other state-of-the-art approaches, generate globally plausible shapes from partial inputs but often leave local geometric inconsistencies. We propose RL-AD-Net, a reinforcement learning (RL) refinement framework that operates in the latent space of a pretrained point autoencoder. The autoencoder encodes completions into compact global feature vectors (GFVs), which are selectively adjusted by an RL agent to improve geometric fidelity. To ensure robustness, a lightweight non-parametric PointNN selector evaluates the geometric consistency of both the original completion and the RL-refined output, retaining the better reconstruction. When ground truth is available, both Chamfer Distance and geometric consistency metrics guide refinement. Training is performed separately per category, since the unsupervised and dynamic nature of RL makes convergence across highly diverse categories challenging. Nevertheless, the framework can be extended to multi-category refinement in future work. Experiments on ShapeNetCore-2048 demonstrate that while baseline completion networks perform reasonable under their training-style cropping, they struggle in random cropping scenarios. In contrast, RL-AD-Net consistently delivers improvements across both settings, highlighting the effectiveness of RL-guided ensemble refinement. The approach is lightweight, modular, and model-agnostic, making it applicable to a wide range of completion networks without requiring retraining.&lt;/p&gt;</description></item><item><guid>2511.20253v1</guid><title>Zoo3D: Zero-Shot 3D Object Detection at Scene Level</title><link>http://arxiv.org/abs/2511.20253v1</link><author>Andrey Lemeshko, Bulat Gabdullin, Nikita Drozdov, Anton Konushin, Danila Rukhovich, Maksim Kolodiazhnyi</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Zoo3D 是首个无训练的 3D 物体检测框架，利用二维实例掩码的图聚类构建 3D 边界框，并通过开放词汇模块进行语义标注。它提供零样本和自监督两种模式，在 ScanNet200 和 ARKitScenes 基准上实现了最先进的开放词汇检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的 3D 检测方法受限于封闭类别，难以识别未见过的物体；现有开放词汇检测器虽然降低了标注需求，但仍需训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种完全不需要训练的 3D 检测框架，以实现对多样且未知物体的识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过对二维实例掩码进行图聚类生成 3D 边界框，随后使用最佳视角选择和视角一致掩码生成的开放词汇模块为框分配语义标签；提供零样本模式和自监督模式，并支持直接处理姿态化或未姿态化图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Zoo3D_0（零样本）和 Zoo3D_1（自监督）在 ScanNet200 和 ARKitScenes 上均取得了最先进的开放词汇检测结果，且零样本模式超过了所有现有自监督方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 训练免费、即插即用的 Zoo3D 展示了在现实 3D 理解任务中的强大适应性和性能，证明了无训练方法的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D物体检测是空间理解的基础。现实环境需要能够识别多样且以前未见过物体的模型，而封闭集方法仍然存在重大限制。现有的开放词汇3D检测器放宽了标注要求，但仍依赖训练场景，无论是点云还是图像。我们进一步提出Zoo3D，首个无训练的3D物体检测框架。该方法通过对二维实例掩码进行图聚类构建3D边界框，然后使用新颖的开放词汇模块进行语义标注，模块包括最佳视角选择和视角一致掩码生成。Zoo3D有两种模式：零样本Zoo3D_0完全不需要训练，和自监督Zoo3D_1通过在Zoo3D_0生成的伪标签上训练一个类别无关检测器来细化3D框预测。我们还将Zoo3D扩展到点云之外，直接处理已姿态化甚至未姿态化的图像。在ScanNet200和ARKitScenes基准上，Zoo3D_0和Zoo3D_1在开放词汇3D检测任务中均取得了最先进的结果。值得注意的是，零样本Zoo3D_0超过了所有现有自监督方法，证明了训练免费、即插即用方法在现实3D理解中的强大适应性。代码可在 https://github.com/col14m/zoo3d 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现零训练、开放词汇的室内 3D 目标检测，能够在不见过任何类别的场景中定位并识别物体。此问题在实际应用中至关重要，因为真实环境中存在大量未标注或新出现的物体，传统闭集方法难以泛化，且收集 3D 标注成本高昂。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 MaskClustering、SAM、CLIP 等基础模型，先用 2D 分割生成实例掩码，再通过图聚类得到 3D 盒子；随后利用视角一致性和 CLIP 语义对齐为盒子赋予标签。该思路在保持无训练的前提下，融合了现有的 2D‑&amp;gt;3D 迁移技术和开放词汇检索方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 3D 检测拆分为无类别 3D 盒子预测和开放词汇标签分配。流程为：输入点云与图像 → 生成 2D 掩码 → 构建掩码图并聚类 → 输出 3D 盒子 → 对每个盒子裁剪点云并投影到最佳视角 → 用 SAM 细化掩码 → 通过 CLIP 计算与文本标签的相似度 → 赋予语义标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 第一次实现完全训练‑free 的 3D 检测；② 通过 MaskClustering 直接生成 3D 盒子；③ 引入最佳视角选择和视角一致性掩码生成的开放词汇模块；④ 在无姿态图像上也能工作（利用 DUSt3R）。与以往需要训练或仅在点云/姿态图像上工作的自监督方法不同，Zoo3D 在零训练条件下即可达到甚至超过自监督性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Zoo3D 展示了一个完全训练‑free、基于基础模型的零训练开放词汇 3D 目标检测框架，在室内场景中实现了最先进的性能，并在无姿态图像上也能直接工作。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .&lt;/p&gt;</description></item><item><guid>2511.20278v1</guid><title>DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion</title><link>http://arxiv.org/abs/2511.20278v1</link><author>Yinghui Li, Qianyu Zhou, Di Shao, Hao Yang, Ye Zhu, Richard Dazeley, Xuequan Lu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了首个将状态空间模型（SSM）应用于域自适应点云补全（DA PCC）的研究，并针对其面临的挑战设计了新框架DAPointMamba。该框架通过三种跨域对齐模块实现了对几何和语义差异的有效补偿，最终在合成与真实数据集上均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 域自适应点云补全旨在弥合标注源域与无标签目标域之间的几何与语义差异。传统方法使用CNN或视觉Transformer，受限于感受野或二次复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究SSM在DA PCC中的适应性，并提出一种高效、全局感受野的框架，以提升跨域补全性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DAPointMamba包含三大模块：1）跨域补丁级扫描，建立补丁几何对应；2）跨域空间SSM对齐，基于相似度调制补丁特征；3）跨域通道SSM对齐，交错对齐特征通道以弥合语义差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 直接将3D点云序列化为1D序列会破坏空间拓扑；忽视域无关表征会削弱适应效果。DAPointMamba通过上述三模块克服这些问题，在多种基准上实现了更低的计算复杂度和推理延迟，并取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DAPointMamba在域自适应点云补全任务中展现出强大的适应性和高效性，证明了SSM在此类任务中的可行性与优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 域自适应点云补全（DA PCC）旨在缩小标注源域与无标签目标域之间的几何和语义差异。现有方法要么受限于感受野，要么因使用CNN或视觉Transformer而导致二次复杂度。本文提出了首个研究状态空间模型（SSM）在DA PCC中适应性的工作，并发现直接将SSM应用于DA PCC会遇到若干挑战：将3D点云序列化为1D序列往往会破坏目标域的空间拓扑和局部几何特征；忽视学习域无关表征的设计会阻碍适应性能。为解决这些问题，我们提出了新框架DAPointMamba，具有跨域强适应性、全局感受野和线性复杂度优势。该框架包含三个新模块。具体而言，跨域补丁级扫描引入补丁级几何对应，能够实现有效的局部对齐；跨域空间SSM对齐通过基于跨域相似度调制补丁特征，进一步加强空间一致性，有效缓解细粒度结构差异；跨域通道SSM对齐通过交错和对齐特征通道，主动解决全局语义差距。对合成和真实世界基准的广泛实验表明，DAPointMamba在保持更低计算复杂度和推理延迟的同时，优于现有最先进方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决域自适应点云完成（DA PCC）中的几何和语义差异，使在源域训练的模型能够在未标记的目标域上保持高质量的重建。该问题重要，因为不同传感器、场景或数据集导致的分布偏移会显著降低模型泛化能力，影响自动驾驶、机器人和虚拟现实等实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统 CNN 受限于感受野、Transformer 受限于二次复杂度，随后关注能够提供全局感受野且线性复杂度的状态空间模型（SSM）Mamba。通过分析直接序列化点云会破坏空间拓扑，作者借鉴了基于 Z‑order 曲线的局部扫描和相似度引导的特征调制技术，结合 Mamba 的优势设计了跨域补丁扫描、空间和通道对齐模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 Mamba 的全局建模能力，并通过三步跨域对齐提升域适应性：1）跨域补丁扫描（CDPS）使用共享归一化和 Z‑order 序列化，使源域和目标域的补丁在空间上对应；2）跨域空间 SSM 对齐（CDSA）根据补丁间相似度调制特征，细化局部对齐；3）跨域通道 SSM 对齐（CDCA）通过全局特征混合和通道调制解决语义差异。实现流程为：输入点云 → CDPS → Mamba 块（含 CDSA、CDCA）→ 解码器生成完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1）首次将 Mamba 引入域自适应点云完成；2）提出跨域补丁扫描保证空间对应；3）设计空间和通道对齐模块实现细粒度与全局语义一致；4）保持线性复杂度和全局感受野，显著降低计算成本。与以往基于 CNN 或 Transformer 的方法相比，DAPointMamba 在保持高性能的同时实现了更高的效率和更强的跨域泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAPointMamba 提出一种基于 Mamba 的线性复杂度框架，通过跨域补丁扫描、空间和通道对齐实现了优于现有 CNN/Transformer 方法的域自适应点云完成。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.&lt;/p&gt;</description></item><item><guid>2511.21925v1</guid><title>OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving</title><link>http://arxiv.org/abs/2511.21925v1</link><author>Alex Richardson, Jonathan Sprinkle</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 OpenTwinMap，一个基于 Python 的开源框架，用于从 LiDAR 扫描和 OpenStreetMap 数据生成高保真 3D 城市数字孪生。该框架解决了现有工具与特定仿真器耦合、难以扩展和技术负担大的问题，并支持将生成的资产导出到 Unreal Engine 进行自动驾驶仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 数字孪生在自动驾驶研究中起着关键作用，可用于仿真、验证和与生成式世界模型集成。然而，现有公开工具往往与特定仿真器紧耦合，难以扩展，且技术开销大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一个可扩展、并行化的开源框架，降低研究者在不同城市环境中适配和扩展数字孪生生成流程的门槛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; OpenTwinMap 通过 Python 处理 LiDAR 和 OSM 数据，完成预处理、道路网格和地形生成，并支持将结果导出到 Unreal Engine；同时提供对 CARLA 的初步集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 目前框架已实现 OSM 与 LiDAR 数据的预处理、基本道路网格和地形生成，并具备对 CARLA 的初步支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenTwinMap 通过强调可扩展性和并行化，提供了一个低门槛、可适配多种城市场景的数字孪生生成管线，为自动驾驶研究提供了有价值的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 数字孪生在城市环境中对推进自动驾驶（AV）研究起着关键作用，它们通过仿真、验证和与新兴生成式世界模型的集成来实现。虽然现有工具已显示出价值，但许多公开可用的解决方案与特定仿真器紧密耦合，难以扩展，或引入显著的技术开销。例如，最广泛使用的开源 AV 仿真器 CARLA 提供的数字孪生框架完全实现为 Unreal Engine 的 C++ 插件，限制了灵活性和快速原型设计。在本研究中，我们提出了 OpenTwinMap，一个基于 Python 的开源框架，用于生成高保真 3D 城市数字孪生。完成的框架将摄取 LiDAR 扫描和 OpenStreetMap（OSM）数据，生成语义分割的静态环境资产，包括道路网络、地形和城市结构，并可导出到 Unreal Engine 进行 AV 仿真。OpenTwinMap 强调可扩展性和并行化，降低了研究者将管线适配和扩展到多样化城市环境的门槛。我们描述了 OpenTwinMap 的当前功能，包括 OSM 和 LiDAR 数据的预处理、基本道路网格和地形生成，以及对 CARLA 的初步支持。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决缺乏灵活、高精度城市数字孪生生成工具的问题。数字孪生对于自动驾驶研究至关重要，因为它们支持真实感仿真、验证和与生成式世界模型的集成，但现有工具往往与特定仿真器紧耦合、难以扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者利用公开的 OSM 与 LiDAR 数据，并借鉴 CARLA、OpenDRIVE、Open3D 等现有工具，构建了一个 Python 框架。该框架模仿 CARLA 的功能，但将其与 Unreal Engine 解耦，便于快速原型和扩展。作者还参考了近期生成式模型研究，以支持未来的集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 LiDAR 点云与 OSM 道路数据融合，先细化几何形状，再转换为 OpenDRIVE 描述，随后生成道路和静态对象的 3D 网格，最后导出到 Unreal Engine 或 CARLA。整个流程由数据摄取、几何细化、转换、网格生成和导出等模块组成，并支持并行化处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：开源、Python 版、可扩展的管线；模块化的 OpenDRIVE 1.4 实现和 OSM‑to‑OpenDRIVE 转换器；对桥梁和高架桥的支持；以及可扩展到其他仿真器的 CARLA stub。与 CARLA 的 Unreal 插件相比，OpenTwinMap 解除耦合、易于定制且支持并行化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenTwinMap 提供了一个灵活、Python 版的框架，将 LiDAR 与 OSM 数据转换为高精度、语义分割的 3D 城市数字孪生，兼容 CARLA 及其他仿真器。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Digital twins of urban environments play a critical role in advancing autonomous vehicle (AV) research by enabling simulation, validation, and integration with emerging generative world models. While existing tools have demonstrated value, many publicly available solutions are tightly coupled to specific simulators, difficult to extend, or introduce significant technical overhead. For example, CARLA-the most widely used open-source AV simulator-provides a digital twin framework implemented entirely as an Unreal Engine C++ plugin, limiting flexibility and rapid prototyping. In this work, we propose OpenTwinMap, an open-source, Python-based framework for generating high-fidelity 3D urban digital twins. The completed framework will ingest LiDAR scans and OpenStreetMap (OSM) data to produce semantically segmented static environment assets, including road networks, terrain, and urban structures, which can be exported into Unreal Engine for AV simulation. OpenTwinMap emphasizes extensibility and parallelization, lowering the barrier for researchers to adapt and scale the pipeline to diverse urban contexts. We describe the current capabilities of the OpenTwinMap, which includes preprocessing of OSM and LiDAR data, basic road mesh and terrain generation, and preliminary support for CARLA integration.&lt;/p&gt;</description></item><item><guid>2511.22404v1</guid><title>UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data</title><link>http://arxiv.org/abs/2511.22404v1</link><author>Longkun Zou, Jiale Wang, Rongqin Liang, Hai Wu, Ke Chen, Yaowei Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了UAV-MM3D合成数据集，包含400K帧多模态数据，支持无人机感知与运动理解，并提供LGFusionNet融合模型和轨迹预测基线，旨在推动低空无人机3D感知研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 低空无人机感知对空域安全和智能系统至关重要，但真实数据收集受限，手工标注成本高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服数据收集与标注难题，构建大规模、多模态、精确标注的合成数据集，以支持无人机感知与运动理解研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 创建UAV-MM3D合成数据集，包含多场景、多天气、多无人机型号和五种模态；每帧提供2D/3D框、6自由度姿态和实例级注释；提出LGFusionNet基于LiDAR的多模态融合模型和轨迹预测基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 数据集覆盖广泛场景与天气，提供丰富注释，LGFusionNet和轨迹预测基线为评测提供基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UAV-MM3D为低空无人机3D感知提供了可控、全面的公开基准，促进相关技术进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确感知低空复杂环境中的无人机对于空域安全和相关智能系统至关重要。开发可靠解决方案需要大规模、精确标注且多模态的数据。然而，现实世界的无人机数据收集受到空域法规、隐私关注和环境变化的限制，手工标注3D姿态和跨模态对应关系既耗时又昂贵。为克服这些挑战，我们提出了UAV-MM3D，一套高保真多模态合成数据集，用于低空无人机感知和运动理解。该数据集包含400K同步帧，覆盖城市、郊区、森林、沿海等多样场景以及晴朗、多云、雨天、雾天等天气条件，包含微型、小型、中型多种无人机型号，并提供RGB、红外、LiDAR、雷达和动态视觉传感器（DVS）五种模态。每帧提供二维/三维边界框、6自由度姿态和实例级注释，支持无人机相关的核心任务，如三维检测、姿态估计、目标跟踪和短期轨迹预测。我们进一步提出了基于LiDAR的多模态融合基线LGFusionNet，以及专门的无人机轨迹预测基线，以便进行基准测试。凭借可控的仿真环境、全面的场景覆盖和丰富的注释，UAV-MM3D为推进无人机三维感知提供了公开基准。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 缺乏大规模、多模态、精确标注的无人机感知数据，限制了低空无人机检测、姿态估计、跟踪和轨迹预测模型的研发。准确的无人机感知对空域安全、基础设施保护和公众隐私至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者利用Unreal Engine 4与CARLA的仿真平台，构建了可控的多天气、多场景、多无人机轨迹环境，并通过Python客户端实现天气、帧同步和坐标转换。设计参考了现有的多模态无人机数据集（如Anti-UAV-RGBT、MMDrone）和自动驾驶多模态数据集（nuScenes、Waymo），并在此基础上扩展到五种传感器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过高保真仿真生成同步的RGB、IR、DVS、LiDAR和雷达数据，并为每帧提供2D/3D框、6-DoF姿态和实例ID。实现流程包括：①在UE4-CARLA服务器中设置场景、天气和无人机轨迹；②Python客户端控制天气、帧同步和坐标转换；③多线程管道完成传感器数据采集、时空对齐和坐标变换；④生成标注并存储为统一的相机坐标系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提供400K帧、5模态、完整3D框和6-DoF标注的合成数据集；②覆盖城市、郊区、森林、海岸等八大场景和多种天气；③引入LiDAR引导的多模态融合基线LGFusionNet和轨迹预测基线；④为无人机感知提供统一评测框架。与以往仅有2D框、单模态或小规模数据集不同，UAV-MM3D在规模、模态、标注深度和任务多样性上实现了突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UAV-MM3D构建了一个大规模、多模态、精确标注的合成无人机感知基准，并提供融合与轨迹预测基线，为低空无人机检测、姿态估计、跟踪和轨迹预测研究提供统一评测平台。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.&lt;/p&gt;</description></item><item><guid>2511.22908v1</guid><title>ViGG: Robust RGB-D Point Cloud Registration using Visual-Geometric Mutual Guidance</title><link>http://arxiv.org/abs/2511.22908v1</link><author>Congjia Chen, Shen Yan, Yufu Qu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于互相引导的RGB-D点云配准方法ViGG，利用视觉与几何信息的互补，提升配准鲁棒性，并在多个数据集上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是3D视觉中的基础任务，现有方法大多仅使用几何信息；RGB-D配准方法多聚焦特征融合或学习，难以充分利用图像信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够充分利用图像信息、提高配准鲁棒性的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 在视觉-几何组合形式下求解团对齐，使用几何引导抑制模糊团；2) 通过视觉引导的几何匹配，利用视觉先验确定搜索空间，提取高质量、抗噪对应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在3DMatch、ScanNet和KITTI数据集上，ViGG在学习无关和学习相关两种设置下均优于最新方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 互相引导策略显著提升了RGB-D配准的鲁棒性，方法适用于多种配准任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准是3D视觉中的基础任务。大多数现有方法仅使用几何信息进行配准。最近提出的RGB-D配准方法主要关注特征融合或改进特征学习，这限制了它们利用图像信息的能力，并阻碍了其实际应用。在本文中，我们提出了ViGG，一种使用互相引导的鲁棒RGB-D配准方法。首先，我们在视觉-几何组合形式下求解团对齐，采用几何引导设计抑制模糊团。其次，为了减轻视觉匹配噪声导致的精度下降，我们提出了一种视觉引导的几何匹配方法，利用视觉先验确定搜索空间，从而提取高质量、抗噪对应。该互相引导策略为我们的方法带来了更优的鲁棒性，使其适用于各种RGB-D配准任务。对3DMatch、ScanNet和KITTI数据集的实验表明，我们的方法在学习无关和学习相关两种设置下均优于最近的最先进方法。代码可在 https://github.com/ccjccjccj/ViGG 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 RGB‑D 点云配准中的鲁棒性问题，尤其是在低重叠、噪声或视觉信息不完整的场景下。点云配准是 3D 视觉、重建和机器人导航等领域的基础任务，缺乏鲁棒方法会直接影响后续处理的精度与可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到视觉匹配具有高质量但易受纹理模糊影响，几何匹配则受局部歧义限制。基于此，他们借鉴了 MAC 的最大团搜索、TEASER 的鲁棒估计以及传统的视觉特征匹配技术，提出了视觉与几何互相引导的配准框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用视觉匹配做粗略配准并通过几何信息消除错误团，再用得到的粗配准作为先验引导几何匹配，提取高质量对应点。实现流程为：①提取图像关键点匹配并映射到 3D；②提取几何特征；③用几何引导的视觉团对齐得到初始变换；④评估该变换的置信度；⑤在该变换的搜索空间内进行几何匹配；⑥利用得到的对应点重新估计精确变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①视觉与几何互相引导的双向策略；②几何引导的视觉团对齐，抑制视觉歧义；③视觉引导的几何匹配，利用粗配准定位搜索区间；④在保持低计算成本的同时兼顾学习‑free 与学习‑based 场景。与以往仅通过网络融合多模态特征或单纯几何匹配的方法不同，ViGG 通过显式的互导机制显著提升了鲁棒性和适用范围。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ViGG 通过视觉与几何的互导配准框架，实现了在多种 RGB‑D 任务中显著提升鲁棒性和精度的点云配准方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration is a fundamental task in 3D vision. Most existing methods only use geometric information for registration. Recently proposed RGB-D registration methods primarily focus on feature fusion or improving feature learning, which limits their ability to exploit image information and hinders their practical applicability. In this paper, we propose ViGG, a robust RGB-D registration method using mutual guidance. First, we solve clique alignment in a visual-geometric combination form, employing a geometric guidance design to suppress ambiguous cliques. Second, to mitigate accuracy degradation caused by noise in visual matches, we propose a visual-guided geometric matching method that utilizes visual priors to determine the search space, enabling the extraction of high-quality, noise-insensitive correspondences. This mutual guidance strategy brings our method superior robustness, making it applicable for various RGB-D registration tasks. The experiments on 3DMatch, ScanNet and KITTI datasets show that our method outperforms recent state-of-the-art methods in both learning-free and learning-based settings. Code is available at https://github.com/ccjccjccj/ViGG.&lt;/p&gt;</description></item><item><guid>2511.23227v2</guid><title>PointCNN++: Performant Convolution on Native Points</title><link>http://arxiv.org/abs/2511.23227v2</link><author>Lihan Li, Haofeng Zhong, Rui Bu, Mingchao Sun, Wenzheng Chen, Baoquan Chen, Yangyan Li</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 PointCNN++，一种新的 3D 点云学习架构，旨在解决传统点基方法的性能瓶颈和体素基方法的几何精度损失问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的 3D 点云卷积学习方法主要分为点基方法和体素基方法，前者保持几何精度但性能有限，后者通过量化实现高效但牺牲几何细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将稀疏卷积从体素推广到点，构建一种既能保持高精度又能实现高性能的点云卷积网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用以点为中心的卷积，将感受野聚焦在原始高精度点坐标上，并设计在原生点上直接执行的计算策略，将卷积转化为矩阵向量乘法与归约问题，随后实现专用的高效 GPU 核心。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 PointCNN++ 在内存占用上比典型点基方法低一个数量级，速度提升数倍；作为体素基骨干的替代方案时，显著提升点云配准精度，同时保持更低的内存使用和更快的速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 保持几何细节与实现高性能并非互斥，PointCNN++ 为高保真高效的 3D 学习开辟了新路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的 3D 点云卷积学习方法可分为两大范式：点基方法能够保持几何精度，但往往面临性能挑战；体素基方法通过量化实现高效，但牺牲了几何细节。几何精度的损失是点云配准等任务的关键瓶颈。我们提出了 PointCNN++，一种新颖的架构设计，根本上缓解了精度与性能之间的权衡。它将稀疏卷积从体素推广到点，将体素卷积视为更一般点卷积的特殊、退化情况。首先，我们引入了以点为中心的卷积，使感受野聚焦在原始高精度点坐标上。其次，为了让这种高保真操作具备良好性能，我们设计了一种在原生点上直接执行的计算策略。我们将原生点卷积表述为矩阵-向量乘法与归约（MVMR）问题，并为此开发了专用的高效 GPU 核心。实验表明，PointCNN++ 的内存占用比代表性点基方法低一个数量级，速度提升数倍。此外，当它作为体素基骨干的简单替代时，显著提升点云配准精度，同时保持更低的内存使用和更快的速度。PointCNN++ 证明了保持几何细节与实现高性能并非互斥，为高保真高效的 3D 学习开辟了新路径。我们的代码将开源。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云卷积中精度与性能之间的权衡问题。传统的体素方法速度快但会因量化失去细节，点云方法保持精度却计算量大。对于需要高精度配准等任务，这种折衷限制了应用效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了体素与点云两种主流范式的缺陷，认为两者的折衷并非不可避免。随后提出以原始点为卷积中心的点中心卷积，并在此基础上借鉴稀疏卷积和GPU优化技术，形成一种既保持几何精度又高效的计算框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将卷积直接作用于原始点云，先用精确的邻域搜索确定邻居，再在每个邻域内做局部体素化以匹配卷积核。实现上把卷积写成矩阵-向量乘法与归约（MVMR）问题，并为此开发专门的GPU核，完成高效计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①点中心卷积将体素卷积视为特殊情况；②局部自适应体素化保持精度；③MVMR形式与专用GPU核实现显著节省内存并提升速度。与以往方法相比，它消除了全局体素化导致的精度损失，同时避免了点云方法的转换开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointCNN++提出一种在原始点云上进行卷积的高效算子，既保留几何细节，又实现与体素方法相当甚至更优的速度和内存表现。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing convolutional learning methods for 3D point cloud data are divided into two paradigms: point-based methods that preserve geometric precision but often face performance challenges, and voxel-based methods that achieve high efficiency through quantization at the cost of geometric fidelity. This loss of precision is a critical bottleneck for tasks such as point cloud registration. We propose PointCNN++, a novel architectural design that fundamentally mitigates this precision-performance trade-off. It $\textbf{generalizes sparse convolution from voxels to points}$, treating voxel-based convolution as a specialized, degraded case of our more general point-based convolution. First, we introduce a point-centric convolution where the receptive field is centered on the original, high-precision point coordinates. Second, to make this high-fidelity operation performant, we design a computational strategy that operates $\textbf{natively}$ on points. We formulate the convolution on native points as a Matrix-Vector Multiplication and Reduction (MVMR) problem, for which we develop a dedicated, highly-optimized GPU kernel. Experiments demonstrate that PointCNN++ $\textbf{uses an order of magnitude less memory and is several times faster}$ than representative point-based methods. Furthermore, when used as a simple replacement for the voxel-based backbones it generalizes, it $\textbf{significantly improves point cloud registration accuracies while proving both more memory-efficient and faster}$. PointCNN++ shows that preserving geometric detail and achieving high performance are not mutually exclusive, paving the way for a new class of 3D learning with high fidelity and efficiency. Our code will be open sourced.&lt;/p&gt;</description></item><item><guid>2512.00264v1</guid><title>HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction</title><link>http://arxiv.org/abs/2512.00264v1</link><author>Zhengda Ma, Abhirup Banerjee</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了基于点云的几何深度学习框架HeartFormer，用于从心动图 MRI 数据重建三维四腔心脏模型，并在公开数据集上实现了优于现有方法的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统心动图 MRI 只能提供二维切片图像，限制了对心脏形态和功能的全面理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服二维限制，构建能够从三维点云完成四腔心脏重建的深度学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了 HeartFormer，包含语义感知双结构变换器网络（SA-DSTNet）和语义感知几何特征细化变换器网络（SA-GFRTNet），并构建了首个公开的 17,000 份高分辨率三维多类心脏网格与点云数据集 HeartCompv1。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 HeartCompv1 和 UK Biobank 数据集上，HeartFormer 在鲁棒性、精度和泛化性方面均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架能够生成高保真、几何一致的三维心脏模型，并为该研究方向提供了可验证的基准数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了首个基于点云表示的几何深度学习框架，用于从心动图 MRI 数据中重建三维四腔心脏。该工作解决了传统心动图 MRI 仅提供二维心脏切片图像的长期局限性，从而限制了对健康和病理条件下心脏形态和生理机制的全面理解。为克服这一问题，我们提出了 “HeartFormer”，一种新颖的点云补全网络，将传统的单类点云补全扩展到多类。HeartFormer 由两个关键组件组成：语义感知双结构变换器网络（SA-DSTNet）和语义感知几何特征细化变换器网络（SA-GFRTNet）。SA-DSTNet 生成包含全局几何特征和子结构几何特征的初始粗点云。借助这些语义-几何表示，SA-GFRTNet 逐步细化粗输出，有效利用全局和子结构几何先验，生成高保真且几何一致的重建结果。我们进一步构建了 “HeartCompv1”，首个公开的大规模数据集，包含 17,000 个高分辨率三维多类心脏网格和点云，用于为这一新兴研究方向建立通用基准。对 HeartCompv1 和 UK Biobank 的跨域实验表明，HeartFormer 在鲁棒性、准确性和可泛化性方面表现出色，始终优于最先进方法。代码和数据集将在接受后发布，网址为：https://github.com/10Darren/HeartFormer。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在从传统的二维心脏磁共振成像（cine MRI）中重建完整的三维四腔心脏点云和网格。二维切片限制了对心脏形态和功能的三维评估，而高质量的三维模型对于生物标志物分析、病理可视化和个体化心脏力学模拟至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先将多视角的二维切片通过分割、轮廓提取和三维配准生成稀疏、错位的点云，然后提出 HeartFormer 这一多类别点云补全网络。该网络借鉴了现有点云补全技术（如 PCN、PoinTr、PCCN 等）和语义感知方法，进一步引入双结构 Transformer（SA‑DSTNet 与 SA‑GFRTNet）来同时捕获全局与子结构信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用语义感知的双结构 Transformer 先生成粗略的多类别点云，再通过两阶段的细化 Transformer 逐步提升几何细节和解剖一致性。实现流程为：输入 cine MRI → 分割与轮廓提取 → 生成稀疏点云 → SA‑DSTNet（全局+子结构聚合）→ 粗点云与特征 → SA‑GFRTNet（两阶段细化）→ 精细点云 → Poisson 重建与 Ball Pivoting → 生成三维网格。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次提出基于点云的全自动三维四腔心脏重建管线；2) 设计了 HeartFormer，包含 SA‑DSTNet 与 SA‑GFRTNet，能够在多类别语义下进行点云补全并纠正错位；3) 构建了首个公开的大规模多类别心脏点云数据集 Heart‑Compv1；4) 在 Heart‑Compv1 与 UK Biobank 上实现了显著优于现有单类别和多类别补全方法的性能。与以往方法相比，HeartFormer 兼顾全局与子结构语义，采用 Transformer 进行上下文聚合，并在细化阶段保持解剖一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HeartFormer 提出了一种语义感知的 Transformer 点云补全框架，能够从稀疏的 cine MRI 切片中重建高保真、解剖一致的三维四腔心脏模型，并通过首个大规模心脏点云数据集验证其优越性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.&lt;/p&gt;</description></item><item><guid>2512.00927v1</guid><title>LAHNet: Local Attentive Hashing Network for Point Cloud Registration</title><link>http://arxiv.org/abs/2512.00927v1</link><author>Wentao Qu, Xiaoshui Huang, Liang Xiao</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 LAHNet 的局部注意哈希网络，用于点云配准，利用局部注意机制和局部敏感哈希实现更广阔的感受野，从而生成更具区分度的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的基于学习的点云描述子主要关注局部信息，缺乏足够的感受野，导致特征区分度不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入局部注意机制和局部敏感哈希，扩展点云描述子的感受野，提高特征的区分度，从而提升点云配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了 Group Transformer 捕捉长距离上下文，使用线性邻域搜索和局部敏感哈希将点云划分为不重叠窗口；采用跨窗口策略进一步扩大感受野；在此基础上引入 Interaction Transformer 计算重叠矩阵，增强窗口间的特征交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 LAHNet 能学习到鲁棒且具有区分度的特征，在室内外真实数据集上实现了显著的配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 局部注意哈希网络通过更合理的感受野和窗口交互，显著提升了点云配准的性能，为未来点云描述子研究提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注点云配准中的特征描述子学习，尤其是如何提升特征的区分度。传统方法只聚焦局部信息，导致在相似几何结构上出现匹配错误。高质量的配准对自动驾驶、机器人导航和三维重建等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 Swin Transformer 的局部注意力思想和 FCGF 的稀疏卷积 U‑Net 结构，提出使用局部敏感哈希（LSH）对点云进行线性窗口划分。随后设计 Group Transformer 以捕捉合理的长程依赖，并在瓶颈处加入 Interaction Transformer 以增强重叠区域的特征交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是通过 LSH 将无序点云划分为非重叠窗口，并在每个窗口内使用局部注意力，同时通过跨窗口交互扩大感受野。实现流程为：输入点云 → 4D 稀疏卷积下采样 → 两层 Group Transformer（U‑Net 编码器）→ Interaction Transformer（瓶颈）→ 上采样解码器恢复尺度并聚合多尺度特征 → 输出对应点的描述子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) LSH 线性窗口划分，避免体素化、KNN 或八叉树的计算开销。2) Group Transformer 结合跨窗口策略，提供合理的长程依赖。3) Interaction Transformer 通过重叠矩阵匹配窗口并进行交叉注意力，提升低重叠场景的特征区分度。与以往仅关注局部或全局注意力的工作不同，LAHNet 在保持计算效率的同时显著提升了特征区分度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LAHNet 通过局部敏感哈希划分窗口、Group Transformer 与 Interaction Transformer 的组合，学习出更具区分度的点云描述子，在室内外高低重叠场景中实现了显著的配准性能提升。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.&lt;/p&gt;</description></item><item><guid>2512.00995v1</guid><title>S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud</title><link>http://arxiv.org/abs/2512.00995v1</link><author>Han Su, Tianyu Huang, Zichen Wan, Xiaohe Wu, Wangmeng Zuo</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出S2AM3D方法，结合2D分割先验与3D一致监督，设计点一致编码器和尺度感知解码器，并构建大规模点云数据集，实验表明在多种评估中表现领先，具有鲁棒性和可控性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云分割面临数据稀缺导致3D模型泛化差，2D预训练知识引入后视角不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决3D点云分割的泛化不足和视角不一致问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; S2AM3D通过点一致编码器聚合多视角2D特征，使用3D对比学习生成全局一致点特征；尺度感知提示解码器实现实时分割粒度调整；并提供100k+样本高质量数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; S2AM3D在多种评估中取得领先性能，表现出卓越的鲁棒性和可控性，能处理复杂结构和尺寸差异显著的部件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; S2AM3D有效融合2D先验与3D监督，提升点云分割性能，并通过大规模数据集提供充分监督，展示了在复杂场景下的优越表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于部件的点云分割近年来在3D计算机视觉中受到广泛关注。然而，现有研究面临两个主要挑战：原生3D模型由于数据稀缺而缺乏泛化能力，而引入2D预训练知识往往导致不同视角下分割结果不一致。为了解决这些挑战，我们提出了S2AM3D，该方法将2D分割先验与3D一致监督相结合。我们设计了一个点一致部件编码器，通过原生3D对比学习聚合多视角2D特征，生成全局一致的点特征。随后提出了一个尺度感知提示解码器，使得通过连续尺度信号能够实时调整分割粒度。同时，我们引入了一个大规模、高质量的部件级点云数据集，包含超过10万份样本，为模型训练提供了充足的监督信号。大量实验表明，S2AM3D在多种评估设置下实现了领先性能，在处理复杂结构和尺寸差异显著的部件时表现出卓越的鲁棒性和可控性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云的部件级分割中数据稀缺导致的泛化不足以及基于2D预训练知识导致的视角不一致问题。部件级分割对于3D内容创作、机器人操作和逆向工程等应用至关重要，因为它能够提供细粒度的几何与语义信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已有的2D转3D迁移方法（如多视图提升、蒸馏）与原生3D对比学习，提出点一致编码器以聚合多视图2D特征并通过3D对比监督提升全局一致性。随后设计了可调尺度的提示解码器，利用正弦嵌入和双向交叉注意力实现实时尺度控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过2D先验与3D对比学习共同构建全局一致的点特征，再用连续尺度信号调制特征并与提示点交互，最终得到每点的分割概率。实现流程包括：①点一致编码器提取点特征；②将尺度映射为正弦嵌入并通过FiLM调制特征；③双向交叉注意力将提示点与全局特征融合；④轻量化头输出分割掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 2D-3D混合训练方案，利用原生3D对比学习提升全局一致性；2) 可连续调节尺度的提示解码器，支持实时细粒度控制；3) 大规模100k+点云数据集，提供丰富监督。与以往方法相比，S2AM3D在保持高精度的同时实现了更好的泛化、尺度可控性和无需后处理的端到端推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S2AM3D提出了一种融合2D先验与3D对比学习的可尺度点云部件分割框架，凭借大规模数据集实现了领先的精度与实时可调粒度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.&lt;/p&gt;</description></item><item><guid>2512.01178v1</guid><title>VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering</title><link>http://arxiv.org/abs/2512.01178v1</link><author>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为VSRD++的弱监督单目三维目标检测框架，利用神经场体素渲染和弱二维监督，消除了对三维标注的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单目三维目标检测是三维场景理解的基础任务，但现有方法高度依赖大量三维标注，通常需要从激光雷达点云中进行繁重的人工标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建弱监督框架，解决三维标注成本高的问题，并实现高质量的单目三维目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; VSRD++采用两阶段流程：第一阶段多视角自动标注，使用签名距离场（SDF）表示物体表面，并通过实例感知体素轮廓渲染生成实例掩码；将SDF拆分为立方体SDF和残差距离场（RDF）以优化三维边界框；通过在边界框属性中加入速度并为每个伪标签赋予置信度来处理动态物体的几何不一致；使用三维属性初始化模块初始化动态边界框参数。第二阶段使用优化后的三维边界框作为伪标签训练单目三维目标检测器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在KITTI-360数据集上，VSRD++在静态和动态场景中显著优于现有弱监督方法，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; VSRD++通过弱监督方式实现了高性能的单目三维目标检测，消除了对三维标注的依赖，为实际应用提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 单目三维目标检测是三维场景理解的基础但具有挑战性。现有方法高度依赖于带有大量三维标注的监督学习，这些标注通常通过激光雷达点云的劳动密集型标注过程获得。为了解决这个问题，我们提出了VSRD++，一种新颖的弱监督框架，消除了对三维标注的依赖，并利用基于神经场的体素渲染与弱二维监督。VSRD++由两阶段流程组成：多视角三维自动标注和随后的单目三维检测器训练。在多视角自动标注阶段，物体表面以签名距离场（SDF）表示，并通过提出的实例感知体素轮廓渲染生成实例掩码。为优化三维边界框，我们将每个实例的SDF拆分为立方体SDF和捕捉与立方体偏差的残差距离场（RDF）。为了解决在动态物体上应用体素渲染方法时常见的几何不一致问题，我们通过在边界框属性中加入速度以及为每个伪标签赋予置信度来对动态物体进行建模。此外，我们还采用三维属性初始化模块来初始化动态边界框参数。在单目三维目标检测阶段，优化后的三维边界框作为伪标签用于训练单目三维目标检测器。在KITTI-360数据集上进行的广泛实验表明，VSRD++在静态和动态场景中显著优于现有弱监督方法。代码可在 https://github.com/Magicboomliu/VSRD_plus_plus 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在没有3D标注的情况下进行单目3D目标检测的问题。由于3D标注需要依赖激光雷达点云并且人工成本高昂，缺乏足够的3D数据限制了自动驾驶等场景中检测系统的普及与扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将神经场体素渲染（NeRF、NeuS）与实例分割的2D监督相结合，提出实例感知体素轮廓渲染和SDF分解技术，并在此基础上加入速度属性和置信度权重。该设计借鉴了VSRD、NeuS、D-NeRF等现有工作，并在此基础上实现了对动态物体的建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多视角的体素渲染将每个目标的表面表示为SDF（由立方体SDF和残差SDF组成），并利用实例感知渲染生成实例掩码，利用2D掩码监督优化3D边界框和速度。流程分为两阶段：第一阶段在多视角图像中自动生成3D伪标签；第二阶段使用这些伪标签（并给每个标签分配置信度）训练单目3D检测器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）实例感知体素轮廓渲染；2）SDF分解为立方体SDF和残差SDF；3）动态物体的速度属性和置信度权重；4）利用自监督深度初始化3D属性。与以往需要LiDAR或合成数据的监督方法不同，VSRD++仅依赖2D实例掩码和多视角图像即可完成3D检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VSRD++提出了一种完全弱监督的单目3D检测框架，通过实例感知体素轮廓渲染和动态SDF优化，从2D掩码生成高质量3D伪标签，实现无需3D标注的精确检测。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance&amp;#x27;s SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus&lt;/p&gt;</description></item><item><guid>2512.01352v1</guid><title>OpenBox: Annotate Any Bounding Boxes in 3D</title><link>http://arxiv.org/abs/2512.01352v1</link><author>In-Jae Lee, Mungyeom Kim, Kwonyoung Ryu, Pierre Musacchio, Jaesik Park</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; OpenBox是一种两阶段自动标注流程，利用二维视觉基础模型将二维图像中的实例信息与三维点云对齐，并根据物体的刚性与运动状态生成自适应尺寸的三维边界框，从而在不需要自训练的情况下提供高质量的三维目标检测标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶等场景中，无监督且开放词汇的三维目标检测受到关注，主要因为降低标注成本和识别未见物体对安全与可扩展性至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有方法统一标注三维边界框、忽略物体物理状态以及需要多轮自训练导致标注质量不佳和计算开销大的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 第一阶段通过跨模态实例对齐，将二维图像中由视觉基础模型提取的实例级线索与对应的三维点云关联；第二阶段对实例按刚性和运动状态分类，并利用类别特定尺寸统计生成自适应边界框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Waymo Open Dataset、Lyft Level 5 Perception dataset和nuScenes dataset上的实验表明，OpenBox在准确性和效率上均优于基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenBox能够在无需自训练的情况下生成高质量的三维边界框标注，并在多数据集上实现了更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; OpenBox是一种两阶段自动标注流程，利用二维视觉基础模型将二维图像中的实例信息与三维点云对齐，并根据物体的刚性与运动状态生成自适应尺寸的三维边界框，从而在不需要自训练的情况下提供高质量的三维目标检测标注。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在自动生成任意类别的 3D 边界框，减少人工标注成本并支持开放词汇检测。此问题在自动驾驶等安全关键场景中至关重要，因为高质量的 3D 目标检测直接影响路径规划与车辆控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了 2D 视觉基础模型（Grounding DINO、SAM2）与 LiDAR 点云的跨模态对齐，借鉴了无监督 3D 检测与多模态融合的思路，但避免了迭代自训练。通过上下文感知细化和物理状态分类，提升了标注质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先将 2D 目标实例投影到 3D 点云并进行上下文细化，再根据实例的刚性与运动状态生成自适应边界框。流程包括：2D 检测+分割 → 投影到点云 → 上下文感知细化 → 物理类型分类 → 依据类别尺寸统计与 SDF 过滤生成 3D 边界框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）无需自训练即可完成标注；2）使用 2D 视觉基础模型提供高质量实例信息；3）上下文感知细化融合 LiDAR 聚类与图像掩码；4）表面感知噪声过滤与基于物理状态的自适应框生成。与以往方法相比，OpenBox 更加关注物理属性，避免了多轮迭代和输出级融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenBox 通过将 2D 视觉基础模型的实例信息与 LiDAR 点云对齐，自动生成高质量、物理状态自适应的 3D 边界框，消除了迭代自训练的需求。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects&amp;#x27; physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.&lt;/p&gt;</description></item><item><guid>2512.01850v1</guid><title>Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching</title><link>http://arxiv.org/abs/2512.01850v1</link><author>Yue Pan, Tao Sun, Liyuan Zhu, Lucas Nunes, Iro Armeni, Jens Behley, Cyrill Stachniss</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种将点云配准视为条件生成的方法，通过学习连续点级速度场将噪声点迁移到已配准场景，从而直接生成配准点云并恢复各视角姿态。该方法在低重叠、不同尺度和传感器模式下表现优异，并支持多机器人SLAM等下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是将多个未姿态点云对齐到公共坐标系的核心步骤，广泛用于三维重建和机器人定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新的配准框架，避免传统对应匹配和多视角变换优化，直接生成配准点云并提高鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用轻量级局部特征提取器和测试时刚性约束，学习连续点级速度场进行条件生成；通过速度场将噪声点迁移到配准场景，并从中恢复姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 与传统方法相比，该模型在配准基准上取得最先进的结果，尤其在低重叠情况下表现突出，并能跨尺度和传感器模式泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法实现了高效、鲁棒的点云配准，可直接生成配准点云，并支持多机器人SLAM、重定位和多会话地图合并等应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准将多个未姿态点云对齐到公共坐标系，是三维重建和机器人定位的核心步骤。在本研究中，我们将配准视为条件生成：学习得到的连续点级速度场将噪声点迁移到已配准场景，从而恢复每个视角的姿态。与以往通过对应匹配估计点云对之间变换并优化多视角变换实现配准的方法不同，我们的模型直接生成配准点云。通过轻量级局部特征提取器和测试时刚性约束，我们的方法在配准基准上取得了最先进的结果，尤其在低重叠情况下表现突出，并能跨尺度和传感器模式泛化。它还支持下游任务，包括重定位、多机器人SLAM和多会话地图合并。源代码可在 https://github.com/PRBonn/RAP 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了将多张未标定的三维点云对齐到同一坐标系的问题，这是三维重建、机器人定位和SLAM等应用的核心步骤。由于真实世界的扫描往往稀疏、噪声大、视角重叠有限，传统的两阶段配准方法在低重叠或大规模场景中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将配准视为条件生成任务，借鉴了RPF、DiffusionReg等基于流匹配和扩散的生成模型，提出单阶段流匹配网络。为保持刚性，他们在采样过程中强制投影到每个视角的SE(3)轨道，并使用刚性误差进行样本选择。整体流程还引入了关键点采样与规范化，以实现跨尺度、跨传感器的泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个神经速度场，将高斯噪声点云逐步变换为合并后的注册点云，并在每一步通过刚性投影保证每个视角的变换保持刚性。实现流程包括：① 采样稀疏关键点并提取局部特征；② 将所有视角规范化到共享相似性不变坐标系；③ 用流匹配Transformer根据条件生成完整的注册点云；④ 在采样过程中对中间结果进行刚性投影并计算刚性误差；⑤ 选取刚性误差最小的生成结果，使用SVD恢复每个视角的刚性变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 单阶段流匹配模型直接输出全局一致的点云，省去两阶段配准和图优化；2) 引入刚性强制采样和刚性误差选择，提升低重叠和大规模场景的鲁棒性；3) 通过大规模多数据集训练，模型在不同尺度、传感器和视角数下均能泛化；4) 采用关键点规范化实现跨尺度一致性。与以往的两阶段配准、RPF等仅限对象级的工作相比，该方法可处理任意数量视角、低重叠和大范围环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种单阶段、基于流匹配的神经网络，能够直接生成多视角点云的全局一致合并结果，并通过刚性强制和误差选择实现高精度、跨尺度、跨传感器的配准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.&lt;/p&gt;</description></item><item><guid>2512.02972v1</guid><title>BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2512.02972v1</link><author>Guowen Zhang, Chenhang He, Liyi Chen, Lei Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种以 LiDAR 为中心的 BEV 膨胀框架 BEVDilation，利用图像特征作为隐式引导，解决传统融合中几何误差导致的性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在 3D 目标检测中，融合 LiDAR 与相机的 BEV 表示已被证明有效，但由于两种传感器几何精度差异，直接拼接往往导致性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种优先考虑 LiDAR 信息、并通过图像隐式引导来缓解空间失配的融合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BEVDilation 通过将图像 BEV 特征视为隐式引导而非简单拼接，提出稀疏体素膨胀块以利用图像先验密化前景体素，并引入语义引导 BEV 膨胀块以增强 LiDAR 特征扩散并捕获长程上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 nuScenes 基准上，BEVDilation 在保持计算效率的同时，优于现有最先进方法，并且对深度噪声更具鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 以 LiDAR 为中心、图像隐式引导的 BEVDilation 能有效缓解传感器几何差异带来的误差，提升 3D 检测性能并增强对深度噪声的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在鸟瞰图（BEV）表示中整合 LiDAR 与相机信息已被证明在 3D 目标检测中有效。然而，由于这些传感器在几何精度上的根本差异，先前方法中无差别的融合往往导致性能下降。本文提出 BEVDilation，一种新颖的以 LiDAR 为中心的框架，优先考虑 LiDAR 信息进行融合。通过将图像 BEV 特征视为隐式引导而非简单拼接，我们的策略有效缓解了图像深度估计误差导致的空间失配。此外，图像引导还能有效帮助以 LiDAR 为中心的范式解决点云的稀疏性和语义限制。具体而言，我们提出了稀疏体素膨胀块，通过图像先验密化前景体素来缓解固有的点稀疏性。我们还引入了语义引导 BEV 膨胀块，以图像语义引导和长程上下文捕获增强 LiDAR 特征扩散处理。在具有挑战性的 nuScenes 基准上，BEVDilation 在保持竞争性计算效率的同时，取得了比最先进方法更好的性能。重要的是，我们的以 LiDAR 为中心的策略相较于简单融合表现出更强的对深度噪声的鲁棒性。源代码可在 https://github.com/gwenzhang/BEVDilation 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进 LiDAR 与相机的多模态融合，以提升 3D 目标检测的精度和鲁棒性。由于 LiDAR 提供精确的几何信息，而相机则缺乏可靠的深度估计，传统的无差别融合往往导致空间失配和性能下降，这在自动驾驶等安全关键场景中尤为重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出 LiDAR 与相机在几何精度上的差距，决定以 LiDAR 为主导并将图像特征作为指导。设计中借鉴了 BEV 融合、Mamba 的全局感受野、以及多模态可变形卷积等现有技术，并在此基础上提出了稀疏体素膨胀块和语义引导 BEV 膨胀块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是以 LiDAR 为主，利用图像特征来补充稀疏性和语义信息。实现流程包括：①分别用 LiDAR 与多视角图像提取 BEV 特征；②使用稀疏体素膨胀块（SVDB）根据前景掩码在 LiDAR 前景中插入可学习的体素并用 Mamba 进行全局细化；③使用语义引导 BEV 膨胀块（SBDB）通过多模态可变形卷积在 LiDAR BEV 上进行特征扩散；④将融合后的 BEV 送入检测头得到 3D 目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出 LiDAR‑centric 融合框架，避免了深度误差导致的空间失配；②SVDB 通过学习体素嵌入和 Mamba 细化有效稀疏化 LiDAR；③SBDB 采用多模态可变形卷积在 LiDAR 上进行语义引导的特征扩散；④整体方法在保持计算效率的同时，在 nuScenes 上实现了新的 state‑of‑the‑art。与以往无差别融合或仅在图像上进行补充的做法不同，BEVDilation 通过图像指导来增强 LiDAR 的几何主导性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BEVDilation 通过 LiDAR‑centric 融合与图像引导的稀疏体素膨胀与语义扩散，显著提升 3D 检测性能并增强对深度噪声的鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Integrating LiDAR and camera information in the bird&amp;#x27;s eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.&lt;/p&gt;</description></item><item><guid>2512.02991v1</guid><title>GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection</title><link>http://arxiv.org/abs/2512.02991v1</link><author>Md Sohag Mia, Md Nahid Hasan, Tawhid Ahmed, Muhammad Abdullah Adnan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出GraphFusion3D框架，结合多模态融合和高级特征学习，显著提升3D目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云数据稀疏、结构不完整、语义信息有限，且远距离物体间的上下文关系难以捕捉，导致3D检测面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决点云检测中的稀疏性、结构缺失和语义不足问题，并有效建模远距离物体的上下文关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) Adaptive Cross-Modal Transformer (ACMT) 适配性地将图像特征融入点云表示，丰富几何和语义信息；2) Graph Reasoning Module (GRM) 通过多尺度图注意力建模邻域关系，捕捉局部几何和全局语义；3) 级联解码器逐步细化检测结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在SUN RGB-D和ScanNetV2数据集上，GraphFusion3D分别取得70.6% AP25、51.2% AP50和75.1% AP25、60.8% AP50的成绩，明显优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GraphFusion3D通过多模态融合和图推理有效提升3D目标检测精度，证明了其在复杂点云场景中的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管在3D目标检测方面取得了显著进展，但由于点云稀疏、结构不完整以及语义信息有限，仍然面临挑战。捕捉远距离物体之间的上下文关系也带来了额外困难。为了解决这些挑战，我们提出了GraphFusion3D，一个统一的框架，结合多模态融合和先进的特征学习。我们的方法引入了自适应跨模态变换器（ACMT），它自适应地将图像特征整合到点云表示中，以丰富几何和语义信息。为了提议细化，我们引入了图推理模块（GRM），一种新机制，建模邻域关系，以同时捕捉局部几何结构和全局语义上下文。该模块采用多尺度图注意力，动态加权提议之间的空间接近度和特征相似度。我们进一步使用级联解码器，通过多阶段预测逐步细化检测。SUN RGB-D（70.6% AP25和51.2% AP50）和ScanNetV2（75.1% AP25和60.8% AP50）上的广泛实验表明，与现有方法相比，性能有显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从稀疏点云中准确检测3D物体的问题，主要挑战是点云稀疏、结构不完整以及缺乏语义信息。该问题在机器人、自动驾驶和增强现实等领域至关重要，因为这些应用需要精确的三维感知来安全、智能地与环境交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的投票式、稀疏卷积和多模态融合方法（如ImVoteNet、EPNet++、TokenFusion等）的基础上，提出了图推理模块和自适应跨模态变压器，以捕捉提议之间的上下文关系并融合图像与点云信息。设计过程中借鉴了Transformer、Deform-DETR以及多尺度注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先通过图推理模块对提议进行多尺度图注意力聚合，随后使用自适应跨模态变压器在点云与图像特征之间进行动态加权融合，最后通过级联解码器逐步细化检测结果。实现流程包括特征提取 → 图推理 → 跨模态融合 → 级联细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 多尺度图注意力的图推理模块，用于捕获局部几何与全局语义关系；2) 自适应跨模态变压器与跨模态门控机制，实现动态权重平衡；3) 级联细化解码器，实现多阶段逐步改进。与之前工作相比，它将图推理嵌入Transformer解码器，动态调节模态贡献，并在两阶段推理中实现更丰富的上下文建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GraphFusion3D通过图推理与自适应跨模态融合相结合，并采用级联细化解码器，显著提升了稀疏点云中的3D目标检测精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.&lt;/p&gt;</description></item><item><guid>2512.03010v1</guid><title>SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting</title><link>http://arxiv.org/abs/2512.03010v1</link><author>Svenja Strobel, Matthias Innmann, Bernhard Egger, Marc Stamminger, Linus Franke</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LiDAR 点云在平坦区域精度高，但易遗漏细小结构和暗色材料；摄影测量可补充细节，但 LiDAR 在无特征区域仍优。本文提出 SurfFill，利用高斯表面元完成 LiDAR 缺失，先分析光束发散导致的伪影，采用密度变化启发式定位缺失点，随后在这些模糊区域进行点生长与重建，并用分治策略扩展到建筑规模，实验表明该方法优于以往方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; LiDAR 被视为主动 3D 重建的金标准，但在薄结构、边缘和暗色吸收材料上易失效；摄影测量可捕获细节，但 LiDAR 在无特征区域的精度仍难以匹敌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 结合 LiDAR 与相机捕获的优势，提出一种基于高斯表面元的 LiDAR 补全方案，以提高点云的完整性和精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 分析 LiDAR 光束发散导致的伪影，利用点云密度变化识别缺失区域；在这些模糊点附近进行点生长，限制高斯表面元重建聚焦于缺失区域；提取并采样高斯原语完成点云；为大规模重建引入分治策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示 SurfFill 在合成和真实场景的 LiDAR 点云补全任务中，性能优于之前的重建方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SurfFill 能有效补全 LiDAR 点云，利用密度启发式和高斯表面元重建实现精细补全，并通过分治方案实现大规模建筑级别的扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; LiDAR 捕获的点云通常被视为主动 3D 重建的金标准。虽然它们在平坦区域的精度异常出色，但捕获过程容易遗漏细小几何结构，并且在暗色、吸收性材料上可能失效。另一种方法是对场景拍摄多张照片并应用 3D 摄影测量技术，这可以推断出这些细节，因为它们往往代表特征丰富的区域。然而，LiDAR 在无特征区域的精度很少能达到。为此，我们通过引入 SurfFill：一种基于高斯表面元的 LiDAR 补全方案，建议结合 LiDAR 与基于相机的捕获优势。我们分析了 LiDAR 捕获，并将 LiDAR 光束发散视为产生伪影的主要因素，主要表现在薄结构和边缘。基于此洞察，我们通过评估点云密度变化引入了一个模糊启发式，用于识别接近缺失区域的点，从而可以从这些点开始生长额外的点以完成扫描。为此点生长，我们将高斯表面元重建（Huang 等 2024）限制在这些模糊区域，聚焦优化和密度化。最后，从模糊区域的重建中提取并采样高斯原语，以完成点云。为应对大规模重建的挑战，我们在该流程中加入了分治方案，用于建筑规模的点云补全。我们在合成和真实场景的 LiDAR 点云补全任务上进行评估，发现我们的方法优于以往的重建方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在补全 LiDAR 点云中因光束发散、暗面或反射面导致的缺失小结构和薄边缘。由于 LiDAR 是三维重建的金标准，缺失细节会影响后续地图构建、机器人导航和建筑测量等应用的精度与可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了 LiDAR 的典型误差来源，提出基于点密度的模糊性启发式来定位可能缺失的区域。随后借鉴 2D Gaussian Splatting、Gaussian surfel 以及多视图立体视觉的技术，将该启发式嵌入到受限的 Gaussian 细化过程中，以在缺失区域进行精细重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先识别点云中密度低的模糊区，然后在这些区内用受限的 Gaussian surfel 进行局部重建，最后从重建的 Gaussian 中采样点补齐原始点云。实现流程包括：① 下采样 LiDAR 并计算模糊性分数；② 用 2D Gaussian Splatting 在模糊区内优化 Gaussian；③ 过滤并采样得到新点；④ 将新点与原点云合并。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 用点密度启发式识别 LiDAR 缺失区域；② 将该启发式与 Gaussian surfel 结合，形成针对缺失区的受限重建；③ 设计了专门的约束、损失和采样策略；④ 采用分块（divide‑and‑conquer）方案处理大规模建筑级点云。与以往的形状补全或光度重建方法不同，SurfFill 直接利用 LiDAR 的高精度信息并在缺失区进行局部补全，显著提升了完整性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SurfFill 通过识别 LiDAR 点云中的模糊区并在这些区内使用受限的 Gaussian surfel 重建，提供了一种高效、精确的大规模点云补全方法，优于现有技术。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.&lt;/p&gt;</description></item><item><guid>2512.03598v1</guid><title>Memory-Guided Point Cloud Completion for Dental Reconstruction</title><link>http://arxiv.org/abs/2512.03598v1</link><author>Jianan Sun, Yukang Huang, Dongzhihan Wang, Mingyu Fan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种检索增强框架，用于完成部分牙齿点云，框架将原型记忆集成到标准的编码-解码管线中，提供结构先验，提升完成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 部分牙齿点云常因遮挡和扫描视角受限而缺失大片区域，导致仅使用编码器的全局特征偏差，迫使解码器产生假象结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用可学习的原型记忆，借助跨样本规律稳定缺失区域推断，释放解码器容量用于细节恢复，从而实现更准确、真实的牙齿点云完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先将部分输入编码为全局描述符，再从可学习记忆中检索最近的原型，并通过置信门控加权与查询特征融合，随后解码。记忆端到端优化，自组织为可复用的牙齿形状原型，无需牙齿位置标签；该模块可插拔，兼容常见完成骨干网络，保持相同训练损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在自制的 Teeth3DS 基准上实验显示，Chamfer Distance 一致下降，视觉效果显示尖锐的咬合尖、脊纹和邻牙过渡更清晰。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 检索增强框架通过利用跨样本规律，提供了简单有效的方式，实现更准确、可信的牙齿点云完成，同时保持解码器对细节的恢复能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 部分牙齿点云常因遮挡和扫描视角受限而缺失大片区域，导致仅使用编码器的全局特征偏差，迫使解码器产生假象结构。我们提出一种检索增强框架，用于牙齿完成，将原型记忆集成到标准的编码-解码管线中。将部分输入编码为全局描述符后，模型从可学习记忆中检索最近的流形原型，并通过置信门控加权与查询特征融合后再解码。记忆端到端优化，自组织为可复用的牙齿形状原型，无需牙齿位置标签，从而提供结构先验，稳定缺失区域推断，释放解码器容量用于细节恢复。该模块可插拔，兼容常见完成骨干网络，保持相同训练损失。在自制的 Teeth3DS 基准上实验显示，Chamfer Distance 一致下降，视觉效果显示尖锐的咬合尖、脊纹和邻牙过渡更清晰。我们的方案提供了一种简单而有效的方式，利用跨样本规律实现更准确、真实的牙齿点云完成。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决牙齿点云在扫描时因遮挡、视角有限而产生的大面积缺失区域，导致传统编码器-解码器网络在完成时产生偏差和细节模糊。准确恢复牙齿形状对于临床诊断、修复和数字化牙科设计至关重要，缺失细节会影响治疗效果和模型精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到编码器产生的全局特征因缺失而偏差，决定在特征形成时主动引入结构先验。借鉴了检索增强和记忆库的思想，设计了双编码器、可学习的原型记忆和置信度门控融合机制，并参考了 FoldingNet、PCN 等现有点云完成网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过检索记忆库中的最近原型来纠正偏差的全局特征，然后与原始特征按置信度加权融合，再由解码器生成完整点云。实现流程包括：①双编码器提取部分点云和完整点云的全局描述符；②在记忆库中检索与部分描述符最近的原型；③使用置信度门控将原型与部分描述符融合得到去偏特征；④解码器根据融合特征生成完成的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①可插拔的原型记忆库，能够自组织捕捉牙齿形状规律；②置信度门控融合机制，动态调节原型对特征的影响；③无需牙齿位置标签即可获得结构先验；④在保持相同训练损失的前提下显著提升 Chamfer Distance。与以往依赖更深网络或通用先验的工作不同，MEM4TEETH直接利用跨样本形状相似性进行检索增强。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MEM4TEETH通过检索并融合可学习的牙齿形状原型，去偏编码器特征，从而在无需额外标签的情况下显著提升牙齿点云完成的准确性和细节恢复。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.&lt;/p&gt;</description></item><item><guid>2512.04996v1</guid><title>A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs</title><link>http://arxiv.org/abs/2512.04996v1</link><author>Qiong Chang, Weimin Wang, Junpei Zhong, Jun Miyazaki</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种内存高效的优化策略，使得高性能点云配准算法VANICP能够在资源受限的嵌入式GPU上轻量化运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; VANICP是一种加速框架，通过将全局最近邻搜索转化为局部过程，显著提升点云应用的计算效率，但其原始实现占用大量内存，限制了在嵌入式系统中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决VANICP在嵌入式环境下的内存占用过高问题，实现轻量化部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出面向GPU的动态内存分配策略，优化膨胀操作的内存使用，并基于该策略构建改进版VANICP框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 改进版VANICP在保持原有性能的同时，内存消耗降低了97%以上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过动态内存分配策略，VANICP可以在嵌入式GPU上实现高效、轻量化的点云配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种内存高效的优化策略，用于高性能点云配准算法VANICP，使其能够在受限硬件资源的嵌入式GPU上轻量化执行。VANICP是一种最近发布的加速框架，显著提升了基于点云的应用的计算效率。通过将全局最近邻搜索转化为通过膨胀式信息传播机制的局部过程，VANICP大幅降低了最近邻搜索的计算复杂度。然而，其原始实现需要大量内存，限制了其在嵌入式系统等资源受限环境中的部署。为解决此问题，我们提出了一种面向GPU的动态内存分配策略，优化了膨胀操作的内存使用。此外，基于该策略，我们构建了VANICP框架的增强版本，在保持原始性能的同时，内存消耗降低了97%以上。源代码已发布在：https://github.com/changqiong/VANICP4Em.git。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了 VANICP 在嵌入式 GPU 上因静态预分配导致的高内存占用问题。高内存占用限制了该算法在资源受限的移动设备和边缘计算场景中的部署，而这些场景正是 3D 感知技术快速发展的关键领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了 VANICP 的内存分配方式，发现其固定 64KB/体素的策略导致大量浪费。随后借鉴了 GPU 的统一内存、哈希表计数和前缀和技术，并参考了 VANICP、HNSW 等加速 ICP 的工作，提出在 GPU 上并行构建体素占用直方图，在 CPU 上计算地址偏移，从而实现动态分配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是根据每个体素实际点数动态分配内存，并通过地址偏移实现间接访问。实现流程包括：① 体素化并在 GPU 上并行统计每个体素的点数；② 在 CPU 上对直方图做前缀和得到每个体素的起始地址；③ 使用这些偏移在统一内存中按需写入点索引；④ 进行体素膨胀和局部最近邻搜索，所有操作均通过间接寻址完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① GPU‑面向的动态内存分配策略；② 分段指针式内存布局与间接寻址；③ 利用统一内存实现 GPU 与 CPU 的无缝协作；④ 在保持原有性能的前提下将内存占用降低 97%。与 VANICP 的静态单块分配相比，这些改进显著提升了嵌入式系统的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种基于体素占用直方图的动态内存分配策略，使得基于膨胀的 ICP 在嵌入式 GPU 上的内存使用降低 97% 而不影响计算速度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.&lt;/p&gt;</description></item><item><guid>2512.05270v1</guid><title>XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots</title><link>http://arxiv.org/abs/2512.05270v1</link><author>Tianyi Wang, Jiseop Byeon, Ahmad Yehia, Huihai Wang, Yiming Xu, Tianyi Zeng, Ziran Wang, Junfeng Jiao, Christian Claudel</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了XR-DT框架，利用扩展现实技术构建数字孪生，帮助移动机器人与人类在共享工作空间中实现安全、高效、可解释的交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着移动机器人在共享工作空间中的应用增多，如何保证人机交互的安全、效率和可解释性成为挑战。虽然已有研究关注人类行为预测，但对人类如何感知、解释和信任机器人推理的关注不足，限制了其在安全关键和社会嵌入环境中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个桥接物理与虚拟空间的XR-DT框架，实现人机双向理解，提升人机交互的可解释性、可信度和适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用分层XR-DT架构，集成虚拟、增强和混合现实层，融合实时传感器数据、Unity游戏引擎中的仿真环境以及可穿戴AR设备收集的人类反馈；在此框架下设计统一扩散策略的移动机器人系统，实现上下文感知任务适配；引入链式思维提示机制，让多模态大型语言模型根据人类指令和环境上下文进行推理；使用AutoGen多智能体协同层提升动态任务的鲁棒性与协作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 初步实验表明，该框架能够准确预测人类和机器人的轨迹，验证了XR-DT在HRI任务中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过将人类意图、环境动态和机器人认知嵌入XR-DT框架，系统实现了可解释、可信且自适应的人机交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着移动机器人在共享工作空间中与人类共同工作，确保安全、高效且可解释的人机交互（HRI）已成为一项紧迫挑战。虽然在预测人类行为方面已取得显著进展，但对人类如何感知、解释和信任机器人推理的关注有限，阻碍了其在安全关键和社会嵌入环境中的部署。本文提出了XR-DT，即一种利用扩展现实增强的数字孪生框架，旨在桥接物理与虚拟空间，实现人机双向理解。我们的分层XR-DT架构集成了虚拟、增强和混合现实层，融合实时传感器数据、Unity游戏引擎中的仿真环境以及通过可穿戴AR设备捕获的人类反馈。在此框架下，我们设计了一个统一扩散策略的移动机器人系统，实现上下文感知的任务适配。我们进一步提出了链式思维提示机制，使多模态大型语言模型能够在考虑人类指令和环境上下文的基础上进行推理，并利用基于AutoGen的多智能体协同层提升动态任务的鲁棒性和协作能力。初步实验结果表明，该框架能够准确预测人类和机器人的轨迹，验证了XR-DT在HRI任务中的有效性。通过将人类意图、环境动态和机器人认知嵌入XR-DT框架，我们的系统实现了可解释、可信且自适应的人机交互。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决移动机器人与人类共享工作空间时的安全、高效、可解释的交互问题。该问题重要，因为缺乏对机器人推理的可理解性会限制其在安全关键或社会嵌入环境中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将数字孪生、扩展现实和代理式人工智能结合，构建了 VR、AR、MR 三层架构，并引入链式思考提示、扩散策略和 AutoGen 多智能体框架。设计借鉴了现有的数字孪生、MR HRI、LLM 推理、扩散模型和多智能体系统等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过实时传感器融合和多模态推理，在虚拟、增强和混合现实层面实现人机双向理解。实现流程为：1）AR 设备采集人类多模态数据，机器人采集环境数据；2）Real‑World Agentic AI 预测轨迹并在 AR 层叠加提示；3）VR 层使用统一扩散策略进行仿真优化；4）MR 层将两者结果融合，生成可执行命令并反馈给机器人和人类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① XR‑DT 三层架构实现物理与虚拟双向同步；② 统一扩散策略兼顾任务通用与特定行为；③ 链式思考提示提升多模态 VLM 推理；④ AutoGen 多智能体协同提升鲁棒性。与以往仅在 VR 或单一任务场景下的 HRI 研究不同，本文提供了实时、可解释、跨模态的全流程解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XR‑DT 通过整合扩展现实、数字孪生与代理式 AI，提供实时、可解释且可信的移动机器人人机交互框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As mobile robots increasingly operate alongside humans in shared workspaces, ensuring safe, efficient, and interpretable Human-Robot Interaction (HRI) has become a pressing challenge. While substantial progress has been devoted to human behavior prediction, limited attention has been paid to how humans perceive, interpret, and trust robots&amp;#x27; inferences, impeding deployment in safety-critical and socially embedded environments. This paper presents XR-DT, an eXtended Reality-enhanced Digital Twin framework for agentic mobile robots, that bridges physical and virtual spaces to enable bi-directional understanding between humans and robots. Our hierarchical XR-DT architecture integrates virtual-, augmented-, and mixed-reality layers, fusing real-time sensor data, simulated environments in the Unity game engine, and human feedback captured through wearable AR devices. Within this framework, we design an agentic mobile robot system with a unified diffusion policy for context-aware task adaptation. We further propose a chain-of-thought prompting mechanism that allows multimodal large language models to reason over human instructions and environmental context, while leveraging an AutoGen-based multi-agent coordination layer to enhance robustness and collaboration in dynamic tasks. Initial experimental results demonstrate accurate human and robot trajectory prediction, validating the XR-DT framework&amp;#x27;s effectiveness in HRI tasks. By embedding human intention, environmental dynamics, and robot cognition into the XR-DT framework, our system enables interpretable, trustworthy, and adaptive HRI.&lt;/p&gt;</description></item><item><guid>2512.05482v1</guid><title>Concept-based Explainable Data Mining with VLM for 3D Detection</title><link>http://arxiv.org/abs/2512.05482v1</link><author>Mai Tsujimoto</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种跨模态框架，利用二维视觉语言模型识别并挖掘驾驶场景中的稀有物体，从而提升基于点云的三维目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶系统中，仅使用点云数据进行稀有物体检测仍是难题；视觉语言模型在图像理解方面表现优异，但其在三维检测中的潜力尚未充分挖掘。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索利用二维视觉语言模型进行稀有物体挖掘，以减少标注工作量并提升三维检测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将目标检测、语义特征提取、降维、Isolation Forest 与 t‑SNE 结合的多维异常检测，并通过概念过滤识别语义上有意义的稀有物体，重点提取如施工车辆、摩托车、障碍物等概念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 nuScenes 数据集上，概念引导的数据挖掘策略在仅使用少量训练数据的情况下，显著提升了三维检测模型的性能，尤其在拖车和自行车等难检测类别上优于随机采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法通过高效挖掘稀有物体概念，既降低了标注成本，又提升了安全关键自动驾驶系统的数据集构建效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 稀有物体检测在自动驾驶系统中仍是一个具有挑战性的任务，尤其当仅依赖点云数据时。尽管视觉语言模型在图像理解方面表现出强大的能力，但它们通过智能数据挖掘提升三维物体检测的潜力尚未得到充分探索。本文提出了一种新颖的跨模态框架，利用二维视觉语言模型识别并挖掘驾驶场景中的稀有物体，从而提升三维物体检测性能。我们的方法将目标检测、语义特征提取、降维以及多维异常检测等互补技术融合成一个可解释的流程，系统地识别驾驶场景中的稀有但关键物体。通过结合 Isolation Forest 与基于 t‑SNE 的异常检测方法以及基于概念的过滤，该框架能够有效识别语义上有意义的稀有物体。该方法的一个关键优势在于能够提取并标注针对性的稀有物体概念，如施工车辆、摩托车和障碍物，从而大幅降低标注负担，并仅关注最有价值的训练样本。对 nuScenes 数据集的实验表明，这种概念引导的数据挖掘策略在仅使用少量训练数据的情况下提升了三维物体检测模型的性能，尤其在拖车和自行车等具有挑战性的物体类别上相较于相同量的随机数据有显著改进。这一发现对安全关键自动驾驶系统中数据集的高效策划具有重要意义。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决自动驾驶中稀有物体检测的难题。稀有物体在训练数据中出现频率低，导致检测模型对这些安全关键场景的鲁棒性不足，影响系统的安全性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了视觉‑语言模型（如 CLIP、Qwen2‑VL）与传统的异常检测技术（t‑SNE、Isolation Forest），并参考了现有的概念瓶颈模型、数据挖掘和多模态融合方法，构建了一个跨模态的可解释数据挖掘框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 2D 图像的语义嵌入来识别稀有物体，并通过概念过滤挑选有价值的训练样本。实现流程包括：① 用 YOLOv8 检测并裁剪物体；② 用 CLIP 提取嵌入；③ 用 t‑SNE 与 Isolation Forest 检测异常；④ 用 Qwen2‑VL 生成描述并匹配概念；⑤ 过滤出稀有概念；⑥ 选取包含目标或稀有概念的场景进行 3D 注释并训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 以概念为基础的可解释数据挖掘框架；② 将 VLM 与异常检测相结合以发现语义稀有物体；③ 通过概念过滤显著降低标注成本；④ 仅使用 20% 数据即可提升稀有类别性能；⑤ 采用“Random 10% + Target 10%”采样策略。与以往仅关注几何特征或单一模态的稀有物体挖掘方法不同，本文强调语义解释与跨模态协同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种基于视觉‑语言模型的概念驱动、可解释数据挖掘管道，能够高效识别并选取稀有安全关键物体，显著提升 3D 检测性能并降低标注成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.&lt;/p&gt;</description></item><item><guid>2512.05663v1</guid><title>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</title><link>http://arxiv.org/abs/2512.05663v1</link><author>Johannes Meier, Jonathan Michel, Oussema Dhaouadi, Yung-Hsu Yang, Christoph Reich, Zuria Bauer, Stefan Roth, Marc Pollefeys, Jacques Kaiser, Daniel Cremers</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为LeAD-M3D的单目3D检测方法，结合三项关键技术，实现了在不使用激光雷达或几何先验的情况下，兼具最高精度和实时推理速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单目3D检测面临深度歧义、视角变化和高计算成本等挑战，现有方法往往依赖激光雷达或牺牲效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在保持高精度的同时，实现单目3D检测的实时推理，且不依赖额外传感器或几何假设。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) A2D2：通过质量和重要性加权的深度特征损失，将干净图像教师的几何知识迁移到混合噪声学生；2) CM3D：将3D重叠度纳入匹配分数，提升预测与真值的对齐；3) CGI3D：仅对高置信度区域执行昂贵的3D回归，提升速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LeAD-M3D在KITTI、Waymo和Rope3D数据集上均取得了最先进的精度，并且比之前高精度方法快3.6倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 高精度与实时效率可以在单目3D检测中同时实现，无需激光雷达、立体视觉或几何假设。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种名为LeAD-M3D的单目3D检测方法，结合三项关键技术，实现了在不使用激光雷达或几何先验的情况下，兼具最高精度和实时推理速度。单目3D检测面临深度歧义、视角变化和高计算成本等挑战，现有方法往往依赖激光雷达或牺牲效率。本文的目标是在保持高精度的同时，实现单目3D检测的实时推理，且不依赖额外传感器或几何假设。方法包括：A2D2通过质量和重要性加权的深度特征损失，将干净图像教师的几何知识迁移到混合噪声学生；CM3D将3D重叠度纳入匹配分数，提升预测与真值的对齐；CGI3D仅对高置信度区域执行昂贵的3D回归，提升速度。主要发现是LeAD-M3D在KITTI、Waymo和Rope3D数据集上均取得了最先进的精度，并且比之前高精度方法快3.6倍。结论是高精度与实时效率可以在单目3D检测中同时实现，无需激光雷达、立体视觉或几何假设。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现实时、无 LiDAR、无额外传感器的单目 3D 目标检测，解决单张 RGB 图像中深度不确定性导致的定位误差，并兼顾高精度与低延迟，满足自动驾驶、机器人和城市监控等场景对即时 3D 感知的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在 YOLOv10 的高效 2D 检测框架基础上，结合知识蒸馏、混合增强、3D IoU 匹配和置信度门控等技术，借鉴了 Mixup、A2D、FD3D、3D IoU 匹配和轻量化推理策略，形成了三大核心模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合增强的学生图像与清晰教师图像进行深度特征蒸馏，使用 3D 兼容匹配确定学生与教师的对应关系，并在推理时仅对高置信度区域执行 3D 回归。实现流程包括：① 训练大模型作为教师；② 用混合图像训练学生，使用质量与重要性加权的特征损失进行蒸馏；③ 在训练和推理中使用 3D‑aware Consistent Matching 进行匹配；④ 推理时采用 Confidence‑Gated 3D Inference 限制 3D 头的计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① A2D2：无 LiDAR 的异构蒸馏，利用 Mixup 产生信息不对称并用质量/重要性加权的特征损失提升深度推理；② CM3D：将 3D IoU 纳入匹配，改进预测与真值的对齐；③ CGI3D：置信度门控的 3D 回归，显著降低推理 FLOPs。与以往依赖 LiDAR 或硬编码几何先验的工作不同，LeAD‑M3D 在保持高精度的同时实现了实时推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LeAD‑M3D 通过无 LiDAR 的异构蒸馏、3D 兼容匹配和置信度门控推理，构建了一个在精度与实时性上均领先的单目 3D 检测框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.&lt;/p&gt;</description></item><item><guid>2512.05698v1</guid><title>OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning</title><link>http://arxiv.org/abs/2512.05698v1</link><author>Xusheng Guo, Wanfa Zhang, Shijia Zhao, Qiming Xia, Xiaolong Xie, Mingming Wang, Hai Wu, Chenglu Wen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为OWL的无监督3D目标检测方法，利用占据感知预热和大模型先验推理来提高伪标签质量，并通过自适应加权自训练提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶中，无监督3D目标检测通过启发式算法发现潜在目标，可降低标注成本，但现有方法生成的伪标签初期往往不准确，影响网络收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决伪标签误差对训练的负面影响，并有效过滤和细化伪标签，以提升无监督3D检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; OWL包含占据感知预热策略初始化骨干网络，实例引导推理模块利用大模型先验评估伪标签质量，权重自适应自训练策略动态重新加权伪标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Waymo Open Dataset和KITTI数据集上，OWL比现有无监督方法提升了15%以上的mAP，验证了方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 占据感知预热与大模型先验推理相结合的OWL方法显著提升了无监督3D目标检测的精度，展示了其在自动驾驶场景中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 无监督3D目标检测利用启发式算法发现潜在目标，为降低自动驾驶中的标注成本提供了一条有前景的途径。现有方法主要生成伪标签并通过自训练迭代进行细化。然而，这些伪标签在训练初期往往不准确，导致优化过程被误导。有效过滤和细化伪标签仍是一个关键挑战。本文提出OWL，采用占据感知预热和大模型先验推理进行无监督3D目标检测。OWL首先使用占据感知预热策略，以空间感知能力初始化骨干网络，减轻错误伪标签对网络收敛的干扰。随后，OWL引入实例引导推理模块，利用大模型的先验知识评估伪标签质量，实现精准过滤和细化。最后，我们设计了权重自适应自训练策略，动态重新加权伪标签，通过自训练提升性能。对Waymo Open Dataset和KITTI的广泛实验表明，OWL在无监督方法中以超过15.0%的mAP优势领先，证明了我们方法的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决无监督 3D 目标检测中的伪标签噪声和网络初始化不稳定问题，降低自动驾驶场景下昂贵的人工标注成本。无监督检测的成功可显著提升大规模点云数据的利用效率，直接关系到自动驾驶安全与可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析现有无监督方法在伪标签初始错误和过滤不当导致的性能瓶颈，借鉴 Occupancy‑MAE 的自监督占据预测、LLM 先验推理以及自训练技术，提出三步策略：占据引导预热（OGW）、实例引导推理（ICR）和权重自适应自训练（WAS）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用占据预测自监督预热网络，使其具备空间感知能力；随后利用大模型先验对伪标签进行推理、筛选和修正；最后通过动态加权的自训练进一步提升检测性能。实现流程为：① 通过动态聚类生成初始伪标签；② OGW 预热网络；③ ICR 依据实例属性和 LLM 先验对伪标签进行筛选/修正；④ WAS 以自适应权重迭代训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① OGW：占据预测自监督预热，解决初始化不稳；② ICR：利用大模型先验进行伪标签推理，超越传统阈值过滤；③ WAS：动态加权自训练，减少误标签累积。与以往仅靠阈值过滤或无预热的无监督方法不同，OWL 在每一步都加入了更鲁棒的先验与自监督机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OWL 通过占据引导预热、大模型推理和自适应自训练，显著提升无监督 3D 目标检测性能，突破了传统方法在伪标签噪声和网络初始化方面的局限。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.&lt;/p&gt;</description></item><item><guid>2512.05710v1</guid><title>Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning</title><link>http://arxiv.org/abs/2512.05710v1</link><author>Jianan Sun, Dongzhihan Wang, Mingyu Fan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种面向流形的点云补全框架，利用测地距离近似和基于流形的特征提取，显著提升了补全结果的几何一致性和语义连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全旨在从不完整或稀疏的3D观测中恢复几何一致的形状，现有方法多依赖欧氏距离，忽视了点云的非线性几何结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过显式引入非线性几何信息，改进点云补全的几何一致性和语义清晰度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入测地距离近似器（GDA）估计点间测地距离，并使用基于测地距离的k近邻分组和测地关系注意力机制的流形感知特征提取器（MAFE）进行层次特征提取。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在基准数据集上持续优于现有最先进方法，提升了重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 面向流形的点云补全框架通过测地距离和关系注意力显著提高了几何一致性和语义连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分或稀疏的3D观测中恢复几何一致的形状。尽管最近的方法已实现了合理的全局形状重建，但它们往往依赖欧氏邻近性，忽视了点云的内在非线性几何结构，导致几何一致性不足和语义模糊。本文提出一种面向流形的点云补全框架，在特征学习管道中显式地整合非线性几何信息。我们的方案引入了两个关键模块：测地距离近似器（GDA），用于估计点间的测地距离以捕捉潜在的流形拓扑；以及流形感知特征提取器（MAFE），利用基于测地距离的k近邻分组和测地关系注意力机制来指导层次特征提取过程。通过整合测地感知的关系注意力，我们的方法促进了重建点云的语义连贯性和结构保真度。在基准数据集上进行的广泛实验表明，我们的方法在重建质量上始终优于最先进的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成问题，即从不完整或稀疏的三维观测中恢复完整、几何一致的形状。该问题在现实中很重要，因为真实世界的扫描往往受遮挡、传感器限制或噪声影响，导致点云缺失；在研究中，点云完成是实现自动驾驶、机器人导航、增强现实和文化遗产数字化等应用的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有方法大多依赖欧氏距离的 k‑NN 组网，忽视了点云的非线性流形结构，导致局部语义模糊。为此，他们借鉴了 DGCNN、AdaPoinTr、PointCFormer 和 PointAttN 等工作中的图卷积和注意力机制，但改为使用基于流形的地理距离。通过引入锚点采样、Dijkstra 最短路计算和地理距离近似，作者设计了 Geodesic Distance Approximator（GDA）和 Manifold‑Aware Feature Extractor（MAFE）两大模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将流形几何信息嵌入特征学习全过程。实现流程包括：① GDA 通过锚点构建稀疏图并使用 Dijkstra 计算锚点间的地理距离，得到近似的流形距离；② MAFE 采用地理 k‑NN 组网、Geodesic‑Relational Attention Transformer（GRA‑T）和流形位置嵌入（MPE）来提取局部与全局流形特征；③ 将提取的特征送入粗略完成模块生成初始点云，再通过多阶段 Transformer 上采样细化为完整稠密点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 锚点基地理距离近似方法，既高效又能捕捉非线性拓扑；2) 地理关系注意力 Transformer，利用地理距离和特征差异生成注意力权重；3) 流形位置嵌入，将全局地理信息注入点特征；4) 将上述模块整合到层次特征学习中，显著提升几何一致性和语义连贯性。与以往仅使用欧氏 k‑NN 或全局注意力的工作不同，该方法在邻域构建和注意力计算上都充分考虑了流形结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种基于锚点地理距离近似和地理关系注意力的流形感知点云完成框架，显著提升了重建质量和语义一致性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.&lt;/p&gt;</description></item><item><guid>2512.05759v1</guid><title>Label-Efficient Point Cloud Segmentation with Active Learning</title><link>http://arxiv.org/abs/2512.05759v1</link><author>Johannes Meyer, Jasper Hoffmann, Felix Schulz, Dominik Merkle, Daniel Buescher, Alexander Reiterer, Joschka Boedecker, Wolfram Burgard</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种简单易行的主动学习策略，用于3D点云语义分割，利用二维网格划分点云并通过网络集成估计不确定性，显著降低标注成本并在多个数据集上取得与最先进方法相当或更优的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云语义分割需要大量人工标注，成本高昂。主动学习通过自动选择最有价值的数据进行标注，能减少总标注量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种新颖且易于实现的点云划分与样本选择方法，以提高主动学习在3D点云分割中的效率和效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用二维网格将点云划分为列，形成可标注区域；采用网络集成对网络输出的不确定性进行估计，从而挑选下一批需标注的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在S3DIS、Toronto-3D和弗莱堡大规模城市点云上实验表明，该方法的性能与复杂的最先进方法相当或更好；并且在点云场景中，标注面积比标注点数更能体现主动学习的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该策略既简单又高效，能够在保持或提升性能的同时显著降低标注成本，为3D点云主动学习提供了实用的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 语义分割3D点云数据往往伴随高昂的标注成本。主动学习自动化选择需要标注的数据，减少实现满意性能所需的总标注量。最近针对3D点云的主动学习方法通常基于复杂的启发式方法，既将点云划分为可标注区域，又选择最有利于后续神经网络训练的数据。在本工作中，我们提出了一种新颖且易于实现的策略，将点云划分为可标注区域。我们的方法利用二维网格将点云细分为列。为识别下一批需标注的数据，我们采用网络集成来估计网络输出的不确定性。我们在S3DIS数据集、Toronto-3D数据集以及我们在弗莱堡市手动标注的一个大规模城市3D点云上评估了我们的方法。广泛的评估表明，我们的方法在所有数据集上都能达到与甚至优于复杂的最先进方法的性能。此外，我们提供了结果，表明在点云的背景下，标注面积可能是主动学习算法比标注点数更有意义的度量。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在降低对三维点云语义分割的标注成本。由于城市点云规模巨大且人工标注耗时昂贵，减少标注量对于机器人、城市规划和环境监测等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用主动学习框架，先用简单的二维网格将点云划分为列，再利用深度集成网络估计不确定性来挑选待标注区域。该思路借鉴了 ReDAL、SSDR‑AL 等混合策略，但去除了复杂的预处理和启发式指标，改为纯集成不确定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是将点云分成易标注的列，并用集成模型的方差或熵衡量每列的不确定性。流程包括：① 用少量已标注数据训练模型；② 用多模型集成预测所有列；③ 计算每列的平均不确定性；④ 选取最高不确定性的列交给人工标注；⑤ 将新标注加入训练集并迭代。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 简单的列划分实现区域分离；② 纯集成不确定性选择策略，避免过度自信的 softmax；③ 用覆盖面积而非点数衡量标注工作量；④ 在 S3DIS、Toronto‑3D 和大规模弗莱堡城市点云上实现与或优于现有复杂方法的性能。与之前工作相比，省去了繁琐的超像素/超体素生成和多种启发式特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种轻量级主动学习框架，通过将点云划分为网格列并使用深度集成不确定性挑选待标注列，在保持或提升分割性能的同时显著降低标注成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.&lt;/p&gt;</description></item><item><guid>2512.06882v1</guid><title>Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion</title><link>http://arxiv.org/abs/2512.06882v1</link><author>Yu Zhu, Naoya Chiba, Koichi Hashimoto</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种分层图像引导的三维分割框架，能够在工业环境中有效处理遮挡和尺度差异问题，并通过实例级到部件级的逐步细化实现高精度分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在工业环境中，复杂场景往往存在密集布局、多尺度物体以及严重遮挡，导致几何边界模糊，传统端到端模型难以同时捕捉粗细细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种分层图像引导的三维分割方法，以提高在遮挡和尺度差异明显的场景中的分割性能，并降低对昂贵标注的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1）实例级分割：渲染俯视图，利用YOLO-World提示的SAM生成掩码，再投影回三维点云；2）部件级分割：对每个实例渲染多视图，重复二维分割与投影，并通过贝叶斯更新融合多视图语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实工厂数据上实验表明，该方法能有效处理遮挡和结构复杂性，取得高的每类mIoU；在公开数据集上的评估进一步验证了其泛化能力、鲁棒性、标注效率和适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该分层图像引导框架在工业三维分割任务中表现出色，具有高效、鲁棒、适应多样环境的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决工业环境中复杂、密集且多尺度物体的 3D 点云分割问题。由于遮挡严重、几何边界模糊以及尺度差异大，传统 3D 分割方法往往难以同时捕捉粗粒度实例和细粒度部件。准确的 3D 分割对于机器人操作、数字孪生和工业自动化等任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有 3D 点云分割需要大量标注且对复杂场景泛化差，随后借鉴了 2D 视觉基础模型（YOLO‑World、SAM）和多视角融合技术。通过将检测与分割分离、采用自适应渲染、以及贝叶斯更新融合，构建了一个分层的图像引导框架，既利用了 2D 模型的强大表示，又解决了跨视角不一致的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从粗到细逐步细化 3D 分割：首先用顶部视图渲染点云，YOLO‑World 检测实例并用 SAM 生成掩码，再投影回 3D 得到实例级标签；随后对每个实例采样多视角，重复检测与分割，得到部件级 2D 掩码，投影回 3D 并通过贝叶斯更新融合多视角信息，最后用 DBSCAN 去除噪声。整个流程可视为检测‑分割‑投影‑融合‑清洗的连贯链条。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 分层框架：先实例后部件，提升对大尺度与细部的兼顾；2) 基于 2D 基础模型的检测‑分割管线，显著降低 3D 标注成本；3) 贝叶斯更新融合多视角掩码，解决跨视角语义不一致；4) 自适应渲染与 DBSCAN 后处理，增强对遮挡和噪声的鲁棒性。与以往单一端到端 3D 网络或单视角 2D 引导方法相比，该方案在工业场景中实现了更高的精度与更低的标注需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 提出一种基于 2D 基础模型和贝叶斯多视角融合的分层图像引导 3D 点云分割框架，能够在工业环境中以低标注成本实现高精度、跨尺度的实例与部件级分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.&lt;/p&gt;</description></item><item><guid>2512.08223v1</guid><title>SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection</title><link>http://arxiv.org/abs/2512.08223v1</link><author>Ching-Hung Cheng, Hsiu-Fu Wu, Bing-Chen Wu, Khanh-Phong Bui, Van-Tin Luu, Ching-Chun Huang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了在三维目标检测任务中使用提示调优方法的有效性，评估了基于大规模Waymo数据集训练的模型是否能作为基础模型并适应其他场景，并提出了场景导向提示池（SOP^2）来进一步提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型（如GPT-3）具备强大的泛化能力，通过微调和提示调优等迁移学习技术，可在自然语言处理等领域以极少的参数调整适配多种下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证提示调优方法在三维目标检测中的适用性，并探索基于Waymo数据集的基础模型在不同场景中的迁移效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 系统评估提示词和提示生成器的影响，随后提出并实验场景导向提示池（SOP^2）以提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 提示池在三维目标检测任务中表现出显著的有效性，能够提升模型在不同场景下的检测表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提示调优技术在三维目标检测领域具有潜在价值，提示池的引入为未来研究提供了新的方向和启示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着大型语言模型（如GPT-3）的崛起，这些模型展现出强大的泛化能力。通过微调和提示调优等迁移学习技术，它们可以以最小的参数调整适配各种下游任务。这种方法在自然语言处理领域尤为常见。本文旨在探讨常见提示调优方法在三维目标检测中的有效性。我们研究了在大规模Waymo数据集上训练的模型是否能作为基础模型，并适应三维目标检测领域的其他场景。本文依次考察了提示词和提示生成器的影响，并进一步提出了场景导向提示池（SOP^2）。我们展示了提示池在三维目标检测中的有效性，旨在激发未来研究者深入探讨提示在三维领域的潜力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注 3D 目标检测模型在不同数据集之间的迁移性能。由于传感器、环境和采集方式的差异，模型在一个大规模数据集（如 Waymo）上训练后往往无法在另一个数据集（如 KITTI）上保持高精度。解决这一域间差距对自动驾驶、机器人和城市规划等实际应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到 NLP 中的 Prompt Tuning、LoRA 等参数高效微调技术，并将其思想迁移到视觉任务。随后参考 VPT、DVPT 和 Prompt Pool（L2P）等工作，提出按场景划分点云并为每个划分分配专门的 Prompt Pool。整个设计借鉴了现有的 Prompt 相关方法，并结合 3D Transformer（DSVT）实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为每个点云划分（X/Y 方向窗口）维护一个 Prompt Pool，并在推理时根据当前划分的特征与池中键进行相似度匹配，挑选最合适的 Prompt 进行拼接。实现流程包括：① 用 Waymo 预训练的 DSVT 作为骨干；② 将点云划分为若干窗口；③ 对每个窗口使用 Prompt Pool 选取 Prompt 并与窗口特征拼接；④ 通过多头自注意力完成特征交互；⑤ 只训练 Prompt Pool，保持骨干冻结。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 引入 Scene‑Oriented Prompt Pool（SOP2）专门针对 3D 点云场景；2) 通过相似度匹配动态选择 Prompt，提升对不同子场景的适应性；3) 在 DSVT 结构上实现 Prompt Pool，保持参数高效。与以往仅使用固定 Prompt 或全模型微调的做法不同，SOP2 在保持骨干不变的前提下显著提升跨域检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种基于场景的 Prompt Pool，能够在冻结 3D Transformer 骨干的同时实现高效、参数友好的跨域目标检测迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.&lt;/p&gt;</description></item><item><guid>2512.08247v1</guid><title>Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</title><link>http://arxiv.org/abs/2512.08247v1</link><author>Haowen Zheng, Hu Zhu, Lu Deng, Weihao Gu, Yang Yang, Yanyan Liang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种稀疏查询的未来时序知识蒸馏方法，能够将离线模型利用未来帧获得的丰富信息迁移到在线模型，从而提升摄像头驱动的三维目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 摄像头基于时序的三维目标检测在自动驾驶中表现突出，离线模型通过使用未来帧显著提高精度，但现有的知识蒸馏方法往往忽略未来帧，难以让在线模型有效学习未来知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够在不严格对齐帧的情况下，将未来帧知识有效传递给在线模型的知识蒸馏框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出Future Temporal Knowledge Distillation (FTKD)，包括未来感知特征重建策略和未来引导的logit蒸馏，利用稀疏查询方式在离线教师与在线学生之间传递未来帧信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，FTKD在两个高性能基线模型上分别提升了最高1.3 mAP和1.3 NDS，并实现了最准确的速度估计，且未增加推理成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FTKD能够有效迁移未来帧知识，显著提升在线三维目标检测的精度与速度估计，证明了未来时序知识蒸馏的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher&amp;#x27;s stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在让在线 3D 目标检测模型在没有未来帧的情况下，学习到离线模型利用未来帧获得的丰富时序信息，从而提升检测精度，尤其是对远距离或被遮挡物体的识别，并改善速度估计，这在自动驾驶等实时场景中至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有知识蒸馏方法大多只关注空间特征或仅利用过去帧的时序关系，忽略了未来帧的价值；他们借鉴了掩码特征重建和 logit 蒸馏的思路，结合稀疏查询的检测框架，设计了未来感知的特征重建和未来引导的 logit 蒸馏两大模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用教师模型聚合未来帧的特征作为重建目标，然后对学生模型的特征随机掩码并用自适应生成器重建，最后通过匈牙利算法匹配教师与学生的查询进行 logit 蒸馏。实现流程包括：冻结教师、计算未来聚合特征、生成掩码、重建学生特征、计算重建损失、匹配查询并计算 logit 损失，完成训练后保持原始学生结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 未来感知的特征重建突破了严格帧对齐限制；② 未来引导的 logit 蒸馏利用背景查询信息；③ 采用稀疏查询的教师与学生，兼顾精度与效率；④ 训练后无额外推理开销。与以往只关注空间或过去时序的蒸馏方法不同，本文显式利用未来帧知识并兼顾背景信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种仅使用相机的知识蒸馏框架，使在线 3D 检测器能够从离线教师的未来帧中学习，提升检测与速度估计精度且不增加推理成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher&amp;#x27;s stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.&lt;/p&gt;</description></item><item><guid>2512.08557v2</guid><title>SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds</title><link>http://arxiv.org/abs/2512.08557v2</link><author>Alexander Dow, Manduhu Manduhu, Matheus Santos, Ben Bartlett, Gerard Dooly, James Riordan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用激光雷达连续扫描运动的技术，通过只关注帧间点云变化的区域来提高目标检测效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统稀疏卷积在处理激光雷达点云时需要对整个点云进行卷积，计算量大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不降低检测精度的前提下，减少卷积运算次数，提高计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用短步长的滑动时间窗口，存储跨帧卷积结果，忽略未变化区域；在此基础上扩展散射卷积，提出带时间数据回收的稀疏散射卷积算法（SSCATeR），仅对变化部分进行运算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法与传统稀疏卷积产生相同的特征图，处理时间可缩短至原来的约1/6.61，显著提升计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过数据重用和时间维度的利用，SSCATeR 在保持精度的同时大幅降低了计算成本，为实时激光雷达目标检测提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在降低无人机 LiDAR 点云 3D 目标检测的延迟。实时检测对无人机在动态环境中的安全至关重要，尤其是在需要快速响应的场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到 LiDAR 连续扫描导致大部分点云在相邻帧中保持不变，于是提出在滑动时间窗口内仅处理变化部分，并重用先前卷积结果。该思路借鉴了之前的散射卷积和稀疏卷积技术，并在先前的流式 LiDAR 研究基础上扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是利用时间稀疏性，记录点云变化地图，存储并重用未变化区域的卷积结果。实现流程包括：滑动 100 ms 时间窗口、生成变化地图、使用稀疏散射卷积与时间数据回收（SSCATeR）处理仅变化的点，输出与传统方法相同的特征图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 在稀疏散射卷积中加入时间数据回收；2) 将活跃点数平均减少 72.8%，处理时间平均降低 59.85%（单层可达 84.88%）；3) 保持与传统卷积相同的特征图和精度。与以往提升精度或使用注意力网络的工作不同，SSCATeR 通过利用时间稀疏性显著提升效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SSCATeR 通过在 LiDAR 点云中重用未变化区域的卷积结果，将 3D 目标检测的处理时间缩短多达 6.6 倍，同时保持原有精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.&lt;/p&gt;</description></item><item><guid>2512.09407v1</guid><title>Generative Point Cloud Registration</title><link>http://arxiv.org/abs/2512.09407v1</link><author>Haobo Jiang, Jin Xie, Jian Yang, Liang Yu, Jianmin Zheng</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新颖的生成式点云配准范式，通过生成跨视角一致且与源点云和目标点云几何对齐的图像对，实现几何与颜色特征融合，从而提升三维配准的鲁棒性和性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统三维配准方法在匹配精度和鲁棒性方面存在局限，尤其在纹理缺失或几何噪声较大的情况下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用先进的二维生成模型与三维匹配任务相结合，构建一种能够生成高质量、跨视角一致图像对的配准框架，以提升配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入 Match-ControlNet，一种可控二维生成模型，利用深度条件生成保证二维-三维几何一致性，并通过耦合条件去噪与提示引导促进跨视角纹理一致性，从而生成符合要求的图像对用于配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 3DMatch 与 ScanNet 数据集上的实验表明，该生成式配准范式显著提升了配准性能，验证了方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 生成式点云配准范式具有通用性，可无缝集成到多种配准方法中，显著提升配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在本文中，我们提出了一种新颖的三维配准范式——生成式点云配准，它将先进的二维生成模型与三维匹配任务相结合，以提升配准性能。我们的核心思路是生成跨视角一致的图像对，并使其与源点云和目标点云良好对齐，从而实现几何-颜色特征融合，促进鲁棒匹配。为确保高质量匹配，生成的图像对应同时具备二维-三维几何一致性和跨视角纹理一致性。为此，我们引入了 Match-ControlNet，一种针对匹配的可控二维生成模型。它利用 ControlNet 的深度条件生成能力，生成与点云派生的深度图几何对齐的图像，保证二维-三维几何一致性。此外，通过结合耦合条件去噪方案和耦合提示引导，Match-ControlNet 进一步促进跨视角特征交互，引导纹理一致性生成。我们的生成式三维配准范式具有通用性，可无缝集成到各种配准方法中以提升其性能。对 3DMatch 和 ScanNet 数据集的广泛实验验证了我们方法的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在提升仅使用几何信息的点云配准精度，方法是通过生成与源点云和目标点云几何一致且纹理相互一致的 RGB 图像，为配准提供补充的颜色信息。此问题在现实中十分重要，因为实际扫描往往存在低重叠、噪声或缺失纹理，传统仅基于几何的配准方法难以获得可靠对应关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 RGB‑D 配准中颜色信息的重要性，结合 Stable Diffusion 的 ControlNet 深度条件生成框架，并在此基础上加入了耦合条件去噪和耦合提示引导，以实现跨视角纹理一致性。同时利用预训练的视觉模型进行零样本特征融合，形成一种可直接插拔的配准增强方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先将源点云和目标点云转换为深度图，再使用 Match‑ControlNet 生成与深度图几何一致且纹理相互一致的 RGB 图像对；随后提取这些图像的零样本特征并与原始几何描述子融合，得到增强的特征；最后通过对应匹配和姿态估计得到最终变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）提出生成点云配准范式，利用合成 RGB 图像补充颜色信息；2）设计 Match‑ControlNet，采用耦合去噪和提示引导实现 2D‑3D 几何一致性与跨视角纹理一致性；3）实现零样本几何‑颜色融合，利用预训练模型无需微调即可提升描述子。与以往仅使用真实 RGB 或独立生成单张图像的方法不同，该方案在不需要真实图像且无需任务特定微调的情况下，生成一致的图像对并直接增强配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过联合生成几何一致且纹理相互一致的 RGB 图像对，并将其颜色特征与几何描述子融合，作者提出了一种可直接插拔的框架，在无需真实 RGB 数据和微调的情况下显著提升点云配准精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.&lt;/p&gt;</description></item><item><guid>2512.10046v1</guid><title>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</title><link>http://arxiv.org/abs/2512.10046v1</link><author>Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SimWorld-Robotics 是一个基于Unreal Engine 5的城市级仿真平台，能够无限生成逼真的城市场景并支持多机器人控制。作者利用该平台创建了两个挑战性基准：多模态指令跟随和多机器人搜索，全面评估机器人在复杂城市环境中的感知、推理、导航与协作能力。实验表明现有视觉语言模型在这些任务中表现不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来基础模型在多模态输入下的通用机器人研究取得进展，但大多聚焦室内家庭场景，缺乏对开放式城市环境的评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个可扩展、逼真的城市仿真平台，并通过新基准评估机器人在真实城市环境中的关键能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Unreal Engine 5实现程序化生成的城市场景，加入行人和交通系统，支持多机器人控制与通信；基于此平台设计两项任务：视觉语言导航与多机器人搜索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示当前最先进的视觉语言模型在多模态指令跟随和多机器人搜索任务中表现不佳，缺乏稳健的感知、推理和规划能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SimWorld-Robotics 及其基准为评估和推动机器人在复杂城市环境中的研究提供了重要工具，现有模型需要进一步提升感知与协作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近在基础模型方面的进展表明，在多模态输入下开发通用机器人能够在开放式场景中执行多样任务具有良好前景。然而，目前的工作主要集中在室内、家庭场景。本研究提出了SimWorld-Robotics（SWR），一个用于大规模、逼真城市环境中具身人工智能的仿真平台。基于Unreal Engine 5，SWR通过程序化生成无限逼真的城市场景，并加入行人和交通系统等动态元素，超过了以往城市仿真的真实性、复杂性和可扩展性。它还支持多机器人控制与通信。凭借这些关键特性，我们构建了两个具有挑战性的机器人基准：（1）多模态指令跟随任务，机器人必须在行人和交通存在的情况下，遵循视觉语言导航指令到达目的地；（2）多智能体搜索任务，两个机器人必须通过通信合作寻找并相遇。与现有基准不同，这两个新基准全面评估了在现实场景中对机器人关键能力的广泛需求，包括多模态指令 grounding、在大环境中的三维空间推理、安全长距离导航与人车交互、多机器人协作以及 grounded communication。我们的实验结果表明，包括视觉语言模型在内的最先进模型在我们的任务中表现不佳，缺乏在城市环境中所需的稳健感知、推理和规划能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在构建一个能够生成无限、光照、天气和人车动态真实的城市环境的仿真平台，以支持机器人在大规模、真实感强的户外场景中进行多模态导航和多机器人协作。现实中现有仿真器大多局限于室内或缺乏动态交互，难以训练和评估能够在复杂城市环境中安全、长距离导航的通用机器人模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有城市仿真器（如CARLA、AirSim、MetaUrban等）的不足，发现它们缺乏光照、天气、动态人车交互和多机器人控制。随后在Unreal Engine 5基础上扩展SimWorld，加入了程序化城市生成、交通系统和四足机器人等功能，并借鉴了这些已有工作中的技术与设计理念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过程序化生成无限、光照、天气和动态人车交互的城市环境，并提供异步多代理控制与丰富感知。实现流程包括：1）道路、建筑、街道元素、交通元素四阶段生成；2）异步多代理控制框架，允许机器人、人车独立行动；3）提供RGB、深度、语义分割等观测；4）基于此平台构建多模态导航与多机器人搜索基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①无限程序化生成的光照、天气真实城市；②多模态感知与异步多代理控制；③支持四足机器人与人车的统一仿真；④提出SIMWORLD-MMNAV和SIMWORLD-MRS两大基准；⑤发布SimWorld-20K大规模训练数据。与之前工作相比，SWR在规模、真实感、动态交互和多机器人协作方面均有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SimWorld‑Robotics提供了一个可无限生成、光照、天气和动态人车交互的高真实感城市仿真平台，并通过新基准和大规模数据集，揭示了当前通用机器人模型在复杂城市环境中的局限。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.&lt;/p&gt;</description></item><item><guid>2512.10386v1</guid><title>Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</title><link>http://arxiv.org/abs/2512.10386v1</link><author>Ge Zhang, Chunyang Wang, Bo Xiao, Xuelian Liu, Bin Liu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自适应双权重重力模型点云去噪方法，利用八叉树并行加速，结合体素占据统计和kNN密度估计快速剔除噪点，并通过密度与距离加权的重力评分函数精细区分噪点与物体点。实验表明该方法在多种噪声条件下相较现有方法在F1、PSNR、CD指标上均有提升，同时单帧处理时间更短，证明其高精度、鲁棒性和实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高质量点云数据是自动驾驶、三维重建等任务的关键基础，但基于LiDAR的点云采集易受多种干扰，产生大量噪点，影响后续目标检测与识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有点云去噪方法在高精度、边缘保持和实时性能之间难以兼顾的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用八叉树进行空间分区实现并行加速；在叶节点内使用自适应体素占据统计和kNN密度估计快速剔除孤立低密度噪点；构建结合密度权重与自适应距离权重的重力评分函数，精细区分噪点与物体点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示该方法在Stanford 3D扫描库、CADC数据集和实验室FMCW LiDAR点云上，在F1、PSNR、CD指标上均优于现有方法，并且单帧处理时间更短。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该自适应双权重重力模型点云去噪方法在多噪声场景下实现了高精度、强边缘保持和实时性能，具有良好的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 高质量的点云数据是自动驾驶和三维重建等任务的关键基础。然而，基于LiDAR的点云采集往往受到多种干扰的影响，导致大量噪点出现，降低后续点云目标检测和识别的准确性。此外，现有的点云去噪方法通常在追求更高去噪精度时牺牲计算效率，或者在提升处理速度时牺牲边缘保持和细节结构的保留，使得难以同时实现高去噪精度、强边缘保持和实时性能。为解决这些局限，本文提出了一种自适应双权重重力模型点云去噪方法。首先，采用八叉树对全局点云进行空间分区，实现并行加速。随后，在每个叶节点内，利用自适应体素占据统计和k近邻密度估计快速去除明显孤立且低密度的噪点，从而减少有效候选集。最后，构建结合密度权重与自适应距离权重的重力评分函数，细致区分噪点与物体点。对Stanford 3D扫描库、加拿大不利驾驶条件（CADC）数据集以及我们实验室采集的FMCW LiDAR点云进行实验，结果表明，与现有方法相比，所提出的方法在各种噪声条件下在F1、PSNR和Chamfer距离（CD）指标上均取得一致提升，同时单帧处理时间更短，从而验证了其在多噪声场景下的高精度、鲁棒性和实时性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 LiDAR 采集的点云中噪声过多导致后续任务（如自动驾驶、三维重建）精度下降的问题。噪声会破坏几何结构，影响检测与识别，因此需要一种既能高效去噪又能保留边缘细节的算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的基于重力特征的去噪方法基础上，结合八叉树空间划分、体素占用统计和 kNN 密度估计，提出了自适应双权重重力评分。文中引用了学习式与非学习式去噪、双边滤波、MLS、LOP 等现有技术，说明了对现有方法的借鉴与改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用八叉树将点云分块，快速剔除明显离群点和低密度噪声，然后对剩余候选点计算结合密度与距离的双权重重力分数，决定是否保留。实现流程为：八叉树划分 → 体素占用剔除 → kNN 低密度剔除 → 计算密度–距离双权重重力分数 → 过滤噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 自适应双权重重力评分函数，兼顾局部密度与空间关系；2) 八叉树块级并行框架，显著提升速度；3) 两阶段噪声剔除（体素+kNN）压缩候选集，降低计算量。与传统重力方法相比，算法更快且更能保留边缘细节；与滤波/优化方法相比，提供了更好的全局结构建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种结合八叉树划分、体素与 kNN 预过滤以及密度–距离双权重重力评分的点云去噪框架，能够在保持边缘细节的同时实现实时高精度去噪。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.&lt;/p&gt;</description></item><item><guid>2512.11354v1</guid><title>A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection</title><link>http://arxiv.org/abs/2512.11354v1</link><author>Qinghan Hu, Haijiang Zhu, Na Sun, Lei Chen, Zhengqiang Fan, Zhiqing Li</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种多模态水下结构光三维成像系统，用于管道检测，结合多源信息融合，提升了检测精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 水下管道易腐蚀，传统人工检测效率低且不安全，需更可靠的实时成像技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种基于多源信息融合的多模态水下结构光三维成像系统，以实现高精度、实时的管道缺陷检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用快速畸变校正、基于因子图的外参优化、适应几何变化的多模态成像策略、适应性扩展卡尔曼滤波以及基于边缘检测的ICP算法，实现图像校正、姿态估计和点云配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该系统在不同工作模式、速度和深度下均表现出优异的精度、适应性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该系统为自主水下管道检测提供了可靠的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 水下管道易受腐蚀，缩短使用寿命并带来安全风险。与人工检查相比，智能实时成像系统已成为更可靠、更实用的解决方案。在众多水下成像技术中，结构光三维成像能够恢复足够的空间细节，精确表征缺陷。因此，本文基于多源信息融合，开发了一种多模态水下结构光三维成像系统（UW‑SLD系统）用于管道检测。首先，采用快速畸变校正方法实现高效的水下图像校正。为克服水下传感器间外参校准的挑战，提出基于因子图的参数优化方法，估计结构光与声学传感器之间的变换矩阵。进一步引入多模态三维成像策略，以适应水下管道几何变化。鉴于水下环境中存在大量干扰，设计了多源信息融合策略和自适应扩展卡尔曼滤波，确保姿态估计稳定、测量精度高。特别地，提出了基于边缘检测的ICP算法，该算法将管道边缘检测网络与增强点云配准相结合，即使在运动条件变化下也能实现稳健、高保真度的缺陷结构重建。对不同工作模式、速度和深度下进行了大量实验，结果表明所开发的系统在精度、适应性和鲁棒性方面均优于现有方法，为自主水下管道检测奠定了坚实基础。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决水下管道腐蚀和缺陷的检测问题。传统人工检查成本高、危险，且难以及时发现细小缺陷。准确、实时的三维成像能帮助维护人员及时评估管道安全，避免泄漏、爆炸等灾害，具有重要的工业和环境意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有声学、被动视觉和结构光技术的局限，决定将结构光与声学、惯性传感器结合。借鉴了结构光快速畸变校正、因子图外参优化、自适应扩展卡尔曼滤波、ICP配准以及深度学习边缘检测等已有技术，并在此基础上提出了多源信息融合与多模式成像方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多源传感器融合和深度学习辅助，实现高精度、鲁棒的水下三维成像。实现流程包括：硬件集成、快速畸变校正、因子图外参优化、姿态估计（AEKF）、多模式成像（平移、旋转、组合）、管道边缘检测网络提取感兴趣区域、基于边缘的ICP配准，最终得到高质量的缺陷三维模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 快速高精度的畸变校正方法；2) 因子图外参优化实现结构光与DVL的统一坐标；3) 分层多频信息融合与自适应卡尔曼滤波提升姿态估计；4) 多模式成像策略与管道边缘检测网络减少噪声；5) 边缘检测ICP算法在动态环境下实现稳健配准。与以往单一技术或单一场景的研究不同，该系统实现了多源融合、实时性和多模式适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种集成结构光、声学、惯性传感器和深度学习的多模式三维成像系统，能够在动态水下环境中实现高精度、鲁棒的管道缺陷检测。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.&lt;/p&gt;</description></item><item><guid>2512.11465v1</guid><title>DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation</title><link>http://arxiv.org/abs/2512.11465v1</link><author>Mohamed Abdelsamad, Michael Ulrich, Bin Yang, Miao Zhang, Yakov Miron, Abhinav Valada</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督学习框架 DOS（Distilling Observable Softmaps），通过仅在可观测（未遮挡）点上蒸馏语义相关软图，避免信息泄漏并提供比离散标记更丰富的监督；同时引入 Zipfian 原型和改进的 Sinkhorn-Knopp 算法 Zipf-Sinkhorn，以在无监督环境下处理语义不平衡问题。实验表明，DOS 在多项基准（nuScenes、Waymo、SemanticKITTI、ScanNet、ScanNet200）上的语义分割和 3D 目标检测任务中均优于现有最先进方法，且不依赖额外数据或标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自监督学习在 3D 点云表示学习中展现出巨大潜力，但由于几何不规则、重建易出现捷径以及语义分布不均衡等挑战，仍难以充分发挥其优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够克服上述挑战、提升 3D 点云自监督学习效果的鲁棒方法，并在语义分割与目标检测任务中实现领先性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1）仅在可观测点上蒸馏软图，防止遮挡区域信息泄漏；2）使用 Zipfian 原型并通过 Zipf-Sinkhorn 算法强制原型使用遵循幂律先验，调节软图的尖锐度；3）在无监督设置下实现对不平衡语义的有效处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DOS 在 nuScenes、Waymo、SemanticKITTI、ScanNet、ScanNet200 等多项基准上，语义分割和 3D 目标检测任务中均超过当前最先进方法，且不需要额外数据或标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可观测点软图蒸馏为学习稳健 3D 表示提供了一种可扩展且有效的范式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近在自监督学习（SSL）方面的进展显示了在没有人工注释的情况下学习 3D 点云表示的巨大潜力。然而，针对 3D 点云的 SSL 仍面临由于几何不规则、易出现捷径的重建以及语义分布不平衡等关键挑战。在本研究中，我们提出了 DOS（Distilling Observable Softmaps），一种新颖的 SSL 框架，它仅在可观测（未遮挡）点上自蒸馏语义相关软图。该策略防止了来自遮挡区域的信息泄漏，并提供了比离散标记到原型分配更丰富的监督。为了解决无监督环境下语义不平衡的挑战，我们引入了 Zipfian 原型，并通过改进的 Sinkhorn-Knopp 算法 Zipf-Sinkhorn 强制原型使用遵循幂律先验，并在训练期间调节目标软图的尖锐度。DOS 在语义分割和 3D 目标检测的多个基准（包括 nuScenes、Waymo、SemanticKITTI、ScanNet 和 ScanNet200）上均优于当前最先进的方法，且不依赖额外数据或注释。我们的结果表明，可观测点软图蒸馏为学习稳健 3D 表示提供了一种可扩展且有效的范式。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决自监督学习在3D点云中的关键挑战，包括不规则几何导致的学习不稳定、重建方法易出现捷径学习以及语义分布极度不平衡的问题。解决这些问题对于在没有人工标注的情况下构建高质量、可迁移的3D表示至关重要，直接影响自动驾驶、机器人导航等实际应用的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在现有的掩码自监督和聚类方法基础上，发现这些方法仍受位置泄漏和均匀原型使用的限制。于是他们提出只在可见点上进行监督、使用语义软图（softmap）来捕捉空间竞争，并引入 Zipf‑Sinkhorn 以匹配真实的长尾语义分布。该设计借鉴了 SwAV 的 Sinkhorn 算法、掩码自监督框架以及 Zipf 定律的概念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学生‑教师框架，仅在学生能看到的可见点上对教师生成的 Zipf‑正则化软图进行蒸馏，从而避免位置泄漏并鼓励空间竞争。实现流程包括：①对点云做两视角增强并随机掩码；②学生仅处理可见点，教师处理完整点云；③两者计算与原型的相似度并归一化得到软图；④教师软图通过 Zipf‑Sinkhorn 进行长尾正则化；⑤用 KL 散度将学生软图对齐到教师软图；⑥更新学生参数并用 EMA 更新教师。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）可见点蒸馏，消除位置泄漏；2）语义软图蒸馏，提供空间竞争的学习信号；3）Zipf‑Sinkhorn 正则化，匹配长尾语义分布。与以往的重建或聚类方法不同，DOS 不依赖重建或均匀原型分配，而是通过软图和 Zipf 先验实现更丰富、更稳健的自监督学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DOS通过在可见点上蒸馏Zipf正则化的语义软图，消除位置泄漏并处理长尾语义，提供了一种无需标注即可学习鲁棒3D点云表示的新框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.&lt;/p&gt;</description></item><item><guid>2512.11926v1</guid><title>TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder</title><link>http://arxiv.org/abs/2512.11926v1</link><author>Qinghao Meng, Chenming Wu, Liangjun Zhang, Jianbing Shen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种联合补全与检测框架，利用Transformer结构提升稀疏区域的检测特征，同时保持成本不变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D目标检测在自动驾驶中至关重要，但在远距离仅有少量激光点的区域检测仍是难点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过稠密化点云来解决点云稀疏问题，并提升稀疏区域的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计TransBridge变压器上采样块，将检测网络与补全网络的特征融合；构建DSRecon模块生成稠密激光点云；利用变压器建立通道与空间关系，得到高分辨率特征图用于补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和Waymo数据集上实验表明，该框架在多种方法上平均精度提升0.7到1.5，且在两阶段检测框架中提升至5.78点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架具有良好的泛化能力，能够持续提升端到端3D目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D目标检测在自动驾驶中至关重要，为移动物体和障碍物提供关键信息。仅凭少量激光点在远距离检测对象仍是挑战，许多策略已被开发以通过稠密化解决点云稀疏问题。本文提出一种联合补全与检测框架，在稀疏区域提升检测特征，同时保持成本不变。具体而言，我们提出TransBridge，一种基于Transformer的上采样块，融合检测网络与补全网络的特征。检测网络可通过获取来自补全网络的隐式补全特征获益。此外，我们设计了Dynamic-Static Reconstruction（DSRecon）模块，为补全网络生成稠密激光点云，满足稠密点云地面真值的需求。进一步地，我们采用Transformer机制建立通道与空间关系，得到用于补全的高分辨率特征图。对nuScenes和Waymo数据集进行的大量实验表明，所提出的框架有效。结果显示，我们的框架在多种方法上平均精度提升0.7到1.5，表明其泛化能力。对于两阶段检测框架，它还将平均精度提升至5.78点。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在稀疏 LiDAR 点云中检测远距离物体的困难，尤其是由于不可见体素导致的误检和漏检。这个问题在自动驾驶中至关重要，因为车辆需要准确感知周围环境以做出安全决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将检测网络和点云补全网络共享编码器，并在解码阶段引入基于 Transformer 的上采样桥（TransBridge）来融合两者特征。方法借鉴了 CenterPoint、VoxelNet 等检测框架和现有点云补全技术，同时改进了稀疏控制和稠密标签生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练检测和补全网络，让补全网络提供隐含的结构信息来提升检测特征。流程包括：1）共享编码器提取多尺度特征；2）TransBridge 上采样并解释特征，生成体素存在预测；3）使用 DSRecon 生成稠密点云标签；4）完成网络监督下的反向传播，最终得到更准确的检测结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）TransBridge 作为 Transformer 上采样桥，兼顾特征上采样和语义桥接；2）DSRecon 模块提供无拖尾的稠密标签；3）稀疏控制模块（SCM）保持高分辨率特征的稀疏性；4）整体框架在保持推理速度的同时提升检测精度。与以往需要额外推理成本的稠密化方法不同，该方法在训练阶段完成补全，推理时无额外开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TransBridge 通过 Transformer 上采样桥实现检测与点云补全的联合训练，在稀疏 LiDAR 数据上显著提升 3D 检测精度，而不增加推理成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.&lt;/p&gt;</description></item><item><guid>2512.12377v1</guid><title>INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset</title><link>http://arxiv.org/abs/2512.12377v1</link><author>Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; INDOOR-LIDAR 是一套综合性的室内 3D LiDAR 点云混合数据集，旨在推动机器人感知研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有室内 LiDAR 数据集规模有限、注释格式不统一、采集过程受人工影响导致变异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过结合仿真环境与真实机器人扫描，提供一致覆盖、可控变异的高质量点云数据，填补现有数据集的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采集密集点云并记录强度值，采用 KITTI 风格注释，包含常见室内物体类别；仿真子集支持灵活布局、点密度和遮挡配置；真实子集记录真实噪声、杂物和特定域伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该数据集兼具可配置的仿真特性和真实世界噪声，支持 3D 目标检测、鸟瞰图感知、SLAM、语义场景理解以及仿真与真实域适配等多种应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; INDOOR-LIDAR 通过桥接合成与真实数据，提供可扩展、真实、可复现的基准，推动复杂室内环境下机器人感知技术进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 INDOOR-LIDAR，一套全面的室内 3D LiDAR 点云混合数据集，旨在推动机器人感知研究。现有室内 LiDAR 数据集往往规模有限、注释格式不一致、采集过程中存在人为变异。INDOOR-LIDAR 通过将仿真环境与使用自主地面机器人获取的真实扫描相结合，提供一致的覆盖范围和在受控变异下的真实传感器行为。每个样本包含密集点云数据，配有强度测量和 KITTI 风格注释。注释方案涵盖各种场景中的常见室内物体类别。仿真子集支持灵活配置布局、点密度和遮挡，而真实子集则捕捉真实传感器噪声、杂物和特定域伪影。INDOOR-LIDAR 支持 3D 目标检测、鸟瞰图感知、SLAM、语义场景理解以及仿真与真实室内域之间的域适配等广泛应用。通过弥合合成与真实世界数据之间的差距，INDOOR-LIDAR 建立了一个可扩展、真实且可复现的基准，推动复杂室内环境下机器人感知技术的进步。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决现有室内 LiDAR 数据集规模小、注释不统一、采集方式受限（如手持导致盲区）等问题，缺乏可直接用于移动机器人 360° 全景感知的数据。此问题重要，因为高质量、完整的机器人视角数据是训练和评估 SLAM、目标检测、语义分割等关键感知算法的基础，且能有效缩小模拟与真实环境之间的差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合现有室内 LiDAR 数据集（如 LiDAR‑Net、3DSES）与模拟平台（NVIDIA Isaac Sim、Unity 等），提出将真实机器人采集与程序化生成的虚拟环境相结合的混合数据集。通过在机器人上安装全景 LiDAR，消除手持盲区，并采用统一的 KITTI‑style 注释格式，确保数据在真实与仿真之间保持一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个机器人视角的 360° LiDAR 混合数据集，包含真实扫描、合成扫描、强度信息和 3D 边界框。实现流程包括：① 在真实环境中使用自主地面机器人采集 LiDAR 点云；② 在仿真平台中生成可配置的室内场景并同步采集；③ 通过 ROS2 管道统一处理数据，生成点云、强度图、3D 边界框、轨迹等多模态信息；④ 提供标准数据划分和基线评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 混合真实与合成数据，提供可扩展的室内场景；② 机器人安装的 360° LiDAR 采集，消除盲区；③ 统一的 intensity、点级注释和 KITTI‑style 3D 边界框；④ 支持多任务（检测、BEV、SLAM 等）并提供基线。与以往仅提供手持扫描或单一真实/合成数据集的工作不同，INDOOR‑LiDAR 通过机器人视角和完整覆盖显著提升了数据的实用性和可迁移性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; INDOOR‑LiDAR 提供了一个机器人视角、360° 全景、带强度和统一 3D 注释的混合室内 LiDAR 数据集，弥合了模拟与真实之间的差距，为多任务感知研究提供了可扩展、可复现的基准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird&amp;#x27;s-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.&lt;/p&gt;</description></item><item><guid>2512.12884v1</guid><title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title><link>http://arxiv.org/abs/2512.12884v1</link><author>Xiangzhong Liu, Jiajie Zhang, Hao Shen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种端到端的跨层融合方法，利用Transformer将智能传感器和V2X模块产生的对象列表与原始摄像头图像结合，用于三维目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 汽车传感器融合系统常使用智能传感器和V2X模块，获取的数据通常是处理后的对象列表，而非传统传感器的原始数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 避免分别处理原始数据后再在对象层面融合，提出一种直接融合不同层次信息的方案，以提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将对象列表作为去噪查询输入Transformer，并与可学习查询一起进行特征聚合；在解码器中加入基于对象列表位置和尺寸先验的可变形高斯掩模，引导注意力并加速训练；通过模拟噪声和误检误漏生成伪对象列表，以弥补缺乏公开数据集的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在nuScenes数据集上相较于仅基于视觉的基线显著提升性能，并能在不同噪声水平和真实检测器下保持良好泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 跨层融合的Transformer框架能够有效结合高层抽象信息与低层原始图像，提升三维目标检测精度，并具有良好的适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在汽车传感器融合系统中，智能传感器和V2X模块被广泛使用。来自这些系统的传感器数据通常仅以处理后的对象列表形式提供，而不是传统传感器的原始数据。与其分别处理其他原始数据然后在对象层面进行融合，我们提出了一种基于Transformer的端到端跨层融合概念，将高度抽象的对象列表信息与原始摄像头图像结合用于三维目标检测。对象列表被作为去噪查询输入Transformer，并与可学习查询一起通过后续特征聚合过程传播。此外，基于对象列表的位置信息和尺寸先验得到的可变形高斯掩模被显式集成到Transformer解码器中，指导注意力聚焦于目标感兴趣区域并加速模型训练收敛。由于没有公开数据集包含单独的对象列表作为模态，我们提出了一种通过模拟状态噪声以及假阳性和假阴性从真实边界框生成伪对象列表的方法。作为首个进行跨层融合的工作，我们的方法在nuScenes数据集上相较于基于视觉的基线表现出显著的性能提升，并展示了对模拟对象列表不同噪声水平以及真实检测器的泛化能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在自动驾驶中，如何将仅提供对象列表的高层感知信息与原始相机图像低层数据进行融合，以提升3D目标检测的精度。由于V2X和智能传感器只能输出对象列表，传统的低层融合难以实现，故需要一种能在端到端模型中同时利用高层和低层信息的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于DETR类的3D检测框架，借鉴了DN-DETR的查询去噪技术和SMCA的空间加权注意力机制，设计了将对象列表转化为去噪查询并与可学习查询拼接的方式，并在Transformer解码器中加入可变形高斯掩码。为缺乏公开对象列表数据，作者还提出了伪对象列表生成模块来模拟不同噪声水平的检测器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将高层对象列表信息作为去噪查询和注意力掩码，直接注入Transformer解码器，实现端到端的跨层融合。实现流程为：相机图像通过骨干网络提取2D特征，生成3D坐标并编码为位置感知特征；对象列表被转换为去噪查询并与学习查询拼接；在解码器中使用去噪查询与位置感知特征进行自注意力和交叉注意力，交叉注意力中加入基于对象列表的可变形高斯掩码；最后将解码器输出的嵌入送入检测头得到3D边界框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次提出跨层融合的Transformer框架，将高层对象列表与低层图像特征在同一网络中联合学习；2) 采用查询去噪技术将对象列表直接作为去噪查询，提升鲁棒性；3) 引入可变形高斯掩码显式引导注意力，聚焦目标区域；4) 伪对象列表生成模块为无公开数据的场景提供训练数据；5) 该模块可无额外参数、无额外计算地插拔。与以往仅在高层或低层进行融合的工作不同，本文实现了端到端的跨层融合并显著提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了首个端到端Transformer框架，通过查询去噪和可变形高斯注意力将高层对象列表与原始相机图像进行跨层融合，显著提升了nuScenes上的3D目标检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.&lt;/p&gt;</description></item><item><guid>2512.13107v2</guid><title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title><link>http://arxiv.org/abs/2512.13107v2</link><author>Zhijian He, Feifei Liu, Yuwei Li, Zhanpeng Luo, Jintao Cheng, Xieyuanli Chen, Xiaoyu Tang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出DiffFusion框架，通过扩散模型恢复受天气影响的图像和点云，并使用双向自适应融合与对齐模块实现多模态融合，显著提升在恶劣天气下的三维目标检测鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态3D目标检测在机器人和自动驾驶中至关重要，但在恶劣天气下由于天气导致的失真和不同模态间的对齐问题，效果受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提升在恶劣天气条件下多模态3D目标检测的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Diffusion-IR图像恢复、PCR点云恢复、BAFAM双向自适应融合与对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个公开数据集上，DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时在清洁数据上保持强劲性能；在真实DENSE数据集的零样本测试也验证了其泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DiffFusion通过扩散模型恢复和自适应融合显著提升了恶劣天气下的检测性能，并具备良好的泛化性，且实现将开源发布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper proposes the DiffFusion framework, which uses diffusion models to restore images and point clouds affected by weather, and employs a bidirectional adaptive fusion and alignment module to achieve multimodal fusion, significantly improving the robustness of 3D object detection in adverse weather conditions.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升多模态3D目标检测在恶劣天气下的鲁棒性。恶劣天气会导致图像模糊、LiDAR点稀疏以及跨模态对齐失效，严重影响自动驾驶和机器人感知的安全性。解决这一问题有助于在真实环境中实现可靠的感知系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为扩散模型在去噪和生成方面表现优异，可用于统一处理多种天气退化。基于此，他们设计了Diffusion-IR和PCR两支分支，并引入BAFAM进行跨模态对齐。该方案借鉴了现有的扩散恢复、点云补全以及BEV融合方法，并在此基础上提出了新的模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用扩散模型恢复被天气破坏的图像和LiDAR点云，再通过自适应融合与对齐实现跨模态信息的有效整合。实现流程为：输入受损图像和点云 → Diffusion-IR恢复图像并提取特征 → 2D检测生成框 → PCR利用图像框和深度信息补全点云 → BAFAM进行特征融合与BEV对齐 → 3D检测头输出目标框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) Diffusion-IR对图像进行条件扩散恢复；2) PCR利用图像提示进行点云补全；3) BAFAM实现双向自适应融合与BEV对齐。与以往仅在单模态或简单融合的工作不同，DiffFusion在同一框架下同时解决图像去噪、点云补全和跨模态对齐，显著提升了恶劣天气下的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DiffFusion通过扩散式恢复与自适应跨模态融合，实现在恶劣天气下的鲁棒3D目标检测，并在公开数据集上刷新了性能记录。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird&amp;#x27;s-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.&lt;/p&gt;</description></item><item><guid>2512.15581v1</guid><title>IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</title><link>http://arxiv.org/abs/2512.15581v1</link><author>Shashank Mishra, Karan Patil, Didier Stricker, Jason Rambach</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于多层知识蒸馏的雷达-相机融合框架IMKD，利用雷达和相机的互补特性实现高性能3D目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的知识蒸馏方法直接将模态特征传递给各传感器，容易扭曲其独特特征，削弱各自优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不使用激光雷达推理的情况下，通过知识蒸馏提升雷达-相机3D检测性能，同时保持各模态的内在特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; IMKD采用三阶段强度感知蒸馏：1）雷达特征蒸馏增强细粒度结构信息；2）融合层蒸馏突出几何深度信息，促进模态互补；3）相机-雷达强度引导融合机制实现特征对齐与校准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes基准上，IMKD取得67.0% NDS和61.0% mAP，优于现有所有基于蒸馏的雷达-相机融合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多层强度感知蒸馏能够有效保留各模态特性并增强互补性，从而显著提升雷达-相机3D检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 通过知识蒸馏，在推理时不使用激光雷达即可实现高性能的雷达-相机3D目标检测。然而，现有的蒸馏方法通常将模态特定特征直接传递给每个传感器，这可能扭曲它们的独特特性并削弱各自的优势。为了解决这一问题，我们提出了IMKD，一种基于多层知识蒸馏的雷达-相机融合框架，能够在保留每个传感器内在特性的同时放大它们的互补优势。IMKD采用三阶段强度感知蒸馏策略，在整个架构中丰富融合表示：(1) 激光雷达到雷达的强度感知特征蒸馏，用细粒度结构线索增强雷达表示；(2) 激光雷达到融合特征的强度引导蒸馏，在融合层选择性突出有用的几何和深度信息，促进模态之间的互补，而不是强迫它们对齐；(3) 相机-雷达强度引导融合机制，促进有效的特征对齐与校准。对nuScenes基准的广泛实验表明，IMKD达到了67.0% NDS和61.0% mAP，优于所有先前基于蒸馏的雷达-相机融合方法。我们的代码和模型可在 https://github.com/dfki-av/IMKD/ 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升仅使用相机和雷达的三维目标检测性能，避免在推理时使用昂贵且范围有限的 LiDAR。此问题重要，因为成本效益高、在恶劣天气和长距离场景下仍能保持可靠感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统跨模态知识蒸馏往往强迫雷达或相机特征模仿 LiDAR，导致特征冲突。借鉴了 LiDAR‑to‑Camera、Fusion‑based KD 等现有工作，提出在多级、强度感知的框架中引入 LiDAR 作为特权教师，利用其强度信息作为置信度先验，指导雷达和融合特征的学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过强度感知的多级知识蒸馏，既保留每个传感器的固有特性，又增强其互补优势。实现流程包括：提取相机和雷达特征并升维到 BEV；使用强度感知的可变形注意力进行相机‑雷达融合；在三个阶段分别进行 LiDAR‑to‑Radar、LiDAR‑to‑Fused 以及 Camera‑Radar 的强度引导蒸馏；同时加入标签蒸馏和一致性损失；训练完成后仅使用相机和雷达进行推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 强度感知的多级蒸馏，利用 LiDAR 强度作为置信度先验；2) 在融合空间而非单一模态中进行蒸馏，提升空间推理；3) 引入强度感知的相机‑雷达融合模块；4) 通过结构化标签监督增强鲁棒性。与以往仅做模态对模态或融合对融合蒸馏的工作不同，IMKD 在保持各模态特性同时实现更高效的互补学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IMKD 通过强度感知的多级知识蒸馏，既保留相机和雷达的独特优势，又显著提升其融合性能，达成无 LiDAR 的三维检测新标杆。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor&amp;#x27;s intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.&lt;/p&gt;</description></item><item><guid>2512.16077v1</guid><title>Auto-Vocabulary 3D Object Detection</title><link>http://arxiv.org/abs/2512.16077v1</link><author>Haomeng Zhang, Kuan-Chuan Peng, Suhas Lohit, Raymond A. Yeh</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自动生成类别的3D目标检测方法AV3DOD，利用2D视觉语言模型生成语义候选，并通过图像描述、伪3D框生成和特征空间语义扩展实现高质量检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的开放词汇3D目标检测方法虽然能定位未见类别，但仍需用户在训练和推理时指定类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究一种无需用户输入即可自动生成检测对象类别的自动词汇3D目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入语义得分评估生成类别名称的质量，构建AV3DOD框架，利用2D视觉语言模型进行图像描述、伪3D框生成和特征空间语义扩展，生成丰富的语义候选。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ScanNetV2和SUNRGB-D数据集上，AV3DOD在定位和语义质量上均达到最先进水平，整体平均精度比CoDA高3.48，ScanNetV2上的语义质量提升24.5%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AV3DOD实现了无需用户输入即可自动生成类别的3D目标检测，并在多个数据集上显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 开放词汇3D目标检测方法能够定位训练期间未见过类别的3D框。尽管名称如此，现有方法在训练和推理时仍依赖用户指定的类别。我们提出研究自动词汇3D目标检测（AV3DOD），在检测对象时无需任何用户输入即可自动生成类别。为此，我们引入语义得分来评估生成类别名称的质量。随后，我们开发了一种新框架AV3DOD，利用2D视觉语言模型通过图像描述、伪3D框生成和特征空间语义扩展生成丰富的语义候选。AV3DOD在ScanNetV2和SUNRGB-D数据集上在定位（平均精度）和语义质量上均实现了最先进的性能。值得注意的是，它在ScanNetV2上比最先进方法CoDA整体平均精度高3.48，并在语义质量上相对提升24.5%。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 它解决了现有开放词汇3D检测仍需用户指定词汇表的问题，使检测器能够自动生成检测到物体的类别标签，从而实现真正的开放世界感知。该能力对机器人、自动驾驶和场景理解等需要识别未知物体的应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在开放词汇3D检测基础上，引入语义得分（Semantic Score）评估指标，并利用2D视觉语言模型（VLM）进行图像描述、伪3D框生成和特征空间语义扩展。框架借鉴了3DETR、CoDA、CLIP等已有技术，并结合了基于文本的语义对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合2D VLM产生的语义候选和特征空间扩展，构建丰富的词汇特征集合，然后将检测到的3D物体特征与该集合对齐以预测自由形式的类别标签。实现流程包括：①使用3DETR生成无类别的3D框和特征；②通过图像描述、伪标签和特征扩展构建超级词汇特征集合；③将每个框的特征与词汇集合匹配，得到类别预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①提出Auto‑Vocabulary 3D Object Detection任务；②设计Semantic Score评估指标；③利用2D VLM进行图像描述和伪3D框生成；④引入特征空间语义扩展以捕获非离散词汇概念；⑤在推理时不需要用户词汇表或2D输入。与以往开放词汇3D检测方法相比，AV3DOD能够自主构建词汇并仅依赖点云进行检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出AV3DOD框架，利用2D视觉语言模型和特征空间语义扩展，使3D检测器能够自动发现并命名物体，且在不需要预定义词汇表的情况下实现领先的检测与语义质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.&lt;/p&gt;</description></item><item><guid>2512.16818v1</guid><title>DenseBEV: Transforming BEV Grid Cells into 3D Objects</title><link>http://arxiv.org/abs/2512.16818v1</link><author>Marius Dähling, Sebastian Krebs, J. Marius Zöllner</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用BEV特征单元直接作为锚点的端到端多摄像头3D目标检测方法，并通过两阶段锚点生成、BEV基非极大值抑制以及混合时序建模实现性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 当前多摄像头3D检测研究中，BEV基变压器被广泛使用，传统模型采用随机查询作为锚点，最近的进展则用辅助网络检测结果替代随机查询。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在提供一种更直观、高效的锚点生成方式，利用BEV网格密集查询单元直接作为潜在目标，从而简化训练流程并提升检测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出两阶段锚点生成；使用BEV基非极大值抑制减少注意力规模问题；将BEV特征直接作为对象查询，天然嵌入时序信息；进一步结合先前检测结果进行混合时序建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，DenseBEV在NDS和mAP上均显著优于基线，尤其在稀疏BEV网格下仍保持优势；对小目标（行人）检测提升3.8% mAP；在Waymo上LET-mAP提升8%；在Waymo Open数据集上达到60.7% LET-mAP，超过前一最佳5.4%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DenseBEV通过直接使用BEV特征单元作为锚点，并结合BEV基NMS与混合时序建模，能够在保持训练效率的同时显著提升多摄像头3D检测性能，尤其对小目标检测效果突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在当前研究中，基于鸟瞰图（BEV）的变压器越来越多地用于多摄像头3D目标检测。传统模型通常使用随机查询作为锚点，并逐步优化它们。最近的进展补充或替代了这些随机查询，使用辅助网络的检测结果。我们提出了一种更直观、高效的方法，直接使用BEV特征单元作为锚点。该端到端方法利用BEV查询的稠密网格，将每个单元视为最终检测任务的潜在目标。因此，我们引入了一种专门为多摄像头3D目标检测设计的两阶段锚点生成方法。为了解决大量查询时注意力的缩放问题，我们应用BEV基非极大值抑制，只让梯度通过未被抑制的对象流动，从而实现高效训练而无需后处理。通过将来自BEVFormer等编码器的BEV特征直接用作对象查询，时序BEV信息天然嵌入。基于已嵌入的时序BEV信息，我们通过整合先前检测结果引入混合时序建模，以进一步提升检测性能。在nuScenes数据集上评估我们的方法，显示出相对于基线在NDS和mAP上的持续显著提升，即使使用更稀疏的BEV网格和更少的初始锚点。它对小目标尤其有效，在nuScenes上行人检测的mAP提升了3.8%，在Waymo上LET-mAP提升了8%。将我们的DenseBEV方法应用于具有挑战性的Waymo Open数据集，取得了LET-mAP 60.7%的最先进性能，超过之前最佳5.4%。代码可在 https://github.com/mdaehl/DenseBEV 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进多摄像头 3D 目标检测中的锚点生成方式。传统方法使用随机锚点或辅助网络产生锚点，导致锚点质量受限、计算开销大，尤其难以检测小目标。准确检测车辆、行人等多种目标对自动驾驶和机器人安全至关重要，因此需要更高效、更可靠的锚点策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 BEVFormer 的 BEV 编码器、Deformable DETR 的两阶段锚点初始化以及 DDQ 的 NMS 过滤思想，并结合 Stream-PETR 的时间记忆机制。通过将 BEV 网格细胞直接作为锚点，并在训练中加入 BEV‑NMS，作者实现了无需辅助网络的密集锚点生成，并在此基础上融合历史检测信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是把 BEV 编码器输出的网格细胞当作初始锚点，先用辅助检测头得到候选框，再用 BEV‑NMS 去除冗余，最后将保留的锚点送入 Transformer 解码器。实现流程包括：图像 → backbone → BEVFormer encoder → BEV 网格 → 辅助检测头 → BEV‑NMS → top‑k 选取 → 解码器（含抑制块）→ 3D 检测头；若有时间信息，还会把前一帧检测结果与当前 BEV 结合，形成混合时间锚点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 直接使用 BEV 网格细胞作为锚点，消除辅助网络；2) 在训练中嵌入 BEV‑NMS，减少冗余并保持高密度锚点；3) 通过记忆队列将历史检测与当前 BEV 结合，形成混合时间建模；4) 端到端实现，参数量低、计算开销小。与以往依赖随机锚点或外部检测器的做法不同，DenseBEV 在保持密集锚点的同时显著提升小目标检测性能，并在 Waymo Open 数据集上刷新了最佳成绩。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DenseBEV 提出了基于 BEV 网格的密集锚点生成与 BEV‑NMS 过滤，并结合混合时间建模，实现高效端到端的多摄像头 3D 目标检测，在大型基准上取得领先性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In current research, Bird&amp;#x27;s-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.&lt;/p&gt;</description></item><item><guid>2512.17012v2</guid><title>4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</title><link>http://arxiv.org/abs/2512.17012v2</link><author>Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了4D-RGPT模型、P4D训练框架和R4D-Bench基准，提升多模态LLM在4D视频问答中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有多模态LLM在3D结构和时间动态推理方面受限，4D感知薄弱；现有4D/3D视频问答基准侧重静态场景，缺乏区域级提示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决4D感知不足和基准缺陷，提升模型对4D视频的理解和推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 4D-RGPT：专门捕捉视频4D表示并增强时间感知；2) P4D：将冻结专家模型的4D表示蒸馏到4D-RGPT；3) R4D-Bench：构建深度感知动态场景的区域级提示基准，采用自动化与人工验证相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 4D-RGPT在现有4D VQA基准和新提出的R4D-Bench上均显著提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过专用模型、蒸馏框架和新基准，显著提升了多模态LLM在4D视频问答中的表现，为未来研究提供了工具和数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管多模态大型语言模型（MLLMs）取得了进展，但它们在推理3D结构和时间动态方面的能力仍然有限，受制于弱的4D感知和时间理解。现有的3D和4D视频问答（VQA）基准也侧重于静态场景，缺乏区域级提示。我们通过引入以下方法来解决这些问题：（a）4D-RGPT，一种专门的MLLM，旨在从视频输入中捕捉4D表示，并增强时间感知；（b）感知4D蒸馏（P4D），一种训练框架，将冻结的专家模型的4D表示转移到4D-RGPT，以实现全面的4D感知；以及（c）R4D-Bench，一个针对深度感知动态场景的区域级提示基准，采用混合自动化和人工验证的管道构建。我们的4D-RGPT在现有的4D VQA基准和提出的R4D-Bench基准上都取得了显著改进。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决区域级 4D 理解问题，即在视频中同时识别深度、运动和时间变化，并将语言查询精确定位到特定区域。该能力对自动驾驶、工业检测等需要精确空间时间信息的应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有多模态 LLM 在 4D 感知和区域提示方面表现不足，借鉴了 3D/4D VQA、区域级 MLLM 以及知识蒸馏技术，提出通过冻结专家 4D 感知模型进行训练时的蒸馏，并加入时间位置编码来提升时序感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用训练时仅存在的蒸馏模块，让学生 4D‑RGPT 学习专家模型的 4D 隐层特征和显式深度/光流信息，并通过时间位置编码增强时序感知。实现流程包括：视频编码 → 视觉投影 → LLM → 训练时 4D 感知解码器和预测头提取 4D 表征；冻结专家模型生成目标特征；对齐隐层和显式特征的蒸馏损失；最终训练得到具备 4D 感知的 MLLM。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 4D‑RGPT 结构，专门捕获 4D 表征；2) P4D 蒸馏框架，结合隐层和显式蒸馏，训练时无额外推理成本；3) 时间位置编码提升时序感知；4) 新的 R4D‑Bench 区域级 4D VQA 基准。与以往仅使用 SFT/RL 或添加额外模块的工作不同，本文通过蒸馏实现高效 4D 感知，并在区域级任务上取得显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种训练时蒸馏框架，使多模态 LLM 获得 4D 感知和区域级推理能力，并在新构建的 R4D‑Bench 上实现领先性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.&lt;/p&gt;</description></item><item><guid>2512.17620v1</guid><title>StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</title><link>http://arxiv.org/abs/2512.17620v1</link><author>Di Wu, Feng Yang, Wenhui Zhao, Jinwen Yu, Pan Liao, Benlian Xu, Dingwen Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多视角三维检测是自动驾驶感知的核心任务，稀疏查询式检测器通过可学习查询高效聚合多视角图像特征。MV2D利用二维检测结果为查询提供高质量先验，提升精度和召回率，但单帧二维检测的深度歧义限制了三维查询的准确性。本文提出StereoMV2D，将时序立体建模融入二维检测引导的多视角三维检测框架，利用相邻帧的跨时差异提升深度感知并细化查询先验，同时在二维感兴趣区域内高效计算。动态置信门控机制通过帧间匹配矩阵和外观一致性评估时序立体线索的可靠性，保证在外观变化和遮挡下的鲁棒检测。实验表明StereoMV2D在nuScenes和Argoverse 2数据集上实现了更优检测性能，且计算开销不显著增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多视角三维检测需要在检测精度与计算效率之间取得平衡。稀疏查询式检测器提供了简洁且端到端的检测范式。MV2D在此基础上利用二维检测结果为查询初始化提供高质量先验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过整合时序立体建模，解决单帧二维检测中深度歧义导致的三维查询生成精度受限的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; StereoMV2D框架在二维检测引导的多视角三维检测器中加入时序立体建模，利用相邻帧中同一目标的跨时差异提升深度感知并细化查询先验。所有计算在二维感兴趣区域内完成，并通过动态置信门控机制评估时序立体线索的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和Argoverse 2数据集上，StereoMV2D在不显著增加计算开销的前提下，取得了比现有方法更优的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; StereoMV2D通过融合时序立体信息和动态置信门控，有效克服了深度歧义问题，提供了一种高效、准确的多视角三维检测方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多视角三维目标检测是自动驾驶感知中的基础任务，如何在检测精度与计算效率之间取得平衡至关重要。稀疏查询式三维检测器通过一组可学习的查询高效聚合多视角图像中的目标相关特征，提供了简洁且端到端的检测范式。在此基础上，MV2D 利用二维检测结果为查询初始化提供高质量的目标先验，从而实现更高的精度和召回率。然而，单帧二维检测中固有的深度歧义仍限制了三维查询生成的准确性。为解决此问题，我们提出了 StereoMV2D，一种将时序立体建模融入二维检测引导的多视角三维检测器的统一框架。通过利用相邻帧中同一目标的跨时差异，StereoMV2D 提升了深度感知并细化了查询先验，同时在二维感兴趣区域内高效完成所有计算。此外，动态置信门控机制通过学习来自帧间匹配矩阵的统计模式以及外观一致性，动态评估时序立体线索的可靠性，确保在目标外观变化和遮挡下的鲁棒检测。对 nuScenes 和 Argoverse 2 数据集的广泛实验表明，StereoMV2D 在不产生显著计算开销的前提下实现了更优的检测性能。代码将发布在 https://github.com/Uddd821/StereoMV2D。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决多视角3D目标检测中单帧2D检测导致的深度不确定性问题。深度不准会导致3D查询初始化错误，影响检测精度。对自动驾驶等实时安全场景而言，既要高精度又要低延迟，解决深度模糊至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先基于MV2D的2D检测引导查询框架，发现单帧深度模糊限制性能。随后借鉴BEVStereo、SOLOFusion等工作中的跨帧几何约束，设计了两阶段查询生成器：先做运动感知软匹配，再在RoI级别构建轻量级立体匹配成本体。最后加入自适应置信门控以融合单帧与立体先验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将RoI级别的时间立体几何信息注入稀疏查询框架，以获得更准确的3D位置先验。实现流程为：多视角图像 → 图像特征提取 → 2D检测 → RoI特征提取 → 单帧与立体两路查询生成 → 运动感知软匹配 → RoI级立体匹配 → 置信门控融合 → 稀疏解码器 → 3D检测头。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) RoI级时间立体匹配，仅在检测框内构建成本体，显著降低计算；2) 动态置信门控学习单帧与立体先验的可靠性并加权融合；3) 将上述技术嵌入稀疏查询检测器，保持高效推理。与以往使用全图或BEV稠密立体、仅基于外观的时间传播不同，StereoMV2D在保持稀疏性的同时引入几何一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; StereoMV2D通过RoI级时间立体匹配和自适应置信门控，在保持稀疏查询效率的同时显著提升多视角3D检测的深度精度与整体性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.&lt;/p&gt;</description></item><item><guid>2512.17908v1</guid><title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title><link>http://arxiv.org/abs/2512.17908v1</link><author>Ananta R. Bhattarai, Helge Rhodin</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种在测试时自监督的框架 Re-Depth Anything，旨在解决单目深度估计在真实图像上的性能下降问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的基础模型如 Depth Anything V2 在训练分布之外的真实图像上表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过融合 DA-V2 与大型 2D 扩散模型的先验知识，弥合域差距并提升深度估计的准确性与真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在输入图像上进行无标签的细化，重新照明预测的深度图并增强输入；使用形状自阴影提示与 Score Distillation Sampling 替代传统的光度重建；冻结编码器，仅更新中间嵌入并微调解码器，以防止优化崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多种基准测试中，Re-Depth Anything 在深度精度和视觉真实性上显著优于 DA-V2，验证了自监督与几何推理相结合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架为利用几何推理增强自监督提供了新的思路，并展示了在单目深度估计领域的显著改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在真实世界图像上，现有基础模型（如Depth Anything V2）产生的深度估计误差。由于这些模型在训练时主要使用合成或有限的真实数据，面对分布外的图像时会出现不准确的深度预测，影响三维重建、自动驾驶、机器人导航等关键应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了自监督测试时适配、2D扩散模型作为先验以及形状与光照的关系。借鉴了DreamFusion、RealFusion等利用扩散模型进行3D重建的工作，并将其思想迁移到单图像深度估计的自监督优化中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用DA‑V2得到初始深度，然后用Blinn‑Phong模型在随机光照下重新渲染该深度，得到增强的图像；再用扩散模型的Score Distillation Sampling（SDS）对该图像进行评分，并将梯度反向传播到深度网络的中间嵌入和解码器权重（编码器保持冻结）。整个流程包括：输入图像 → DA‑V2初始深度 → 计算法向量 → 随机光照渲染 → SDS损失 → 目标嵌入/解码器优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 在测试时使用单图像自监督优化；2) 通过重新照明而非光度重建，将深度与图像关联；3) 采用SDS损失将2D扩散模型的先验引入深度优化；4) 只更新嵌入和解码器，避免过拟合。与以往多视角NeRF或全局光照重建不同，Re‑Depth Anything 在单视角下即可提升深度精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Re‑Depth Anything 在测试时通过对深度预测进行随机照明并利用扩散模型的SDS损失进行自监督优化，显著提升了基础模型在真实图像上的深度估计精度，且不需要额外标注。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.&lt;/p&gt;</description></item><item><guid>2512.18187v1</guid><title>ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</title><link>http://arxiv.org/abs/2512.18187v1</link><author>Janghyun Baek, Mincheol Chang, Seokha Moon, Seung Joon Lee, Jinkyu Kim</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 ALIGN 方法，改进 3D 目标检测中的查询初始化，提升遮挡和拥挤场景下的检测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有基于查询的 3D 检测方法在相机和 LiDAR 输入下表现良好，但随机或 BEV 热图采样的查询初始化往往低效，导致遮挡或拥挤物体的准确率下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决查询初始化低效导致的精度下降问题，提出一种对遮挡鲁棒、面向对象的查询初始化方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ALIGN 由三部分组成：Occlusion-aware Center Estimation（OCE）利用 LiDAR 几何和图像语义估计物体中心；Adaptive Neighbor Sampling（ANS）从 LiDAR 聚类生成候选物体，并在其周围采样空间和语义对齐的点；Dynamic Query Balancing（DQB）自适应平衡前景和背景查询。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 nuScenes 基准上，ALIGN 在多种先进检测器上均提升性能，最大提升 0.9 mAP 和 1.2 NDS，尤其在遮挡或密集人群场景中效果显著。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ALIGN 通过改进查询初始化显著提升 3D 检测精度，特别适用于遮挡和拥挤环境，且代码将公开。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近基于查询的 3D 目标检测方法使用相机和 LiDAR 输入已显示出强劲的性能，但现有的查询初始化策略，如随机采样或基于 BEV 热图的采样，往往导致查询使用效率低下和准确率下降，尤其是在遮挡或拥挤物体的情况下。为了解决这一限制，我们提出 ALIGN（Advanced query initialization with LiDAR and Image GuidaNce），一种针对遮挡鲁棒、面向对象的查询初始化的新方法。我们的模型由三个关键组件组成：（i）Occlusion-aware Center Estimation（OCE），它整合 LiDAR 几何和图像语义来准确估计物体中心；（ii）Adaptive Neighbor Sampling（ANS），它从 LiDAR 聚类生成物体候选，并通过在其周围采样空间和语义对齐的点来补充每个物体；（iii）Dynamic Query Balancing（DQB），它自适应地在前景和背景区域之间平衡查询。我们在 nuScenes 基准上的广泛实验表明，ALIGN 在多种最先进的检测器上持续提升性能，尤其在遮挡或密集人群的挑战性场景中，提升幅度高达 +0.9 mAP 和 +1.2 NDS。我们的代码将在发布后公开。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文解决了查询式3D目标检测中查询初始化不合理导致的效率低下和对遮挡或拥挤场景识别不佳的问题。准确的目标检测是自动驾驶和机器人感知的核心，尤其在复杂城市环境中，遮挡和密集目标的检测准确性直接影响安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了随机采样和BEV热图采样的局限，随后结合LiDAR几何信息与图像语义，提出了OCE、ANS和DQB三模块。设计中借鉴了DBSCAN聚类、Deformable DETR的邻域采样、BEV热图与图像语义融合等已有技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态信息精确估计目标中心，随后在聚类核心点周围采样邻域查询，并动态平衡前景与背景查询。实现流程为：1）将LiDAR点投影到图像，利用分割掩码和局部同伦估计中心；2）对LiDAR点云做DBSCAN聚类，采样核心点及其邻域并语义过滤；3）根据剩余查询预算按比例分配邻域查询和随机背景查询；4）将所有查询送入多模态Transformer解码器完成检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① Occlusion‑Aware Center Estimation (OCE) 通过图像分割与LiDAR投影精确定位中心；② Adaptive Neighbor Sampling (ANS) 在聚类核心点周围采样语义一致的邻域点；③ Dynamic Query Balancing (DQB) 根据场景密度动态分配前景与背景查询。与以往仅使用随机或热图采样的做法不同，ALIGN提供了显式的目标中心估计和自适应邻域采样，显著提升了遮挡和密集场景下的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALIGN通过多模态目标中心估计、邻域采样和动态查询平衡，提出了一种高效、遮挡鲁棒的查询初始化策略，显著提升了3D目标检测的准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent query-based 3D object detection methods using camera and LiDAR inputs have shown strong performance, but existing query initialization strategies,such as random sampling or BEV heatmap-based sampling, often result in inefficient query usage and reduced accuracy, particularly for occluded or crowded objects. To address this limitation, we propose ALIGN (Advanced query initialization with LiDAR and Image GuidaNce), a novel approach for occlusion-robust, object-aware query initialization. Our model consists of three key components: (i) Occlusion-aware Center Estimation (OCE), which integrates LiDAR geometry and image semantics to estimate object centers accurately (ii) Adaptive Neighbor Sampling (ANS), which generates object candidates from LiDAR clustering and supplements each object by sampling spatially and semantically aligned points around it and (iii) Dynamic Query Balancing (DQB), which adaptively balances queries between foreground and background regions. Our extensive experiments on the nuScenes benchmark demonstrate that ALIGN consistently improves performance across multiple state-of-the-art detectors, achieving gains of up to +0.9 mAP and +1.2 NDS, particularly in challenging scenes with occlusions or dense crowds. Our code will be publicly available upon publication.&lt;/p&gt;</description></item><item><guid>2512.18684v1</guid><title>A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</title><link>http://arxiv.org/abs/2512.18684v1</link><author>Huimin Wu, Kwang-Ting Cheng, Stephen Lin, Zhirong Wu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了将视频预训练的视觉变换器用于多视角几何任务，如光流估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多视角几何任务需要同时处理时空信息，传统方法往往需要定制网络结构和任务特定的预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究通用视频预训练模型在多视角几何任务中的迁移效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在视频预训练的Transformer骨干上添加线性解码器，并通过迭代细化提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通用模型的注意力机制能学习时空信息，最小适配即可迁移到多视角任务；线性解码器足以获得良好结果；迭代细化可达到最先进水平；在光流估计上实现跨数据集最佳泛化，并在在线测试基准上创下新纪录；在3D深度估计和立体匹配上也表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 视频预训练模型在几何视觉任务中具有高度通用性和卓越性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了对多视角几何任务（如光流估计）进行视觉变换器学习的研究，通过微调视频基础模型实现。与以往需要定制架构和任务特定预训练的方法不同，我们的研究发现，通用的、在视频上预训练的模型可以在最小适配下直接迁移到多视角问题。核心见解是，通用的注意力机制在补丁之间学习时空信息，从而实现几何推理。我们证明，在Transformer骨干后添加线性解码器即可获得令人满意的结果，迭代细化进一步将性能提升到最先进水平。该概念上简单的方法在光流估计的跨数据集泛化上取得了顶尖成绩，端点误差（EPE）分别为0.69、1.78和3.15（Sintel clean、Sintel final和KITTI）。我们的方案在在线测试基准上也创下了新纪录，EPE分别为0.79、1.88，F1值为3.79。对3D深度估计和立体匹配的应用也显示出强劲表现，说明视频预训练模型在解决几何视觉任务方面的多样性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在研究如何将通用的视频预训练视觉变压器（ViT）迁移到多视角几何任务（如光流、立体匹配和深度估计）中，并实现高性能。该问题重要，因为这些几何任务是计算机视觉的基础，现有方法往往需要复杂的专用网络和任务特定的预训练，限制了模型的通用性和部署效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为跨帧自注意力能够捕捉匹配信息，决定将预训练的 3D ViT 适配为两帧任务，并在其上添加线性解码器和迭代细化。设计借鉴了 RAFT 的迭代细化思想、Flowformer 的 Transformer 结构以及 CroCo 的适配模块，但避免了专门的预训练任务和复杂的成本体积。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视频预训练的 Transformer 编码器的跨帧注意力来提取几何特征，并通过一个简单的线性头直接回归几何量，再用 ConvGRU 进行迭代细化。实现流程包括：1）调整空间和时间位置编码以适配两帧输入；2）将图像对（或已 warp 的图像）送入预训练 ViT 生成特征；3）线性头输出初始预测；4）在每一步迭代中将上一步预测 warp 目标图像，重新编码并用 ConvGRU 预测残差，累加得到更精细的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）证明通用视频预训练 ViT 能直接迁移到几何任务；2）仅使用线性头即可获得强性能；3）提出不依赖成本体积的迭代细化框架；4）在光流、立体匹配和深度估计上实现新的 state‑of‑the‑art。与以往需要专门预训练、复杂管线或成本体积的工作不同，GeoViT 只需最小化改动即可获得高性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们展示了一个通用的视频预训练 Transformer，配合线性解码器和迭代细化即可在光流、立体匹配和深度估计等多视角几何任务中实现 state‑of‑the‑art 性能，提供了一种简单而通用的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.&lt;/p&gt;</description></item><item><guid>2512.19083v2</guid><title>CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</title><link>http://arxiv.org/abs/2512.19083v2</link><author>Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; CoDrone 是一种云-边缘-端协同计算框架，利用基础模型提升资源受限无人机的自主导航性能。通过使用灰度图像、边缘深度估计和一维占据网格导航，结合深度强化学习调度器和视觉语言交互模块，CoDrone 能在不同飞行速度和网络条件下显著提高飞行距离和导航质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 无人机自主导航受限于机载计算资源，导致只能使用浅层深度网络，难以处理复杂环境；将任务卸载到边缘服务器会产生高延迟，系统设计面临权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决机载计算受限和边缘延迟带来的挑战，构建一种协同计算框架，提升无人机在复杂环境中的导航性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CoDrone 采用灰度图像进行导航模型，必要时使用边缘基础模型 Depth Anything V2 进行深度估计，并引入一维占据网格导航方法；通过深度强化学习神经调度器将深度估计与导航决策融合；同时加入 UAV 专用视觉语言交互模块，支持低级飞行指令与云模型的交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，CoDrone 在不同飞行速度和网络条件下比基线方法提升平均飞行距离 40% ，平均导航质量提升 5%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CoDrone 通过云-边缘-端协同和基础模型的集成，显著提升了资源受限无人机的自主导航能力，验证了其在复杂环境中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在资源受限的无人机上实现可靠自主导航的问题，避免使用过重的深度神经网络，同时减少因将任务卸载到远程服务器而产生的高延迟。该问题重要，因为无人机在物流、监测和灾害响应等应用中需要在复杂环境中安全、无碰撞飞行，而计算资源和能源往往十分有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了轻量化 DNN 与纯边缘卸载方案的不足，然后借鉴了 Depth Anything V2 等深度估计模型和视觉语言模型的优势，构建了一个混合架构。该架构使用灰度图像进行本地推理，必要时向边缘服务器请求深度估计，并在复杂场景下调用云端视觉语言模型；整个过程由 DRL 调度器学习何时卸载，借鉴了 adaDrone、DRL 调度等已有工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CoDrone 的核心思想是让无人机运行极轻量的导航网络，必要时向边缘服务器请求深度估计，再在更复杂或未知场景下调用云端视觉语言模型。实现流程为：捕获灰度图像 → 本地网络输出转向角和碰撞风险；若需要更细致感知，则将图像发送给边缘服务器，得到深度并构建占据格栅，再反馈给导航网络；若情况复杂，调度器触发云端视觉语言模型，模型通过低级飞行动作原语给出语义指导；所有决策实时完成，保持低延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）云‑边‑端协同框架，将基础模型与轻量化本地推理结合；2）使用灰度图像降低计算和带宽开销；3）边缘深度估计与一维占据格栅实现细粒度感知；4）基于 DRL 的神经调度器学习卸载策略；5）无人机专用视觉语言交互模块，提供可调用的低级飞行动作原语。与以往仅使用浅层 DNN 或简单边缘卸载的方案不同，CoDrone 同时利用深度与语义推理，保持实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoDrone 展示了通过协调轻量化本地导航、边缘深度估计和云端视觉语言推理，在资源受限且网络条件变化的环境下显著提升无人机自主导航性能的可行性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.&lt;/p&gt;</description></item><item><guid>2512.19150v1</guid><title>AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</title><link>http://arxiv.org/abs/2512.19150v1</link><author>Ruikai Li, Xinrun Li, Mengwei Xie, Hao Shan, Shoumeng Qiu, Xinyuan Chang, Yizhe Fan, Feng Xiong, Han Jiang, Yilong Ren, Haiyang Yu, Mu Xu, Yang Long, Varun Ojha, Zhiyong Cui</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; AMap提出了一种前瞻性在线高清地图构建框架，通过教师-学生蒸馏方式将未来时序信息压缩到仅使用当前帧的轻量模型，从而在不增加推理成本的前提下显著提升前方道路的感知质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在线高清地图构建是自动驾驶的关键技术。现有方法多利用历史时序融合来提升性能，但其本质上是“向后看的”，主要改善已行驶区域的地图重建，对未见的前方道路改进有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补现有方法在前方感知上的安全缺口，使得前方误差不再导致危险驾驶行为，同时保持单帧推理的高效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用“distill-from-future”范式，先用拥有未来时序上下文的教师模型训练，再将其知识蒸馏到仅使用当前帧的学生模型；技术细节包括多层BEV蒸馏、空间遮罩以及非对称查询适配模块，以有效迁移前瞻性表征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和Argoverse 2基准上，AMap在当前帧感知上显著优于现有时序模型，尤其在关键前方区域表现更佳，同时保持单帧推理的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 前瞻性在线高清地图构建能够在不增加推理成本的情况下提升前方感知安全性，为自动驾驶提供更可靠的地图信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在线高清地图构建是自动驾驶的关键技术。虽然最近的方法利用历史时序融合来提升性能，但我们发现这种范式存在一个关键的安全缺陷：它本质上是“向后看的”。这些方法主要提升已行驶区域的地图重建，对未见的前方道路改进有限。我们的下游规划任务分析显示，后向感知误差往往可以容忍，而前向区域的不准确直接导致危险驾驶行为。为弥补这一安全缺口，我们提出了AMap，一种前瞻性在线高清地图构建框架。我们开创了“distill-from-future”范式，使用拥有未来时序上下文的教师模型指导仅受限于当前帧的轻量学生模型。该过程隐式地将未来知识压缩到学生模型中，使其在零推理时成本下具备“前瞻”能力。技术上，我们引入了多层BEV蒸馏策略，配合空间遮罩和非对称查询适配模块，有效地将前瞻性表征迁移到学生模型的静态查询。对nuScenes和Argoverse 2基准的广泛实验表明，AMap显著提升了当前帧感知。最值得注意的是，它在关键前方区域的表现超过了最先进的时序模型，同时保持了单帧推理的效率。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文指出现有在线高清地图构建方法主要利用历史帧进行融合，导致模型在车辆已通过的后方区域表现良好，而对前方未见道路的感知提升有限。前方区域的准确性对路径规划和安全决策至关重要，后方误差对后续任务影响较小，因此需要一种能够提升前方感知的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先通过实验量化了前后区域对下游任务的不同影响，发现前方信息更关键。随后借鉴知识蒸馏、BEV 表示和向量化地图构建等已有技术，提出“distill-from-future”框架：先训练拥有未来时序信息的教师模型，再将其未来感知知识蒸馏给仅使用当前帧的学生模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用未来帧提供的先验知识，通过知识蒸馏将前瞻性表示压缩进仅使用当前帧的学生网络，从而在推理时获得“看前”能力。实现流程包括：①训练教师模型加入未来帧上下文；②使用BEV遮罩、多层BEV特征蒸馏和不对称查询匹配模块，将教师的未来感知信息迁移给学生；③学生在推理时仅输入当前帧即可得到前方感知增强的地图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①首次系统识别并量化了后向偏差，并提出前向感知评估指标 A‑mAP；②提出“distill-from-future”蒸馏范式，将未来时序信息作为教师监督；③设计了BEV遮罩、多层BEV蒸馏和查询匹配转移模块，解决异构上下文蒸馏难题；④在保持单帧推理效率的同时，使前方感知精度超过现有多帧融合模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AMap通过将未来时序信息蒸馏给仅使用当前帧的学生模型，实现零推理成本的前瞻性高清地图构建，显著提升前方区域的感知精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking.&amp;quot; These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future&amp;quot; paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead&amp;quot; capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student&amp;#x27;s static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.&lt;/p&gt;</description></item><item><guid>2512.19871v1</guid><title>HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</title><link>http://arxiv.org/abs/2512.19871v1</link><author>Jong Wook Kim, Wonseok Roh, Ha Dam Baek, Pilhyeon Lee, Jonghyun Choi, Sangpil Kim</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 HyGE-Occ 框架，通过融合连续高斯深度表示与离散深度分箱以及边缘先验，提升 3D 全景占据预测的几何一致性与边界感知，实验表明其在 Occ3D-nuScenes 数据集上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在 3D 场景重建中，需要对每个占据区域进行语义类别和实例身份预测，以实现细粒度的 3D 理解。然而现有方法往往难以保持精确几何和捕捉实例的空间范围，导致全景分离效果不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为克服上述局限，提出 HyGE-Occ，旨在通过混合视图转换分支和边缘先验提升几何一致性和边界感知，从而实现更精确的 3D 全景占据预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; HyGE-Occ 采用混合视图转换分支，将连续高斯深度表示与离散深度分箱融合，生成具有更好几何一致性和结构连贯性的 BEV 特征；同时从 BEV 特征中提取边缘图并作为辅助信息学习边缘线索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 Occ3D-nuScenes 数据集上，HyGE-Occ 的性能优于现有方法，显示出更强的 3D 几何推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; HyGE-Occ 通过融合高斯深度与边缘先验，显著提升了 3D 全景占据预测的几何一致性和边界感知，为复杂环境下的精细 3D 理解提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 全景占据预测旨在通过预测 3D 空间中每个占据区域的语义类别和实例身份来重建密集体素场景图。实现这种细粒度的 3D 理解需要精确的几何推理和跨复杂环境的空间一致场景表示。然而，现有方法往往难以保持精确几何并捕捉 3D 实例的精确空间范围，这对于稳健的全景分离至关重要。为克服这些限制，我们提出了 HyGE-Occ，一种利用混合视图转换分支与 3D 高斯和边缘先验的新框架，以增强 3D 全景占据预测中的几何一致性和边界感知。HyGE-Occ 采用混合视图转换分支，将连续基于高斯的深度表示与离散深度分箱形式融合，生成具有改进几何一致性和结构连贯性的 BEV 特征。与此同时，我们从 BEV 特征中提取边缘图并将其用作辅助信息来学习边缘线索。在 Occ3D-nuScenes 数据集上的广泛实验中，HyGE-Occ 超越了现有工作，展示了卓越的 3D 几何推理能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 3D 视景占用预测（3D Panoptic Occupancy Prediction）问题，即在三维空间中为每个占用区域同时预测语义类别和实例身份。该问题在自动驾驶等场景中至关重要，因为完整的三维场景理解能够支持安全的运动规划和长期预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为离散深度投影（LSS）在空间定位上精确，但会产生量化误差；连续高斯投影能提供几何连续性和不确定性建模。于是他们结合两种方法，并借鉴了 Lift‑Splat‑Shoot、Gaussian Splatting 以及边缘监督技术，设计了混合视图转换分支和边缘预测模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将离散深度和连续高斯深度的 BEV 特征通过 α‑混合融合，得到既精确又连续的空间表示；随后利用从语义标签生成的伪边缘监督提升边界清晰度。实现流程为：多视图图像 → 共享 backbone 提取特征 → 两个投影分支（LSS 与 Gaussian） → 混合 BEV 特征 → 边缘预测模块 → 体素解码器 → 语义与实例预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 混合视图转换分支：融合离散与连续深度，兼顾几何一致性与空间精度；2) 边缘预测模块：使用伪边缘监督显式强化边界信息；3) 两个模块可无缝集成到现有 BEV 基础模型。与以往仅使用离散深度或仅使用连续深度、缺乏边缘监督的工作相比，HyGE‑Occ 在几何精度和分割边界上均有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HyGE‑Occ 通过混合离散与连续深度投影并加入边缘监督，构建了更精确、更连贯的 BEV 表示，显著提升了 3D 视景占用预测的几何与分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.&lt;/p&gt;</description></item><item><guid>2512.20217v1</guid><title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title><link>http://arxiv.org/abs/2512.20217v1</link><author>Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的多模态3D检测器LiteFusion，利用LiDAR点云作为相机检测的几何补充，去掉了传统3D骨干网络，提升了部署友好性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态3D检测器依赖复杂架构和LiDAR，导致在无LiDAR时性能下降，且难以在NPU、FPGA等硬件上部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 重新评估LiDAR在相机-LiDAR融合中的角色，设计一种不依赖3D骨干、易部署且鲁棒的检测方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; LiteFusion将LiDAR点云的几何信息嵌入四元数空间，与图像特征融合，保持正交约束，形成紧凑的跨模态嵌入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，LiteFusion相较基线视觉检测器提升了约20.4% mAP和19.7% NDS，仅增加1.1%参数，且在无LiDAR输入时仍保持强劲表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LiteFusion通过简化LiDAR处理，兼顾性能与部署，展示了在多种融合与硬件场景下的鲁棒性与有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D目标检测是安全可靠的智能交通系统的基础。当前多模态3D目标检测器往往依赖复杂的架构和训练策略以获得更高的检测精度。然而，这些方法高度依赖激光雷达传感器，导致在缺失激光雷达时性能大幅下降，影响了自动驾驶系统在实际场景中的鲁棒性和安全性。此外，现有多模态检测器由于使用3D稀疏卷积操作，主要针对NVIDIA GPU进行优化，难以在多样化的硬件平台（如NPU和FPGA）上部署。为了解决这些挑战，我们重新审视了激光雷达在相机-激光雷达融合范式中的作用，并提出了一种新型多模态3D检测器LiteFusion。LiteFusion不将激光雷达点云视为独立模态并使用单独的特征提取骨干网络，而是将激光雷达数据作为相机检测的几何信息补充来源。该直观方法完全消除了对3D骨干的依赖，使方法高度易于部署。具体而言，LiteFusion将激光雷达点的互补特征嵌入四元数空间，在网络训练过程中保持正交约束，从而帮助模型学习跨模态的域特定关系，得到紧凑的跨模态嵌入。在nuScenes数据集上的实验表明，LiteFusion在不使用专门的激光雷达编码器的情况下，将基线视觉检测器的mAP提升了20.4%，NDS提升了19.7%，参数增加仅1.1%。值得注意的是，即使在缺失激光雷达输入的情况下，LiteFusion仍保持强劲的结果，凸显了其在多种融合范式和部署场景下的鲁棒性和有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在让基于相机的3D目标检测器在加入激光雷达（LiDAR）后，既能显著提升检测精度，又能保持对LiDAR缺失的鲁棒性，并且易于在不同硬件平台上部署。这个问题重要，因为在自动驾驶等实际场景中，LiDAR可能失效或成本高昂，而高精度的3D感知是安全决策的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统多模态检测器对LiDAR的过度依赖和对3D稀疏卷积的硬件依赖，决定把LiDAR视为补充的几何信息而非独立模态。借鉴了BEVFormer、深度感知嵌入（Depth‑Aware Embedding）和几何感知嵌入（Geometry‑Aware Embedding）等现有技术，并引入四元数空间来实现跨模态融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过四元数空间将LiDAR的3D几何信息映射到相机的2D特征中，并在网络的每个阶段逐步注入这些几何特征。实现流程包括：①将点云投影为视角图（PV）和鸟瞰图（BEV）；②分别使用DAE和GAE模块生成深度和几何嵌入；③将嵌入逐层融合到相机特征中；④最终通过BEV解码器和检测头得到3D框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 去掉了独立的3D点云编码器，使用轻量级几何整合器；2) 在四元数空间实现跨模态融合，保持正交约束；3) 采用逐层响应框架，逐步增强视觉特征；4) 仅使用标准conv2d，易于在NPU、FPGA等平台部署。与以往需要稀疏卷积、复杂双流结构且对LiDAR高度依赖的模型不同，LiteFusion在参数增量极小的情况下即可获得相当甚至更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiteFusion通过在四元数空间中轻量级地将LiDAR几何信息注入相机特征，实现了从视觉到多模态的无缝转换，既提升了3D检测精度，又保持了对LiDAR缺失的鲁棒性和跨平台的部署友好性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.&lt;/p&gt;</description></item><item><guid>2512.20557v1</guid><title>Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</title><link>http://arxiv.org/abs/2512.20557v1</link><author>Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出DSR Suite，包含自动化生成动态空间推理问题的数据集、基准测试以及轻量化几何选择模块，显著提升视觉语言模型在动态空间推理任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视觉语言模型在一般理解任务表现优异，但在动态空间推理（即随时间变化的三维几何与关系推理）方面仍弱，主要原因是缺乏可扩展的四维感知训练资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补数据集、基准与模型三方面的不足，构建支持动态空间推理的完整体系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 设计自动化流水线，从真实视频中提取相机姿态、点云、物体掩码、方向和三维轨迹，生成多项选择问答对；构建DSR-Train用于学习，DSR-Bench用于评估；2) 开发Geometry Selection Module，将预训练的四维重建先验压缩为几何令牌，精准提取与问题相关的几何知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 将DSR-Train与GSM集成到Qwen2.5-VL-7B后，模型在动态空间推理任务上显著提升，同时保持了在通用视频理解基准上的准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DSR Suite通过丰富的数据、严格的基准和高效的几何模块，为视觉语言模型在动态空间推理领域提供了实质性进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉语言模型（VLM）在一般理解任务上表现出色，但在动态空间推理（DSR）方面仍显薄弱，即关于物体几何和关系随时间在三维空间中演变的推理，主要由于缺乏可扩展的四维感知训练资源。为弥补数据集、基准和模型三方面的差距，我们提出了DSR Suite。首先，我们提出了一套自动化流水线，从真实视频中生成多项选择问答对用于DSR。利用现代视觉基础模型，该流水线提取丰富的几何和运动信息，包括相机姿态、局部点云、物体掩码、方向和三维轨迹。这些几何线索使我们能够构建用于学习的DSR-Train以及进一步人工精炼的用于评估的DSR-Bench。与以往工作相比，我们的数据强调：（i）真实视频来源；（ii）物体和场景级别的三维需求；（iii）视角变换；（iv）多物体交互；（v）细粒度、程序化答案。除了数据，我们提出了轻量化的几何选择模块（GSM），无缝将几何先验整合到VLM中，它将问题语义压缩并从预训练的四维重建先验中提取与问题相关的知识，形成紧凑的几何令牌集合。这种针对性提取避免了模型被无关知识淹没。实验表明，将DSR-Train和GSM集成到Qwen2.5-VL-7B后，显著提升了其动态空间推理能力，同时保持了在通用视频理解基准上的准确性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升视觉语言模型在动态空间推理（DSR）方面的能力。DSR要求模型理解物体在三维空间随时间演变的几何关系，这对机器人、自动驾驶、AR/VR 等需要实时空间感知的应用至关重要，但现有模型在这方面表现薄弱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出缺乏可扩展的 4D 训练资源、评测基准和有效模型的瓶颈，随后构建了自动化数据生成管线，并结合现有的视觉基础模型（如 Grounded SAM2、Orient Anything、π³）提取几何信息。该思路借鉴了静态空间推理的数据与模型设计，但扩展到动态、视角变换和多物体交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过三阶段管线：视频筛选 → 几何线索提取 → QA 生成，构建大规模训练集 DSR‑Train 和人类校准的评测集 DSR‑Bench。随后提出 Geometry Selection Module（GSM），利用双 Q‑Former 先压缩问题语义，再从 4D 先验中挑选与问题相关的几何 token，避免无关噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 自动化生成 4D QA 对，覆盖视角变换、多物体交互和细粒度答案；2) DSR‑Train 与 DSR‑Bench 两套资源，兼顾规模与质量；3) GSM 轻量化、可控的几何知识注入方式，兼顾 DSR 与通用视频理解；4) 在 Qwen2.5‑VL‑7B 上实现了 DSR 领先性能而不牺牲其他基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提供了一个可扩展的 4D 数据管线、评测基准和轻量化几何注入模块，使视觉语言模型在动态空间推理上实现显著提升，同时保持通用视频理解能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.&lt;/p&gt;</description></item><item><guid>2512.20770v1</guid><title>OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective</title><link>http://arxiv.org/abs/2512.20770v1</link><author>Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了语义场景完成（SSC）在移动机器人中的重要性，并指出在空中场景中SSC研究不足。为此，作者提出了首个基于相机的空中SSC基准数据集OccuFly，并开发了无激光雷达的数据生成框架，利用传统三维重建和2D掩码迁移显著降低了三维标注工作量。通过在OccuFly上对现有方法进行基准测试，揭示了高空视角下的挑战，为空中三维场景理解提供了全面的视觉基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; SSC在地面自动驾驶等领域已得到广泛研究，但在无人机等空中场景中应用有限。激光雷达是SSC数据生成的主流方式，但受限于飞行法规、重量和能耗，且高空激光点云稀疏，难以满足需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个真实世界、基于相机的空中SSC基准数据集OccuFly，并提出一种无激光雷达的数据生成框架，以降低三维标注成本并促进空中SSC研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用相机采集不同季节、不同高度的图像，利用传统三维重建技术生成点云；通过将部分已标注的二维掩码投影到重建点云，实现自动标签迁移；在OccuFly数据集上对现有SSC方法进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 高空视角下的SSC面临独特挑战；无激光雷达的相机基准数据集可有效支持空中SSC研究；自动标签迁移显著减少了人工三维标注工作量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OccuFly为空中三维场景理解提供了全面的视觉基准，展示了相机驱动的SSC方法的可行性，并为后续研究指明了方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提供首个真实世界的无人机视角下的语义场景完成（SSC）基准数据集，并提出一种无需 LiDAR 的数据生成方法。该问题重要，因为现有 SSC 数据集主要基于地面车辆，缺乏空中视角的数据，且 LiDAR 在无人机上受重量、能耗和稀疏性限制，阻碍了空中三维感知的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有 SSC 数据集（如 SemanticKITTI、nuScenes 等）和空中重建技术，采用多视角结构光与多视角立体（SfM+MVS）生成稠密点云，并通过 2D–3D 对应关系将少量 2D 标注提升到 3D。随后使用实例分离、表面重建和体素化等步骤完成数据集构建，并参考了 Occ3D、OpenOccupancy 等相关工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用相机图像进行稠密 3D 重建，然后将少量 2D 语义标注投射到点云中，融合多视角信息并进行平滑；接着对实例类进行稠密化、对地面类进行表面重建、对其他类直接体素化；最后对每帧相机视角进行视锥裁剪得到固定大小的语义体素网格，并提供对应的深度图。整体流程包括：重建 → 2D 标注 → 3D 标注投射与融合 → 稠密化/表面重建 → 体素化 → 视锥裁剪 → 深度图生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①首个真实空中 SSC 数据集，覆盖 9 场景、20,000+ 样本、22 类；②基于相机的 LiDAR‑free 数据生成框架，显著降低 3D 标注成本；③利用 2D–3D 对应关系实现高效标注迁移；④实例级稠密化与表面重建的分组处理；⑤提供深度图并训练 Depth‑Anything‑V2。与以往 LiDAR 基础的数据集不同，OccuFly 解决了空中视角稀疏性和硬件限制问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OccuFly 提供了首个真实空中语义场景完成基准数据集，并通过相机驱动的稠密重建与 2D 标注迁移，构建了可扩展、低成本的 SSC 数据生成管线。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.&lt;/p&gt;</description></item><item><guid>2512.20988v1</guid><title>PUFM++: Point Cloud Upsampling via Enhanced Flow Matching</title><link>http://arxiv.org/abs/2512.20988v1</link><author>Zhi-Song Liu, Chenhang He, Roland Maier, Andreas Rupp</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; PUFM++ 是一种改进的流匹配框架，旨在从稀疏、噪声和部分观测中重建高质量、密集且准确的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来，生成模型在点云上采样方面表现出强大潜力，但现有方法在几何保真度、鲁棒性和与后续表面任务的一致性方面仍有不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提升流匹配的几何保真度、对不完美输入的鲁棒性，并确保与下游表面任务的一致性，从而实现更高质量的点云重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用两阶段流匹配策略：先学习稀疏输入到密集目标的直线路径流，再用噪声扰动样本进行细化；引入数据驱动的自适应时间调度器提高采样效率；在采样过程中施加在流形上的约束，保证生成点与底层表面对齐；使用递归接口网络加强层次特征交互，提升重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成基准和真实扫描数据上，PUFM++ 在点云上采样任务中实现了新的最先进水平，提供了更好的视觉保真度和定量精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PUFM++ 通过改进流匹配技术，显著提升了点云重建的质量，成为点云上采样领域的领先方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近年来，生成模型在高质量点云上采样方面展现出强大潜力。在本研究中，我们提出了 PUFM++，一种改进的流匹配框架，用于从稀疏、噪声和部分观测中重建密集且准确的点云。PUFM++ 在流匹配方面在三个关键方向上取得了提升：几何保真度、对不完美输入的鲁棒性以及与下游基于表面的任务的一致性。我们引入了两阶段流匹配策略，首先学习从稀疏输入到密集目标的直接直线路径流，然后使用噪声扰动样本进行细化，以更好地逼近终端边缘分布。为加速和稳定推理，我们提出了基于数据的自适应时间调度器，基于插值行为提高采样效率。我们进一步在采样过程中施加流形约束，确保生成的点保持与底层表面对齐。最后，我们加入了递归接口网络（RIN），以加强层次特征交互并提升重建质量。广泛实验表明，PUFM++ 在点云上采样任务中设定了新的最先进水平，在视觉保真度和定量精度方面均优于现有方法。代码和预训练模型已公开可用，地址为 https://github.com/Holmes-Alan/Enhanced_PUFM。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在将稀疏、噪声或不完整的点云转换为高密度、精确的点云。高质量的点云是机器人导航、工业检测、自动驾驶和三维重建等领域的基础，传统方法往往需要昂贵的硬件或手工设计，难以普适。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在先前的 PUFM 和 PU-DM 等流匹配与扩散模型工作基础上，提出了两阶段流匹配、时间自适应采样、流形约束和递归接口网络等改进。通过对现有方法的局限性（如步数多、缺乏全局一致性、对噪声敏感）进行分析，设计了更高效、更鲁棒的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先在补丁空间学习稀疏到稠密的直线流匹配，再用噪声扰动的样本进行细化；在推理时采用自适应时间步长加速采样，并在采样过程中强制点保持在表面流形上。实现流程包括：提取稀疏与稠密补丁对，训练两阶段流匹配网络，推理时使用自适应 ODE 步长生成稠密点，最后通过 kNN 后处理和 RIN 进一步提升质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 两阶段流匹配训练，提升精度与鲁棒性；2) 基于训练轨迹的自适应时间采样，显著减少采样步数；3) 流形感知先验和 kNN 后处理，保证点云与表面一致；4) 递归接口网络（RIN）提供长期全局上下文。与 PUFM 相比，PUFM++ 在效率、表面一致性和细节保留方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PUFM++ 通过两阶段自适应流匹配、流形约束和递归特征交互，显著提升点云上采样的速度、精度和鲁棒性，成为目前最先进的稀疏到稠密点云生成方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.&lt;/p&gt;</description></item><item><guid>2512.21641v1</guid><title>TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</title><link>http://arxiv.org/abs/2512.21641v1</link><author>Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在动态三维驾驶场景中通过多帧观测实现基于语言的目标定位，并提出了TrackTeller框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在交互式自动驾驶系统中，理解自然语言对对象的引用至关重要，尤其是许多表达依赖于最近运动或短期交互，单靠静态外观或几何无法解决。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨利用多帧观测进行时间语言驱动的三维定位，目标是在当前帧中识别被引用的对象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出TrackTeller，一个融合激光雷达与图像、语言条件解码和时间推理的统一架构，构建与文本语义对齐的共享场景表示，生成语言感知的三维候选框，并通过运动历史和短期动态细化定位决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在NuPrompt基准上实验表明，TrackTeller 在平均多目标跟踪精度上相对提升70%，误报频率降低3.15-3.4倍，显著优于强基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; TrackTeller 通过整合多模态信息和时间推理，显著提升了基于语言的三维跟踪性能，验证了时间多模态定位方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解动态三维驾驶场景中自然语言对对象的引用对于交互式自动驾驶系统至关重要。实际上，许多指代表达通过最近的运动或短期交互来描述目标，这些信息无法仅凭静态外观或几何来解决。我们研究基于时间语言的三维定位，目标是利用多帧观测在当前帧中识别被引用的对象。我们提出了TrackTeller，一个整合激光雷达-图像融合、语言条件解码和时间推理的统一多模态定位框架。TrackTeller 构建了与文本语义对齐的共享 UniScene 表示，生成语言感知的三维候选框，并利用运动历史和短期动态细化定位决策。对 NuPrompt 基准的实验表明，TrackTeller 在平均多目标跟踪精度上相对提升 70%，误报频率降低 3.15-3.4 倍，持续优于强基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在动态驾驶场景中，通过自然语言表达的对象引用需要考虑最近运动或短期交互的情况，从单帧静态信息无法满足的挑战。该问题对自动驾驶和交互式机器人至关重要，因为它们需要实时理解并定位被语言描述的目标，以实现安全、高效的决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出多模态融合和时间推理是关键挑战，并参考了 LanguageRefer、NS3D、GroundFlow 等先前研究。基于这些工作，他们设计了统一的 UniScene 表示、语言对齐解码器以及记忆式时间推理模块，以实现跨帧、跨模态的语义对齐与动态行为理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 LiDAR 点云与多视角图像融合成统一的 UniScene 令牌，然后通过语言引导的注意力将文本语义注入场景表示，生成候选 3D 框；随后利用历史记忆和运动信息对候选框进行时间推理，最终选取与语言描述最匹配的对象。整体流程包括：多模态融合 → 语言对齐 → 3D 解码 → 时间记忆更新 → 运动推理 → 最终定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）UniScene 统一表示融合 LiDAR 与图像语义；2）语言对齐解码器实现文本条件的 3D 框生成；3）记忆式时间推理模块结合历史上下文和短期运动；4）在 NuPrompt 基准上实现显著性能提升。与以往仅关注单帧或静态场景的工作不同，TrackTeller 能够处理基于运动和行为的语言引用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrackTeller 提出了一个统一的多模态与时间推理框架，能够在动态驾驶场景中准确定位基于运动和行为的自然语言对象引用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.&lt;/p&gt;</description></item><item><guid>2512.21714v1</guid><title>AstraNav-World: World Model for Foresight Control and Consistency</title><link>http://arxiv.org/abs/2512.21714v1</link><author>Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; AstraNav-World 是一种端到端的世界模型，能够在统一的概率框架内同时预测未来视觉状态和行动序列，并通过同步的滚动方式将预测的场景与计划的动作实时更新，从而提升具身导航的轨迹精度和成功率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在开放且动态的环境中，具身导航需要准确预测世界的演变以及行动随时间展开的方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够联合推理未来视觉状态和行动序列的模型，以减少传统“先想象再规划”流程中的累计误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 整合扩散式视频生成器与视觉-语言策略，采用同步滚动；训练时同时优化两项互补目标：生成动作条件的多步视觉预测和基于预测视觉的轨迹规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在多种具身导航基准上提升了轨迹准确性和成功率；消融实验显示视觉-动作紧耦合和统一训练是必要的；在真实世界测试中表现出卓越的零样本适应能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过在单一生成模型中统一视觉预见与控制，AstraNav-World 使具身代理在开放式真实环境中更可靠、可解释且通用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在开放且动态的环境中，具身导航需要准确预测世界的演变以及行动随时间展开的方式。我们提出 AstraNav-World，一个端到端的世界模型，在统一的概率框架内同时推理未来视觉状态和行动序列。我们的框架将扩散式视频生成器与视觉-语言策略相结合，支持同步滚动，使预测的场景和计划的动作能够同时更新。训练时优化两项互补目标：生成动作条件的多步视觉预测以及基于这些预测视觉的轨迹规划。这种双向约束使视觉预测可执行，并使决策保持在物理一致、任务相关的未来之中，从而减轻了传统“先想象再规划”流程中常见的累计误差。实验在多种具身导航基准上显示出更高的轨迹准确性和成功率。消融实验确认了紧密的视觉-动作耦合和统一训练的必要性，移除任一分支都会降低预测质量和策略可靠性。在真实世界测试中，AstraNav-World 展示了卓越的零样本能力，能够在未见过的场景中适应，而无需任何真实世界的微调。这些结果表明 AstraNav-World 捕捉了可迁移的空间理解和与规划相关的导航动力学，而非仅仅对仿真特定的数据分布过拟合。总体而言，通过在单一生成模型中统一预见视觉和控制，我们更接近于在开放式真实环境中可靠、可解释且通用的具身代理。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在开放、动态环境中实现可靠的机器人导航所需的未来世界预测与动作规划的耦合问题。传统方法缺乏对物理规律和时间动态的建模，导致预测误差累积，影响全局规划的有效性。准确的未来感知与可执行规划对于让机器人在真实世界中自主行动至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为“想象未来”和“规划未来”应在同一模型中同步进行，于是将视觉语言模型（VLM）作为高层规划器，结合扩散式视频生成器和动作策略头，形成统一的概率框架。设计过程中借鉴了世界模型、VLM‑VLA、CoT‑VLA 等现有工作，并在此基础上引入双向约束和同步滚动，以减少误差传播。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将未来视觉状态预测与动作序列生成紧密耦合，在同一概率模型中通过双向约束实现一致性。实现流程为：VLM 处理指令和历史视觉，输出嵌入；嵌入通过交叉注意力注入扩散视频生成器和策略头；视频生成器预测多步未来帧；策略头根据相同嵌入和预测帧生成动作；两者在训练时同步滚动并共同优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）统一生成框架，将视觉预测与动作规划放在同一模型中；2）双向约束与同步滚动，强化物理与因果一致性；3）使用扩散模型同时生成未来视频和动作序列；4）实现零样本迁移到真实机器人。与以往“想象‑然后‑规划”分离的管线不同，本文通过联合训练和紧耦合显著降低误差累积，提高导航成功率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AstraNav‑World 将视觉语言推理、扩散式视频预测和动作生成统一到一个概率模型中，通过双向约束和同步滚动实现物理一致的未来感知与可执行规划，显著提升导航性能并实现零样本真实世界迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled &amp;quot;envision-then-plan&amp;quot; pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.&lt;/p&gt;</description></item><item><guid>2512.21831v1</guid><title>End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration</title><link>http://arxiv.org/abs/2512.21831v1</link><author>Zhenwei Yang, Yibo Ai, Weidong Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为XET-V2X的端到端多模态融合跟踪框架，旨在通过统一多视角多模态感知来提升自动驾驶中的三维时空理解，尤其在遮挡、视角受限和V2X通信延迟的情况下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶场景中，多视角协同感知与多模态融合对于可靠的三维时空理解至关重要，尤其在存在遮挡、有限视角和V2X通信延迟时更显重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在V2X协作中统一多视角多模态感知的共享时空表示框架，以提升检测与跟踪性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; XET-V2X采用双层空间交叉注意力模块，基于多尺度可变形注意力实现不同视角和模态的高效对齐；先聚合多视角图像特征以增强语义一致性，再通过更新后的空间查询引导点云融合，实现跨模态交互并降低计算开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实V2X-Seq-SPD数据集以及模拟V2X-Sim-V2V和V2X-Sim-V2I基准上，XET-V2X在不同通信延迟条件下均表现出检测和跟踪性能的持续提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; XET-V2X在复杂交通场景中实现了稳健且时序稳定的感知效果，验证了其在V2X协作中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多视角协同感知和多模态融合对于自动驾驶中的可靠三维时空理解至关重要，尤其在遮挡、有限视角和V2X通信延迟的情况下。本研究提出了XET-V2X，一种多模态融合端到端跟踪框架，统一多视角多模态感知于共享时空表示。为高效对齐异构视角和模态，XET-V2X引入基于多尺度可变形注意力的双层空间交叉注意力模块。首先聚合多视角图像特征以增强语义一致性，然后在更新后的空间查询指导下进行点云融合，实现有效的跨模态交互并降低计算开销。实验在真实V2X-Seq-SPD数据集以及模拟V2X-Sim-V2V和V2X-Sim-V2I基准上显示，在不同通信延迟下检测和跟踪性能均持续提升。定量结果和定性可视化均表明XET-V2X在复杂交通场景中实现了稳健且时序稳定的感知。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在自动驾驶中因遮挡、视角受限和 V2X 通信延迟导致的 3D 时空感知不完整和不稳定的问题。该问题对车辆安全至关重要，因为缺失或错误的感知会导致碰撞风险增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将多视角协同、跨模态融合和时空建模整合到一个端到端可学习的框架中，借鉴了 V2VNet、Where2Comm、Transformer‑based BEV、RecurrentBEV 等现有技术，并在此基础上提出了双层空间交叉注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双层空间交叉注意力实现视角和模态的高效对齐，先聚合图像特征提升语义一致性，再用更新后的空间查询引导点云融合，随后在共享 BEV 表示上使用 Transformer 进行时空编码，最终输出检测与跟踪结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 端到端统一的多视角、多模态、时空感知框架；② 双层跨模态跨视角注意力模块；③ 共享 BEV 表示实现几何对齐与信息补全；④ 在 V2X-Seq-SPD 与 V2X-Sim 基准上实现显著性能提升。与以往模块化、单模态或仅空间/时间融合的方法不同，XET‑V2X 在一次优化中同时处理所有维度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XET‑V2X 提供了一个统一的端到端框架，将多视角协同、跨模态融合与时空建模结合，实现了在 V2X 通信延迟下的 3D 感知与跟踪的最先进性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.&lt;/p&gt;</description></item><item><guid>2512.22207v1</guid><title>GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks</title><link>http://arxiv.org/abs/2512.22207v1</link><author>Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了GamiBench，一个基于折纸灵感的折叠任务，用来评估多模态大语言模型在空间推理和二维到三维规划方面的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大语言模型在感知和指令跟随方面表现出色，但在跨视角和时间的空间推理上仍存在困难。现有基准多聚焦于静态图像或最终输出，未能体现空间推理的序列性和视角依赖性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个全面评估空间推理过程的基准，衡量模型在多视角一致性、物理可行性和中间折叠步骤解释等方面的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GamiBench包含186个可行和186个不可行的二维折痕图案，配以对应的三维折叠形状，并从六个不同视角生成。设计了三类视觉问答任务：预测三维折叠配置、区分有效视角、检测不可行图案，并引入视角一致性（VC）和不可行折叠选择率（IFSR）两项诊断指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，即使是领先模型如GPT-5和Gemini-2.5-Pro在单步空间理解上也表现不佳；GamiBench能够完整评估模型的推理过程并揭示其在不同复杂度折叠任务中的弱点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GamiBench为多模态大语言模型的几何理解和空间推理提供了标准化的评估框架，凸显了当前模型的不足并为后续改进提供了参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大型语言模型（MLLMs）在感知和指令跟随方面表现出色，但它们在空间推理方面仍然存在困难：在多视角和时间上心理跟踪和操作物体的能力。空间推理是人类智能的关键组成部分，但大多数现有基准侧重于静态图像或最终输出，未能考虑到这一技能的序列性和视角依赖性。为弥补这一差距，我们引入了GamiBench，这是一个通过折纸灵感的折叠任务来评估MLLMs空间推理和二维到三维规划的基准。GamiBench包括186个常规和186个不可能的二维折痕图案及其对应的三维折叠形状，这些形状来自六个不同视角，跨越三种视觉问答（VQA）任务：预测三维折叠配置、区分有效视角和检测不可能的图案。与仅评估最终预测的先前基准不同，GamiBench全面评估整个推理过程——测量跨视角一致性、通过不可能折叠检测评估物理可行性以及对中间折叠步骤的解释。它还引入了新的诊断指标——视角一致性（VC）和不可能折叠选择率（IFSR），以衡量模型在不同复杂度折叠上的处理能力。我们的实验表明，即使是领先模型如GPT-5和Gemini-2.5-Pro在单步空间理解上也表现不佳。这些贡献为评估MLLMs的几何理解和空间推理建立了标准化框架。数据集和代码：https://github.com/stvngo/GamiBench.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决多模态大型语言模型在空间推理方面的不足，尤其是从二维折纸图到三维结构的映射以及多视角一致性。空间推理是人类认知和许多实际任务（如家具组装、机器人操作）的核心能力，现有模型在这方面表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出现有基准多聚焦于静态图像或最终结果，缺乏多视角和序列推理。随后借鉴了原有空间推理基准（如GSR‑Bench、LEGO‑Puzzles、3DSRBench）和折纸工具（Oriedita、Origami Simulator、Flat‑Folder），构建了包含可折叠与不可折叠图案的折纸数据集，并引入多视角验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过折纸任务将二维折痕图映射到三维结构，并在六个视角下评估模型的空间一致性和物理可行性。实现流程包括：①收集186个可折叠和186个不可折叠的折痕图；②使用折纸仿真器生成对应的三维模型；③设计三类VQA多选题（单步空间理解、多步推理、不可折叠检测）；④让模型根据提示和图像输出答案；⑤用视角一致性（VC）和不可折叠选择率（IFSR）等指标评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①多视角、序列化的空间推理基准；②引入视角一致性和不可折叠选择率两项诊断指标；③将文本指令与视觉状态结合的折纸任务；④对折叠复杂度进行分级控制。与以往基准相比，GamiBench不再仅关注最终结果，而是全面评估推理过程、跨视角一致性和物理可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GamiBench提供了一个多视角、序列化的折纸基准，揭示并量化了多模态大型语言模型在二维到三维空间推理、视角一致性和物理可行性方面的显著局限。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.&lt;/p&gt;</description></item><item><guid>2512.22351v1</guid><title>VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement</title><link>http://arxiv.org/abs/2512.22351v1</link><author>Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文通过解决三大关键挑战，弥补了多模态大型语言模型在复杂三维场景操作中的不足。首先，针对模型视觉定位弱的问题，提出了基于MCP的API，将交互从脆弱的原始代码改为更稳健的函数级更新；其次，利用一套专门的视觉工具增强模型对三维场景的理解，收集空间信息并验证操作结果，形成感知反馈循环；再次，构建协作多智能体框架，分别负责规划、执行和验证，提升对多步指令的鲁棒性并纠正中间错误。实验在25个复杂物体排列任务上验证，显著优于现有基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大型语言模型在二维视觉-语言任务上取得显著进展，但在复杂三维场景操作方面仍未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补多模态大型语言模型在三维物体排列任务中的应用空白，提升其在三维场景中的操作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 引入MCP‑based API，实现函数级更新以增强视觉定位；2. 结合专用视觉工具进行场景状态分析、空间信息收集和动作验证，形成感知反馈循环；3. 设计协作多智能体框架，分工规划、执行与验证，处理迭代更新中的错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在25个复杂物体排列任务中，所提方法显著优于现有基线，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过函数级API、视觉反馈工具和多智能体协作，本文成功提升了多模态大型语言模型在三维场景操作中的鲁棒性和精确度，为未来相关研究奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管多模态大型语言模型在二维视觉-语言任务上取得了显著进展，但它们在复杂三维场景操作中的应用仍未得到充分探索。本文通过解决三大关键挑战，弥补了这一空白。首先，为了解决多模态大型语言模型在视觉定位方面的弱点，我们引入了基于MCP的API，将交互从脆弱的原始代码改为更稳健的函数级更新。其次，我们通过一套专门的视觉工具增强模型对三维场景的理解，分析场景状态、收集空间信息并验证动作结果，形成感知反馈循环，弥合了基于语言的更新与精确三维感知操作之间的差距。再次，为了管理迭代且易出错的更新，我们提出了一个协作多智能体框架，设定规划、执行和验证的专门角色。该分解使系统能够稳健地处理多步指令并从中间错误中恢复。我们在25个复杂物体排列任务上验证了该方法的有效性，显著优于现有基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 解决复杂3D物体排列任务，让AI能根据文字指令和图像逐步移动、旋转或放置物体，保持物理可行性。重要因为人类日常会重新布置环境，机器人和AI需要具备空间推理和多步规划能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有MLLM只能做一次性编辑，缺乏多步推理。借鉴了MLLM工具调用、Model Context Protocol、FirePlace、ScanEdit等工作，设计了Planner-Executor-Evaluator三代理框架，并引入MCP工具和约束求解器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是让MLLM在计划-执行-评估循环中逐步完成任务。流程：Planner根据指令和历史渲染生成动作和目标坐标；Executor用视觉工具选取对象、构造几何约束并求解碰撞自由姿态；Evaluator检查物理和语义质量；若不合格则回溯，循环直至完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①工具增强的MLLM与约束求解器；②协作多代理分工；③自适应回溯搜索；④新建25个多步排列基准。与以往单步、无工具、无回溯的3D排列方法不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VULCAN通过多代理、工具调用和自适应回溯，使MLLM能够从文字和图像中迭代完成物理可行的3D物体排列，显著优于以往单步方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM&amp;#x27;s 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io&lt;/p&gt;</description></item><item><guid>2512.22439v2</guid><title>SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems</title><link>http://arxiv.org/abs/2512.22439v2</link><author>Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LiDAR感知受限于固定的垂直束分辨率和环境遮挡导致的束掉落。本文提出SuperiorGAT，一种基于图注意力的框架，利用束感知图和门控残差融合与前馈细化，能够在不加深网络的情况下准确重建稀疏点云中的缺失高程信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶系统中，LiDAR感知受限于固定的垂直束分辨率，并因环境遮挡导致束掉落，进一步削弱了点云的稠密度和精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不增加网络深度的前提下，重建稀疏LiDAR点云中的缺失高程信息，以提升感知质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将LiDAR扫描建模为束感知图，采用图注意力网络；通过门控残差融合和前馈细化实现高效重建；在KITTI数据集上通过每四束移除一次的方式模拟结构化束掉落，进行实验评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SuperiorGAT在多种KITTI环境（人、道路、校园、城市）中，重建误差更低，几何一致性更好，优于基于PointNet的模型和更深的GAT基线；X-Z投影显示其能保持结构完整，垂直失真最小。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过架构优化，可在不增加硬件的情况下，以计算效率高的方式提升LiDAR分辨率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model&amp;#x27;s ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; The paper tackles the problem of missing elevation data in sparse LiDAR point clouds caused by beam dropout. This issue is critical because LiDAR is a core sensor for autonomous vehicles, and missing height information can degrade object detection, mapping, and path planning. Low‑resolution LiDAR sensors are common in cost‑effective vehicles, so an efficient reconstruction method is needed for real‑time operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; The authors built on their earlier conference work that used multi‑layer GATs for z‑reconstruction. They realized that increasing depth made the model heavy and unstable, so they redesigned the architecture to be lightweight while still powerful. They incorporated ideas from GCNs, GATs, and residual connections, and used realistic beam‑dropout simulation to train and test the model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; The core idea is to treat a LiDAR scan as a graph where each point is a node and edges reflect the sensor’s beam pattern. A single‑layer graph attention network with multi‑head attention learns how to weight neighboring points, and a gated residual fusion layer blends the raw and refined features. A feed‑forward block further refines the output, producing accurate z‑values for the missing points.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; Key innovations include a realistic beam‑dropout simulation framework, a beam‑aware graph construction that uses beam indices, a lightweight single‑layer GAT with gated residual fusion and feed‑forward refinement, and multi‑head attention. Compared to previous work, the method achieves higher accuracy with far fewer parameters, avoiding the need for deep stacks of attention layers and dense CNNs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SuperiorGAT delivers a real‑time, beam‑aware graph attention network that accurately reconstructs missing LiDAR elevation data, outperforming deeper GATs and PointNet while keeping the model lightweight.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model&amp;#x27;s ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.&lt;/p&gt;</description></item><item><guid>2512.22463v1</guid><title>MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression</title><link>http://arxiv.org/abs/2512.22463v1</link><author>Kai-Hsiang Hsieh, Monyneath Yim, Wen-Hsiao Peng, Jui-Chiu Chiang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为MEGA-PCC的端到端学习框架，用于点云几何与属性的联合压缩，利用共享编码器、双解码器和基于Mamba的熵模型，实现了更高效的压缩性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云几何与属性的联合压缩对高效3D数据表示至关重要，但现有方法往往需要后处理重色和手工调节比特率，阻碍了端到端优化并增加了系统复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服上述局限，提出MEGA-PCC，实现无重色、数据驱动的比特率分配，并简化压缩流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MEGA-PCC包含主压缩模型：共享编码器将几何与属性编码为统一潜在表示，随后两个解码器按顺序重建几何和属性；以及Mamba基熵模型，利用空间和通道相关性提升概率估计。两模型均基于Mamba架构，能捕捉长程依赖与丰富上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，MEGA-PCC在率失真性能和运行时效率上均优于传统和基于学习的基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MEGA-PCC为AI驱动的点云压缩提供了强有力的解决方案，具备更高效、简化的端到端流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云几何与属性的联合压缩对于高效的3D数据表示至关重要。现有方法往往依赖后期重色处理和手动调节几何与属性比特流之间的比特率，这阻碍了端到端优化并增加了系统复杂度。为克服这些限制，我们提出了MEGA-PCC，一种完全端到端、基于学习的框架，包含两个专门的联合压缩模型。主压缩模型使用共享编码器将几何和属性信息编码为统一的潜在表示，然后通过双解码器依次重建几何和属性。补充地，基于Mamba的熵模型（MEM）通过捕捉空间和通道相关性来提升熵编码的概率估计。两种模型均基于Mamba架构，以有效建模长程依赖和丰富的上下文特征。通过消除重色和启发式比特率调节的需求，MEGA-PCC在训练期间实现了数据驱动的比特率分配，并简化了整体流程。大量实验表明，MEGA-PCC在率失真性能和运行时效率方面优于传统和基于学习的基线，为AI驱动的点云压缩提供了强大的解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云几何和属性的联合压缩问题。传统方法需要在几何和属性之间进行 recoloring，并手动分配比特率，导致压缩效率低且难以端到端优化。高效的点云压缩对沉浸式技术、自动驾驶和远程感知等领域至关重要，因为它们需要在有限带宽下传输大量三维数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了 recoloring 步骤和手工比特率分配的瓶颈，然后提出使用共享编码器和双解码器的统一架构，以消除中间步骤。该设计借鉴了 Mamba 结构的状态空间模型（SSM）以及之前的 Mamba-PCGC、SparsePCGC 等学习式点云压缩工作，并将其扩展到同时处理几何和属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个共享的稀疏卷积+Mamba 编码器将几何和属性编码为统一潜在表示，然后先解码几何，再利用解码后的几何引导属性解码。整个流程包括：体素化点云 → 共享编码器（多方向 SSM）→ 潜在表示 → 先解码几何 → 以几何为条件解码属性 → 采用 Mamba‑based Entropy Model 进行熵编码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 单编码器双解码器的统一架构，消除 recoloring；2) 在编码器、解码器和熵模型中使用 Mamba 以捕获长程依赖；3) 端到端训练实现自动比特率分配，避免手工匹配。与以往方法相比，MEGA‑PCC 不需要手工调节比特率，也不需要耗时的模型匹配，且在压缩质量和运行速度上均优于传统和学习式基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MEGA‑PCC 提供了一种基于 Mamba 的端到端点云几何与属性联合压缩框架，自动学习比特率分配，消除 recoloring 步骤，并在压缩效率和速度上显著优于现有方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Joint compression of point cloud geometry and attributes is essential for efficient 3D data representation. Existing methods often rely on post-hoc recoloring procedures and manually tuned bitrate allocation between geometry and attribute bitstreams in inference, which hinders end-to-end optimization and increases system complexity. To overcome these limitations, we propose MEGA-PCC, a fully end-to-end, learning-based framework featuring two specialized models for joint compression. The main compression model employs a shared encoder that encodes both geometry and attribute information into a unified latent representation, followed by dual decoders that sequentially reconstruct geometry and then attributes. Complementing this, the Mamba-based Entropy Model (MEM) enhances entropy coding by capturing spatial and channel-wise correlations to improve probability estimation. Both models are built on the Mamba architecture to effectively model long-range dependencies and rich contextual features. By eliminating the need for recoloring and heuristic bitrate tuning, MEGA-PCC enables data-driven bitrate allocation during training and simplifies the overall pipeline. Extensive experiments demonstrate that MEGA-PCC achieves superior rate-distortion performance and runtime efficiency compared to both traditional and learning-based baselines, offering a powerful solution for AI-driven point cloud compression.&lt;/p&gt;</description></item><item><guid>2512.22489v2</guid><title>Tracking by Predicting 3-D Gaussians Over Time</title><link>http://arxiv.org/abs/2512.22489v2</link><author>Tanish Baranwal, Himanshu Gaurav Singh, Jathushan Rajasegaran, Jitendra Malik</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 提出了Video-GMAE，一种自监督方法，将视频序列编码为随时间移动的高斯斑点集合，利用高斯表示的先验偏置捕捉动态三维场景的投影特征。通过预训练网络，模型自然学习到跟踪能力，映射高斯轨迹到图像平面即可实现零样本跟踪，性能与最先进方法相当。对Kinetics和Kubric数据集进行小规模微调后，模型分别提升了34.6%和13.1%，超过了现有自监督视频方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在视频表示学习中，自监督方法需要有效的先验偏置来捕捉空间与时间的关联。传统方法往往缺乏对三维动态场景的自然建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将视频表示为移动的高斯斑点，提供一种合理的先验偏置，使模型能够学习到动态三维结构并实现跟踪与表示学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计Video-GMAE架构，使用自监督预训练将视频序列编码为一组随时间移动的高斯斑点；随后将高斯轨迹投影到图像平面，用于零样本跟踪；在Kinetics和Kubric上进行小规模微调以提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 预训练后模型自然出现跟踪能力；零样本跟踪性能与最先进方法相当；微调后在Kinetics和Kubric上分别提升34.6%和13.1%，优于现有自监督视频方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Video-GMAE通过高斯斑点表示为视频提供了有效的先验偏置，既能实现自然跟踪，又能显著提升自监督视频表示学习的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了视频高斯掩码自编码器（Video-GMAE），这是一种自监督的表示学习方法，它将一系列图像编码为随时间移动的一组高斯斑点。将视频表示为一组高斯斑点强加了一个合理的归纳偏置：二维视频往往是动态三维场景的合理投影。我们发现，当使用这种架构对网络进行预训练时，跟踪能力会自然出现。将学习到的高斯轨迹映射到图像平面即可实现零样本跟踪，其性能与最先进方法相当。通过小规模微调，我们的模型在Kinetics数据集上提升了34.6%，在Kubric数据集上提升了13.1%，超过了现有的自监督视频方法。项目页面和代码公开可用，网址为 https://videogmae.org/ 和 https://github.com/tekotan/video-gmae。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在通过自监督学习从无标签视频中学习像素级对应关系，从而实现点跟踪。点跟踪是理解场景结构、运动和三维重建的基础，对计算机视觉和机器人等领域具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现传统的自监督视频预训练（如VideoMAE、MAE-ST）在点跟踪上效果不佳，认为缺乏对时间一致性的强约束。于是借鉴了可微分渲染中的三维高斯分布（Gaussian Splatting）和自监督掩码自编码器的思想，设计了在视频中预测高斯原语及其时间增量的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用可微分的三维高斯原语来表示视频帧，并通过预测第一帧的原语以及后续帧的增量来强制时间对应。实现流程包括：用ViT编码器对掩码视频帧进行特征提取；解码器生成第一帧的高斯原语和后续帧的增量；将增量递归累加得到每帧原语；用可微分高斯渲染重建视频并计算重建损失；在此基础上可直接从高斯轨迹生成流场，实现零样本点跟踪。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 将三维高斯分布与自监督掩码自编码器结合，首次在视频预训练中显式强制时间对应；2) 通过预测增量而非完整原语，保持原语身份并实现长时序对应；3) 提供零样本点跟踪算法，并在Kinetics、Kubric等数据集上显著提升性能。与以往仅关注图像重建或对比学习的自监督方法不同，本文通过可微分渲染直接学习三维运动表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Video-GMAE通过自监督学习三维高斯原语及其时间增量，构建了可微分的时间对应表示，实现零样本点跟踪并在多项视频任务中取得领先表现。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.&lt;/p&gt;</description></item><item><guid>2512.22503v1</guid><title>SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration</title><link>http://arxiv.org/abs/2512.22503v1</link><author>Xin Chen, Kang Luo, Yangyi Xiao, Hesheng Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了SCAFusion，一种针对月球探测任务的多模态3D目标检测模型，显著提升了对小型不规则目标的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在月球表面探测中，精确检测小型不规则物体（如陨石碎片和岩石）对自主导航至关重要，但现有地面自动驾驶的多模态3D感知方法在外星环境中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种专门针对月球机器人任务的多模态3D检测模型，以克服特征对齐不足、模态协同有限和小目标检测弱的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在BEVFusion框架基础上，SCAFusion集成了认知适配器、对比对齐模块、相机辅助训练分支以及专为小目标设计的分区感知坐标注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes验证集上，SCAFusion实现了69.7%的mAP和72.1%的NDS，分别比基线提升5.0%和2.7%；在Isaac Sim构建的模拟月球环境中，mAP达到90.93%，比基线提升11.5%，在检测小型陨石类障碍物方面表现尤为突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SCAFusion在保持参数和计算量几乎不变的前提下，显著提升了月球环境中小型不规则目标的检测效果，为月球机器人自主导航提供了可靠的感知支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可靠且精确地检测小型不规则物体（如陨石碎片和岩石）对于月球表面探测中的自主导航和操作至关重要。现有为陆地自动驾驶设计的多模态3D感知方法在外星环境中往往表现不佳，原因包括特征对齐不佳、模态协同有限以及小目标检测弱。本文提出了SCAFusion，一种专为月球机器人任务定制的多模态3D目标检测模型。基于BEVFusion框架，SCAFusion集成了认知适配器以高效调优相机骨干网络、对比对齐模块以增强相机与激光雷达特征的一致性、相机辅助训练分支以强化视觉表征，并最重要的是，设计了分区感知坐标注意力机制，专门提升小型不规则目标的检测性能。通过几乎不增加参数和计算量，模型在nuScenes验证集上实现了69.7%的mAP和72.1%的NDS，分别比基线提升5.0%和2.7%。在Isaac Sim构建的模拟月球环境中，SCAFusion实现了90.93%的mAP，超过基线11.5%，在检测小型陨石类障碍物方面取得显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提高月球表面探测任务中对小型、形状不规则物体（如陨石碎片、岩石）的三维检测精度。准确识别这些障碍物对机器人路径规划、避障和任务安全至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了地面自动驾驶中 BEVFusion 的局限性，发现其在小目标检测、模态对齐和视觉信息利用方面不足。随后在 BEVFusion 基础上引入 Cognitive Adapter、Contrastive Alignment Module、Camera Auxiliary Training Branch 和 Section‑aware Coordinate Attention，借鉴并改进了现有多模态融合与注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在 BEV 空间中实现高效的多模态特征对齐与增强，专门提升小目标的检测能力。实现流程为：提取相机与 LiDAR 特征 → 在相机骨干中使用 Cognitive Adapter 进行参数高效调优 → 通过 Contrastive Alignment Module 对 RGB 与深度特征进行对齐 → 将相机 BEV 与 LiDAR BEV 融合 → 通过 Section‑aware Coordinate Attention 强化小目标特征 → 在训练阶段使用 Camera Auxiliary Branch 进一步提升视觉表示 → 最终通过检测头输出 3D 检测框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) Section‑aware Coordinate Attention 机制专为小目标设计；2) Cognitive Adapter 使相机骨干可在不大幅增加参数的情况下高效适配；3) Contrastive Alignment Module 在 Lift‑Splat‑Shoot 过程中对 RGB 与深度特征进行对齐；4) Camera Auxiliary Training Branch 在训练时充分利用视觉信息。与 BEVFusion 等前置工作相比，SCAFusion 在模态对齐、视觉利用和小目标检测上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCAFusion 通过引入高效相机适配、对齐增强和面向小目标的注意力机制，在保持参数与计算成本不变的前提下，显著提升了月球表面多模态 3D 检测的精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.&lt;/p&gt;</description></item><item><guid>2512.22706v1</guid><title>SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis</title><link>http://arxiv.org/abs/2512.22706v1</link><author>Paul Dobre, Jackson Cooper, Xin Wang, Hongzhou Yang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个统一的仿真框架SCPainter，结合3D高斯散点汽车资产表示和3D场景点云，并利用扩散模型生成高质量图像，从而实现真实的3D资产插入和新视角合成，评估显示能生成多样化、真实的驾驶数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自动驾驶训练需要多样化数据，现有3D资产重建缺乏光照阴影真实感，NVS方法通常与资产插入分离，难以实现场景交互与多样化场景生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个统一框架，融合3D资产插入与新视角合成，以生成更丰富、真实的驾驶训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用3D高斯散点车资产表示和3D场景点云，投影到新视角后作为条件输入给扩散模型，生成高质量图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Waymo Open数据集上，SCPainter能够实现真实的3D资产插入和新视角合成，生成多样化且真实的驾驶数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 统一的3D资产插入与新视角合成框架可有效提升训练数据多样性和真实性，进而增强自动驾驶模型的鲁棒性与安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 资产插入和新视角合成（NVS）是自动驾驶仿真的关键组成部分，可增强训练数据的多样性。通过更丰富、覆盖更广范围情景（包括长尾驾驶场景）的训练数据，自动驾驶模型可以变得更稳健、更安全。这激发了一个统一的仿真框架，能够同时处理插入的 3D 资产的真实集成和 NVS。最近的 3D 资产重建方法能够从视频中重建动态演员，并支持将其重新插入到仿真驾驶场景中。虽然整体结构和外观可以准确，但在通过光照或阴影捕捉 3D 资产的真实感方面仍然存在困难，尤其是在将其插入场景时。与此同时，最近的 NVS 方法在合成原始记录轨迹之外的视角方面显示出有前景的结果。然而，现有方法大多将资产插入和 NVS 能力视为孤立的。为了与场景其余部分交互并实现更丰富的新场景创建，真实的 3D 资产插入应与 NVS 结合。为此，我们提出了 SCPainter（Street Car Painter），一个统一框架，将 3D 高斯散点（GS）汽车资产表示和 3D 场景点云与基于扩散的生成相结合，联合实现真实的 3D 资产插入和 NVS。3D GS 资产和 3D 场景点云一起投影到新视角，并将这些投影用于调节扩散模型以生成高质量图像。在 Waymo Open 数据集上的评估表明，我们的框架能够实现 3D 资产插入和 NVS，促进了多样化且真实的驾驶数据的创建。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在自动驾驶仿真中实现逼真的3D资产插入和新视角合成的问题。该问题重要，因为高质量、多样化的训练数据能提升自动驾驶模型的鲁棒性和安全性，而现有方法往往分别处理资产插入或新视角合成，缺乏统一且真实的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将3D高斯分裂（Gaussian Splatting）资产表示与场景点云结合，并利用扩散模型进行图像生成，借鉴了Amodal3R、VGGT、Stable Video Diffusion、R3D2和GEN3C等已有技术，形成了一个统一的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用Amodal3R重建车辆的完整3D高斯模型，再将其插入由VGGT生成的彩色点云场景中；随后将资产与场景一起投影到用户指定的新相机轨迹，得到渲染图像和掩码；最后将这些渲染结果作为条件输入到Stable Video Diffusion模型，生成具有真实光照、阴影和时间一致性的最终视频。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①统一框架同时处理资产插入和新视角合成；②使用3D高斯分裂资产与点云的联合投影作为扩散模型的条件；③通过资产专属掩码实现光照和阴影的真实渲染；④利用视频扩散模型保证时间一致性。与之前的工作相比，R3D2仅处理单帧插入，GEN3C仅做新视角合成，SCPainter将两者结合并保持连续性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCPainter通过将3D高斯分裂资产与场景点云的联合渲染作为条件，利用视频扩散模型实现了逼真且时间一致的3D资产插入与新视角合成，为自动驾驶仿真提供了统一且多样化的数据生成方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.&lt;/p&gt;</description></item><item><guid>2512.22771v1</guid><title>Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2512.22771v1</link><author>Yiqian Li, Wen Jiang, Kostas Daniilidis</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这项研究提出了一种基于主动学习的视角选择方法，利用Fisher信息评估帧的有用性，以提升语义和动态场景建模的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 对具身代理而言，理解语义和动态信息至关重要，但相关任务的数据冗余远高于静态场景理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过主动学习框架，优先选择能为模型训练提供最大信息增益的帧，从而改进渲染质量和语义分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了一个结合语义高斯参数和变形网络的Fisher信息主动学习算法，能够同时处理语义推理和动态场景建模，并在多摄像头设置中挑选信息丰富的帧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在大规模静态图像和动态视频数据集上实验表明，该方法持续提升渲染质量和语义分割准确率，优于随机选择和基于不确定性的启发式方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提出的Fisher信息主动学习视角选择策略为语义与动态建模提供了更系统、更有效的解决方案，显著优于传统启发式方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 这项研究通过将视角选择问题视为主动学习问题，提出了一种基于Fisher信息的算法，用以量化候选视角在语义高斯参数和变形网络方面的有用性。该方法能够同时处理语义推理和动态场景建模，并在多摄像头设置中挑选信息丰富的帧。实验结果显示，该方法在大规模静态图像和动态视频数据集上持续提升渲染质量和语义分割性能，优于随机选择和基于不确定性的启发式方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在动态语义场景中如何高效选择最具信息量的视角，以减少训练成本并提升几何重建与语义分割质量。该问题在机器人、AR/VR 和大规模数字内容创建等应用中至关重要，因为这些场景往往拥有大量冗余或无信息的视角，随时间变化的几何与语义不一致会导致重建不完整或语义漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将视角选择视为主动学习问题，借鉴了 FisherRF 的 Fisher 信息量度量，并结合 4D-GS 的动态 Gaussian 以及 Feature 3DGS 的语义扩展。通过对 Gaussian 参数和变形网络分别计算 Fisher 信息，并采用对角线近似与梯度外积迹估计，形成统一的视角评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 Fisher 信息量化候选视角对模型参数（几何、颜色、语义特征和时间变形）的信息增益。实现流程包括：① 训练当前视角集合并累计 Fisher 信息；② 对每个候选视角计算其 Fisher 信息（对角线近似）并与累计信息相乘得到期望信息增益；③ 选取信息增益最高的视角作为下一训练视角；④ 重复该过程直至满足训练目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 首次将 Fisher 信息驱动的 NBV 引入动态语义 3D Gaussian splatting；② 将对角线近似扩展到语义特征和变形网络；③ 提出利用梯度外积迹估计变形 MLP 的 Fisher 信息；④ 将 NBV 直接集成到 3DGS 后端，实现统一的几何、语义与动态建模。与以往随机、基于不确定性或黑盒模型的 NBV 方法不同，该方法在动态语义场景中显著提升重建与分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种基于 Fisher 信息的下一最佳视角策略，能够同时优化 3D Gaussian splatting 的几何、语义和时间变形，从而在动态场景中实现高效、精确的重建与分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.&lt;/p&gt;</description></item><item><guid>2512.22819v1</guid><title>Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild</title><link>http://arxiv.org/abs/2512.22819v1</link><author>Hualie Jiang, Ziyang Song, Zhiqiang Lou, Rui Xu, Minglang Tan</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; DA360通过学习偏移参数和圆形填充，提升全景深度估计的零样本泛化性能，在室内外基准上显著降低误差，成为新SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 全景深度估计在室内已成熟，但在开放世界的零样本泛化差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥合室内与开放世界全景深度估计的性能差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在Depth Anything V2基础上学习偏移参数，将输出转为尺度不变估计，并在DPT解码器中加入圆形填充，消除缝隙伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DA360在室内基准降低50%误差，户外基准降低10%误差，且相对PanDA提升约30%，成为零样本全景深度估计的SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过偏移参数学习和圆形填充，DA360显著提升全景深度估计的零样本泛化能力，验证了视角域迁移的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 全景深度估计提供了一个全面的解决方案，用于捕捉完整的360度环境结构信息，为机器人和AR/VR应用带来显著好处。然而，虽然在室内环境中已被广泛研究，但其在开放世界领域的零样本泛化远不及视角图像，这些视角图像受益于丰富的训练数据。这种差距使得将视角域的能力迁移成为一个有吸引力的解决方案。为弥合这一差距，我们提出了DA360，即Depth Anything在360度全景中的适配版本。我们的关键创新是从ViT骨干学习一个偏移参数，将模型的尺度和偏移不变输出转换为尺度不变估计，直接生成良好的3D点云。此方法还将圆形填充集成到DPT解码器中，以消除缝隙伪影，确保空间连贯的深度图并保持球面连续性。在标准室内基准和我们新收集的户外数据集Metropolis上评估，DA360相较于其基础模型取得显著提升，在室内和户外基准上分别降低了50%和10%的相对深度误差。此外，DA360显著优于稳健的全景深度估计方法，在所有三个测试数据集上相对误差提升约30%，并确立了零样本全景深度估计的新状态。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现 360 度全景图像的零样本深度估计，并使输出的深度直接可用于生成无缝 3D 点云。全景深度在机器人导航、AR/VR 等应用中至关重要，但现有方法大多受限于室内数据、只能输出仿射不变深度，难以在野外环境中泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先评估了现有零样本视角深度模型，发现 DAV2 在全景任务中具有最小的域差距。随后他们在 DAV2 的 DPT 结构上加入了从 ViT 类 token 学习的偏移量模块和圆形填充，以实现尺度不变深度和无缝边界。该设计借鉴了 Depth Anything、PanDA 等先行工作，并在此基础上做了关键改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习一个全局偏移量，将 DAV2 输出的仿射不变视差转换为尺度不变视差，并使用圆形填充消除全景图像的边界伪影。实现流程为：①用预训练的 DAV2 作为起点；②在 ViT 的 class token 上接一个 MLP 预测偏移量；③在 DPT 解码器中将标准零填充改为圆形填充；④将真实深度反转为视差，使用尺度不变损失对网络进行微调；⑤在合成全景数据上训练后得到可直接生成 3D 点云的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①学习偏移量实现尺度不变深度；②在解码器中使用圆形填充消除左右边界伪影；③构建了新的户外全景基准 Metropolis；④在零样本设置下直接生成可用的 3D 点云。与之前的 PanDA 等方法不同，前者仅输出仿射不变深度并需后处理，且未解决边界问题；本工作在保持零样本优势的同时，显著提升了尺度精度和空间连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DA360 通过在预训练的视角深度模型中学习全局偏移量并采用圆形填充，将零样本视角深度迁移到全景图像，实现可直接生成无缝 3D 点云的尺度不变深度估计，并在室内外基准上刷新了零样本全景深度的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model&amp;#x27;s scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.&lt;/p&gt;</description></item><item><guid>2512.23042v1</guid><title>3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds</title><link>http://arxiv.org/abs/2512.23042v1</link><author>Ryousuke Yamada, Kohsuke Ide, Yoshihiro Fukuhara, Hirokatsu Kataoka, Gilles Puy, Andrei Bursuc, Yuki M. Asano</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督学习框架LAM3C，利用从无标签视频中生成的点云来学习3D表示，并通过构建RoomTours数据集和噪声正则化损失实现了在室内语义与实例分割任务上的显著性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 收集大规模3D场景扫描仍然昂贵且劳动密集，限制了3D自监督学习的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究是否可以仅凭无标签视频而不使用真实3D传感器来学习3D表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 构建RoomTours数据集：从网络收集房间漫游视频并使用现成的前馈重建模型生成49,219个点云场景；2) 设计LAM3C框架：采用拉普拉斯感知多层3D聚类与Sinkhorn-Knopp算法；3) 引入噪声正则化损失：通过局部几何平滑和特征稳定性提升学习效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在室内语义与实例分割任务中，LAM3C在没有任何真实3D扫描数据的情况下，性能超过了之前的自监督方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 无标签视频是3D自监督学习的丰富数据来源，可有效替代昂贵的3D扫描。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管最近在3D自监督学习方面取得了进展，但收集大规模3D场景扫描仍然昂贵且劳动密集。在这项工作中，我们研究了是否可以从未标记的视频中学习3D表示，而不使用任何真实的3D传感器。我们提出了Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp（LAM3C），一种自监督框架，能够从未标记视频生成的点云中学习。我们首先介绍了RoomTours，这是一个通过收集网络上的房间漫游视频（例如房地产导览）并使用现成的前馈重建模型生成49,219个场景的点云数据集。我们还提出了一种噪声正则化损失，通过强制局部几何平滑并确保在噪声点云下特征的稳定性来稳定表示学习。值得注意的是，在没有使用任何真实3D扫描的情况下，LAM3C在室内语义和实例分割任务上取得了比以前的自监督方法更高的性能。这些结果表明，未标记的视频是3D自监督学习的丰富数据来源。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文想解决如何在没有昂贵的真实 3D 扫描数据的情况下，利用无标签视频学习 3D 表示。因为 3D 扫描难以大规模获取，限制了 3D 自监督学习的发展，解决这一问题能让室内场景理解更易推广。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为视频中蕴含丰富的几何信息，现代前馈重建模型可以直接从视频生成点云。于是他们收集室内视频，使用 π³ 等模型生成点云，构建 RoomTours 数据集，并在此基础上设计 LAM3C，借鉴了 Sonata 的教师-学生框架、Sinkhorn‑Knopp 聚类、拉普拉斯平滑和噪声一致性等已有技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用视频生成的点云做大规模自监督预训练，并通过噪声正则化的聚类学习稳定的 3D 表示。实现流程包括：①收集并分割室内视频；②用前馈重建模型生成点云，构建 RoomTours；③训练 LAM3C，使用教师‑学生、聚类损失、拉普拉斯平滑和噪声一致性；④在下游任务上微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①创建了 49k 场景的 RoomTours 视频生成点云数据集；②提出 LAM3C，加入拉普拉斯平滑和噪声一致性损失，使模型能在噪声点云上稳定学习；③在不使用任何真实 3D 扫描的情况下，取得比现有自监督方法更好的室内分割性能。与之前的工作不同，它完全摆脱了对昂贵 3D 扫描的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LAM3C 证明了可以利用视频生成的点云进行大规模自监督 3D 表示学习，从而在无需真实 3D 扫描的情况下实现室内场景理解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.&lt;/p&gt;</description></item><item><guid>2512.23141v1</guid><title>Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset</title><link>http://arxiv.org/abs/2512.23141v1</link><author>Wuhao Xie, Kanji Tanaka</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在大规模城市环境中，杆状结构作为长期机器人定位的几何锚点的识别可靠性下降的问题，并通过建立专门的评估框架和数据集，系统比较了对比学习和监督学习在杆标志物特征鲁棒性上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 杆状结构被广泛认为是长期机器人定位的稳定几何锚点，但在典型的大规模城市环境中，杆距观测导致其识别可靠性显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将研究焦点从描述符设计转向系统性探讨描述符鲁棒性，并为此建立评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建了小杆标志物数据集（SPL），通过自动跟踪关联管道获取多视角、多距离的同一物理标志物观测；利用该框架对对比学习和监督学习两种范式进行比较分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 对比学习在稀疏几何特征空间中产生更鲁棒的特征，尤其在5到10米范围内的检索性能优于监督学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提供了实证基础和可扩展的方法，用于在挑战性真实场景中评估标志物的区分度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然杆状结构被广泛认为是长期机器人定位的稳定几何锚点，但在大规模城市环境中典型的杆距观测下，它们的识别可靠性显著下降。本文将焦点从描述符设计转向对描述符鲁棒性的系统性研究。我们的主要贡献是建立了一个以小杆标志物（SPL）数据集为中心的专门评估框架。该数据集通过自动跟踪关联管道构建，捕获同一物理标志物的多视角、多距离观测，无需人工标注。利用该框架，我们对对比学习（CL）和监督学习（SL）范式进行了比较分析。研究结果表明，对比学习在稀疏几何特征空间中产生更鲁棒的特征，尤其在5到10米范围内的检索性能更优。本文为在具有挑战性的真实场景中评估标志物区分度提供了实证基础和可扩展的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在评估在稀疏、长距离观测（Pole-at-Distance）下，基于杆状地标的描述子在机器人定位中的鲁棒性。杆状结构是长期稳定的定位锚点，但在大尺度城市环境中易出现辨识不清，导致定位误差。评估其鲁棒性对于实现可靠的长周期定位和地图维护至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为关键在于构建一个无人工标注、可扩展的评估数据集，并比较不同学习范式的表现。借鉴了已有的杆检测器 Polex、Pole-Image 描述子以及对比学习（Contrastive Learning）和监督学习的研究，构建了自动跟踪关联管道来生成多视角、不同距离的匹配对。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自动跟踪生成稀疏观测与密集观测的匹配对，使用 Pole-Image 将三维点云投影为二维图像，再用轻量级 ResNet 编码为嵌入向量。流程包括：①从 NCLT 数据中检测并跟踪杆；②构建 SPL 数据集（仅保留点数低于阈值的观测）；③分别用监督学习和对比学习训练相同网络；④在不同距离区间评估检索性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) SPL 数据集的自动化构建，消除人工标注成本；2) 在稀疏、长距离条件下系统性比较对比学习与监督学习的鲁棒性；3) 发现对比学习在 5–10 m 范围内显著优于监督学习。与以往仅在密集点云下评估的工作不同，本文聚焦于真实环境中稀疏观测的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提供了一个可扩展的自动化评估框架，证明对比学习在稀疏、长距离杆状地标定位中比监督学习更具鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While pole-like structures are widely recognized as stable geometric anchors for long-term robot localization, their identification reliability degrades significantly under Pole-at-Distance (Pad) observations typical of large-scale urban environments. This paper shifts the focus from descriptor design to a systematic investigation of descriptor robustness. Our primary contribution is the establishment of a specialized evaluation framework centered on the Small Pole Landmark (SPL) dataset. This dataset is constructed via an automated tracking-based association pipeline that captures multi-view, multi-distance observations of the same physical landmarks without manual annotation. Using this framework, we present a comparative analysis of Contrastive Learning (CL) and Supervised Learning (SL) paradigms. Our findings reveal that CL induces a more robust feature space for sparse geometry, achieving superior retrieval performance particularly in the 5--10m range. This work provides an empirical foundation and a scalable methodology for evaluating landmark distinctiveness in challenging real-world scenarios.&lt;/p&gt;</description></item><item><guid>2512.23147v1</guid><title>GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection</title><link>http://arxiv.org/abs/2512.23147v1</link><author>Jingyu Li, Xiaolong Zhao, Zhe Liu, Wenxiao Wu, Li Zhang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 GeoTeacher，利用教师模型的几何知识和体素级数据增强，提升半监督 3D 目标检测中学生模型对物体几何关系的感知能力，并在 ONCE 与 Waymo 数据集上取得新一代最优结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 半监督 3D 目标检测通过利用未标注数据提升检测性能，已有方法通过异构教师模型或特征一致性实现改进，但忽视了模型对有限标注数据下物体几何的低敏感性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 增强学生模型在有限训练数据，尤其是未标注数据下，捕捉物体几何关系的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计基于关键点的几何关系监督模块，将教师模型的几何知识迁移给学生；引入体素级数据增强并加入距离衰减机制以保持远距离物体完整；可与多种半监督 3D 检测方法结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ONCE 与 Waymo 数据集上实验表明 GeoTeacher 在提升学生模型几何感知方面显著有效，且与其他方法结合后进一步提升性能，达成新最优。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GeoTeacher 通过几何监督与增强策略显著提升半监督 3D 检测性能，具有良好的通用性和可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 半监督 3D 目标检测旨在利用未标注数据提升 3D 目标检测器的性能，近年来已成为活跃的研究领域。一些先前的方法通过使用异构教师模型提供高质量伪标签，或在教师与学生网络之间强制特征视角一致性，已取得显著改进。然而，这些方法忽视了模型在有限标注数据下通常对物体几何的敏感性较低，难以捕捉几何信息，而几何信息对于提升学生模型的物体感知和定位能力至关重要。本文提出 GeoTeacher，旨在增强学生模型在有限训练数据，尤其是未标注数据下，捕捉物体几何关系的能力。我们设计了基于关键点的几何关系监督模块，将教师模型对物体几何的知识迁移给学生，从而提升学生对几何关系的理解能力。此外，我们引入了体素级数据增强策略，增加物体几何多样性，进一步提升学生模型对几何结构的理解能力。为在增强过程中保持远距离物体的完整性，我们在该策略中加入了距离衰减机制。GeoTeacher 还可以与不同的半监督 3D 检测方法结合，以进一步提升其性能。对 ONCE 和 Waymo 数据集的广泛实验表明，我们的方法具有有效性和泛化性，并取得了新的最优结果。代码将发布在 https://github.com/SII-Whaleice/GeoTeacher&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决半监督3D目标检测中标注数据稀缺的问题。通过利用大量未标注点云数据提升检测性能，减少昂贵的人工标注成本。该问题在自动驾驶、机器人等需要高精度3D感知的场景中尤为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有半监督方法主要关注伪标签质量或特征一致性，忽视了物体内部几何关系。于是提出基于关键点的几何关系监督和体素级数据增强，并借鉴了教师-学生框架、伪标签加权、距离衰减等已有技术。该方法在设计上兼顾了对几何信息的利用和数据多样性的提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让学生模型通过教师模型学习物体的几何关系，并通过体素级增强增加几何多样性。实现流程分两阶段：第一阶段训练高性能GeoTeacher；第二阶段使用GeoTeacher对学生进行监督，加入几何关系监督损失和距离衰减的体素增强，并在训练中使用伪标签的置信度加权。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 关键点（中心、边缘中点、角点）构成的几何关系监督模块；2) 距离衰减的体素级数据增强策略；3) 通过置信度加权减轻伪标签噪声；4) 可与现有半监督方法无缝结合。与以往仅关注伪标签或特征一致性的工作不同，本文显式利用几何信息和数据多样性来提升学生模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoTeacher通过关键点几何关系监督和距离感知的体素增强，使半监督3D目标检测在利用未标注数据时显著提升检测精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model&amp;#x27;s ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model&amp;#x27;s ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model&amp;#x27;s knowledge of object geometry to the student, thereby improving the student&amp;#x27;s capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model&amp;#x27;s ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher&lt;/p&gt;</description></item><item><guid>2512.23176v1</guid><title>GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</title><link>http://arxiv.org/abs/2512.23176v1</link><author>Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于图像的3D目标检测框架GVSynergy-Det，通过协同学习高斯-体素表示来提升检测精度，避免使用深度传感器或密集3D监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的基于点云的3D检测需要昂贵的深度传感器，而仅使用RGB图像的检测方法往往需要密集的3D监督才能达到高精度，缺乏监督时难以准确提取几何信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种无需深度或密集3D监督即可实现高精度3D目标检测的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用双重表示架构：一方面利用可泛化的高斯光散射提取细粒面细节特征；另一方面通过交叉表示增强机制将高斯场的几何细节注入体素特征；两种表示通过可学习的融合实现特征协同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ScanNetV2和ARKitScenes室内基准上，GVSynergy-Det在不使用任何深度或密集3D几何监督的情况下，显著超过现有方法，取得最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 协同高斯-体素表示能够有效弥补单一表示的不足，实现高精度的图像基3D检测，并且不依赖昂贵的深度传感器或密集3D监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于图像的3D目标检测旨在仅使用RGB图像识别并定位三维空间中的目标，从而消除了点云方法所需的昂贵深度传感器。现有的基于图像的方法面临两个关键挑战：实现高精度的方法通常需要密集的三维监督，而不使用此类监督的方法则难以仅凭图像提取准确的几何信息。在本文中，我们提出了GVSynergy-Det，一种通过协同高斯-体素表示学习提升3D检测的新框架。我们的核心见解是，连续高斯和离散体素表示能够捕获互补的几何信息：高斯擅长建模细粒表面细节，而体素提供结构化的空间上下文。我们引入了双重表示架构：1）适配可泛化的高斯光散射以提取用于检测任务的互补几何特征；2）开发交叉表示增强机制，用高斯场的几何细节丰富体素特征。与以前依赖耗时的逐场景优化或仅将高斯表示用于深度正则化的方法不同，我们的协同策略通过可学习的集成直接利用两种表示的特征，从而实现更准确的目标定位。大量实验表明，GVSynergy-Det在具有挑战性的室内基准上实现了最先进的结果，在ScanNetV2和ARKitScenes数据集上显著优于现有方法，且不需要任何深度或密集三维几何监督（例如点云或TSDF）。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现仅使用 RGB 图像的 3D 目标检测，避免依赖昂贵的深度传感器或稠密 3D 注释。该问题重要，因为它能降低成本、扩大部署场景，并解决点云稀疏或缺失导致的几何不确定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到连续高斯表示能捕捉细腻表面细节，而离散体素表示提供结构化空间上下文。基于此，他们借鉴了 3D Gaussian Splatting、体素检测和 MVSDet 等现有技术，提出了双表示架构和跨表示增强机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将连续高斯和离散体素两种几何表示在特征层面融合，以获得更完整的空间信息。实现流程包括：提取 2D 图像特征 → 通过几何反投影构建体素体积 → 用可泛化的高斯 Splatting 预测高斯原语 → 将高斯特征体素化并与体素特征进行自适应融合 → 在融合后的特征上执行检测头。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次在检测任务中实现高斯与体素的深度交叉学习；2) 设计自适应跨表示增强模块，动态加权两种特征；3) 无需逐场景优化或深度监督即可获得高精度检测。与之前仅将高斯用于深度正则化或需要昂贵 3D 注释的工作不同，GVSynergy-Det 直接利用两种表示的互补优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GVSynergy-Det 通过联合学习和融合连续高斯与离散体素表示，在仅使用 RGB 图像且不依赖深度或稠密 3D 注释的条件下，实现了室内场景中最先进的 3D 目标检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).&lt;/p&gt;</description></item><item><guid>2512.23180v1</guid><title>GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation</title><link>http://arxiv.org/abs/2512.23180v1</link><author>Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo Zhang, Wuxiong Huang, Hesheng Wang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于3D高斯场景表示的统一驾驶世界模型框架，能够实现3D场景理解与多模态生成，并通过文本与3D场景的早期对齐提升生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有驾驶世界模型缺乏3D场景理解能力，且仅能基于输入数据生成内容，无法解释或推理驾驶环境；当前方法使用点云或BEV特征，无法准确将文本信息与3D场景对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够同时实现3D场景理解和多模态生成的统一框架，并通过上下文丰富提升理解与生成任务的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将丰富的语言特征嵌入每个高斯原语，实现早期模态对齐；设计任务感知的语言引导采样策略，去除冗余高斯并注入精确紧凑的3D令牌；构建双条件多模态生成模型，利用视觉语言模型捕获的信息作为高层语言条件与低层图像条件共同引导生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和NuInteract数据集上进行的综合实验表明，该框架在多模态生成任务上达到了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提出的基于3D高斯表示的驾驶世界模型框架在理解与生成方面均表现出色，且将公开代码以促进后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在解决驾驶场景中缺乏统一的 3D 场景理解与多模态生成的难题。现有驾驶世界模型只能生成基于输入的内容，无法解释或推理环境信息，限制了其在风险预测、路径规划和训练数据生成等关键应用中的实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 3D Gaussian 场景表示（3DGS）、LangSplat、CLIP、SAM 等技术，并参考了 BEV‑based HERMES 等工作，提出将语言特征嵌入 3D 高斯体素并通过投影对齐到文本空间。随后设计了任务感知的语言引导采样和双条件多模态生成框架，以实现高效的理解与生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将图像信息编码为带语言嵌入的 3D 高斯体素，投影为统一的 token，按查询进行稀疏采样后输入 LLM 进行理解，得到文本答案和高层语言特征；随后将高层语言特征与低层图像特征作为条件，驱动扩散式生成器产生 RGB、深度等多模态输出。整体流程包括：世界分词器 → 投影器 → 任务感知采样 → LLM 理解 → 双条件生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首个基于 3D Gaussian 的统一世界模型，兼顾场景理解与生成；2) 语言嵌入与 3D 高斯的直接对齐，提升跨模态精度；3) 任务感知语言引导采样，解决 token 过长与冗余问题；4) 双条件多模态生成，结合高层语言与低层图像特征；与 HERMES 等 BEV 方法相比，提供更精确的空间对齐和更高效的 token 表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GaussianDWM 通过将 3D 高斯场景表示与视觉‑语言推理相结合，实现了驾驶场景的精准理解与连贯多模态生成，弥合了感知与合成之间的鸿沟。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.&lt;/p&gt;</description></item><item><guid>2512.23215v1</guid><title>AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding</title><link>http://arxiv.org/abs/2512.23215v1</link><author>Jongoh Jeong, Taek-Jin Song, Jong-Hwan Kim, Kuk-Jin Yoon</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个新的模拟环境下的道路障碍检测数据集AVOID，并在该数据集上评估了实时网络和多任务网络的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 道路场景理解对自动驾驶至关重要，尤其是在不同恶劣天气和光照条件下实时检测小型意外障碍物。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有数据集中缺乏同一视觉域下道路障碍物的问题，并提供支持多视觉感知任务的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建AVOID数据集，包含不同天气和时间条件下的道路障碍物图像、语义图、深度图、LiDAR数据和航路点；使用高性能实时网络进行障碍物检测基准测试，并开展多任务网络的消融实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; AVOID数据集丰富了道路障碍物样本，实验表明实时网络在该数据集上表现良好，多任务网络在语义分割、深度预测和航路点预测任务上具有可观的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AVOID为在恶劣视觉条件下的实时障碍物检测提供了有效的数据资源，并证明了多任务网络在相关任务中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解道路场景以实现视觉感知仍然是智能自动驾驶汽车的关键。特别是，在各种恶劣条件（例如天气和昼夜）下实时可靠地检测意外的小型道路障碍物是非常重要的。然而，现有的道路驾驶数据集仅提供在正常或恶劣场景下获取的大规模图像，并且通常不包含与其他类别在同一视觉域中捕获的道路障碍物。为了解决这个问题，我们引入了一个名为AVOID（Adverse Visual Conditions Dataset）的新数据集，用于在模拟环境中进行实时障碍物检测。AVOID包含在各种天气和时间条件下沿每条路径捕获的大量意外道路障碍物。每张图像都配有相应的语义图和深度图、原始和语义LiDAR数据以及航路点，从而支持大多数视觉感知任务。我们在高性能实时网络上对障碍物检测任务进行了基准测试，并使用全面的多任务网络对语义分割、深度和航路点预测任务进行了消融研究。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在恶劣天气和光照条件下，实时检测道路上突发小型障碍物的问题。此类障碍物往往无预警出现，导致交通事故和人员伤亡，尤其在自动驾驶系统中对安全性构成严重威胁。现有数据集缺乏在同一视觉域下包含障碍物的标注，限制了模型在真实环境中的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有道路场景数据集的不足，发现它们要么只包含正常或恶劣条件，要么缺少障碍物标注。随后借鉴CARLA模拟器和之前的TransFuser、SHIFT等基于模拟的道路数据集，设计了在模拟环境中放置多种障碍物并同步采集多模态数据的流程。通过手工设置障碍物位置和避障路径，确保数据在不同天气和时间条件下的真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个大规模、标注完整的AVOID数据集，包含在42种恶劣环境组合下的道路场景，并提供语义、深度、LiDAR和航路点等多模态标注。实现流程包括：①在CARLA中导入45种障碍物模型并手动调整位置；②使用A*规划和PID控制器生成避障路径；③同步采集立体RGB、深度相机、语义LiDAR等数据；④自动生成像素级语义标签和障碍物类别。随后在此数据集上对单任务和多任务网络进行基准测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提供了首个包含障碍物类别的恶劣条件道路数据集，规模达21,403帧；②在同一视觉域下同步采集立体RGB、深度、语义LiDAR和航路点，支持多任务学习；③引入了多天气-时间组合（42种），并保持环境不随帧变化，提升真实性；④在此数据集上实现了实时轻量级障碍物检测网络，取得51.81% IoU、18.87 FPS的性能。与之前的数据集相比，AVOID在障碍物标注、模态多样性和环境多样性方面均有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AVOID提供了一个大规模、标注完整的恶劣视觉条件下的道路障碍物数据集，并通过多模态同步采集和实时检测网络验证了其在自动驾驶感知任务中的价值。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding road scenes for visual perception remains crucial for intelligent self-driving cars. In particular, it is desirable to detect unexpected small road hazards reliably in real-time, especially under varying adverse conditions (e.g., weather and daylight). However, existing road driving datasets provide large-scale images acquired in either normal or adverse scenarios only, and often do not contain the road obstacles captured in the same visual domain as for the other classes. To address this, we introduce a new dataset called AVOID, the Adverse Visual Conditions Dataset, for real-time obstacle detection collected in a simulated environment. AVOID consists of a large set of unexpected road obstacles located along each path captured under various weather and time conditions. Each image is coupled with the corresponding semantic and depth maps, raw and semantic LiDAR data, and waypoints, thereby supporting most visual perception tasks. We benchmark the results on high-performing real-time networks for the obstacle detection task, and also propose and conduct ablation studies using a comprehensive multi-task network for semantic segmentation, depth and waypoint prediction tasks.&lt;/p&gt;</description></item><item><guid>2512.23318v1</guid><title>PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering</title><link>http://arxiv.org/abs/2512.23318v1</link><author>Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了PCR-ORB框架，通过深度学习点云细化技术提升动态环境下的视觉SLAM性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在动态环境中，移动物体会破坏跟踪精度和地图一致性，给视觉SLAM带来挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 降低动态物体干扰，提高vSLAM在复杂场景中的定位与地图构建可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在ORB-SLAM3基础上加入YOLOv8语义分割与CUDA加速，实现实时处理；采用多阶段过滤策略，包括地面平面估计、天空区域剔除、边缘过滤和时间一致性验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在KITTI数据集上，PCR-ORB在部分序列（如序列04）显著提升ATE RMSE和中位数；但不同序列表现不一，效果受场景影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 动态物体过滤仍具挑战，但PCR-ORB展示了在复杂环境中实现鲁棒导航的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉同步定位与地图构建（vSLAM）系统在动态环境中面临重大挑战，移动物体会破坏跟踪精度和地图一致性。本文提出了PCR-ORB（点云细化ORB），一种增强的ORB-SLAM3框架，集成了基于深度学习的点云细化技术，以减轻动态物体干扰。我们的方法使用YOLOv8进行语义分割，并结合CUDA加速处理，实现实时性能。系统实现了多阶段过滤策略，包括地面平面估计、天空区域移除、边缘过滤和时间一致性验证。对KITTI数据集（序列00-09）的全面评估展示了不同环境条件和场景类型下的性能特征。在特定序列中观察到显著改进，序列04在ATE RMSE上提升了25.9%，ATE中位数提升了30.4%。然而，结果在不同序列中表现混合，表明效果取决于场景。实现为动态物体过滤挑战和在复杂环境中实现鲁棒导航的机遇提供了见解。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决视觉SLAM在动态环境中因移动物体导致的跟踪误差和地图不一致问题。动态物体会破坏静态世界假设，导致定位漂移、地图错误和回环检测失败。随着自动驾驶、机器人和无人机等系统在真实世界中部署，能够在动态场景下保持高精度定位变得尤为重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在ORB‑SLAM3的基础上加入了一个点云精炼模块，利用YOLOv8进行语义分割并结合多阶段几何与时序过滤。设计过程中参考了ORB‑SLAM3的三线程架构、YOLO系列的实时检测能力以及CUDA加速技术，并借鉴了先前使用YOLOv5进行动态物体过滤的工作。通过将深度学习与传统SLAM的并行线程相结合，保持了实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度学习得到的语义掩码来识别并剔除动态物体、天空和地面等不可靠点云，再将精炼后的点云输入SLAM流程。实现流程包括：①预处理帧并送入YOLOv8得到分割掩码；②对掩码进行后处理，执行地面估计、天空剔除、边缘过滤和时序一致性校验；③将过滤后的点云交给跟踪线程和地图线程，完成定位与地图构建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①无缝集成YOLOv8语义分割到ORB‑SLAM3；②多阶段过滤策略结合语义、几何和时序信息；③CUDA加速的并行线程实现实时性能；④专门的点云过滤线程。与以往仅使用YOLOv5或简单统计滤波的动态SLAM方法不同，PCR‑ORB提供更精细的动态物体识别和更完整的过滤流程，显著提升了在复杂动态场景下的定位精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PCR‑ORB通过CUDA加速的YOLOv8语义分割和多阶段点云精炼，显著提升了ORB‑SLAM3在动态环境中的定位精度与地图一致性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.&lt;/p&gt;</description></item><item><guid>2512.23365v1</guid><title>SpatialMosaic: A Multiview VLM Dataset for Partial Visibility</title><link>http://arxiv.org/abs/2512.23365v1</link><author>Kanghee Lee, Injae Lee, Minseok Kwak, Kwonyoung Ryu, Jungi Hong, Jaesik Park</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个可扩展的多视角数据生成与标注流程，构建了包含两百万问答对的 SpatialMosaic 数据集，并推出了 SpatialMosaic-Bench 评测基准和 SpatialMosaicVLM 框架，以提升视觉语言模型在真实环境中对三维空间的推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大型语言模型在三维场景理解和空间推理方面取得快速进展，但现有方法多依赖预构建的三维表示或现成的重建管线，限制了可扩展性和实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在真实环境中部分可见、遮挡和低重叠等条件下，基于多视角图像进行空间推理的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了可扩展的多视角数据生成与标注管线，生成真实的空间推理问答对；构建了包含两百万问答对的 SpatialMosaic 数据集；推出了包含一百万问答对、六个任务的 SpatialMosaic-Bench 评测基准；并提出了将三维重建模型作为几何编码器集成到视觉语言模型中的 SpatialMosaicVLM 框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，所提出的数据集和问答任务能显著提升模型在具有挑战性的多视角条件下的空间推理性能，验证了数据生成管线的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过构建大规模、多样化的空间推理问答数据以及相应的评测基准和混合框架，本文为提升视觉语言模型在真实三维场景中的空间推理能力提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 快速发展的多模态大型语言模型解锁了增强三维场景理解和空间推理的潜力。然而，现有方法往往依赖预构建的三维表示或现成的重建管线，限制了可扩展性和现实应用。最近的一系列工作探索了直接从多视角图像学习空间推理，使视觉语言模型能够在没有显式三维重建的情况下理解三维场景。然而，在现实环境中经常出现的部分可见、遮挡和低重叠等关键挑战仍未得到充分探讨。为解决这些限制，我们提出了一个可扩展的多视角数据生成与标注管线，构建了包含两百万问答对的 SpatialMosaic 数据集。我们进一步推出了 SpatialMosaic-Bench，一个在现实且具有挑战性场景下评估多视角空间推理的基准，包含六个任务的一百万问答对。此外，我们提出了 SpatialMosaicVLM，一个将三维重建模型作为几何编码器集成到视觉语言模型中的混合框架，以实现稳健的空间推理。大量实验表明，我们提出的数据集和问答任务能有效提升在具有挑战性的多视角条件下的空间推理能力，验证了数据生成管线在构建真实多样化问答对方面的有效性。代码和数据集将很快发布。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在让多模态大型语言模型在面对部分可见、遮挡和低重叠的多视角图像时，仍能进行可靠的三维空间推理。当前模型往往依赖预构建的三维地图或重建管线，成本高且难以扩展到动态环境；而在现实场景中，摄像机视角稀疏、物体遮挡常见，准确的空间推理对机器人导航、场景理解等任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有多模态模型在稀疏视角下的不足，随后设计了一套基于ScanNet++稠密三维注释的可扩展标注管线，用于计算遮挡比例、视角重叠并筛选符合部分可见条件的图像与实例。随后他们自动生成符合六类空间推理任务的问答对，并提出SpatialMosaicVLM，将最新的三维重建模型作为几何编码器与视觉语言模型融合。该方案借鉴了Flamingo、BLIP‑2、MiniGPT‑4等多模态模型，以及DUSt3R、MASt3R、VGGT等三维重建网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自动化的数据生成和几何编码，将稀疏、多视角的视觉信息与三维几何知识融合，以实现对部分可见、遮挡和低重叠场景的空间推理。实现流程包括：① 计算每个实例的遮挡比例和图像对的重叠比例；② 过滤满足部分可见条件的图像与实例；③ 依据空间关系填充模板生成问答对；④ 训练SpatialMosaicVLM，将重建模型产生的几何标记与每视角视觉特征融合后输入语言模型进行推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文提出了可扩展的标注框架、规模达200万条问答的训练集和100万条的评测集，专门针对部分可见、遮挡和低重叠场景；引入SpatialMosaicVLM，将三维重建模型的几何编码器与视觉语言模型结合，实现跨视角一致性和鲁棒推理；与以往仅使用2–3视角或视频帧、依赖预构建三维地图的Benchmark不同，SpatialMosaic提供2–5视角的多样化、稀疏视角设置，并在规模与任务多样性上实现突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpatialMosaic通过构建大规模、真实多视角问答数据集与基于几何编码的混合模型，首次实现了在部分可见、遮挡和低重叠条件下的鲁棒三维空间推理。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.&lt;/p&gt;</description></item><item><guid>2512.23472v1</guid><title>MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration</title><link>http://arxiv.org/abs/2512.23472v1</link><author>Shuyuan Lin, Wenwu Peng, Junjie Huang, Qiang Qi, Miaohui Wang, Jian Weng</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种多域上下文集成网络MCI-Net，用于改进点云配准中的特征学习和配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有基于深度学习的方法主要依赖欧氏邻域策略，难以有效捕捉点云中的隐含语义和结构一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统方法在特征提取和配准中的局限性，提升特征表达和配准的鲁棒性与辨别力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MCI-Net包含三大模块：全局图邻域聚合模块构建整体结构关系；渐进式上下文交互模块通过域内特征解耦和域间上下文交互提升辨别力；动态内点选择模块利用多次位姿估计的残差信息优化内点权重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在室内RGB-D和室外LiDAR数据集上，MCI-Net显著优于现有最先进方法，在3DMatch数据集上实现了96.4%的最高配准召回率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多域上下文集成网络通过全局结构捕捉、上下文交互和动态内点选择，有效提升了点云配准的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种多域上下文集成网络（MCI-Net），通过聚合来自不同域的上下文线索来改进特征表示和配准性能。具体而言，作者提出了一个图邻域聚合模块，构建全局图以捕捉点云中的整体结构关系；随后提出了渐进式上下文交互模块，通过域内特征解耦和域间上下文交互来增强特征辨别力；最后设计了动态内点选择方法，利用多次位姿估计的残差信息优化内点权重，从而提升配准的准确性和鲁棒性。实验结果表明，MCI-Net在室内RGB-D和室外LiDAR数据集上显著优于现有最先进方法，在3DMatch数据集上实现了最高96.4%的配准召回率。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云配准中的鲁棒性和精度问题，尤其是传统方法仅依赖欧氏邻域难以捕捉全局语义和结构一致性。点云配准是三维重建、机器人定位和自动驾驶等领域的核心任务，鲁棒的配准直接影响后续处理的质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有基于欧氏邻域的特征提取方法的局限，随后借鉴图神经网络、注意力机制和历史残差信息等技术，提出了全局图聚合、进阶上下文交互和动态内点选择三大模块。该设计在多篇前沿工作（如DCP、CoFiNet、Hyperbolic embedding等）的基础上进一步融合多域信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多域（坐标、特征、全局图）上下文整合来提升特征判别力，并通过迭代内点权重更新提高配准鲁棒性。实现流程为：①使用PAConv提取点/块特征；②将特征映射到坐标、特征和全局图域；③GNAM构建全局图并自适应聚合邻域；④PCIM在各域内进行特征分解并跨域交互；⑤匹配得到对应关系；⑥DISM利用多轮残差动态更新内点权重；⑦最终求解刚性变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①全局图邻域聚合模块（GNAM）捕获整体结构；②进阶上下文交互模块（PCIM）实现局部-全局分解与跨域注意力；③动态内点选择（DISM）通过历史残差迭代优化对应权重。与以往仅使用局部欧氏邻域或单轮对应的配准方法不同，MCI‑Net在特征提取、上下文融合和内点更新上均实现了多域协同与迭代改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MCI‑Net通过全局图聚合、跨域上下文交互和动态内点迭代，构建了一个多域上下文整合框架，显著提升了点云配准的鲁棒性和精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\% on 3DMatch. Source code is available at http://www.linshuyuan.com.&lt;/p&gt;</description></item><item><guid>2512.23635v1</guid><title>Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</title><link>http://arxiv.org/abs/2512.23635v1</link><author>Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的时空对齐模块HAT，能够让每个目标对象在没有直接监督的情况下，从多个假设中自适应地解码出最佳对齐方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在端到端自动驾驶感知中，时空对齐对于时间建模至关重要，现有方法多依赖注意力机制和统一的简化运动模型，忽视了不同类别和帧间运动状态与特征的差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入多种显式运动模型和多假设解码，提升目标对齐的精度和鲁棒性，从而改善后续的检测与跟踪性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; HAT首先利用多种显式运动模型生成历史实例的空间锚点和运动感知特征提案；随后通过将语义与运动线索嵌入缓存的目标查询中进行多假设解码，最终为目标帧提供最优对齐方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，HAT在多种基线模型中均提升了3D时序检测器和跟踪器的性能，单独与DETR3D配合实现了46.0% AMOTA的最先进跟踪结果；在端到端自动驾驶方法中，HAT提升了感知精度（+1.3% mAP，+3.1% AMOTA），并将碰撞率降低32%；当语义信息被破坏时，HAT的运动建模增强了感知与规划的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 显式运动建模与自适应多假设对齐显著提升了端到端自动驾驶感知与规划的性能，HAT为时空对齐提供了一种有效且可推广的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注在自动驾驶中端到端3D感知的时空对齐问题。准确的时空对齐能为检测、跟踪和规划提供结构和纹理先验，提升感知精度并降低碰撞风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有方法往往只使用单一运动假设，且需要手工调参，导致对不同物体类别和动态场景的适应性差。为此，他们借鉴了传统的Kalman滤波、显式运动模型（如常速、常加、转弯等）以及查询传播和注意力机制，提出多假设生成与自适应解码的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用多种显式运动模型生成多组锚点和特征假设，再通过嵌入在缓存查询中的运动线索自适应地解码出最佳对齐方案。实现流程包括：1）时空对齐模块生成多假设锚点和特征；2）空间对齐模块利用查询中的运动信息为每个假设分配动态权重，挑选最优对齐；3）融合并细化锚点与特征后送入检测或跟踪头。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）可插拔的HAT模块，能无监督地集成到多种端到端感知框架；2）多假设显式-隐式混合对齐，克服单假设的局限；3）自适应解码器利用查询中的运动线索，无需手工调参。与以往方法相比，HAT不再依赖单一运动模型或人工规则，显著提升检测、跟踪和整体系统性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HAT通过多假设显式运动模型与自适应解码，提供一种可插拔、无监督的时空对齐方案，显著提升自动驾驶端到端3D感知与跟踪的精度与鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.&lt;/p&gt;</description></item><item><guid>2512.23786v1</guid><title>Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments</title><link>http://arxiv.org/abs/2512.23786v1</link><author>Ankan Aich, Yangming Lee</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出利用Depth Anything V2架构的高保真合成先验，并通过Dynamic Vector Low-Rank Adaptation (DV-LORA)高效适配医学领域，解决单目深度估计在镜面、液体环境下的边界崩溃问题，并在SCARED数据集上引入物理分层评估，取得98.1%的准确率，显著优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单目深度估计在机器人手术中至关重要，但在镜面、液体充满的内镜环境中易失效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过高保真合成先验和低秩适配，提升在高镜面环境下的深度估计鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Depth Anything V2的合成先验，结合Dynamic Vector Low-Rank Adaptation (DV-LORA)进行参数高效迁移，并在SCARED数据集上采用物理分层评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 方法在SCARED数据集上实现98.1%的准确率，平方相对误差比基线降低17%以上，显示出在恶劣手术照明下的优越鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在高镜面手术环境中实现了新的最优性能，证明了合成先验与低秩适配的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确的单目深度估计（MDE）对机器人手术至关重要，但在镜面、液体充满的内镜环境中仍然脆弱。现有的自监督方法通常依赖于使用嘈杂真实世界伪标签训练的基础模型，往往在薄型手术工具和透明表面上出现边界崩溃。在本研究中，我们通过利用Depth Anything V2架构的高保真合成先验来解决这一问题，该架构本身能够捕捉薄结构的精确几何细节。我们使用Dynamic Vector Low-Rank Adaptation（DV-LORA）高效地将这些先验适配到医学领域，最小化参数预算，同时弥合合成到真实的差距。此外，我们在SCARED数据集上引入了物理分层评估协议，以严格量化高镜面环境下的性能，这些环境往往被聚合指标掩盖。我们的方案确立了新的最优性能，准确率在1.25以内达到98.1%，并将平方相对误差降低了超过17%，相较于既定基线，展示了在恶劣手术照明下的卓越鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本论文旨在解决在光照强烈、含液体的内窥镜环境中，单目深度估计容易出现边界崩溃和透明表面误差的问题。准确的深度信息对机器人手术的安全性、定位、跟踪和增强现实等关键任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出传统自监督方法在真实世界伪标签噪声下对薄工具和透明表面处理不佳的缺陷，随后借鉴了EndoDAC的参数高效微调框架和DV‑LoRA技术，并将其与Depth Anything V2的合成先验相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是冻结在合成数据上预训练的DAv2 Transformer骨干，插入可学习的DV‑LoRA模块以适应手术图像的光照与纹理，并通过卷积颈部恢复高频细节。整体流程包括DepthNet与Pose‑Intrinsics Net的联合训练，使用视图合成的自监督损失进行优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）利用合成先验而非噪声伪标签；2）使用DV‑LoRA实现极低参数量的高效微调；3）引入物理分层评估协议，专门衡量高光反射区域的性能；4）在这些改进下实现了在SCARED数据集上新的最优指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过将高保真合成深度先验与参数高效的DV‑LoRA相结合，并提出物理分层评估，本研究实现了在光照强烈、含液体的内窥镜环境中最先进的单目深度估计。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (&amp;lt; 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.&lt;/p&gt;</description></item><item><guid>2512.23972v1</guid><title>SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration</title><link>http://arxiv.org/abs/2512.23972v1</link><author>Liangtao Feng, Zhenchang Liu, Feng Zhang, Xuefeng Ren</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 SHIELD，一种基于球面投影的混合前沿集成方法，用于高效的 LiDAR 无人机探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然激光 LiDAR 具有宽视场优势，但在无人机探索中仍面临观测质量低于深度相机、传统前沿方法计算量大、无点云区域难以判定空旷等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为解决 LiDAR 观测质量不一致、计算负担重以及无点云区域判定困难等问题，提出 SHIELD 方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SHIELD 通过维护观测质量占据图并在该图上进行光线投射，采用混合前沿策略减轻计算负担并克服点云质量限制，同时引入向外球面投射光线投射策略，以保证开放区域的飞行安全和探索效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仿真和飞行实验表明 SHIELD 在 LiDAR 无人机探索中具有显著的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SHIELD 能有效提升 LiDAR 探索性能，且将开源以促进研究社区发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了 SHIELD，一种用于高效 LiDAR 无人机探索的球面投影混合前沿集成方法。尽管激光 LiDAR 具有宽视场的优势，但其在无人机探索中的应用仍面临若干挑战。LiDAR 点云的观测质量通常不如深度相机。基于已知和未知区域的传统前沿方法在处理 LiDAR 的宽视场时会产生沉重的计算负担。此外，缺乏点云的区域也难以通过光线投射来分类为空旷空间。为解决这些问题，提出了 SHIELD。它维护一个观测质量占据图，并在该图上执行光线投射，以解决探索过程中点云质量不一致的问题。采用混合前沿方法来同时解决计算负担和点云质量探索的局限性。此外，还提出了一种向外球面投射光线投射策略，以在开放区域中共同确保飞行安全和探索效率。仿真和飞行实验证明了 SHIELD 的有效性。本工作将开源，以贡献于研究社区。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决使用激光雷达进行无人机大规模开放环境探索时，宽视场导致的观测质量不均、无返回点难以判定自由空间以及前沿检测计算量大的问题。该问题重要，因为激光雷达在无人机自主探索中提供了广阔视野，但若无法准确建图和规划，飞行安全与效率将受到严重影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了激光雷达的局限性，借鉴了 EPIC、FLARE、SOAR 等现有前沿与观测质量方法，提出将表面法线信息与观测角度结合形成质量度量，并设计了混合前沿策略和外向球面投影射线投射。通过多线程架构平衡负载，最终形成 SHIELD 系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将观测质量融入占据图、前沿检测与射线投射三大模块，并利用球面投影补全无返回区域。实现流程包括：① 通过点云与法线计算质量并构建质量占据图；② 依据质量进行射线投射；③ 检测质量前沿与未知前沿，聚类并生成视点；④ 采用外向球面投影射线投射标记无返回方向的自由空间；⑤ 规划路径完成探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 基于表面法线的观测质量占据图与质量射线投射；② 混合前沿策略同时考虑质量前沿和未知前沿；③ 外向球面投影射线投射与自校准方法，解决无返回点的自由空间判定；④ 通过多线程实现高效实时运行。与以往仅使用统一占据图或仅关注未知前沿的工作不同，SHIELD 在观测质量和射线投射上做了更细粒度的处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SHIELD 通过观测质量映射、混合前沿规划和外向球面投影射线投射，构建了一套高效安全的激光雷达无人机探索框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper introduces SHIELD, a Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone exploration method. Although laser LiDAR offers the advantage of a wide field of view, its application in UAV exploration still faces several challenges. The observation quality of LiDAR point clouds is generally inferior to that of depth cameras. Traditional frontier methods based on known and unknown regions impose a heavy computational burden, especially when handling the wide field of view of LiDAR. In addition, regions without point cloud are also difficult to classify as free space through raycasting. To address these problems, the SHIELD is proposed. It maintains an observation-quality occupancy map and performs ray-casting on this map to address the issue of inconsistent point-cloud quality during exploration. A hybrid frontier method is used to tackle both the computational burden and the limitations of point-cloud quality exploration. In addition, an outward spherical-projection ray-casting strategy is proposed to jointly ensure flight safety and exploration efficiency in open areas. Simulations and flight experiments prove the effectiveness of SHIELD. This work will be open-sourced to contribute to the research community.&lt;/p&gt;</description></item><item><guid>2512.23983v1</guid><title>DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation</title><link>http://arxiv.org/abs/2512.23983v1</link><author>Yuang Jia, Jinlong Wang, Jiayi Zhao, Chunlam Li, Shunzhou Wang, Wei Gao</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种仅使用图像和可选相机位姿的视角外推方法，结合点云估计、四维高斯重建和视频扩散模型，能够在自动驾驶场景中生成高质量的新视角图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法依赖昂贵的 LiDAR 点云、三维边界框和车道标注等先验信息，限制了实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不使用昂贵传感器或繁重标注的前提下，实现自动驾驶场景中的高质量视角外推。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先估计全局静态点云和每帧动态点云并融合为统一表示；使用可变形四维高斯框架重建场景；训练四维高斯模型渲染退化和伪图像以训练视频扩散模型；随后用扩散模型迭代细化偏移的高斯渲染，并将增强结果回馈给四维高斯模型，直至完成视角外推。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在新外推视角下生成的图像质量显著优于基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提供了一种无需昂贵传感器或繁重标注的有效视角外推方案，能够在自动驾驶场景中生成高质量图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种在自动驾驶场景中进行视角外推的有效解决方案。最近的方法侧重于使用扩散模型从给定视角生成偏移的新视角图像。然而，这些方法严重依赖于 LiDAR 点云、三维边界框和车道注释等先验信息，这需要昂贵的传感器或劳动密集型标注，限制了其在真实世界部署中的适用性。在本工作中，仅使用图像和可选的相机位姿，我们首先估计全局静态点云和每帧动态点云，并将它们融合成统一的表示。随后，我们采用可变形四维高斯框架重建场景。最初训练的四维高斯模型渲染出退化和伪图像，用于训练视频扩散模型。随后，逐步偏移的高斯渲染被扩散模型迭代细化，增强的结果再作为训练数据回馈给四维高斯模型。该过程持续进行，直至视角外推达到目标视角。与基线方法相比，我们的方法在新外推视角下生成的图像质量更高。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文旨在解决在自动驾驶场景中，仅使用图像（可选相机位姿）生成远距离视角的新颖图像的问题。该问题重要，因为逼真的闭环仿真和感知系统需要在大视角偏移下保持高质量渲染，而现有方法往往依赖昂贵的 LiDAR、3D 边框或车道标注，限制了实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了显式重建方法（NeRF、3D Gaussian Splatting）和扩散模型的优势，首先从图像中直接分离静态与动态点云并融合，随后使用 4D Gaussian 模型进行场景重建。接着训练一个以伪图像和动态掩码为条件的视频扩散模型，并在推理时通过扩散模型生成偏移视角，再将生成结果回馈给 Gaussian 模型进行迭代训练。该设计借鉴了 Drive‑Dreamer4D、ReconDreamer 等工作中的多条件扩散和逐步恢复策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将图像仅能获取的稠密点云估计与 4D Gaussian Splatting 结合，再利用条件视频扩散模型逐步恢复并生成偏移视角。实现流程为：①从图像估计静态与每帧动态点云；②用这些点云初始化 Gaussian 并渲染退化图像；③训练以伪图像和动态掩码为条件的视频扩散模型；④推理时使用扩散模型生成偏移视角图像；⑤将生成图像作为新训练数据重新训练 Gaussian，循环直到达到目标视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①仅使用图像的训练范式，去除了 LiDAR、3D 边框和车道标注等先验；②将静态与动态点云分离并融合，使用 4D Gaussian 进行动态建模；③在视频扩散模型中加入伪图像和动态掩码的多条件输入；④采用逐步恢复循环，将扩散生成的图像反馈给 Gaussian 进行迭代训练。与之前工作相比，它不依赖昂贵传感器或人工标注，并将显式重建与扩散生成有效结合，显著提升了大视角偏移下的图像质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种仅基于图像的管线，结合 4D Gaussian Splatting 与条件视频扩散模型，并通过迭代恢复实现高质量的驾驶视角外推，且不需要任何外部先验。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents an effective solution for view extrapolation in autonomous driving scenarios. Recent approaches focus on generating shifted novel view images from given viewpoints using diffusion models. However, these methods heavily rely on priors such as LiDAR point clouds, 3D bounding boxes, and lane annotations, which demand expensive sensors or labor-intensive labeling, limiting applicability in real-world deployment. In this work, with only images and optional camera poses, we first estimate a global static point cloud and per-frame dynamic point clouds, fusing them into a unified representation. We then employ a deformable 4D Gaussian framework to reconstruct the scene. The initially trained 4D Gaussian model renders degraded and pseudo-images to train a video diffusion model. Subsequently, progressively shifted Gaussian renderings are iteratively refined by the diffusion model,and the enhanced results are incorporated back as training data for 4DGS. This process continues until extrapolation reaches the target viewpoints. Compared with baselines, our method produces higher-quality images at novel extrapolated viewpoints.&lt;/p&gt;</description></item><item><guid>2512.24201v1</guid><title>BATISNet: Instance Segmentation of Tooth Point Clouds with Boundary Awareness</title><link>http://arxiv.org/abs/2512.24201v1</link><author>Yating Cai, Yanghui Xu, Zehua Hu, Jiazhou Chen, Jing Huang</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对牙齿点云的边界感知实例分割网络 BATISNet，旨在提高牙齿分割的准确性和鲁棒性，尤其在缺牙、牙位异常等复杂临床场景下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 牙齿点云分割对于诊断、临床辅助和治疗规划具有重要意义。现有方法多采用语义分割，关注不同牙齿类型的语义特征，但由于牙齿紧密排列、边界不清晰以及缺牙、牙位异常等复杂情况，语义分割往往难以获得满意结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决牙齿点云分割中因牙齿结构紧密、边界模糊和复杂病例导致的分割困难，提出一种能够同时学习语义特征和实例特征的网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BATISNet 由特征提取骨干网络和实例分割模块组成，能够提取不同牙齿类型的语义特征并学习单颗牙齿的实例特征；同时设计了边界感知损失函数，对实例间的边界进行专门监督，降低牙齿粘连和边界模糊。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，BATISNet 在牙齿完整性分割任务上优于现有方法，能够在缺牙、牙位异常等复杂场景下实现更稳健、更精确的分割，并有效缓解牙齿粘连和边界不清问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BATISNet 为牙齿点云分割提供了更可靠、细致的数据支持，可为临床诊断和治疗规划提供更有价值的参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 牙齿点云的准确分割对于诊断、临床辅助和治疗规划具有重要意义。现有方法大多采用语义分割，侧重于不同牙齿类型之间的语义特征。然而，由于牙齿紧密排列、边界不清晰以及缺牙、牙位异常等复杂病例的多样性，语义分割在处理复杂牙科案例时往往难以获得令人满意的结果。为解决这些问题，本文提出了 BATISNet，一种针对牙齿点云分割的边界感知实例网络。该网络模型由特征提取骨干网络和实例分割模块组成。它不仅关注提取不同牙齿类型的语义特征，还学习单颗牙齿的实例特征，从而在缺牙、牙位异常等复杂临床场景中实现更稳健、更精确的牙齿实例分割。此外，为进一步提升牙齿边界分割的完整性和准确性，本文设计了边界感知损失函数，专门监督实例间的边界分割，能够有效缓解牙齿粘连和边界模糊问题。大量实验结果表明，BATISNet 在牙齿完整性分割任务上优于现有方法，为实际临床应用提供了更可靠、更细致的数据支持。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决三维牙齿点云的实例分割问题，尤其是在缺牙、牙位异常或牙齿紧密排列等复杂临床场景下。准确的牙齿分割对于诊断、治疗规划和手术导航等口腔医学应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 PointMLP 的点云特征提取、U‑Net 的编码解码结构以及 2D 视觉中的 OneFormer、Mask2Former 的实例分割思路，构建了一个无候选框的实例分割框架，并加入了边界感知损失来解决牙齿边界模糊的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将牙齿分割视为实例分割任务，并通过边界感知损失提升边界精度。实现流程为：输入点云 → PointMLP‑U‑Net 提取局部与全局特征 → 通过可学习查询的 Transformer 解码器生成实例掩码、类别和置信度 → 非极大抑制得到最终实例掩码，并用边界损失进行细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①无候选框的牙齿实例分割网络 BATISNet；②全局‑局部特征聚合的 PointMLP‑U‑Net；③轻量化的边界感知损失；③端到端训练。与以往依赖语义标签或边框/中心点候选的语义/实例分割方法不同，BATISNet 能直接学习实例掩码并在复杂病例中保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BATISNet 提出了一种无候选框、边界感知的实例分割框架，能够在复杂牙齿点云中准确分离每颗牙齿，并在多项实验中优于现有方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate segmentation of the tooth point cloud is of great significance for diagnosis clinical assisting and treatment planning. Existing methods mostly employ semantic segmentation, focusing on the semantic feature between different types of teeth. However, due to the tightly packed structure of teeth, unclear boundaries, and the diversity of complex cases such as missing teeth, malposed teeth, semantic segmentation often struggles to achieve satisfactory results when dealing with complex dental cases. To address these issues, this paper propose BATISNet, a boundary-aware instance network for tooth point cloud segmentation. This network model consists of a feature extraction backbone and an instance segmentation module. It not only focuses on extracting the semantic features of different types of teeth but also learns the instance features of individual teeth. It helps achieve more robust and accurate tooth instance segmentation in complex clinical scenarios such as missing teeth and malposed teeth. Additionally, to further enhance the completeness and accuracy of tooth boundary segmentation, a boundary-aware loss function is designed to specifically supervise the boundary segmentation between instances. It mitigates effectively tooth adhesion and boundary ambiguity issues. Extensive experimental results show that BATISNet outperforms existing methods in tooth integrity segmentation, providing more reliable and detailed data support for practical clinical applications.&lt;/p&gt;</description></item><item><guid>2512.24212v1</guid><title>RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation</title><link>http://arxiv.org/abs/2512.24212v1</link><author>Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu</author><pubDate>Fri, 02 Jan 2026 20:28:50 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 RANGER 框架，利用单目摄像头实现零样本、开放词汇语义导航，克服了传统方法对深度和姿态信息的依赖，并具备强大的上下文学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在复杂环境中高效寻找目标是现实世界机器人应用的核心需求。多模态基础模型的进步使得零样本目标导航成为可能，但现有方法仍受限于对精确深度和姿态的依赖以及缺乏上下文学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统零样本导航方法对深度/姿态的高度依赖以及缺乏快速适应新环境的上下文学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; RANGER 通过关键帧三维重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应航路点选择以及低层动作执行，实现仅用单目摄像头完成目标导航，并通过观看短视频快速适应新环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 HM3D 基准和真实环境实验中，RANGER 在导航成功率和探索效率方面与现有方法竞争，并在无需预先 3D 地图的情况下表现出更优的上下文学习适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RANGER 展示了在不依赖深度和姿态信息的情况下，利用单目摄像头实现零样本、开放词汇导航的可行性，并通过短视频实现快速环境适应，具有显著的实用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要：高效地在复杂环境中寻找目标是现实世界具身应用的基础。最近多模态基础模型的进展使得零样本目标导航成为可能，允许机器人在无需微调的情况下搜索任意对象。然而现有方法存在两个主要限制：（1）过度依赖模拟器提供的精确深度和姿态信息，限制了其在真实世界中的适用性；（2）缺乏上下文学习（ICL）能力，使得难以快速适应新环境，例如利用短视频。为了解决这些挑战，我们提出了 RANGER，一种新颖的零样本、开放词汇语义导航框架，仅使用单目摄像头即可运行。借助强大的三维基础模型，RANGER 消除了对深度和姿态的依赖，并展现出强大的上下文学习能力。仅通过观察新环境的短视频，系统即可显著提升任务效率，而无需架构修改或微调。该框架集成了关键帧三维重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应航路点选择以及低层动作执行等关键组件。在 HM3D 基准和真实环境实验中，RANGER 在导航成功率和探索效率方面表现出竞争力，同时显示出优越的上下文学习适应性，且不需要先前的 3D 环境映射。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现仅使用单目 RGB 摄像头的零样本语义导航，消除对深度传感器和精确位姿的依赖，并通过短视频实现快速适应。此问题在实际机器人部署中至关重要，因为它降低了硬件成本、提升了部署灵活性，并能在未知环境中高效定位目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了多模态基础模型、VLM、LLM 以及现有的单目 3D 重建技术（如 MASt3R-SLAM、Grounding DINO、Mobile SAM、CLIP），并在此基础上构建了统一的关键帧记忆库。设计思路借鉴了前沿的零样本导航、语义地图构建和上下文学习方法，但将它们整合为一个无需微调、可实时运行的系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过关键帧记忆库同时存储几何、语义和探索价值信息，利用单目 RGB 实时重建、语义点云融合、2D 地图投影和 VLM 驱动的高层规划来实现导航。实现流程包括：接收 RGB 观测 → MASt3R-SLAM 进行 3D 重建与位姿估计 → Grounding DINO+Mobile SAM+CLIP 生成语义点云 → 投影为 2D 障碍/前沿/价值地图 → 高层规划器选取 waypoint → 低层控制器执行离散动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 仅使用 RGB 进行零样本语义导航；2) 通过短视频实现无微调的上下文学习；3) 关键帧记忆库整合几何、语义与价值；4) VLM 驱动的探索价值评估；5) 兼顾在线重建与离线视频适应。与以往依赖深度、精确位姿或全局地图、需要微调的零样本方法不同，RANGER 在硬件成本、适应性和实时性上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RANGER 通过单目 RGB、关键帧记忆库和 VLM 驱动的规划，实现了无需深度传感器、位姿或微调的零样本语义导航，并能通过短视频快速适应新环境。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.&lt;/p&gt;</description></item><item><guid>2512.24271v1</guid><title>Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</title><link>http://arxiv.org/abs/2512.24271v1</link><author>Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展，但它们存在一个关键漏洞：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。由于文本和视频之间固有的数据不平衡，这一限制很难解决，因为收集和标注反事实数据的成本很高。为了解决这个问题，我们引入了DualityForge，这是一个新颖的反事实数据合成框架，它采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，我们构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，为了充分利用我们配对数据的对比性质，我们提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。我们将开源我们的数据集和代码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展，但它们存在一个关键漏洞：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决多模态大语言模型在处理反事实视频时存在的视觉无根据的幻觉问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入DualityForge，这是一个新颖的反事实数据合成框架，采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入DualityForge和DualityVidQA数据集，以及Duality-Normalized Advantage Training (DNA-Train)训练机制，我们有效地减少了多模态大语言模型在处理反事实视频时的幻觉问题，并展示了强大的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展。然而，它们存在一个关键问题：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。由于文本和视频之间固有的数据不平衡，这一限制很难解决，因为收集和标注反事实数据的成本很高。为了解决这个问题，我们引入了DualityForge，这是一个新颖的反事实数据合成框架，它采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，我们构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，为了充分利用我们配对数据的对比性质，我们提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。我们将开源我们的数据集和代码。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型（MLLMs）在视频理解中存在的过度依赖语言先验的问题，导致在处理反事实视频时出现视觉无根据的幻觉。这个问题在现实研究中很重要，因为MLLMs在视频理解中的应用越来越广泛，但幻觉问题限制了它们的准确性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，如利用AI生成内容（AIGC）和视觉强化学习，设计出了一种新的数据合成框架DualityForge，通过可控的视频编辑将真实视频转换为反事实场景，并构建了DualityVidQA数据集。作者也借鉴了现有的视频理解数据集和视觉强化学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过嵌入结构化上下文信息到视频编辑和问答生成过程中，自动产生高质量的反事实视频和问答对，从而提高MLLMs的视频理解能力。整体实现流程包括使用DualityForge框架生成反事实视频，然后使用这些视频和原始视频构建对比性问答对，最后通过Duality-Normalized Advantage Training（DNA-Train）方法进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括DualityForge框架，它利用可控的视频编辑生成反事实场景，并嵌入结构化上下文信息；DualityVidQA数据集，它包含大量反事实视频和问答对；以及DNA-Train训练方法，它通过对比性问答训练提高MLLMs的视频理解能力。相比之前的工作，这些创新点提供了更系统、更自动化的方法来减少MLLMs的幻觉问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个通过反事实视频生成和对比性问答训练来提高MLLMs视频理解能力的新方法，有效减少了模型在反事实视频上的幻觉问题。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.&lt;/p&gt;</description></item><item><guid>2512.24323v1</guid><title>Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention</title><link>http://arxiv.org/abs/2512.24323v1</link><author>Haijing Liu, Zhiyuan Song, Hefeng Wu, Tao Pu, Keze Wang, Liang Lin</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Ego-RVOS旨在从第一人称视频中根据语言查询分割出积极参与人类动作的特定对象，这对理解人类行为至关重要。然而，由于第一人称视频中的模糊性和训练数据中的偏差，实现鲁棒的分割具有挑战性。现有的方法常常从数据集中学习虚假的相关性，并受到第一人称视角的基本视觉混淆因素的影响，如快速运动和频繁的遮挡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Ego-RVOS任务对于理解人类行为非常重要，但由于第一人称视频的模糊性和训练数据的偏差，实现鲁棒的分割具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决这些限制，引入了Causal Ego-REferring Segmentation (CERES)，这是一个插件式因果框架，将强预训练的RVOS骨干网络适应到第一人称领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CERES实现了双模态因果干预：应用后门调整原则来抵消从数据集统计中学习到的语言表示偏差，并利用前门调整概念来解决视觉混淆，通过智能地整合语义视觉特征与几何深度信息，创建对第一人称扭曲更鲁棒的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CERES在Ego-RVOS基准上实现了最先进的性能，突出了应用因果推理构建更可靠模型以实现更广泛的第一人称视频理解的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CERES通过因果推理方法提高了第一人称视频中对象分割的鲁棒性，展示了其在第一人称视频理解中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Ego-RVOS旨在从第一人称视频中根据语言查询分割出积极参与人类动作的特定对象，这对理解人类行为至关重要。然而，由于第一人称视频中的模糊性和训练数据中的偏差，实现鲁棒的分割具有挑战性。现有的方法常常从数据集中学习虚假的相关性，并受到第一人称视角的基本视觉混淆因素的影响，如快速运动和频繁的遮挡。为了解决这些限制，引入了Causal Ego-REferring Segmentation (CERES)，这是一个插件式因果框架，将强预训练的RVOS骨干网络适应到第一人称领域。CERES实现了双模态因果干预：应用后门调整原则来抵消从数据集统计中学习到的语言表示偏差，并利用前门调整概念来解决视觉混淆，通过智能地整合语义视觉特征与几何深度信息，创建对第一人称扭曲更鲁棒的表示。CERES在Ego-RVOS基准上实现了最先进的性能，突出了应用因果推理构建更可靠模型以实现更广泛的第一人称视频理解的潜力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是在第一人称视频中，根据语言查询分割出特定参与人类动作的对象的问题。这个问题在现实或研究中非常重要，因为它有助于机器更深入地理解人类行为和交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的不足，特别是数据集偏差和视觉混淆因素，设计了CERES框架。这个方法借鉴了因果推理的原则，并参考了现有的RVOS和因果推理相关工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是使用双模态因果干预来减少语言和视觉偏差。整体实现流程包括使用后门调整来处理语言偏差，使用前门调整和视觉-深度中介来处理视觉偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用因果推理原则来解决Ego-RVOS中的鲁棒性问题，使用后门调整来减少语言偏差，以及使用前门调整和视觉-深度中介来减少视觉偏差。与之前的工作相比，这种方法更全面地考虑了语言和视觉偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于因果推理的Ego-RVOS框架，通过双模态因果干预提高了模型的鲁棒性和泛化能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.&lt;/p&gt;</description></item><item><guid>2512.24327v1</guid><title>Topological Spatial Graph Coarsening</title><link>http://arxiv.org/abs/2512.24327v1</link><author>Anna Calissano, Etienne Lasalle</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了空间图的简化问题，旨在找到一个小型的空间图，同时保留初始图的整体结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 空间图是节点在空间中局部化的图，例如公共交通网络、分子和分支生物结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在空间图中进行图简化，同时保留初始图的主要拓扑特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出了一种基于新框架的拓扑空间图粗化方法，该方法通过折叠短边来实现粗化，并适应了经典拓扑描述符（持久图）的构建，以捕获拓扑信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法是无参数的，并且对初始空间图的旋转、平移和缩放具有等变性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在合成和真实空间图上评估了性能，并显示它显著减少了图的大小，同时保留了相关的拓扑信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究了空间图的简化问题，旨在找到一个小型的空间图，同时保留初始图的整体结构。空间图是节点在空间中局部化的图，例如公共交通网络、分子和分支生物结构。在空间图中进行图简化，同时保留初始图的主要拓扑特征。提出了一种基于新框架的拓扑空间图粗化方法，该方法通过折叠短边来实现粗化，并适应了经典拓扑描述符（持久图）的构建，以捕获拓扑信息。该方法是无参数的，并且对初始空间图的旋转、平移和缩放具有等变性。该方法在合成和真实空间图上评估了性能，并显示它显著减少了图的大小，同时保留了相关的拓扑信息。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空间图简化的问题，即如何在减少节点数量的同时保留空间图的主要拓扑特征。这个问题在现实研究中非常重要，因为空间图通常包含大量节点和边，简化它们可以帮助更好地理解和分析复杂的数据结构，如交通网络、分子结构等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴拓扑数据分析（TDA）中的工具，特别是持久图和三角感知图过滤，来设计这个方法。他们考虑了现有工作在图数据上的应用，并提出了一个新的过滤方法来适应空间图数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过合并短边来简化空间图，同时保留其主要的拓扑特征。整体实现流程包括：定义一个参数θ来控制合并的边长阈值，根据这个参数将图中的节点合并成超节点，并重新计算超节点的位置，最后通过比较原始图和简化图的持久图来选择合适的简化程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出了一种新的三角感知图过滤方法来适应空间图数据，并定义了一个基于持久图的评分系统来指导简化程度的选择。与之前的工作相比，这个方法更注重在简化图的同时保留空间图的主要拓扑特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于拓扑数据分析的空间图简化方法，能够在减少节点数量的同时保留空间图的主要拓扑特征。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatial graphs are particular graphs for which the nodes are localized in space (e.g., public transport network, molecules, branching biological structures). In this work, we consider the problem of spatial graph reduction, that aims to find a smaller spatial graph (i.e., with less nodes) with the same overall structure as the initial one. In this context, performing the graph reduction while preserving the main topological features of the initial graph is particularly relevant, due to the additional spatial information. Thus, we propose a topological spatial graph coarsening approach based on a new framework that finds a trade-off between the graph reduction and the preservation of the topological characteristics. The coarsening is realized by collapsing short edges. In order to capture the topological information required to calibrate the reduction level, we adapt the construction of classical topological descriptors made for point clouds (the so-called persistent diagrams) to spatial graphs. This construction relies on the introduction of a new filtration called triangle-aware graph filtration. Our coarsening approach is parameter-free and we prove that it is equivariant under rotations, translations and scaling of the initial spatial graph. We evaluate the performances of our method on synthetic and real spatial graphs, and show that it significantly reduces the graph sizes while preserving the relevant topological information.&lt;/p&gt;</description></item><item><guid>2512.24331v1</guid><title>Spatial-aware Vision Language Model for Autonomous Driving</title><link>http://arxiv.org/abs/2512.24331v1</link><author>Weijie Wei, Zhipeng Luo, Ling Feng, Venice Erin Liong</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中展现出巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。为了解决这个问题，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。实验表明，LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中具有巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决VLMs在自动驾驶中的局限性，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 我们的工作强调了在构建可信的基于VLMs的自动驾驶系统中，明确3D度量数据的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中展现出巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。为了解决这个问题，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。实验表明，LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。我们的工作强调了在构建可信的基于VLMs的自动驾驶系统中，明确3D度量数据的重要性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉语言模型（VLMs）在自动驾驶场景中缺乏精确的3D空间理解能力的问题。这个问题在现实研究中非常重要，因为准确的3D空间理解是自动驾驶安全规划和决策的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有VLMs和LiDAR技术的优势，设计了一个名为LVLDrive的框架，该框架结合了图像和LiDAR点云数据，并引入了Gradual Fusion Q-Former来逐步融合LiDAR特征，同时保持VLMs的现有知识。这个设计借鉴了OmniDrive中的Q-Former 3D块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过Gradual Fusion Q-Former逐步融合LiDAR点云特征，以增强VLMs的3D空间理解能力。整体实现流程包括使用三个预训练编码器处理文本、图像和点云数据，然后通过Gradual Fusion Q-Former融合这些数据，最后通过语言模型生成任务特定的响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括引入Gradual Fusion Q-Former来逐步融合LiDAR特征，以及构建一个空间感知的视觉问答（SA-QA）数据集来增强模型的3D空间推理能力。与之前的工作相比，LVLDrive在融合LiDAR和图像数据方面更加稳定，并且通过SA-QA数据集提供了更精确的空间理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入Gradual Fusion Q-Former和SA-QA数据集，显著提升了视觉语言模型在自动驾驶场景中的3D空间理解能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&amp;#x27;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.&lt;/p&gt;</description></item><item><guid>2512.24373v1</guid><title>Skim-Aware Contrastive Learning for Efficient Document Representation</title><link>http://arxiv.org/abs/2512.24373v1</link><author>Waheed Ahmed Abro, Zied Bouraoui</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; transformer-based models 在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。相比之下，人类通常通过浏览文本，专注于重要部分来理解整体信息。基于这种人类策略，我们引入了一种新的自监督对比学习框架，以增强长文档的表示。我们的方法随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。这模拟了人类综合信息的方式，从而产生了更丰富且计算效率更高的表示。在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然基于 transformer 的模型在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入一种新的自监督对比学习框架，随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过模拟人类综合信息的方式，产生了更丰富且计算效率更高的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然基于 transformer 的模型在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。相比之下，人类通常通过浏览文本，专注于重要部分来理解整体信息。基于这种人类策略，我们引入了一种新的自监督对比学习框架，以增强长文档的表示。我们的方法随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。这模拟了人类综合信息的方式，从而产生了更丰富且计算效率更高的表示。在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决长文档的有效表示问题，特别是在法律和医学领域。这个问题重要，因为现有的方法在处理长文档时效率低且难以捕捉完整文档的上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了人类浏览文本的策略，设计了一种自监督对比学习框架。他们使用了现有的层次化 transformer 模型和 Longformer 模型，并提出了一个新的块预测编码器（CPE）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习来增强长文档的表示。整体流程包括随机遮蔽文档的一部分，并使用基于自然语言推理的对比目标来对齐相关部分，同时与不相关部分保持距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括提出块预测编码器（CPE），使用自监督对比学习来增强文档表示，以及利用自然语言推理（NLI）来模拟人类浏览文本的过程。与之前的工作相比，这种方法更注重捕捉文档中不同部分之间的关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的自监督对比学习框架，通过模拟人类浏览文本的策略来增强长文档的表示，从而在法律和医学领域实现了更高的准确性和效率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.&lt;/p&gt;</description></item><item><guid>2512.24384v1</guid><title>Geometric Multi-Session Map Merging with Learned Local Descriptors</title><link>http://arxiv.org/abs/2512.24384v1</link><author>Yanlong Ma, Nakul S. Joshi, Christa S. Robison, Philip R. Osteen, Brett T. Lopez</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于学习的局部描述符框架GMLD，用于大规模多会话点云地图合并，该框架能够系统地对跨不同会话收集的地图进行对齐，特别是在重叠区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多会话地图合并对于大规模环境中的自主操作至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够有效合并多会话地图的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用关键点感知编码器和基于平面的几何转换器提取判别性特征，用于回环检测和相对位姿估计，并在因子图优化阶段加入会话间扫描匹配成本因素以增强全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在公开数据集和自收集数据上评估，结果显示出准确且鲁棒的地图合并，误差低，学习到的特征在回环检测和相对位姿估计中表现出色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GMLD框架能够有效地进行大规模多会话点云地图合并。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种基于学习的局部描述符框架GMLD，用于大规模多会话点云地图合并，该框架能够系统地对跨不同会话收集的地图进行对齐，特别是在重叠区域。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多会话地图合并的问题，即在大型环境中进行扩展的自主操作时，如何将不同会话收集的地图在重叠区域进行系统地对齐。这个问题在现实或研究中非常重要，因为准确的地图合并能够帮助自主系统在未知环境中进行有效的规划和导航。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的地图合并、位置识别和注册以及位姿图优化工作，设计出了一种基于学习的局部描述符框架。该方法结合了关键点感知编码器和平面几何转换器，以提取具有判别性的特征，用于回环检测和相对位姿估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用学习到的局部描述符来提高地图合并的准确性和鲁棒性。整体实现流程包括三个主要模块：关键点和描述符生成、回环检测和注册、以及地图合并。首先，从密集点云中提取关键点和局部描述符；然后，通过计算局部描述符之间的距离来检测潜在的回环，并估计相对变换；最后，通过位姿图优化来合并地图，确保全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：关键点感知的局部描述符生成、基于平面的几何转换器编码器、以及扫描匹配成本感知的会话间位姿图优化。相比之前的工作，这些创新点提高了地图合并的准确性和鲁棒性，并能够更好地处理大型环境中的复杂情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于学习的局部描述符框架，能够有效地进行多会话地图合并，提高地图的准确性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.&lt;/p&gt;</description></item><item><guid>2512.24385v1</guid><title>Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems</title><link>http://arxiv.org/abs/2512.24385v1</link><author>Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了如何从多模态车载传感器数据中构建真正的空间智能，特别是在自动驾驶车辆和无人机等自主系统中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着自主系统的快速发展，从多模态车载传感器数据中整合能力以创建统一理解的需求日益增加，但这一挑战依然存在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文旨在提出一个全面的框架，用于多模态预训练，并识别推动该目标实现的核心技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文分析了基础传感器特征与学习策略之间的相互作用，评估了平台特定数据集在这些进步中的作用，并提出了一个统一的预训练范式分类法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 本文提出了一个统一的预训练范式分类法，从单模态基线到复杂的统一框架，这些框架学习整体表示以用于高级任务，如3D物体检测和语义占用预测。此外，本文还研究了文本输入和占用表示的集成，以促进开放世界的感知和规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文确定了关键瓶颈，如计算效率和模型可扩展性，并提出了一个路线图，以实现通用多模态基础模型，这些模型能够实现稳健的空间智能，适用于实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文探讨了如何从多模态车载传感器数据中构建真正的空间智能，特别是在自动驾驶车辆和无人机等自主系统中的应用。随着自主系统的快速发展，从多模态车载传感器数据中整合能力以创建统一理解的需求日益增加，但这一挑战依然存在。本文旨在提出一个全面的框架，用于多模态预训练，并识别推动该目标实现的核心技术。本文分析了基础传感器特征与学习策略之间的相互作用，评估了平台特定数据集在这些进步中的作用，并提出了一个统一的预训练范式分类法。本文提出了一个统一的预训练范式分类法，从单模态基线到复杂的统一框架，这些框架学习整体表示以用于高级任务，如3D物体检测和语义占用预测。此外，本文还研究了文本输入和占用表示的集成，以促进开放世界的感知和规划。本文确定了关键瓶颈，如计算效率和模型可扩展性，并提出了一个路线图，以实现通用多模态基础模型，这些模型能够实现稳健的空间智能，适用于实际部署。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从多模态传感器数据中训练出具有空间智能的自主系统。这个问题在现实或研究中非常重要，因为自主系统（如自动驾驶汽车和无人机）需要准确感知和理解环境，才能安全有效地运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有单模态和跨模态学习方法，以及基础模型在多模态场景中的应用，设计了多模态数据预训练框架。他们借鉴了现有工作，特别是自监督学习、跨模态交互和知识蒸馏等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态预训练，从不同传感器（如摄像头和LiDAR）中提取统一的表示，以实现空间智能。整体流程包括单模态预训练、跨模态交互和统一框架预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括提出统一的多模态预训练框架，以及将文本输入和占用表示整合到预训练中。与之前的工作相比，这篇论文更全面地分析了不同模态和平台的数据集，以及预训练方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文为从多模态传感器数据中训练具有空间智能的自主系统提供了一个全面的框架和路线图。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.&lt;/p&gt;</description></item><item><guid>2512.24404v1</guid><title>Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning</title><link>http://arxiv.org/abs/2512.24404v1</link><author>Soham Pahari, M. Srinivas</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多模态智能在视觉理解和高级推理方面取得了显著进展，但大多数推理系统仍依赖文本信息进行推断，这在空间任务（如视觉导航和地理定位）中限制了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大多数推理系统依赖文本信息，这在空间任务中限制了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 讨论该领域的潜在范围，并提出一个视觉推理范式Geo-Consistent Visual Planning，即ViReLoc框架，该框架仅使用视觉表示进行规划和定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ViReLoc学习空间依赖和几何关系，通过在视觉域中编码逐步推理并使用基于强化优化的目标进行规划，同时整合对比学习和自适应特征交互来对齐跨视图视角并减少视角差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验在不同导航和定位场景中显示出空间推理准确性和跨视图检索性能的持续改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 视觉推理作为导航和定位的强大补充方法，表明这些任务可以在没有实时全球定位系统数据的情况下完成，从而实现更安全的导航解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态智能在视觉理解和高级推理方面取得了显著进展，但大多数推理系统仍依赖文本信息进行推断，这在空间任务（如视觉导航和地理定位）中限制了其有效性。本文讨论了该领域的潜在范围，并提出一个视觉推理范式Geo-Consistent Visual Planning，即ViReLoc框架，该框架仅使用视觉表示进行规划和定位。ViReLoc学习空间依赖和几何关系，通过在视觉域中编码逐步推理并使用基于强化优化的目标进行规划，同时整合对比学习和自适应特征交互来对齐跨视图视角并减少视角差异。实验在不同导航和定位场景中显示出空间推理准确性和跨视图检索性能的持续改进。这些结果表明视觉推理作为导航和定位的强大补充方法，表明这些任务可以在没有实时全球定位系统数据的情况下完成，从而实现更安全的导航解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决地面到空中的定位问题，通过视觉推理引导规划。这个问题在现实研究中很重要，因为它可以实现无需GPS的导航，提高导航的安全性和隐私性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过结合视觉推理和强化学习来设计这个方法，借鉴了现有工作中的特征提取、对比学习和强化学习等技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过视觉推理引导规划，整体实现流程包括构建画布、交叉视图地理定位和通过强化学习的视觉规划三个阶段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括统一的架构、视觉推理模块和可微分的规划系统。相比之前的工作，这个方法能够同时进行检索和规划，并且通过视觉推理来引导规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过视觉推理引导规划的方法，实现了地面到空中的定位，为无需GPS的导航提供了一种新的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.&lt;/p&gt;</description></item><item><guid>2512.24428v1</guid><title>Subsecond 3D Mesh Generation for Robot Manipulation</title><link>http://arxiv.org/abs/2512.24428v1</link><author>Qian Wang, Omar Abdellall, Tony Gao, Xiatao Sun, Daniel Rakita</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D网格是计算机科学和工程中的一种基本表示形式，在机器人学中尤其有价值，因为它们能够以与机器人如何与物理世界交互一致的方式捕捉物体，从而实现预测稳定抓取、检测碰撞和模拟动力学等核心功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 尽管自动3D网格生成方法近年来取得了显著进展，但仍然存在两个关键挑战：生成高保真网格的速度过慢，难以满足实时应用需求，通常每个物体需要数十秒；仅生成网格本身是不够的，在机器人学中，网格必须被正确地从场景中分割并注册到适当的尺度和姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 引入一个端到端系统，解决上述挑战，从单个RGB-D图像中在不到一秒的时间内生成高质量的、具有上下文信息的3D网格。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 集成开放词汇对象分割、加速的基于扩散的网格生成和鲁棒的点云注册，每个步骤都针对速度和准确性进行了优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在现实世界的操作任务中展示了其有效性，表明它能够使网格成为机器人感知和规划的实用按需表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该系统有效地解决了3D网格生成中的实时性和上下文信息问题，为机器人学提供了实用的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D网格是计算机科学和工程中的一种基本表示形式，在机器人学中尤其有价值，因为它们能够以与机器人如何与物理世界交互一致的方式捕捉物体，从而实现预测稳定抓取、检测碰撞和模拟动力学等核心功能。尽管自动3D网格生成方法近年来取得了显著进展，但仍然存在两个关键挑战：生成高保真网格的速度过慢，难以满足实时应用需求，通常每个物体需要数十秒；仅生成网格本身是不够的，在机器人学中，网格必须被正确地从场景中分割并注册到适当的尺度和姿态。引入一个端到端系统，解决上述挑战，从单个RGB-D图像中在不到一秒的时间内生成高质量的、具有上下文信息的3D网格。集成开放词汇对象分割、加速的基于扩散的网格生成和鲁棒的点云注册，每个步骤都针对速度和准确性进行了优化。在现实世界的操作任务中展示了其有效性，表明它能够使网格成为机器人感知和规划的实用按需表示。该系统有效地解决了3D网格生成中的实时性和上下文信息问题，为机器人学提供了实用的解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D网格生成在机器人操作中的实时性问题。在现实和研究中，这个问题非常重要，因为3D网格能够帮助机器人更好地理解和交互物理世界，实现抓取、避障和动力学模拟等关键任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合现有技术并加以改进来设计这个方法。他们借鉴了开放词汇对象分割、加速扩散模型和鲁棒点云注册等技术，并对这些技术进行了优化，以实现快速且准确的3D网格生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将开放词汇分割、加速网格生成和鲁棒点云注册三个步骤紧密集成，以实现快速且准确的3D网格生成。整体流程包括：首先使用 Florence-2 和 SAM2 进行开放词汇分割；然后使用 FlashVDM 加速的 Hunyuan3D 2.0 生成高保真网格；最后通过 RANSAC 和 ICP 进行点云注册，对齐网格与观测点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：使用 FlashVDM 加速扩散模型生成网格，减少生成时间；采用分层 SDF 解码和自适应键值选择技术进一步加速推理；结合 Florence-2 和 SAM2 实现开放词汇分割；使用 RANSAC 和 ICP 进行点云注册，无需纹理网格。这些创新点使得该方法在速度和准确性上都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种快速、准确的3D网格生成方法，通过整合开放词汇分割、加速扩散模型和鲁棒点云注册，实现了亚秒级的网格生成，为实时机器人应用提供了新的可能性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.&lt;/p&gt;</description></item><item><guid>2512.24438v1</guid><title>Exploring Compositionality in Vision Transformers using Wavelet Representations</title><link>http://arxiv.org/abs/2512.24438v1</link><author>Akshad Shyam Purushottamdas, Pranav K Nayak, Divya Mehul Rajparia, Deekshith Patel, Yashmitha Gogineni, Konda Reddy Mopuri, Sumohana S. Channappayya</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究通过分析视觉Transformer（ViT）编码器学习到的表示，探讨了其组合性。研究引入了一个类似于先前用于测量表示学习中组合性的框架，以测试ViT编码器的组合性。研究使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 先前对Transformer模型的工作原理的了解主要通过对它们在语言任务上的行为进行分析而获得。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本研究旨在通过组合性的视角，调查ViT编码器学习到的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 研究引入了一个类似于先前用于测量表示学习中组合性的框架，并使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 来自一阶DWT分解的基元在潜在空间中近似地组合，产生了编码器表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ViT编码器在表示空间中尊重组合性，提供了一种新的视角来理解ViT如何结构化信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究通过分析视觉Transformer（ViT）编码器学习到的表示，探讨了其组合性。研究引入了一个类似于先前用于测量表示学习中组合性的框架，以测试ViT编码器的组合性。研究使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想解决如何测试视觉Transformer（ViT）编码器表示的组成性。这个问题在现实或研究中很重要，因为理解ViT如何学习和组织信息可以帮助提高模型的解释性和性能，特别是在计算机视觉领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作中关于表示学习的组成性框架，并使用离散小波变换（DWT）来生成图像的基集（输入特定的原始元素）。这个方法的设计思路是将图像分解为视觉上有意义的原始元素，并检查这些原始元素在ViT编码器中的表示是否具有组成性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是使用DWT将图像分解为原始元素，并检查这些原始元素在ViT编码器中的表示是否可以重新组合以近似原始图像的表示。整体实现流程包括使用DWT分解图像，提取原始元素，然后检查这些原始元素在ViT编码器中的表示是否具有组成性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用DWT来分析ViT编码器的组成性，以及提出一个框架来测试ViT编码器表示的组成性。与之前的工作相比，这篇论文首次将DWT应用于ViT编码器的组成性分析，并提供了实证结果来支持这一方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过使用离散小波变换来分析视觉Transformer编码器的组成性，提供了一种新的视角来理解ViT如何结构化信息。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While insights into the workings of the transformer model have largely emerged by analysing their behaviour on language tasks, this work investigates the representations learnt by the Vision Transformer (ViT) encoder through the lens of compositionality. We introduce a framework, analogous to prior work on measuring compositionality in representation learning, to test for compositionality in the ViT encoder. Crucial to drawing this analogy is the Discrete Wavelet Transform (DWT), which is a simple yet effective tool for obtaining input-dependent primitives in the vision setting. By examining the ability of composed representations to reproduce original image representations, we empirically test the extent to which compositionality is respected in the representation space. Our findings show that primitives from a one-level DWT decomposition produce encoder representations that approximately compose in latent space, offering a new perspective on how ViTs structure information.&lt;/p&gt;</description></item><item><guid>2512.24470v1</guid><title>Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</title><link>http://arxiv.org/abs/2512.24470v1</link><author>Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavoned, Martin Steinert</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 摘要介绍了IMO MASS Code对自主和远程监督的海上船舶的要求，包括检测操作设计域的偏离、进入预定义的回退模式通知操作员、允许立即人工覆盖以及未经批准不更改航行计划。摘要还讨论了如何在这些要求中实现快速、可人工覆盖的回退操作，并介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的海上自主系统在正确操作依赖于语义理解的情况下（例如，潜水下降标志意味着有人在水中，火势附近意味着危险）难以应对。摘要指出，视觉语言模型（VLMs）可以提供语义意识，以应对这些分布外的情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 摘要旨在展示视觉语言模型（VLMs）如何提供语义意识，以及如何通过快速-慢速异常管道和短视距、可人工覆盖的回退操作，使IMO MASS Code的要求在实际操作中可行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 摘要介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器，能够在持续的人工授权下从水有效、世界锚定的轨迹中选择一个谨慎的操作（或保持停泊）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 摘要通过在40个港口场景中测量每调用场景理解、延迟、与人类共识的一致性（模型多数投票）、火灾危险场景的短视距风险缓解以及水上警报-&amp;gt;回退操作-&amp;gt;操作员接管，验证了Semantic Lookout模型的有效性。结果表明，亚10秒的模型保留了大多数较慢的先进模型的意识，回退操作选择器优于仅基于几何学的基线，并在火灾场景中增加了安全距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 摘要支持视觉语言模型作为语义回退操作选择器，符合IMO MASS Code的要求，并在实际延迟预算内有效。摘要还鼓励未来研究在领域适应的混合自主系统中，将基础模型的语义与多传感器鸟瞰感知和短视距重规划相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要介绍了IMO MASS Code对自主和远程监督的海上船舶的要求，包括检测操作设计域的偏离、进入预定义的回退模式通知操作员、允许立即人工覆盖以及未经批准不更改航行计划。摘要还讨论了如何在这些要求中实现快速、可人工覆盖的回退操作，并介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自主船舶在遇到需要语义理解的异常情况时，如何进行安全、有效的Fallback操作的问题。这个问题在现实中非常重要，因为自主船舶在遇到传统几何方法无法处理的情况（如旗帜、火灾等）时，需要能够安全地应对并等待人类接管，以确保航行安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，特别是视觉-语言模型（VLMs）和快速-慢速异常检测管道，设计出了一种基于VLM的Fallback操作选择器。该方法借鉴了Sinha等人的工作，并结合了IMO MASS Code的要求，设计出了一种短时程、可被人类覆盖的Fallback操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用VLM提供语义理解能力，选择一个谨慎的Fallback操作，直到人类接管。整体实现流程包括：使用摄像头获取图像，通过VLM理解图像内容，选择一个安全的轨迹，并在人类接管前保持船舶安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：使用VLM进行语义理解，选择一个谨慎的Fallback操作，并确保人类可以立即接管。相比之前的工作，这个方法更加注重语义理解，并且能够在短时程内做出反应，同时保持人类控制权。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于VLM的Fallback操作选择器，能够在自主船舶遇到需要语义理解的异常情况时，安全、有效地进行Fallback操作，并确保人类可以立即接管。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert-&amp;gt;fallback maneuver-&amp;gt;operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird&amp;#x27;s-eye-view perception and short-horizon replanning.&lt;/p&gt;</description></item><item><guid>2512.24504v1</guid><title>Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments</title><link>http://arxiv.org/abs/2512.24504v1</link><author>Zhiwei Wei, Yuxing Liu, Hua Liao, Wenjia Xu</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了基础模型（FM）代理在符号地图环境中的探索、记忆和推理能力，并提出了一个交互式评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地图环境是表示空间结构的基本媒介，理解基础模型代理如何理解和在地图环境中行动对于实现可靠的基于地图的推理和应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文旨在分析基础模型代理如何在符号地图环境中探索、记忆和推理，并揭示不同组件的功能角色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文提出了一种交互式评估框架，代理在部分可观察的基于网格的地图上进行增量式探索，地图包含道路、交叉口和兴趣点（POI），代理在每个步骤只接收局部观察。使用六种空间任务来评估空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 探索主要影响经验获取，但对最终推理准确性的影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是顺序和基于图的表示）显著提高路径规划等结构密集型任务的性能；推理方案进一步塑造存储的空间知识的使用方式，高级提示支持更有效的多步推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 空间推理性能在模型版本和规模超过一定能力阈值后会饱和，表明改进基于地图的空间理解需要针对空间表示和推理的机制，而不仅仅是规模扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究了基础模型（FM）代理在符号地图环境中的探索、记忆和推理能力，并提出了一个交互式评估框架。地图环境是表示空间结构的基本媒介，理解基础模型代理如何理解和在地图环境中行动对于实现可靠的基于地图的推理和应用至关重要。本文旨在分析基础模型代理如何在符号地图环境中探索、记忆和推理，并揭示不同组件的功能角色。本文提出了一种交互式评估框架，代理在部分可观察的基于网格的地图上进行增量式探索，地图包含道路、交叉口和兴趣点（POI），代理在每个步骤只接收局部观察。使用六种空间任务来评估空间理解能力。探索主要影响经验获取，但对最终推理准确性的影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是顺序和基于图的表示）显著提高路径规划等结构密集型任务的性能；推理方案进一步塑造存储的空间知识的使用方式，高级提示支持更有效的多步推理。空间推理性能在模型版本和规模超过一定能力阈值后会饱和，表明改进基于地图的空间理解需要针对空间表示和推理的机制，而不仅仅是规模扩展。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何评估基础模型（FM）代理在地图环境中的空间认知能力问题。这个问题在现实或研究中很重要，因为理解和评估FM代理在地图环境中的空间认知能力对于开发可靠的基于地图的推理系统和应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有在物理或感知环境、地图环境中的空间能力评估方法，设计了一个交互式评估框架。这个框架让代理逐步探索部分可观察的网格地图，并通过一系列任务评估其空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过交互式探索和推理来评估FM代理的空间认知能力。整体实现流程包括代理逐步探索地图环境，构建内部空间表示，并通过一系列任务（如方向判断、距离估计、路径规划等）评估其空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出一个交互式评估框架，设计一套涵盖多个城市的探测任务，系统地分析不同探索策略、记忆表示和提示方法对空间理解的影响。相比之前的工作，这个方法更注重动态、经验驱动的空间认知评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过交互式评估框架，揭示了基础模型代理在地图环境中探索、记忆和推理空间的能力，为设计更可靠的基于地图的推理系统提供了重要见解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.&lt;/p&gt;</description></item><item><guid>2512.24513v1</guid><title>From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting</title><link>http://arxiv.org/abs/2512.24513v1</link><author>Zhiwei Wei, Mengzi Zhang, Boyan Lu, Zhitao Deng, Nai Yang, Hua Liao</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究探讨了动态元素如行人和车辆对城市感知的影响，通过构建带和不带动态元素的街景图像对，进行感知实验和机器学习分析，发现移除动态元素会显著降低感知的活力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的城市分析研究通常将城市场景视为静态，忽略了动态元素如行人和车辆的作用，这可能导致感知偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一个控制框架，通过构建带和不带动态元素的街景图像对，研究动态元素对城市感知的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用语义分割和MLLM引导的生成性修复技术构建图像对，进行感知实验，并训练机器学习模型分析多模态视觉特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 移除动态元素导致感知活力降低30.97%，其他维度变化较小；光照条件、人类存在和深度变化是导致感知变化的关键因素；65%的参与者对活力感知有显著变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于静态图像的城市感知评估可能低估城市的活力，动态元素对城市感知有显著影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解城市感知从街景图像已成为城市分析和以人为本的城市设计中的核心主题。然而，大多数现有研究将城市场景视为静态，很大程度上忽略了行人、车辆等动态元素的作用，这引发了基于感知的城市分析中潜在偏差的担忧。为了解决这个问题，我们提出一个控制框架，通过构建带和不带行人和车辆的街景图像对，使用语义分割和MLLM引导的生成性修复技术，隔离动态元素的感知效应。基于中国东莞的720对图像，进行感知实验，参与者评估原始和编辑场景的六个感知维度。结果表明，移除动态元素导致感知活力一致降低30.97%，其他维度变化较小且异质；为了进一步探索潜在机制，我们使用多模态视觉特征训练了11个机器学习模型，发现光照条件、人类存在和深度变化是导致感知变化的关键因素。在个体层面，65%的参与者对活力感知有显著变化，而其他维度为35-50%；性别对安全感知有边缘调节作用。除了控制实验，训练的模型被扩展到城市规模的数据集，以预测移除动态元素后的活力变化。城市级别的结果表明，这种感知变化是普遍存在的，具有空间结构，影响73.7%的位置和32.1%的图像，表明仅基于静态图像的城市感知评估可能严重低估城市的活力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有研究大多将城市场景视为静态，忽略了动态元素（如行人和车辆）对城市感知的影响。这个问题在现实中很重要，因为动态元素实际上显著影响人们对城市的感知，而忽略它们可能导致城市分析中的偏见。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作，如使用街景图像、语义分割和机器学习来研究城市感知。他们设计了通过生成式填充去除动态元素的方法，并构建了配对街景图像进行感知实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过控制实验来隔离动态元素对城市感知的影响。整体流程包括收集街景图像，使用语义分割和生成式填充去除动态元素，然后进行感知实验，最后分析结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括使用生成式填充技术去除动态元素，并进行配对图像的感知实验。与之前工作不同，他们特别关注动态元素对城市感知的影响，并进行了大规模的实验和分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过实验和分析揭示了动态元素对城市感知的重要影响，强调了在大型感知驱动城市研究中考虑瞬时城市特征的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding urban perception from street view imagery has become a central topic in urban analytics and human centered urban design. However, most existing studies treat urban scenes as static and largely ignore the role of dynamic elements such as pedestrians and vehicles, raising concerns about potential bias in perception based urban analysis. To address this issue, we propose a controlled framework that isolates the perceptual effects of dynamic elements by constructing paired street view images with and without pedestrians and vehicles using semantic segmentation and MLLM guided generative inpainting. Based on 720 paired images from Dongguan, China, a perception experiment was conducted in which participants evaluated original and edited scenes across six perceptual dimensions. The results indicate that removing dynamic elements leads to a consistent 30.97% decrease in perceived vibrancy, whereas changes in other dimensions are more moderate and heterogeneous. To further explore the underlying mechanisms, we trained 11 machine learning models using multimodal visual features and identified that lighting conditions, human presence, and depth variation were key factors driving perceptual change. At the individual level, 65% of participants exhibited significant vibrancy changes, compared with 35-50% for other dimensions; gender further showed a marginal moderating effect on safety perception. Beyond controlled experiments, the trained model was extended to a city-scale dataset to predict vibrancy changes after the removal of dynamic elements. The city level results reveal that such perceptual changes are widespread and spatially structured, affecting 73.7% of locations and 32.1% of images, suggesting that urban perception assessments based solely on static imagery may substantially underestimate urban liveliness.&lt;/p&gt;</description></item><item><guid>2512.24532v1</guid><title>From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning</title><link>http://arxiv.org/abs/2512.24532v1</link><author>Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 在大型语言模型（LLMs）中进行空间推理的研究由于在导航和规划中的应用而受到越来越多的关注。尽管LLMs具有强大的通用语言能力，但它们在结构化环境中的空间变换和多步规划方面仍然存在困难。我们提出了一种两阶段方法，将空间推理分解为原子构建块及其组合。首先，我们对基本的空间变换（如旋转、平移和缩放）进行监督微调，以使模型具备基本的物理空间知识。然后，我们冻结这个具有物理知识的模型，并在GRPO框架内训练轻量级的LoRA适配器，以在基于谜题的环境中通过闭环方式学习组合这些构建块的多步规划策略。为了支持这一流程，我们合成了一个ASCII艺术数据集，并构建了一个相应的基于ASCII的强化学习环境。我们的方法在动态环境（具有明确的状态更新）和静态环境（模型必须依赖其内部状态跨步骤）中始终优于基线，包括通用主干、具有物理知识的模型和端到端RL模型。此外，与从头开始进行端到端强化学习相比，所提出的方法收敛更快，训练更稳定。最后，我们分析了注意力模式，以评估微调是否在空间理解方面带来了有意义的改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; We analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在大型语言模型（LLMs）中进行空间推理的研究由于在导航和规划中的应用而受到越来越多的关注。尽管LLMs具有强大的通用语言能力，但它们在结构化环境中的空间变换和多步规划方面仍然存在困难。我们提出了一种两阶段方法，将空间推理分解为原子构建块及其组合。首先，我们对基本的空间变换（如旋转、平移和缩放）进行监督微调，以使模型具备基本的物理空间知识。然后，我们冻结这个具有物理知识的模型，并在GRPO框架内训练轻量级的LoRA适配器，以在基于谜题的环境中通过闭环方式学习组合这些构建块的多步规划策略。为了支持这一流程，我们合成了一个ASCII艺术数据集，并构建了一个相应的基于ASCII的强化学习环境。我们的方法在动态环境（具有明确的状态更新）和静态环境（模型必须依赖其内部状态跨步骤）中始终优于基线，包括通用主干、具有物理知识的模型和端到端RL模型。此外，与从头开始进行端到端强化学习相比，所提出的方法收敛更快，训练更稳定。最后，我们分析了注意力模式，以评估微调是否在空间理解方面带来了有意义的改进。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大型语言模型（LLMs）在空间推理和多步规划方面的不足。这个问题在现实或研究中很重要，因为空间推理能力对于机器人导航、语言导航任务等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过将空间推理分解为原子构建块及其组合来设计这个方法，借鉴了现有工作，如使用监督微调来学习基本空间变换，并应用强化学习来优化这些构建块的组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将空间理解分解为一系列基本变换，并使用监督微调来学习这些变换，然后通过强化学习来优化这些变换的组合。整体实现流程包括两个阶段：监督微调阶段和强化学习阶段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括将空间推理分解为原子构建块，使用ASCII艺术数据集和强化学习环境，以及通过LoRA适配器来优化策略。相比之前的工作，这个方法更注重构建块的组合和强化学习的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合监督微调和强化学习的新型空间推理方法，显著提升了LLMs在多步空间规划任务中的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.&lt;/p&gt;</description></item><item><guid>2512.24593v1</guid><title>3D Semantic Segmentation for Post-Disaster Assessment</title><link>http://arxiv.org/abs/2512.24593v1</link><author>Nhut Le, Maryam Rahnemoonfar</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。3D语义分割对于灾害后的评估至关重要，但现有的深度学习模型缺乏专门针对灾害后环境的数据库。为了解决这个问题，我们使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。我们评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现，揭示了现有方法在受灾地区的重大局限性。这些发现强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决现有的深度学习模型缺乏专门针对灾害后环境的数据库的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 现有方法在受灾地区的重大局限性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。3D语义分割对于灾害后的评估至关重要，但现有的深度学习模型缺乏专门针对灾害后环境的数据库。为了解决这个问题，我们使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。我们评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现，揭示了现有方法在受灾地区的重大局限性。这些发现强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有深度学习模型缺乏针对灾后环境设计的3D语义分割数据集的问题。这个问题在现实中非常重要，因为准确的灾后评估对于救援行动和减少经济损失至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的3D重建技术和深度学习模型，设计了一个针对灾后环境的3D语义分割数据集。他们使用了无人机拍摄的数据，并应用了Structure-from-Motion和Multi-View Stereo技术来重建3D点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是创建一个专门针对灾后环境的3D语义分割数据集，并评估现有深度学习模型在该数据集上的性能。整体流程包括数据收集、3D点云重建、生成地面真实验证标签，以及使用Fast Point Transformer、Point Transformer v3和OA-CNNs模型进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点在于构建了一个专门针对灾后环境的3D语义分割数据集，并评估了现有深度学习模型在该数据集上的性能。与之前的工作相比，这篇论文强调了现有模型在灾后环境中的局限性，并提出了改进的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建一个针对灾后环境的3D语义分割数据集，揭示了现有深度学习模型在该场景下的局限性，并强调了改进3D语义分割技术的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses. While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments. To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds. We evaluated the state-of-the-art (SOTA) 3D semantic segmentation models, Fast Point Transformer (FPT), Point Transformer v3 (PTv3), and OA-CNNs on this dataset, exposing significant limitations in existing methods for disaster-stricken regions. These findings underscore the urgent need for advancements in 3D segmentation techniques and the development of specialized 3D benchmark datasets to improve post-disaster scene understanding and response.&lt;/p&gt;</description></item><item><guid>2512.24605v1</guid><title>MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding</title><link>http://arxiv.org/abs/2512.24605v1</link><author>Panquan Yang, Junfei Huang, Zongzhangbao Yin, Yingsong Hu, Anni Xu, Xinyi Luo, Xueqi Sun, Hai Wu, Sheng Ao, Zhaoxing Zhu, Chenglu Wen, Cheng Wang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D视觉定位旨在将自然语言句子中的对象定位到3D点云场景中。这对路边基础设施系统在复杂交通环境中解释自然语言和定位相关目标至关重要。然而，大多数现有的数据集和方法都集中在室内和室外驾驶场景，由于缺乏路边基础设施传感器捕获的配对点云-文本数据，室外监控场景仍然未被探索。本文引入了一个新的任务，即室外监控场景的3D视觉定位，它使基础设施级别的交通场景理解超越了自车视角。为了支持这个任务，我们构建了MoniRefer，这是第一个真实世界的路边级3D视觉定位多模态数据集。该数据集包含约136,018个对象和411,128个自然语言表达，收集自多个复杂的交通十字路口。为了确保数据集的质量和准确性，我们对所有语言描述和3D标签进行了人工验证。此外，我们还提出了一种新的端到端方法，名为Moni3DVG，它利用图像提供的丰富外观信息和几何信息以及点云的光学信息进行多模态特征学习和3D对象定位。在提出的基准上的大量实验和消融研究证明了我们方法的优势和有效性。我们的数据集和代码将被发布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大多数现有的3D视觉定位数据集和方法都集中在室内和室外驾驶场景，而室外监控场景由于缺乏配对点云-文本数据而未被探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 引入室外监控场景的3D视觉定位任务，并构建一个真实世界的路边级3D视觉定位多模态数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出了一种新的端到端方法Moni3DVG，利用图像和点云的多模态特征学习和3D对象定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MoniRefer数据集包含约136,018个对象和411,128个自然语言表达，通过人工验证确保了质量和准确性。Moni3DVG方法在实验中证明了其优势和有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提出的室外监控场景的3D视觉定位任务和Moni3DVG方法为交通场景理解提供了新的途径，并展示了其在真实世界数据集上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决户外监控场景下的3D视觉定位问题，即如何根据自然语言描述在3D点云场景中定位相关对象。这个问题在现实研究中非常重要，因为对于路边基础设施系统来说，能够解释自然语言并定位复杂交通环境中的相关目标至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D视觉定位研究的局限性，特别是户外监控场景中数据稀缺、语言多样性有限和视角不一致等问题，设计出MoniRefer数据集和Moni3DVG方法。该方法借鉴了现有2D视觉定位和3D视觉定位的研究成果，并针对户外监控场景进行了优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; Moni3DVG方法的核心思想是通过整合图像和点云的多模态特征进行3D对象定位。整体实现流程包括数据采集、多模态特征学习和3D对象定位。首先，使用LiDAR和相机采集多模态数据；然后，提取图像和点云的 appearance、几何和光学信息进行特征学习；最后，利用这些特征匹配自然语言描述，定位目标对象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括构建了第一个大规模户外监控场景的多模态数据集MoniRefer，提出了Moni3DVG方法，并实现了在户外监控场景下的高效3D视觉定位。相比之前的工作，MoniRefer数据集更注重户外场景和语言多样性，Moni3DVG方法则更有效地整合了多模态信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建MoniRefer数据集和提出Moni3DVG方法，为户外监控场景下的3D视觉定位任务提供了新的基准和解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;</description></item><item><guid>2512.24708v1</guid><title>BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework</title><link>http://arxiv.org/abs/2512.24708v1</link><author>András Millinghoffer, András Formanek, András Antos, Péter Antal</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; BandiK是一种新的多任务辅助任务子集选择方法，使用多臂老虎机来解决问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 有效地在多个任务之间转移知识非常重要，但在基础模型下游任务中仍然存在挑战。转移的性质，特别是其可传递性和不可传递性，仍然是一个开放的问题，负迁移仍然是一个重大障碍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决选择有益辅助任务集的约束，这些约束通常受到高计算成本、大量合理的候选辅助集以及目标任务之间选择复杂性的阻碍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BandiK引入了一种三阶段的多任务辅助任务子集选择方法，使用多臂老虎机。每个臂拉都通过在一个随机训练-测试数据集分割上训练和测试一个多输出神经网络来评估候选辅助集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; BandiK首先估计任务之间的成对迁移，这有助于识别哪些任务可能从联合学习中受益。在第二阶段，它基于初始估计为每个目标任务构建线性数量的候选辅助任务集。第三阶段，它采用多臂老虎机框架，其中臂对应于候选辅助集的性能，这些辅助集作为多个输出神经网络在训练-测试数据集分割上实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BandiK通过将单个任务特定的MAB集成到多臂老虎机结构中来提高效率。这种解决方案利用了相同的神经网络实现了多个臂，这些臂对应于给定候选集的不同单个老虎机。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 有效地在多个任务之间转移知识非常重要，但在基础模型下游任务中仍然存在挑战。转移的性质，特别是其可传递性和不可传递性，仍然是一个开放的问题，负迁移仍然是一个重大障碍。选择有益辅助任务集的约束通常受到高计算成本、大量合理的候选辅助集以及目标任务之间选择复杂性的阻碍。BandiK引入了一种三阶段的多任务辅助任务子集选择方法，使用多臂老虎机。每个臂拉都通过在一个随机训练-测试数据集分割上训练和测试一个多输出神经网络来评估候选辅助集。BandiK首先估计任务之间的成对迁移，这有助于识别哪些任务可能从联合学习中受益。在第二阶段，它基于初始估计为每个目标任务构建线性数量的候选辅助任务集。第三阶段，它采用多臂老虎机框架，其中臂对应于候选辅助集的性能，这些辅助集作为多个输出神经网络在训练-测试数据集分割上实现。BandiK通过将单个任务特定的MAB集成到多臂老虎机结构中来提高效率。这种解决方案利用了相同的神经网络实现了多个臂，这些臂对应于给定候选集的不同单个老虎机。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多任务学习中如何高效选择辅助任务集的问题。这个问题在现实或研究中非常重要，因为选择合适的辅助任务集可以显著提高多任务学习的效率和性能，同时避免负迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴多臂老虎机（MAB）的方法，设计了一个三阶段的多任务辅助任务子集选择方法。这个方法借鉴了现有工作中关于多臂老虎机在超参数优化和神经架构搜索中的应用，以及多任务学习的相关研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用多臂老虎机来选择最佳的辅助任务集。整体实现流程包括三个阶段：首先估计任务之间的成对迁移效应，并构建正向和负向迁移图；然后基于初始估计构建候选辅助任务集；最后使用多臂老虎机框架选择每个目标任务的最佳多任务神经网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用半重叠臂的多臂老虎机框架，以及将单个任务特定的多臂老虎机集成到多臂老虎机结构中。相比之前的工作，这个方法可以更有效地选择辅助任务集，并且减少了计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于多臂老虎机的高效多任务辅助任务子集选择方法，可以显著提高多任务学习的效率和性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The challenge of effectively transferring knowledge across multiple tasks is of critical importance and is also present in downstream tasks with foundation models. However, the nature of transfer, its transitive-intransitive nature, is still an open problem, and negative transfer remains a significant obstacle. Selection of beneficial auxiliary task sets in multi-task learning is frequently hindered by the high computational cost of their evaluation, the high number of plausible candidate auxiliary sets, and the varying complexity of selection across target tasks.   To address these constraints, we introduce BandiK, a novel three-stage multi-task auxiliary task subset selection method using multi-bandits, where each arm pull evaluates candidate auxiliary sets by training and testing a multiple output neural network on a single random train-test dataset split. Firstly, BandiK estimates the pairwise transfers between tasks, which helps in identifying which tasks are likely to benefit from joint learning. In the second stage, it constructs a linear number of candidate sets of auxiliary tasks (in the number of all tasks) for each target task based on the initial estimations, significantly reducing the exponential number of potential auxiliary task sets. Thirdly, it employs a Multi-Armed Bandit (MAB) framework for each task, where the arms correspond to the performance of candidate auxiliary sets realized as multiple output neural networks over train-test data set splits. To enhance efficiency, BandiK integrates these individual task-specific MABs into a multi-bandit structure. The proposed multi-bandit solution exploits that the same neural network realizes multiple arms of different individual bandits corresponding to a given candidate set. This semi-overlapping arm property defines a novel multi-bandit cost/reward structure utilized in BandiK.&lt;/p&gt;</description></item><item><guid>2512.24763v1</guid><title>UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning</title><link>http://arxiv.org/abs/2512.24763v1</link><author>Ankit Dhiman, Srinath R, Jaswanth Reddy, Lokesh R Boregowda, Venkatesh Babu Radhakrishnan</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。一个关键挑战是 2D 实例标签跨视图的不一致性，导致 3D 预测效果不佳。现有方法采用两阶段方法，一些依赖于对比学习和超参数敏感的聚类，而另一些则预处理标签以实现一致性。我们提出一个统一框架，将这些步骤合并，通过引入可学习的特征嵌入来减少训练时间并提高性能，该嵌入用于高斯原语中的分割。然后通过一种新的“嵌入到标签”过程高效解码成实例标签，有效地集成了优化。尽管这个统一框架提供了显著的好处，但我们观察到在物体边界处存在伪影。为了解决物体边界问题，我们提出沿着这些边界进行硬挖掘样本。然而，直接将硬挖掘应用于特征嵌入被证明是不稳定的。因此，我们在计算三元组损失之前对光栅化特征嵌入应用线性层，这稳定了训练并显著提高了性能。我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决 2D 实例标签跨视图的不一致性，提高 3D 预测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出一个统一框架，将对比学习和标签预处理步骤合并，引入可学习的特征嵌入来减少训练时间并提高性能，通过“嵌入到标签”过程高效解码成实例标签，并应用线性层进行硬挖掘以稳定训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 统一框架提供了显著的好处，但在物体边界处存在伪影。应用线性层进行硬挖掘稳定了训练并显著提高了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。一个关键挑战是 2D 实例标签跨视图的不一致性，导致 3D 预测效果不佳。现有方法采用两阶段方法，一些依赖于对比学习和超参数敏感的聚类，而另一些则预处理标签以实现一致性。我们提出一个统一框架，将这些步骤合并，通过引入可学习的特征嵌入来减少训练时间并提高性能，该嵌入用于高斯原语中的分割。然后通过一种新的“嵌入到标签”过程高效解码成实例标签，有效地集成了优化。尽管这个统一框架提供了显著的好处，但我们观察到在物体边界处存在伪影。为了解决物体边界问题，我们提出沿着这些边界进行硬挖掘样本。然而，直接将硬挖掘应用于特征嵌入被证明是不稳定的。因此，我们在计算三元组损失之前对光栅化特征嵌入应用线性层，这稳定了训练并显著提高了性能。我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从2D图像中不一致的实例分割标签生成一致的三维分割标签的问题。这个问题在现实或研究中非常重要，因为三维场景理解对于增强现实、自动驾驶和路径规划等领域至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的三维表示方法如NeRF和3DGS，并利用对比学习的思想设计出这个方法。他们参考了Contrastive-Lift的工作，但对其进行了改进，以实现更高效的训练和更准确的分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过对比学习将二维分割标签直接解码为三维分割标签。整体实现流程包括：将二维图像渲染为三维高斯点云，为每个三维高斯点云添加一个d维向量嵌入，通过对比损失优化这些嵌入，然后通过线性层和三重损失进一步优化，最后通过“嵌入到标签”过程生成三维分割标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：提出了一种统一的单阶段方法，直接将学习到的三维嵌入解码为一致的三维分割标签；使用基于三重损失的新型对比损失来减少类间方差，从而实现更准确的三维分割。与之前的工作相比，这个方法更高效，因为它不需要额外的后处理步骤，并且具有更快的推理时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于对比学习的统一三维实例分割方法，该方法能够直接从二维分割标签生成一致的三维分割标签，并在多个数据集上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel &amp;quot;Embedding-to-Label&amp;quot; process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.&lt;/p&gt;</description></item><item><guid>2512.24834v1</guid><title>GenZ: Foundational models as latent variable generators within traditional statistical models</title><link>http://arxiv.org/abs/2512.24834v1</link><author>Marko Jojic, Nebojsa Jojic</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; GenZ是一种混合模型，它通过可解释的语义特征连接了基础模型和统计模型。该方法通过迭代过程发现语义特征描述，对比通过统计建模错误识别的项目组，而不是仅仅依赖基础模型的领域理解。该方法被表述为一个通用的EM算法，联合优化语义特征描述和统计模型参数。该方法提示一个冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为对潜在二元特征的噪声观察，这些特征通过学习的统计关系预测实值目标。该方法在两个领域进行了演示：房价预测（享乐回归）和冷启动协同过滤用于电影推荐。在房价预测中，该模型使用从多模态列表数据中发现的语义特征实现了12%的中值相对误差，大大优于依赖LLM的通用领域知识的GPT-5基线（误差为38%）。对于Netflix电影嵌入，该模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式（例如，建筑细节预测当地房地产市场，系列成员身份预测用户偏好），这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型拥有广泛的领域知识，但它们通常无法捕捉对预测任务至关重要的数据集特定模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出GenZ模型，通过可解释的语义特征连接基础模型和统计模型，以解决大型语言模型在捕捉数据集特定模式方面的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过迭代过程对比通过统计建模错误识别的项目组，发现语义特征描述，并使用通用的EM算法联合优化语义特征描述和统计模型参数。提示一个冻结的基础模型根据发现的特征对项目进行分类，并将这些判断视为对潜在二元特征的噪声观察。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在房价预测中，GenZ模型使用发现的语义特征实现了12%的中值相对误差，远优于依赖LLM的通用领域知识的GPT-5基线。对于Netflix电影嵌入，GenZ模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式，这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GenZ模型通过可解释的语义特征有效地连接了基础模型和统计模型，在房价预测和电影推荐领域取得了显著的性能提升，并揭示了数据集特定的模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了GenZ，一种通过可解释的语义特征连接基础模型和统计模型的混合模型。虽然大型语言模型拥有广泛的领域知识，但它们通常无法捕捉对预测任务至关重要的数据集特定模式。我们的方法通过迭代过程对比通过统计建模错误识别的项目组，发现语义特征描述，而不是仅仅依赖基础模型的领域理解。我们将此表述为一个通用的EM算法，联合优化语义特征描述和统计模型参数。该方法提示一个冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为对潜在二元特征的噪声观察，这些特征通过学习的统计关系预测实值目标。我们在两个领域演示了该方法：房价预测（享乐回归）和冷启动协同过滤用于电影推荐。在房价预测中，我们的模型使用发现的语义特征实现了12%的中值相对误差，远优于依赖LLM的通用领域知识的GPT-5基线。对于Netflix电影嵌入，我们的模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式，这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将大型语言模型（LLM）的广泛领域知识与特定数据集的模式相结合，以改进预测任务。这个问题在现实或研究中很重要，因为LLM虽然知识广泛，但往往无法捕捉到特定数据集的独特模式，这限制了它们在预测任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴概念瓶颈模型（CBM）的思想，设计了一个混合模型，该模型通过迭代过程对比通过统计建模错误识别的项目组来发现语义特征描述，而不是仅仅依赖LLM的领域理解。这个方法借鉴了现有工作，但引入了新的元素，如不确定性参数和基于统计模型后验分布的对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用LLM作为解释器，通过对比统计模型识别的项目组来发现语义特征，并使用这些特征来改进预测模型。整体实现流程包括使用LLM对项目进行分类，通过统计模型的后验分布对比项目组，使用这些对比信息来更新语义特征描述，并最终联合优化语义特征和统计模型参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用冻结的LLM作为解释器，通过对比统计模型识别的项目组来发现语义特征，以及支持任意映射的统计模型。与之前的工作相比，这个方法更注重发现数据集特定的模式，而不是仅仅依赖LLM的领域知识，并且能够处理高维实值目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的混合模型，通过结合LLM和统计建模来发现数据集特定的语义特征，从而提高预测任务的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model&amp;#x27;s domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM&amp;#x27;s general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model&amp;#x27;s domain knowledge alone.&lt;/p&gt;</description></item><item><guid>2512.24845v1</guid><title>ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation</title><link>http://arxiv.org/abs/2512.24845v1</link><author>Qiuyi Gu, Yuze Sheng, Jincheng Yu, Jiahao Tang, Xiaolong Shan, Zhaoyang Shen, Tinghao Yi, Xiaodan Liang, Xinlei Chen, Yu Wang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。为弥补这一差距，我们提出了ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。该方法利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。我们将这些运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。大量实际实验表明，ArtiSG在功能元素召回和关节估计精度方面显著优于基线。此外，我们证明构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补3D场景图在功能信息方面的不足，特别是对于关节对象的功能信息，通过构建功能3D场景图来指导机器人在真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。将运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ArtiSG在功能元素召回和关节估计精度方面显著优于基线。构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ArtiSG框架能有效构建功能3D场景图，弥补现有方法的不足，并显著提升机器人在真实环境中的操作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。为弥补这一差距，我们提出了ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。该方法利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。我们将这些运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。大量实际实验表明，ArtiSG在功能元素召回和关节估计精度方面显著优于基线。此外，我们证明构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景图中缺乏功能信息的问题，特别是对于需要物理操作的关节对象。这个问题在现实研究中很重要，因为机器人需要理解物体的功能属性才能进行有效的物理操作和任务执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，如利用人类演示进行机器人学习和3D场景图构建，设计出ArtiSG方法。他们注意到现有方法的局限性，如视觉歧义和缺乏功能细节，因此提出了一个结合人类演示和视觉基础模型的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将人类演示编码到结构化的机器人记忆中，以构建功能3D场景图。整体流程包括三个阶段：初始化功能场景图，利用便携式设备进行视角鲁棒的关节估计，以及交互增强的图细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括视角鲁棒的关节跟踪、交互增强的功能元素检测和开放词汇场景构建。与之前的工作相比，ArtiSG能够更准确地估计关节机制，并识别被视觉感知忽略的细微功能元素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ArtiSG通过结合人类演示和视觉基础模型，构建了功能3D场景图，使机器人能够更好地理解和操作关节对象。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.&lt;/p&gt;</description></item><item><guid>2512.24866v1</guid><title>Characterization of Transfer Using Multi-task Learning Curves</title><link>http://arxiv.org/abs/2512.24866v1</link><author>András Millinghoffer, Bence Bolgár, Péter Antal</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。我们假设通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，可以更根本地描述迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，以更根本地描述迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 我们使用多任务学习曲线来定量地建模迁移效应，这些曲线近似于不同样本数量下的归纳性能。我们描述了一种高效的方法来近似多任务学习曲线，类似于训练期间的任务亲和分组方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 我们的结果表明，学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。我们假设通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，可以更根本地描述迁移效应。为了捕捉这种现象，我们使用多任务学习曲线来定量地建模迁移效应，这些曲线近似于不同样本数量下的归纳性能。我们描述了一种高效的方法来近似多任务学习曲线，类似于训练期间的任务亲和分组方法。我们比较了迁移的统计和计算方法，这表明之前的计算成本要高得多，但具有更好的性能和更广泛的适用性。我们使用基准药物靶点相互作用数据集进行了评估。我们的结果表明，学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多任务学习中迁移效应的表征问题。这个问题在现实或研究中非常重要，因为迁移效应直接影响机器学习模型的性能和泛化能力，特别是在处理大规模、多任务问题时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多任务学习方法，特别是任务亲和分组方法，设计出一种新的方法来表征迁移效应。他们借鉴了现有工作，但提出了新的学习曲线方法来更有效地捕捉迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过学习曲线来表征迁移效应，特别是通过样本数量来观察模型性能的变化。整体实现流程包括使用多任务学习曲线来建模和评估迁移效应，通过系统地比较不同任务组合下的性能变化来分析迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出新的多任务学习曲线方法来表征迁移效应，以及通过系统地比较统计和计算方法来分析迁移效应。相比之前的工作，这个方法更注重样本数量对模型性能的影响，并且能够更有效地捕捉迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出新的多任务学习曲线方法，有效地表征和分析了多任务学习中的迁移效应。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Transfer effects manifest themselves both during training using a fixed data set and in inductive inference using accumulating data. We hypothesize that perturbing the data set by including more samples, instead of perturbing the model by gradient updates, provides a complementary and more fundamental characterization of transfer effects. To capture this phenomenon, we quantitatively model transfer effects using multi-task learning curves approximating the inductive performance over varying sample sizes. We describe an efficient method to approximate multi-task learning curves analogous to the Task Affinity Grouping method applied during training. We compare the statistical and computational approaches to transfer, which indicates considerably higher compute costs for the previous but better power and broader applicability. Evaluations are performed using a benchmark drug-target interaction data set. Our results show that learning curves can better capture the effects of multi-task learning and their multi-task extensions can delineate pairwise and contextual transfer effects in foundation models.&lt;/p&gt;</description></item><item><guid>2512.24880v1</guid><title>mHC: Manifold-Constrained Hyper-Connections</title><link>http://arxiv.org/abs/2512.24880v1</link><author>Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 近期研究，如Hyper-Connections (HC)，通过扩展残差流宽度和多样化连接模式，扩展了过去十年建立的普遍残差连接范式。虽然这带来了显著的性能提升，但这种多样化从根本上破坏了残差连接的恒等映射特性，导致严重的训练不稳定性、有限的扩展性，并增加了显著的内存访问开销。为了解决这些挑战，我们提出了Manifold-Constrained Hyper-Connections (mHC)，一个将HC的残差连接空间投影到特定流形上的通用框架，以恢复恒等映射特性，同时结合严格的架构优化以确保效率。实证实验表明，mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。我们预计，作为HC的一种灵活且实用的扩展，mHC将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Hyper-Connections (HC)通过扩展残差流宽度和多样化连接模式提升了性能，但破坏了恒等映射特性，导致训练不稳定性、有限扩展性和内存访问开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决HC带来的训练不稳定性、有限扩展性和内存访问开销问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出Manifold-Constrained Hyper-Connections (mHC)，将残差连接空间投影到特定流形上，恢复恒等映射特性，并结合严格的架构优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; mHC作为HC的灵活且实用的扩展，将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近期研究，如Hyper-Connections (HC)，通过扩展残差流宽度和多样化连接模式，扩展了过去十年建立的普遍残差连接范式。虽然这带来了显著的性能提升，但这种多样化从根本上破坏了残差连接的恒等映射特性，导致严重的训练不稳定性、有限的扩展性，并增加了显著的内存访问开销。为了解决这些挑战，我们提出了Manifold-Constrained Hyper-Connections (mHC)，一个将HC的残差连接空间投影到特定流形上的通用框架，以恢复恒等映射特性，同时结合严格的架构优化以确保效率。实证实验表明，mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。我们预计，作为HC的一种灵活且实用的扩展，mHC将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Hyper-Connections（HC）在扩展连接复杂度时导致的训练不稳定和可扩展性问题。这个问题在研究中很重要，因为HC虽然能提升性能，但其无约束的连接方式破坏了残差连接的恒等映射特性，导致大规模训练时信号放大或衰减，影响训练效率和模型稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，特别是HC的设计思想，提出了Manifold-Constrained Hyper-Connections（mHC）方法。mHC借鉴了将残差连接空间投影到特定流形上的思想，利用Sinkhorn-Knopp算法将残差连接矩阵投影到Birkhoff多面体上，以恢复恒等映射特性，同时结合基础设施优化确保效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; mHC的核心思想是将HC的残差连接空间投影到由双随机矩阵构成的流形上，以恢复恒等映射特性。整体实现流程包括：1) 利用Sinkhorn-Knopp算法将残差连接矩阵投影到Birkhoff多面体上；2) 通过矩阵的行和列和为1的特性，确保信号在传播过程中的均值和范数得到有效控制；3) 结合基础设施优化，如内核融合、混合精度内核和通信重叠等，提高效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; mHC的关键创新点包括：1) 将残差连接空间投影到特定流形上，恢复恒等映射特性，提高训练稳定性；2) 结合基础设施优化，如内核融合、混合精度内核和通信重叠等，提高效率。相比之前的工作，mHC在保持HC拓扑复杂度的同时，解决了其训练不稳定和可扩展性问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; mHC通过将残差连接空间投影到特定流形上，恢复了恒等映射特性，同时结合基础设施优化，提高了大规模训练的稳定性和可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.&lt;/p&gt;</description></item><item><guid>2512.24896v1</guid><title>Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing</title><link>http://arxiv.org/abs/2512.24896v1</link><author>Andrii Gamalii, Daniel Górniak, Robert Nowak, Bartłomiej Olber, Krystian Radlak, Jakub Winter</author><pubDate>Fri, 02 Jan 2026 14:33:45 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该报告介绍了DARTS项目内开发的一种半自动化数据标注流程，旨在创建大规模、多模态的波兰驾驶场景数据集。手动标注此类异构数据既昂贵又耗时。为应对这一挑战，提出的解决方案采用人机协作方法，结合人工智能与人类专业知识，以降低标注成本和时长。系统自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。核心工具依赖于3D目标检测算法生成初步标注。总体而言，开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 手动标注大规模、多模态的波兰驾驶场景数据集既昂贵又耗时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 创建大规模、多模态的波兰驾驶场景数据集，并降低标注成本和时长。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用人机协作方法，结合人工智能与人类专业知识，自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该报告介绍了DARTS项目内开发的一种半自动化数据标注流程，旨在创建大规模、多模态的波兰驾驶场景数据集。手动标注此类异构数据既昂贵又耗时。为应对这一挑战，提出的解决方案采用人机协作方法，结合人工智能与人类专业知识，以降低标注成本和时长。系统自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。核心工具依赖于3D目标检测算法生成初步标注。总体而言，开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶车辆测试中多传感器数据集的手动标注成本高、耗时长的问题。这个问题在现实或研究中很重要，因为高质量的标注数据集是训练和评估自动驾驶感知系统的关键，而手动标注难以满足大规模、多模态数据集的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过结合人工智能和人类专业知识，设计了一个半自动化的数据标注流程。这个方法借鉴了现有的3D目标检测算法和人类在环验证技术，并对这些技术进行了改进和整合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用人工智能自动生成初始标注，然后通过人类验证和修正来提高标注质量。整体实现流程包括数据预处理、自动标注生成、人类验证、质量控制和模型迭代训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括结合人工 intelligence 和人类专业知识进行半自动化标注，以及使用3D目标检测算法和领域适应技术来提高标注效率和准确性。相比之前的工作，这个方法更加注重效率和准确性，并且能够处理多模态数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种半自动化的数据标注方法，通过结合人工智能和人类专业知识，有效地提高了自动驾驶车辆测试中多传感器数据集的标注效率和准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project&amp;#x27;s standardized format, strengthening the technological base for autonomous vehicle research in Poland.&lt;/p&gt;</description></item><item><guid>2512.24922v1</guid><title>Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</title><link>http://arxiv.org/abs/2512.24922v1</link><author>Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak</author><pubDate>Fri, 02 Jan 2026 11:54:02 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于神经元激活模式的激光雷达领域适应方法，只需对目标域中少量精选且多样的样本进行标注，即可实现最先进的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D目标检测器是自动驾驶感知系统的核心，但在不同地区或数据分布之间的迁移能力不足，例如在美国训练的模型在亚洲或欧洲表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在开发一种能够在极小标注预算下实现跨域适应的技术，并通过后训练策略防止模型权重漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用神经元激活模式挑选目标域的代表性样本进行少量标注，随后结合受持续学习启发的后训练技术，对模型进行微调，以保持原始模型的权重稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该领域适应方法在精度上超过线性探测和现有最先进的领域适应技术，并且只需极少的标注成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过正确选择少量多样化的目标域样本并使用持续学习的后训练手段，可在保持原模型性能的同时实现高效的跨域适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D目标检测器是自动驾驶感知系统的基础组件。虽然这些检测器在标准的自动驾驶基准上取得了显著的性能，但它们常常难以在不同领域之间实现泛化——例如，在美国训练的模型在亚洲或欧洲等地区可能表现不佳。本文提出了一种基于神经元激活模式的全新激光雷达领域适应方法，证明只要正确选择目标域中少量具有代表性且多样性的样本进行标注，就能实现最先进的性能。该方法只需极少的标注预算，并且结合受持续学习启发的后训练技术，可防止模型权重从原始模型漂移。实证评估表明，所提出的领域适应方法在性能上优于线性探测和最先进的领域适应技术。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D目标检测模型在不同地区或传感器条件下的域适应问题，即如何用极少量标注的目标域数据使模型在新环境中保持高性能。这在自动驾驶实际部署中非常关键，因为不同地区的车辆尺寸、道路布局和LiDAR配置差异会导致模型性能大幅下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为低层特征对跨域检测至关重要，因而聚焦于细粒度的微调，并借鉴了持续学习防止灾难性遗忘的技术、L2‑SP 权重正则化以及已有的点云密度和边框尺寸对齐方法。同时，他们创新性地使用神经元激活模式来挑选最具代表性的目标域样本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过提取ROI头的激活向量，剪枝并二值化后衡量样本多样性，选出少量代表性帧进行微调。整体流程为：从预训练模型中获取激活模式 → 处理得到稀疏二进制特征 → 计算多样性并挑选样本 → 使用小学习率或L2‑SP 等正则化进行后训练，同时加入持续学习技巧以保持源域性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 基于神经激活模式的多样性样本选择算法；② 证明仅需少量目标域样本即可实现有效适应；③ 将持续学习正则化与微调相结合防止权重漂移；④ 在保持实现简洁的同时超越了线性探测和其他先进域适应方法。与以往依赖对抗学习、伪标签或大规模未标注数据的做法不同，本文侧重于少量高质量标注和激活驱动的选择。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于激活模式的多样性驱动微调加持续学习正则化的域适应方案，使3D LiDAR目标检测在仅使用少量精选目标域样本的情况下实现跨地区高效迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.&lt;/p&gt;</description></item><item><guid>2512.25008v1</guid><title>FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</title><link>http://arxiv.org/abs/2512.25008v1</link><author>Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</author><pubDate>Fri, 02 Jan 2026 11:54:02 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 FoundationSLAM，一种基于学习的单目稠密 SLAM 系统，通过将光流估计与几何推理相结合，解决了以往光流方法缺乏几何一致性的问题，实现了精准且鲁棒的跟踪与建图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往基于光流的 SLAM 方法在几何一致性方面存在不足，导致跟踪和映射的精度和鲁棒性受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在构建一个能够在几何一致性约束下进行准确、鲁棒且实时的单目稠密 SLAM 系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 核心思路是利用基础深度模型的引导，将光流估计与几何推理相结合。具体包括：1）设计混合光流网络，生成几何感知的对应关系，实现跨关键帧的一致深度和位姿推断；2）提出双一致性束束优化层，在多视图约束下联合优化关键帧位姿和深度，以保证全局一致性；3）引入可靠性感知的细化机制，通过区分可靠和不确定区域动态调整光流更新，形成匹配与优化的闭环反馈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 大量实验表明，FoundationSLAM 在多个具有挑战性的数据集上实现了更高的轨迹精度和稠密重建质量，且能够以 18 帧每秒的实时速度运行，展示了对各种场景的强泛化能力和实际应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FoundationSLAM 通过融合光流与深度的几何约束，成功提升了单目稠密 SLAM 的精度、鲁棒性和实时性，具备良好的通用性和实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 FoundationSLAM，这是一种基于学习的单目稠密 SLAM 系统，解决了以往基于光流的方法在几何一致性方面的缺失，从而实现了准确且鲁棒的跟踪和建图。我们的核心思想是通过利用基础深度模型的引导，将光流估计与几何推理相结合。为此，我们首先开发了一个混合光流网络，生成几何感知的对应关系，使得在不同关键帧之间能够进行一致的深度和位姿推断。为了强制全局一致性，我们提出了双一致性束束优化层，在多视图约束下联合优化关键帧位姿和深度。此外，我们引入了可靠性感知的细化机制，通过区分可靠和不确定区域动态调整光流更新过程，在匹配和优化之间形成闭环反馈。大量实验表明，FoundationSLAM 在多个具有挑战性的数据集上实现了卓越的轨迹精度和稠密重建质量，同时以 18 FPS 的实时速度运行，展示了对各种场景的强泛化能力和我们方法的实际适用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文针对现有单目密集 SLAM 系统仅依赖光流而缺乏几何一致性的问题，导致位姿估计和稠密重建出现结构错误和不完整。几何一致性是机器人导航和三维建模的核心，直接影响系统的鲁棒性和精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为需要将几何先验引入光流匹配，并在多视图优化中强化几何约束，于是结合了已有的流式 SLAM（如 DROID‑SLAM）和基础深度模型（如 FoundationStereo）的思路，设计了融合深度特征的混合流网络和双向束调整层。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让光流估计受深度先验指导，并在闭环中通过多视图几何约束共同优化深度和位姿。整体流程是：先用混合流网络利用基础深度特征预测光流；随后双向束调整层利用流和几何残差联合优化关键帧的深度与位姿；最后可靠性感知的细化模块根据优化残差更新流的可靠性掩码并再次迭代。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新包括：① 将基础深度模型的几何特征注入光流网络，实现几何感知的对应估计；② 提出双向一致性束调整层，显式加入正向和反向几何一致性约束；③ 可靠性感知的流细化机制，根据优化残差动态调整流更新。相比仅基于像素相关性的传统流式 SLAM，这些设计实现了更强的全局几何一致性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文通过融合基础深度先验、双向几何约束和残差驱动的流细化，构建了一个几何一致且实时的单目稠密 SLAM 系统。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.&lt;/p&gt;</description></item></channel></rss>