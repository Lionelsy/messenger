<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Arxiv论文推荐</title><link>https://arxiv.org/</link><description>Arxiv论文推荐</description><language>zh-CN</language><lastBuildDate>Wed, 07 Jan 2026 14:10:01 +0800</lastBuildDate><item><guid>2512.24138v1</guid><title>GARDO: Reinforcing Diffusion Models without Reward Hacking</title><link>http://arxiv.org/abs/2512.24138v1</link><author>Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了通过在线强化学习微调扩散模型以提升文本到图像的对齐效果，并提出了GARDO框架来解决奖励代理不匹配导致的奖励黑客问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在强化学习中使用代理奖励优化视觉任务时，难以精确指定真实目标，导致模型往往被误导，出现奖励黑客、图像质量下降和多样性崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在保持样本效率和有效探索的同时，降低奖励黑客并提升生成多样性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GARDO采用门控与自适应正则化，针对不确定样本进行惩罚；周期性更新参考模型以匹配在线策略；对高质量且多样化的样本放大奖励，鼓励模式覆盖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多种代理奖励和未见指标上实验表明，GARDO 能有效抑制奖励黑客，提升多样性，同时不牺牲样本效率或探索能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GARDO 是一种兼容多种强化学习算法的稳健框架，能够在样本效率、探索和奖励黑客之间取得良好平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 通过在线强化学习微调扩散模型已显示出提升文本到图像对齐的巨大潜力。然而，由于难以精确指定视觉任务的真实目标，模型往往使用仅部分捕捉真实目标的代理奖励进行优化。这种不匹配常导致奖励黑客，即代理分数上升而真实图像质量下降，生成多样性崩溃。常见的解决方案是对参考策略进行正则化以防止奖励黑客，但这会降低样本效率并阻碍对新高奖励区域的探索，因为参考策略通常并非最优。为兼顾样本效率、有效探索和奖励黑客缓解，我们提出了门控与自适应正则化结合多样性感知优化（GARDO）框架，适用于多种强化学习算法。我们的核心见解是正则化不必普遍应用，而是针对表现出高不确定性的样本进行选择性惩罚。为解决探索难题，GARDO 引入自适应正则化机制，周期性更新参考模型以匹配在线策略的能力，确保正则化目标相关。为解决 RL 中的模式崩溃问题，GARDO 对既高质量又高多样性的样本放大奖励，鼓励模式覆盖而不破坏优化过程。通过在多种代理奖励和未见指标上的广泛实验，GARDO 一致显示出能抑制奖励黑客并提升生成多样性，同时不牺牲样本效率或探索，凸显其有效性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.&lt;/p&gt;</description></item><item><guid>2601.00095v2</guid><title>Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning</title><link>http://arxiv.org/abs/2601.00095v2</link><author>Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; MetaJuLS 是一种元强化学习方法，能够学习通用的约束传播策略，适用于多语言和多任务的结构化推理，无需针对每个任务单独训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大型语言模型的普及，结构化推理需求日益增长，包括 JSON 模式强制和多语言解析，输出必须满足复杂约束。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在不同语言和任务间快速适应、提高推理速度并保持高精度的通用约束传播框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将结构化推理视为自适应约束传播，使用图注意力网络结合元学习进行训练，形成可迁移的策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MetaJuLS 在 GPU 优化基线上实现 1.5 到 2.0 倍的速度提升，准确率与最先进解析器相差不超过 0.2%；在十种语言的 Universal Dependencies 以及 LLM 受限生成任务中，单一策略可在 5 到 10 次梯度更新（5 到 15 秒）内适应新语言和任务；机制分析显示策略学习了类似人类的易先解析策略和新颖的非直观启发式；减少传播步骤降低了 LLM 推理的碳足迹。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MetaJuLS 提供了一种高效、可迁移且绿色的结构化推理解决方案，显著提升了多语言、多任务环境下的推理性能与可持续性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型越来越需要结构化推理，从 JSON 模式强制到多语言解析，输出必须满足复杂约束。我们提出 MetaJuLS，一种元强化学习方法，学习可在不同语言和任务中通用的约束传播策略，无需针对每个任务单独训练。通过将结构化推理表述为自适应约束传播，并使用图注意力网络与元学习进行训练，MetaJuLS 在 GPU 优化基线上实现 1.5 到 2.0 倍的速度提升，同时保持与最先进解析器相差不超过 0.2% 的准确率。在 Universal Dependencies 的十种语言以及 LLM 受限生成任务（LogicBench、GSM8K-受限）中，MetaJuLS 展示了快速的跨域适应：在英语解析上训练的策略可在 5 到 10 次梯度步骤（5 到 15 秒）内适应新语言和任务，而不需要数小时的任务特定训练。机制分析揭示该策略发现了类似人类的易先解析策略和新颖的非直观启发式。通过减少 LLM 部署中的传播步骤，MetaJuLS 通过直接降低推理碳足迹为绿色 AI 做出了贡献。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.&lt;/p&gt;</description></item><item><guid>2601.00551v2</guid><title>SlingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging with arbitrary array geometries</title><link>http://arxiv.org/abs/2601.00551v2</link><author>Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为SlingBAG Pro的三维光声成像重建算法，能够在不规则阵列下实现高质量成像，并显著加快重建速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 三维光声成像因其高分辨率和无创性受到关注，但受限于空间和成本，传统迭代重建方法在不规则阵列上计算复杂、内存占用大、重建时间长。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种兼容任意阵列几何、减少探头数量且保持高重建质量的高效算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SlingBAG Pro基于滑动球自适应增长点云迭代思想，采用层次优化策略：先进行零梯度滤波去除冗余点云，再逐步提高时间采样率，快速收敛并缩短重建时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 与原SlingBAG相比，SlingBAG Pro在不规则阵列下的点云重建速度提升至2.2倍；通过仿真和活体小鼠实验验证了其性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SlingBAG Pro实现了在更少探头下获得高质量三维光声图像，并显著降低了计算成本和重建时间，为临床应用提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; High‑quality three‑dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high‑quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero‑gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2‑fold speed improvement in point cloud‑based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.&lt;/p&gt;</description></item><item><guid>2601.00832v1</guid><title>ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI</title><link>http://arxiv.org/abs/2601.00832v1</link><author>Israk Hasan Jone, D. M. Rafiun Bin Masud, Promit Sarker, Sayed Fuad Al Labib, Nazmul Islam, Farhad Billah</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究利用深度学习自动识别虾类疾病，使用多种预训练模型和数据增强技术，最终ConvNeXt-Tiny模型在测试集上取得最高准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虾类是全球消费量最大的水产品之一，养殖业为许多地区带来收入，但疾病爆发严重影响可持续生产。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种及时、准确的自动化疾病分类方法，以提高虾类养殖的可持续性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 收集1149张四类疾病图像，去除背景并使用Keras管道标准化预处理；采用六种预训练模型（ResNet50、EfficientNet、DenseNet201、MobileNet、ConvNeXt-Tiny、Xception）；使用FGSM进行对抗训练；应用CutMix和MixUp增强；使用Grad-CAM、Grad-CAM++、XGrad-CAM进行可解释性分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ConvNeXt-Tiny模型在测试集上取得最高准确率96.88%，并在1000次迭代后置信区间为[0.953,0.971]。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 深度学习方法，尤其是ConvNeXt-Tiny，能够有效自动识别虾类疾病，为养殖业提供可靠的诊断工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虾类是全球最广泛消费的水生物种之一，因其营养价值和经济重要性而受到重视。虾养殖在许多地区是重要的收入来源；然而，与其他水产养殖形式一样，它受到疾病爆发的严重影响。这些疾病对可持续虾类生产构成重大挑战。为解决这一问题，自动化疾病分类方法可以提供及时准确的检测。本研究提出了一种基于深度学习的虾类疾病自动分类方法。使用包含1149张图像、涵盖四种疾病类别的数据集。部署并评估了六种预训练深度学习模型：ResNet50、EfficientNet、DenseNet201、MobileNet、ConvNeXt-Tiny和Xception。图像背景被移除后，通过Keras图像管道进行标准化预处理。使用Fast Gradient Sign Method（FGSM）通过对抗训练增强模型鲁棒性。实施了CutMix和MixUp等高级增强策略，以减轻过拟合并提高泛化能力。为支持可解释性并可视化模型关注区域，应用了后置解释方法Grad-CAM、Grad-CAM++和XGrad-CAM。探索性结果表明，ConvNeXt-Tiny取得最高性能，在测试集上达到96.88%的准确率。经过1000次迭代后，该模型的99%置信区间为[0.953,0.971]。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].&lt;/p&gt;</description></item><item><guid>2601.00993v1</guid><title>WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift</title><link>http://arxiv.org/abs/2601.00993v1</link><author>Julian D. Santamaria, Claudia Isaza, Jhony H. Giraldo</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这篇论文讨论了利用深度学习模型自动识别野生动物图像的挑战，尤其是模型在不同地理区域的泛化能力，并提出了结合文本描述的WildIng模型来提升跨地区的识别准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 野生动物监测对研究生物多样性丧失和气候变化至关重要，摄像机陷阱图像提供了非侵入式的数据来源，但人工分析耗时且资源消耗大。深度学习模型在同一地区数据上表现良好，但在新地区会出现显著性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有视觉模型在地理域迁移时的泛化不足，提升在不同地区的野生动物识别准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计WildIng模型，将文本描述与图像特征融合，构建对地理域变化更稳健的表征；使用适配器训练CLIP等基础模型，并在不同地区数据集上进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; WildIng在跨地区测试中将基础模型BioCLIP的准确率提升了约30%，显著改善了在美国与非洲数据集之间的性能差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入文本信息，WildIng能够有效缓解地理域迁移导致的性能下降，为野生动物监测提供更可靠的自动识别工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 野生动物监测对于研究生物多样性丧失和气候变化至关重要。摄像机陷阱图像提供了一种非侵入式的方法，用于分析动物种群并识别随时间变化的生态模式。然而，人工分析耗时且资源密集。深度学习，尤其是基础模型，被用于自动化野生动物识别，并在与训练集相同地理位置的数据上表现出强劲性能。然而，尽管前景看好，这些模型在新地理区域的泛化能力仍然存在困难，导致显著的性能下降。例如，在非洲数据集上使用CLIP加适配器训练的先进视觉语言模型可获得84.77%的准确率，但在美国数据集上测试时准确率显著下降至16.17%。这一局限部分源于现有模型主要依赖图像表示，对背景、光照和环境条件等地理数据分布变化敏感。为了解决这一问题，我们提出了WildIng，即针对地理域迁移的野生动物图像不变表示模型。WildIng将文本描述与图像特征相结合，创建更稳健的表示，以应对地理域迁移。通过利用文本描述，我们的方法捕捉到一致的语义信息，例如对物种外观的详细描述，从而提升不同地理位置的泛化能力。实验表明，WildIng在地理域迁移条件下将基础模型BioCLIP的准确率提升了30%。我们在来自美国和非洲的两个不同地区收集的数据集上评估了WildIng。代码和模型已公开发布在 https://github.com/Julian075/CATALOG/tree/WildIng。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.&lt;/p&gt;</description></item><item><guid>2601.01014v1</guid><title>Geometric and Dynamic Scaling in Deep Transformers</title><link>http://arxiv.org/abs/2601.01014v2</link><author>Haoran Su, Chenyu You</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文指出深度Transformer在极深层数时会出现表示冗余、秩下降、最终崩溃的问题，并认为这主要是几何原因。为此提出Manifold-Geometric Transformer（MGT），通过限制残差更新方向和引入可擦除的深度增量学习来解决。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度Transformer在极深层数时表现不佳，现有解释是优化不稳定或梯度消失，但无法解释在现代归一化和初始化下仍然崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 阐明深度Transformer崩溃的根本原因是几何问题，并提出统一的几何框架来解决。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出两条原则：1) 在局部切空间内限制残差更新方向，防止无控制的几何漂移；2) 采用深度增量学习实现数据相关、非单调更新，允许反射和擦除冗余特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通过这些机制可以解耦特征更新的方向和符号，保持几何演化稳定，避免秩崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 几何有效性与动态擦除是防止超深网络秩崩溃的关键，深度本身不是限制因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管Transformer在实践中取得了显著成功，但将其架构推向极深层数往往会导致一种悖论性失败：表示变得越来越冗余，秩下降，最终崩溃。现有解释主要归因于优化不稳定或梯度消失，但这些解释无法说明即使在现代归一化和初始化方案下，崩溃仍然存在。本文认为深度Transformer的崩溃本质上是几何问题。标准的残差更新隐式假设特征累积总是有益的，但没有机制限制更新方向或擦除过时信息。随着深度增加，这导致系统性漂移离开语义流形并出现单调特征累积，导致表示退化。我们提出一个统一的几何框架，通过两条互补原则来解决这些失败。首先，流形约束的超连接将残差更新限制在有效的局部切向方向，防止无控制的流形漂移。其次，深度增量学习引入数据相关、非单调更新，使得可以反射和擦除冗余特征，而不是无条件累积。两种机制共同解耦了特征更新的方向和符号，产生了跨深度的稳定几何演化。我们将这种架构称为Manifold-Geometric Transformer（MGT）。我们的分析预测，强制几何有效性并允许动态擦除是避免超深网络秩崩溃的关键。我们概述了一个评估协议，用于测试超过100层的Transformer，以验证几何而非深度本身是深度表示学习的关键限制因素。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.&lt;/p&gt;</description></item><item><guid>2601.01044v1</guid><title>Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data</title><link>http://arxiv.org/abs/2601.01044v1</link><author>Jin Wang, Angelo De Castro, Yuxi Zhang, Lucas Basolli Borsatto, Yuechen Guo, Victoria Bastos Primo, Ana Beatriz Montevecchio Bernardino, Gota Morota, Ricardo C Chebel, Haipeng Yu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究评估了在小型奶牛场使用大规模农场预训练模型进行体重预测的有效性，并比较了深度图像与点云两种数据模态在三种实验设计下的预测表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 计算机视觉为奶牛监测提供了自动化、无创且可扩展的工具，常用迁移学习从图像预测体重，但其在畜牧业中的有效性和最佳微调策略尚不清楚，尤其是超出 ImageNet 或 COCO 预训练权重的情况；同时，深度图像与三维点云在奶牛体重预测中的直接比较有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 1) 评估从大型农场迁移学习是否能提升小型农场有限数据下的体重预测；2) 在三种实验设计下比较深度图像与点云方法的预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 收集大型、中型、小型奶牛场的顶视深度图像和点云数据，样本量分别为1,201、215、58头；使用 ConvNeXt、MobileViT 处理深度图像，使用 PointNet、DGCNN 处理点云；比较迁移学习、单源学习和联合学习三种策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 迁移学习在小型农场的四种模型中显著提升体重预测，优于单源学习，且与联合学习相当或更好；预训练表示在不同农场和奶牛群体间具有良好泛化；深度图像与点云模型之间无显著性能差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 迁移学习非常适合小型农场预测场景，尤其在跨农场数据共享受限时，只需使用预训练模型权重即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 计算机视觉为监测奶牛提供了自动化、无创且可扩展的工具，从而支持管理、健康评估和表型数据收集。虽然迁移学习常用于从图像预测体重，但其有效性和最佳微调策略在畜牧业应用中尚不清楚，尤其是超出 ImageNet 或 COCO 预训练权重的情况。除此之外，深度图像和三维点云数据已被用于体重预测，但在奶牛中的直接比较有限。因此，本研究的目标是：1）评估从大型农场迁移学习是否能提升小型农场有限数据下的体重预测；2）在三种实验设计下比较深度图像和点云方法的预测性能。我们从大型、中型和小型奶牛场分别收集了1,201、215和58头奶牛的顶视深度图像和点云数据。评估了四种深度学习模型：ConvNeXt 和 MobileViT 用于深度图像，PointNet 和 DGCNN 用于点云。迁移学习在小型农场的四种模型中显著提升了体重预测，优于单源学习，并实现了与联合学习相当或更高的收益。这些结果表明，预训练表示在不同的农场、不同的成像条件和奶牛群体之间具有良好的泛化能力。未观察到深度图像和点云模型之间的持续性能差异。总体而言，这些发现表明，迁移学习非常适合小型农场预测场景，在跨农场数据共享受限于隐私、物流或政策约束时，只需访问预训练模型权重，而不需要原始数据。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.&lt;/p&gt;</description></item><item><guid>2601.01454v1</guid><title>PartImageNet++ Dataset: Enhancing Visual Models with High-Quality Part Annotations</title><link>http://arxiv.org/abs/2601.01454v1</link><author>Xiao Li, Zilong Liu, Yining Liu, Zhuhong Li, Na Dong, Sitian Qin, Xiaolin Hu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了PartImageNet++（PIN++）数据集，包含100K张图像，每类100张带有详细部件标注，覆盖ImageNet-1K所有类别。基于此数据集，作者设计了多尺度部件监督识别模型（MPM），先用PIN++训练部件分割网络，再为未标注图像生成伪部件标签，随后将传统识别网络与辅助旁路层结合，联合监督伪标签和真实标签。作者在PIN++上开展了部件分割、物体分割和少样本学习等实验，探讨了部件标注在下游任务中的多种利用方式。实验表明，部件监督能显著提升模型的鲁棒性，并为多项下游任务奠定了强有力的基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有数据集中高质量部件标注稀缺，限制了基于部件的视觉模型发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建全面的部件标注数据集PIN++，并利用其提出MPM模型以提升ImageNet-1K的分类性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 用PIN++训练部件分割网络；2) 为剩余未标注图像生成伪部件标签；3) 将传统识别网络与辅助旁路层结合，使用伪标签和真实标签共同监督；4) 在PIN++上进行部件分割、物体分割和少样本学习实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 部件监督显著提升了模型在ImageNet-1K上的鲁棒性，并为多项下游任务提供了强基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 部件标注在提升模型性能方面具有重要潜力，PIN++数据集和MPM方法为未来研究提供了有价值的资源和思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 为了解决现有数据集中高质量部件标注稀缺的问题，我们提出了PartImageNet++（PIN++）数据集，该数据集为ImageNet-1K的所有类别提供了详细的部件标注。每个类别都有100张标注图像，总计100K张图像，PIN++是覆盖多种对象类别的最全面的数据集。利用PIN++，我们提出了多尺度部件监督识别模型（MPM），以在ImageNet-1K上实现鲁棒分类。我们首先使用PIN++训练了部件分割网络，并利用它为剩余未标注的图像生成伪部件标签。MPM随后将传统识别架构与辅助旁路层相结合，联合监督伪部件标签和原始部件标注。此外，我们在PIN++上进行了广泛的实验，包括部件分割、物体分割和少样本学习，探索了在下游任务中利用部件标注的多种方式。实验结果表明，我们的方法不仅提升了基于部件的模型在鲁棒对象识别方面的性能，还为多项下游任务建立了强有力的基线，凸显了部件标注在提升模型性能方面的潜力。数据集和代码可在 https://github.com/LixiaoTHU/PartImageNetPP 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;To address the scarcity of high-quality part annotations in existing datasets, we introduce PartImageNet++ (PIN++), a dataset that provides detailed part annotations for all categories in ImageNet-1K. With 100 annotated images per category, totaling 100K images, PIN++ represents the most comprehensive dataset covering a diverse range of object categories. Leveraging PIN++, we propose a Multi-scale Part-supervised recognition Model (MPM) for robust classification on ImageNet-1K. We first trained a part segmentation network using PIN++ and used it to generate pseudo part labels for the remaining unannotated images. MPM then integrated a conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and the original part annotations. Furthermore, we conducted extensive experiments on PIN++, including part segmentation, object segmentation, and few-shot learning, exploring various ways to leverage part annotations in downstream tasks. Experimental results demonstrated that our approach not only enhanced part-based models for robust object recognition but also established strong baselines for multiple downstream tasks, highlighting the potential of part annotations in improving model performance. The dataset and the code are available at https://github.com/LixiaoTHU/PartImageNetPP.&lt;/p&gt;</description></item><item><guid>2601.01473v2</guid><title>Accelerating Storage-Based Training for Graph Neural Networks</title><link>http://arxiv.org/abs/2601.01473v2</link><author>Myung-Hwan Jang, Jeong-Min Park, Yunyong Ko, Sang-Wook Kim</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了AGNES框架，改进了存储式GNN训练中的I/O瓶颈，显著提升了大规模图数据的训练效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着真实世界图规模不断扩大，传统GNN训练难以满足需求，研究者开始利用外部存储（如NVMe SSD）在单机上进行大规模GNN训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决存储式GNN训练中因大量小I/O导致的数据准备瓶颈问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; AGNES采用块级存储I/O处理，充分利用高性能存储设备的带宽，并结合基于图特征的超批处理策略，进一步提升每次I/O的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在五个真实图数据集上实验表明，AGNES比四种最先进方法快最多4.1倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AGNES通过块级I/O和超批处理有效缓解了存储I/O瓶颈，显著提升了大规模GNN训练的速度和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图神经网络（GNN）因其强大的表达能力，在各类真实世界下游任务中取得了突破。随着真实世界图规模的持续增长，研究者开始探索基于存储的GNN训练方法，该方法利用外部存储（如NVMe SSD）在单台机器上处理大规模网络图。虽然这些基于存储的GNN训练方法在大规模训练中展现出良好潜力，但我们发现它们在数据准备阶段存在严重瓶颈，因为它们忽视了一个关键挑战：如何处理大量小规模的存储I/O。为解决这一挑战，本文提出了一种新型基于存储的GNN训练框架——AGNES，该框架采用块级存储I/O处理方法，充分利用高性能存储设备的I/O带宽。此外，为进一步提升每次存储I/O的效率，AGNES基于真实图的特性采用了简单而有效的超批处理策略。对五个真实图数据集的全面实验表明，AGNES始终优于四种最先进方法，最快可比最佳竞争者快4.1倍。代码已发布在 https://github.com/Bigdasgit/agnes-kdd26。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, a storage-based approach to GNN training has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: how to handle a large number of small storage I/Os. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named AGNES, that employs a method of block-wise storage I/O processing to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, AGNES employs a simple yet effective strategy, hyperbatch-based processing based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that AGNES consistently outperforms four state-of-the-art methods, by up to 4.1X faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.&lt;/p&gt;</description></item><item><guid>2601.01554v1</guid><title>MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization</title><link>http://arxiv.org/abs/2601.01554v2</link><author>MOSI. AI, Donghua Yu, Zhengyuan Lin, Chen Yang, Yiyang Zhang, Hanfu Chen, Jingqi Chen, Ke Chen, Liwei Fan, Yi Jiang, Jie Zhu, Muchen Li, Wenxuan Wang, Yang Wang, Zhe Xu, Yitian Gong, Yuqian Zhang, Wenbo Zhang, Zhaoye Fei, Qinyuan Cheng, Shimin Li, Xipeng Qiu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; MOSS Transcribe Diarize 是一种统一的多模态大语言模型，能够端到端完成带说话人标签和时间戳的转录任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的说话人归属时间戳转录系统很少采用端到端方法，且受限于上下文窗口小、长距离说话人记忆弱以及无法输出时间戳等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服上述限制，提供一种能够处理长达 90 分钟输入、具有 128k 上下文窗口的高效转录系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在大量真实野外数据上训练，使用统一的多模态大语言模型实现端到端的说话人归属时间戳转录。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多项公开和内部基准测试中，MOSS Transcribe Diarize 的性能优于现有最先进的商业系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型在规模、泛化能力和实时性方面表现出色，为会议转录等应用提供了可靠的技术方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.&lt;/p&gt;</description></item><item><guid>2601.01720v1</guid><title>FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing</title><link>http://arxiv.org/abs/2601.01720v2</link><author>Xijie Huang, Chengming Xu, Donghao Luo, Xiaobin Hu, Peng Tang, Xu Peng, Jiangning Zhang, Chengjie Wang, Yanwei Fu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个新的大规模视频数据集FFP-300K，并基于该数据集开发了一种完全无引导的第一帧传播（FFP）框架。该框架通过自适应时空位置编码（AST-RoPE）和自蒸馏身份传播任务，解决了保持第一帧外观与保留源视频运动之间的矛盾，并在EditVerseBench基准上显著优于现有学术和商业模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的FFP方法依赖繁琐的运行时引导，主要原因是训练数据集过短、分辨率低且缺乏任务多样性，无法学习到稳健的时间先验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补训练数据不足的根本缺口，构建一个大规模、多样化的视频数据集，并设计一种真正无引导的FFP框架，以提升视频编辑的可控性和稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 构建FFP-300K数据集：300K对720p、81帧长度的视频，采用两轨流程实现多样化的局部与全局编辑。2) 设计AST-RoPE：动态重映位置编码，分离外观与运动参考。3) 引入自蒸馏身份传播任务：作为正则化器，保证长期时间稳定性并防止语义漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在EditVerseBench基准上，所提方法相较于现有学术和商业模型提升了约0.2的PickScore和0.3的VLM分数，显示出显著的性能优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过大规模数据集和创新的无引导框架，FFP技术可以实现更高质量、更稳定的视频编辑，证明了无引导FFP的可行性与优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 第一帧传播（FFP）为可控视频编辑提供了一种有前景的范式，但现有方法受制于繁琐的运行时引导。我们认为这一限制的根本原因是当前训练数据集不足，往往过短、低分辨率且缺乏足够的任务多样性，无法教授稳健的时间先验。为填补这一基础数据缺口，我们首先推出了FFP-300K——一个包含30万对高保真720p分辨率、81帧长度视频的新大规模数据集，该数据集通过两轨原则化流程构建，涵盖多样化的局部与全局编辑。基于此数据集，我们提出了一种旨在实现真正无引导FFP的新框架，解决了保持第一帧外观与保留源视频运动之间的关键张力。架构层面，我们引入了自适应时空RoPE（AST-RoPE），动态重映位置编码以解耦外观与运动参考；在目标层面，我们采用自蒸馏策略，将身份传播任务作为强有力的正则化器，确保长期时间稳定性并防止语义漂移。对EditVerseBench基准的全面实验表明，我们的方法在PickScore和VLM分数上分别比现有学术和商业模型提升约0.2和0.3，显示出显著的性能优势。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.&lt;/p&gt;</description></item><item><guid>2601.01856v1</guid><title>GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection</title><link>http://arxiv.org/abs/2601.01856v1</link><author>Joongwon Chae, Lihui Luo, Yang Liu, Runming Wang, Dongmei Yu, Zeming Liang, Xi Yuan, Dayan Zhang, Zhenglin Chen, Peiwu Qin, Ilmoon Chae</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 传统方法侧重同类异常评分，忽略跨类路由问题。2. 实际部署需要在类别未知时进行任务无关的持续异常检测。3. 现有跨头路由规则因分数分布差异不可靠。4. 提出 GCR 框架，通过几何一致路由在共享冻结的补丁嵌入空间中选择专家。5. 路由后仅在选定专家内部使用原型评分生成异常图。6. 该方法分离跨头决策与内部评分，避免分数可比性问题。7. 在 MVTec AD 与 VisA 数据集实验中，GCR 提升路由稳定性，几乎无遗忘，保持竞争性的检测与定位性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 工业检测中，基于特征的异常检测因大型预训练视觉编码器的强大表示能力而被广泛采用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在类别不断扩展且测试时类别未知的场景下，稳定任务无关的持续异常检测，解决跨专家路由不稳定的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建轻量级混合专家框架 GCR；在共享冻结的补丁嵌入空间中，通过最小化到各类别原型库的最近原型距离进行几何一致路由；随后仅在被路由专家内部使用标准原型评分规则生成异常图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明几何一致路由显著提升路由稳定性，缓解持续性能崩溃，几乎无遗忘，同时保持检测与定位性能；许多先前归因于表示遗忘的失败实际上是由跨头决策规则不稳定导致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 几何一致路由通过分离跨头决策与内部评分，避免了分数可比性问题，实现了稳定的持续异常检测，并将遗忘降至近零。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior. We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning. Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.   We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.   Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR&lt;/p&gt;</description></item><item><guid>2601.01874v1</guid><title>CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving</title><link>http://arxiv.org/abs/2601.01874v1</link><author>Shuhang Chen, Yunqiu Xu, Junjie Xie, Aojun Lu, Tao Feng, Zeying Huang, Ning Zhang, Yi Sun, Yi Yang, Hangjie Yuan</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了CogFlow框架，解决多模态大语言模型在视觉数学推理中的困难，通过三阶段流程提升感知、内部化和推理，并通过奖励机制和算法保证视觉信息的有效利用，验证了其优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大语言模型在视觉数学问题求解上仍存在挑战，现有工作主要关注视觉感知的改进，忽视了视觉线索在后续推理中的整合与利用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究并解决视觉线索在推理过程中的有效整合问题，提出一种认知启发的三阶段框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CogFlow框架包括感知、内部化、推理三阶段；使用协同视觉奖励提升感知；内部化阶段引入知识内部化奖励模型；推理阶段采用视觉门控策略优化算法；并构建MathCog数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明CogFlow在常用视觉数学推理基准上优于现有方法，证明了三阶段流程和奖励机制的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CogFlow通过模拟人类认知流程，显著提升了多模态模型在视觉数学推理中的表现，验证了视觉信息整合的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管取得了显著进展，多模态大型语言模型在视觉数学问题求解方面仍面临挑战。近期工作认识到视觉感知是视觉数学推理的瓶颈，但其解决方案仅限于改进视觉输入的提取和解释。值得注意的是，它们忽视了提取的视觉线索是否被忠实整合并在后续推理中得到恰当利用这一关键问题。为此，我们提出了CogFlow，一种新颖的认知启发的三阶段框架，包含知识内部化阶段，明确模拟人类推理的层级流程：感知→内部化→推理。与此层级流程保持一致，我们全面提升了各阶段。我们设计了协同视觉奖励，以提升参数空间和语义空间的感知能力，联合改进符号和图表的视觉信息提取。为保证提取的视觉线索在后续推理中的忠实整合，我们在内部化阶段引入了知识内部化奖励模型，桥接感知与推理。此外，我们设计了视觉门控策略优化算法，进一步强制推理以视觉知识为基础，防止模型寻找看似连贯但视觉上不扎根的推理链。我们还贡献了新的数据集MathCog用于模型训练，包含超过12万条高质量的感知-推理对齐注释。对常用视觉数学推理基准的全面实验和分析验证了所提出的CogFlow的优越性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\Rightarrow$internalization$\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.&lt;/p&gt;</description></item><item><guid>2601.02088v1</guid><title>PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction</title><link>http://arxiv.org/abs/2601.02088v2</link><author>Jiahao Bao, Huazhen Liu, Yu Zhuang, Leran Tao, Xinyu Xu, Yongtao Shi, Mengjia Cheng, Yiming Wang, Congshuang Ku, Ting Zeng, Yilang Du, Siyi Chen, Shunyao Shen, Suncheng Xiang, Hongbo Yu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 PhysSFI-Net，利用物理信息的几何深度学习框架，预测正颌手术后软组织变形，结合层次图模块、LSTM 预测器和生物力学模块，训练于135例患者，结果显示预测误差低于现有方法，具有临床应用潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 正颌手术通过重新定位颌骨来恢复咬合并改善面部美学，准确的术后面部形态模拟对术前规划至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发并验证一种物理信息几何深度学习框架，以精确预测正颌手术后软组织变形。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建 PhysSFI-Net，包含层次图模块（结合颅面与手术计划编码器及注意力机制）、基于 LSTM 的序列预测器以及生物力学启发的高分辨率面部表面重建模块；使用点云形状误差、表面偏差误差和标志点定位误差评估模型；在135例患者数据上训练和验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; PhysSFI-Net 的点云形状误差约1.07毫米，表面偏差误差约1.30毫米，标志点定位误差约2.45毫米，优于 ACMT-Net。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PhysSFI-Net 能以可解释、高分辨率的方式预测术后面部形态，准确性优于现有方法，显示出在正颌手术规划与模拟中的临床应用前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 正颌手术通过重新定位颌骨来恢复咬合并提升面部美学。术后面部形态的准确模拟对于术前规划至关重要。然而，传统的生物力学模型计算量大，而几何深度学习方法往往缺乏可解释性。本文开发并验证了一种名为 PhysSFI-Net 的物理信息几何深度学习框架，用于精确预测正颌手术后软组织变形。PhysSFI-Net 由三部分组成：一个层次图模块，结合颅面和手术计划编码器并使用注意力机制提取骨骼-面部交互特征；一个基于 LSTM 的序列预测器，用于逐步预测软组织变形；以及一个受生物力学启发的高分辨率面部表面重建模块。模型性能通过点云形状误差（Hausdorff 距离）、表面偏差误差和面部标志点定位误差（面部标志点的欧氏距离）与真实数据进行比较。共计 135 名接受综合正畸和正颌治疗的患者用于模型训练和验证。定量分析显示，PhysSFI-Net 的点云形状误差为 1.070 ± 0.088 毫米，表面偏差误差为 1.296 ± 0.349 毫米，标志点定位误差为 2.445 ± 1.326 毫米。与现有方法 ACMT-Net 的比较实验表明，PhysSFI-Net 在预测精度上更优。综上所述，PhysSFI-Net 能以可解释、高分辨率的方式预测术后面部形态，具有优越的准确性，显示出在正颌手术规划和模拟中的强大临床应用潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.&lt;/p&gt;</description></item><item><guid>2601.02088v2</guid><title>PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction</title><link>http://arxiv.org/abs/2601.02088v2</link><author>Jiahao Bao, Huazhen Liu, Yu Zhuang, Leran Tao, Xinyu Xu, Yongtao Shi, Mengjia Cheng, Yiming Wang, Congshuang Ku, Ting Zeng, Yilang Du, Siyi Chen, Shunyao Shen, Suncheng Xiang, Hongbo Yu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种物理信息几何深度学习框架PhysSFI-Net，用于精准预测正颌手术后软组织变形。通过三大模块实现骨骼-面部交互特征提取、序列化软组织变形预测以及高分辨率面部表面重建。实验显示该模型在形状误差、表面偏差和关键点定位误差方面均优于现有方法，并具有良好的可解释性和临床应用潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 正颌手术需要准确模拟术后面部形态以指导术前规划。传统的生物力学模型计算量大，几何深度学习方法缺乏可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发并验证一种物理信息几何深度学习框架，以实现对正颌手术后软组织变形的精确预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PhysSFI-Net由层次图模块、LSTM序列预测器和生物力学启发模块组成。层次图模块结合颅面与手术计划编码器并使用注意力机制提取骨骼-面部交互特征；LSTM预测器逐步生成软组织变形；生物力学模块实现高分辨率面部表面重建。使用135例患者数据进行训练与验证，并通过点云形状误差、表面偏差误差和关键点定位误差评估模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在点云形状误差约1.07毫米、表面偏差误差约1.30毫米、关键点定位误差约2.45毫米的水平上表现优异，并在与ACMT-Net的对比实验中取得更高的预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PhysSFI-Net能够以可解释、高分辨率的方式预测正颌手术后的面部形态，准确性优于现有方法，具有显著的临床应用前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 正颌手术通过重新定位颌骨来恢复咬合并提升面部美学。术后面部形态的准确模拟对于术前规划至关重要。然而，传统的生物力学模型计算量大，而几何深度学习方法往往缺乏可解释性。在本研究中，我们开发并验证了一种名为PhysSFI-Net的物理信息几何深度学习框架，用于精确预测正颌手术后的软组织变形。PhysSFI-Net由三部分组成：一个层次图模块，结合颅面和手术计划编码器，并使用注意力机制提取骨骼-面部交互特征；一个基于长短期记忆网络的序列预测器，用于逐步预测软组织变形；以及一个受生物力学启发的模块，用于高分辨率面部表面重建。通过点云形状误差（Hausdorff距离）、表面偏差误差以及关键点定位误差（颅面关键点的欧氏距离）评估模型性能。我们使用135例接受综合正畸和正颌治疗的患者进行模型训练和验证。定量分析表明，PhysSFI-Net实现了约1.07毫米的点云形状误差、约1.30毫米的表面偏差误差和约2.45毫米的关键点定位误差。对比实验表明，PhysSFI-Net在预测精度上优于最先进的方法ACMT-Net。总之，PhysSFI-Net能够以可解释、高分辨率的方式预测术后面部形态，并具有优越的准确性，显示出在正颌手术规划和模拟中的强大临床应用潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.&lt;/p&gt;</description></item><item><guid>2601.02401v1</guid><title>Spiking Heterogeneous Graph Attention Networks</title><link>http://arxiv.org/abs/2601.02401v1</link><author>Buqing Cao, Qian Peng, Xiang Xie, Liang Chen, Min Shi, Jianxun Liu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新的异构图注意力网络SpikingHAN，利用脉冲神经网络的低能耗特性，减少了传统HGNN的计算成本，同时保持了良好的节点分类性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现实世界中的图网络往往是异构的，包含多种节点和关系。传统的异构图神经网络能够有效处理这些信息，但其复杂结构导致内存占用大、推理时间长，难以在资源受限的设备上部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决传统HGNN在资源受限环境下的高计算成本问题，本文旨在设计一种更轻量、能耗更低的异构图学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SpikingHAN采用单层共享参数的图卷积聚合元路径邻居信息，随后使用语义层注意力机制捕捉不同元路径的重要性并进行语义聚合，最后通过脉冲神经网络将异构信息编码为脉冲序列，得到一位二值化表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个真实异构图数据集上的实验表明，SpikingHAN在节点分类任务中与传统HGNN性能相当，同时参数更少、推理更快、内存占用和能耗显著降低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SpikingHAN通过结合脉冲神经网络的低能耗特性，成功实现了轻量化的异构图学习，证明了在资源受限设备上部署高效图模型的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Real-world graphs or networks are usually heterogeneous, involving multiple types of nodes and relationships. Heterogeneous graph neural networks (HGNNs) can effectively handle these diverse nodes and edges, capturing heterogeneous information within the graph, thus exhibiting outstanding performance. However, most methods of HGNNs usually involve complex structural designs, leading to problems such as high memory usage, long inference time, and extensive consumption of computing resources. These limitations pose certain challenges for the practical application of HGNNs, especially for resource-constrained devices. To mitigate this issue, we propose the Spiking Heterogeneous Graph Attention Networks (SpikingHAN), which incorporates the brain-inspired and energy-saving properties of Spiking Neural Networks (SNNs) into heterogeneous graph learning to reduce the computing cost without compromising the performance. Specifically, SpikingHAN aggregates metapath-based neighbor information using a single-layer graph convolution with shared parameters. It then employs a semantic-level attention mechanism to capture the importance of different meta-paths and performs semantic aggregation. Finally, it encodes the heterogeneous information into a spike sequence through SNNs, simulating bioinformatic processing to derive a binarized 1-bit representation of the heterogeneous graph. Comprehensive experimental results from three real-world heterogeneous graph datasets show that SpikingHAN delivers competitive node classification performance. It achieves this with fewer parameters, quicker inference, reduced memory usage, and lower energy consumption. Code is available at https://github.com/QianPeng369/SpikingHAN.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Real-world graphs or networks are usually heterogeneous, involving multiple types of nodes and relationships. Heterogeneous graph neural networks (HGNNs) can effectively handle these diverse nodes and edges, capturing heterogeneous information within the graph, thus exhibiting outstanding performance. However, most methods of HGNNs usually involve complex structural designs, leading to problems such as high memory usage, long inference time, and extensive consumption of computing resources. These limitations pose certain challenges for the practical application of HGNNs, especially for resource-constrained devices. To mitigate this issue, we propose the Spiking Heterogeneous Graph Attention Networks (SpikingHAN), which incorporates the brain-inspired and energy-saving properties of Spiking Neural Networks (SNNs) into heterogeneous graph learning to reduce the computing cost without compromising the performance. Specifically, SpikingHAN aggregates metapath-based neighbor information using a single-layer graph convolution with shared parameters. It then employs a semantic-level attention mechanism to capture the importance of different meta-paths and performs semantic aggregation. Finally, it encodes the heterogeneous information into a spike sequence through SNNs, simulating bioinformatic processing to derive a binarized 1-bit representation of the heterogeneous graph. Comprehensive experimental results from three real-world heterogeneous graph datasets show that SpikingHAN delivers competitive node classification performance. It achieves this with fewer parameters, quicker inference, reduced memory usage, and lower energy consumption. Code is available at https://github.com/QianPeng369/SpikingHAN.&lt;/p&gt;</description></item><item><guid>2601.02409v1</guid><title>Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis</title><link>http://arxiv.org/abs/2601.02409v1</link><author>Longwei Wang, Ifrat Ikhtear Uddin, KC Santosh</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出双框架解决方案，结合专家引导的可解释少样本学习和可解释引导的主动学习，以提高医学影像分析的准确性和可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 医学影像分析面临标注数据稀缺和模型可解释性不足的挑战，阻碍临床AI部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在解决数据稀缺的同时提升模型预测的透明度和可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) EGxFSL：利用放射科医生标注的感兴趣区域作为空间监督，通过Grad-CAM的Dice损失与原型分类共同优化，实现可解释的少样本学习。2) xGAL：在主动学习中迭代采样，优先考虑预测不确定性和注意力偏差，形成闭环框架，使可解释性指导训练与样本选择。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在BraTS、VinDr-CXR和SIIM-COVID-19数据集上，准确率分别达到92%、76%和62%，均优于无引导基线；在样本极少的情况下，xGAL仅用680个样本即可获得76%的准确率，远高于随机采样的57%；Grad-CAM可视化显示模型聚焦于诊断相关区域，并在乳腺超声上验证了跨模态适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 专家引导的可解释学习与可解释引导的主动学习相结合，可显著提升医学影像模型的准确性和可解释性，具有广泛的临床应用潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 医学影像分析面临两个关键挑战：标注数据稀缺和模型可解释性不足，这两者都阻碍了临床人工智能的部署。少样本学习（FSL）解决了数据限制问题，但在预测时缺乏透明度。主动学习（AL）方法优化了数据获取，但忽视了所获取样本的可解释性。我们提出了双框架解决方案：专家引导的可解释少样本学习（EGxFSL）和可解释引导的主动学习（xGAL）。EGxFSL通过将放射科医生定义的感兴趣区域作为空间监督，结合基于Grad-CAM的Dice损失，与原型分类共同优化，实现可解释的少样本学习。xGAL引入了迭代样本获取，优先考虑预测不确定性和注意力不匹配，创建了一个闭环框架，使可解释性在训练和样本选择中相互协同。我们在BraTS（MRI）、VinDr-CXR（胸部X光）和SIIM-COVID-19（胸部X光）数据集上取得了92%、76%和62%的准确率，始终优于无引导基线。在严重数据限制下，xGAL仅用680个样本即可达到76%的准确率，而随机采样仅能达到57%。Grad-CAM可视化表明，指导模型聚焦于诊断相关区域，并在乳腺超声上验证了跨模态适用性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.&lt;/p&gt;</description></item><item><guid>2601.02414v1</guid><title>MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion</title><link>http://arxiv.org/abs/2601.02414v1</link><author>Jichao Zhu, Jun Yu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的多模态情感识别方法MIAR，利用模态间特征交互和对齐来提升情感识别性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往方法主要关注模态融合，却未充分考虑模态间分布差异、贡献差异以及文本模型特征的泛化能力，导致多模态场景下性能受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决模态分布差异和贡献不均衡问题，并提升跨文本模型的泛化能力，以提高多模态情感识别效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MIAR通过特征交互生成四个全局特征标记，表示每个模态从其他模态提取信息的方式，并采用对比学习和归一化策略对不同模态进行对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在CMU-MOSI和CMU-MOSEI两个基准数据集上，MIAR的实验结果优于现有最先进的多模态情感识别方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MIAR通过模态交互与对齐显著提升了多模态情感识别性能，证明了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态情感识别（MER）旨在通过语言、视觉和音频三种模式感知人类情感。以往方法主要关注模态融合，却未充分解决模态间显著的分布差异或考虑它们对任务的不同贡献。它们还缺乏在多样化文本模型特征上的稳健泛化能力，从而限制了多模态场景下的性能。因此，我们提出了一种名为模态交互与对齐表示（MIAR）的新方法。该网络通过特征交互整合不同模态的上下文特征，生成特征标记来表示每个模态从其他模态提取信息的全局表示。这四个标记代表了每个模态如何从其他模态提取信息的全局表示。MIAR使用对比学习和归一化策略对不同模态进行对齐。我们在CMU-MOSI和CMU-MOSEI两个基准数据集上进行了实验，实验结果表明MIAR优于现有最先进的MER方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.&lt;/p&gt;</description></item><item><guid>2601.02427v1</guid><title>NitroGen: An Open Foundation Model for Generalist Gaming Agents</title><link>http://arxiv.org/abs/2601.02427v1</link><author>Loïc Magne, Anas Awadalla, Guanzhi Wang, Yinzhen Xu, Joshua Belofsky, Fengyuan Hu, Joohwan Kim, Ludwig Schmidt, Georgia Gkioxari, Jan Kautz, Yisong Yue, Yejin Choi, Yuke Zhu, Linxi "Jim" Fan</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了 NitroGen，一种面向通用游戏代理的视觉-动作基础模型，利用 40,000 小时的游戏视频训练，覆盖 1,000 多款游戏。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 当前游戏代理往往针对单一游戏，缺乏跨游戏泛化能力，缺少大规模公开可用的玩家动作数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在多种游戏中表现出色、并能迁移到未见游戏的通用游戏代理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①构建互联网规模的视频-动作数据集，自动从公开游戏视频中提取玩家动作；②设计多游戏基准环境，用于评估跨游戏泛化；③训练统一的视觉-动作模型，采用大规模行为克隆。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; NitroGen 在 3D 动作游戏的战斗、2D 平台游戏的高精度控制以及程序生成世界的探索等多领域表现出色；在未见游戏中迁移效果显著，任务成功率相较从零训练的模型提升高达 52%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过发布数据集、评估套件和模型权重，推动通用具身代理研究的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 NitroGen，一种面向通用游戏代理的视觉-动作基础模型，训练于 40,000 小时的游戏视频，覆盖 1,000 多款游戏。我们整合了三大关键要素：①通过自动从公开游戏视频中提取玩家动作构建的互联网规模视频-动作数据集；②可测量跨游戏泛化的多游戏基准环境；③采用大规模行为克隆训练的统一视觉-动作模型。NitroGen 在多种领域表现出强大能力，包括 3D 动作游戏中的战斗、2D 平台游戏中的高精度控制以及程序生成世界中的探索。它能有效迁移到未见游戏，任务成功率相较从零训练的模型提升高达 52%。我们发布数据集、评估套件和模型权重，以推动通用具身代理研究。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.&lt;/p&gt;</description></item><item><guid>2601.02439v1</guid><title>WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</title><link>http://arxiv.org/abs/2601.02439v1</link><author>Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了WebGym，这是目前最大的开源环境，用于训练真实视觉网页代理。WebGym包含近30万个任务，覆盖多样化的真实网站和不同难度级别，并提供基于评分标准的评估。作者使用简单的强化学习方法，利用代理自身的交互轨迹和任务奖励进行训练，并通过高吞吐量的异步采样系统将轨迹采样速度提升4-5倍。进一步扩大任务集的广度、深度和规模，持续提升性能。对强大的视觉语言模型Qwen-3-VL-8B-Instruct进行微调后，在从未见过的网站上测试成功率从26.2%提升至42.9%，显著优于基于GPT-4o和GPT-5-Thinking的代理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 真实网站具有非平稳性和多样性，人工或小规模任务集不足以实现鲁棒的策略学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个大规模、真实多样化的网页任务环境WebGym，以支持更稳健的视觉网页代理训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 创建近30万个基于评分标准的网页任务；2) 采用简单的强化学习方案，利用代理自身的交互轨迹和任务奖励进行学习；3) 开发高吞吐量的异步采样系统，将轨迹采样速度提升4-5倍；4) 扩大任务集的广度、深度和规模；5) 在WebGym上微调强大的视觉语言模型Qwen-3-VL-8B-Instruct。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在未见过的网站测试中，成功率从26.2%提升至42.9%，显著优于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）。此外，异步采样系统实现了4-5倍的轨迹采样速度提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; WebGym通过大规模、多样化的真实网页任务集和高效的强化学习扩展，显著提升了视觉网页代理在未知网站上的性能，证明了其在训练鲁棒代理方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文介绍了WebGym，这是目前最大的开源环境，用于训练真实视觉网页代理。真实网站具有非平稳性和多样性，人工或小规模任务集不足以实现鲁棒的策略学习。WebGym包含近30万个任务，覆盖多样化的真实网站和不同难度级别，并提供基于评分标准的评估。作者使用简单的强化学习方法，利用代理自身的交互轨迹和任务奖励进行训练，并通过高吞吐量的异步采样系统将轨迹采样速度提升4-5倍。进一步扩大任务集的广度、深度和规模，持续提升性能。对强大的视觉语言模型Qwen-3-VL-8B-Instruct进行微调后，在从未见过的网站上测试成功率从26.2%提升至42.9%，显著优于基于GPT-4o和GPT-5-Thinking的代理。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&amp;#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.&lt;/p&gt;</description></item><item><guid>2601.02451v1</guid><title>mHC-GNN: Manifold-Constrained Hyper-Connections for Graph Neural Networks</title><link>http://arxiv.org/abs/2601.02451v1</link><author>Subhankar Mishra</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的图神经网络架构mHC-GNN，利用流形约束的超连接方法来缓解深层网络的过平滑问题，并提升网络的表达能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统图神经网络在深层时容易出现过平滑现象，且其表达能力受限于1-Weisfeiler-Leman测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将Transformer中提出的流形约束超连接方法迁移到图神经网络，以解决过平滑和表达能力受限的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; mHC-GNN通过在n个并行流中扩展节点表示，并使用Sinkhorn-Knopp归一化将流混合矩阵约束在Birkhoff多面体上，从而实现流形约束。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明mHC-GNN的过平滑速率指数级减慢，能够区分超出1-WL的图；在10个数据集、4种GNN架构上均有持续提升；在深度实验中，mHC-GNN在128层时仍保持超过74%的准确率，极深层提升超过50个百分点；消除流形约束会导致高达82%的性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; mHC-GNN通过流形约束的超连接方法有效缓解了深层图神经网络的过平滑问题，并提升了网络的表达能力，表现出显著的性能优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图神经网络（GNN）在深层架构中容易出现过平滑问题，并且其表达能力受限于1-Weisfeiler-Leman（1-WL）测试。我们将最近在Transformer中提出的流形约束超连接（Manifold-Constrained Hyper-Connections，MHC）方法应用到图神经网络中，提出了mHC-GNN。该方法通过在n个并行流中扩展节点表示，并使用Sinkhorn-Knopp归一化将流混合矩阵约束在Birkhoff多面体上。我们证明mHC-GNN的过平滑速率指数级减慢，并且能够区分超出1-WL的图。对10个数据集、4种GNN架构的实验显示，mHC-GNN在各个层数上均有持续提升。深度实验从2层到128层表明，传统GNN在超过16层后性能接近随机，而mHC-GNN在128层时仍保持超过74%的准确率，极深层时提升超过50个百分点。消除流形约束的消融实验表明该约束至关重要，去除后性能下降高达82%。代码已发布在GitHub上。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph Neural Networks (GNNs) suffer from over-smoothing in deep architectures and expressiveness bounded by the 1-Weisfeiler-Leman (1-WL) test. We adapt Manifold-Constrained Hyper-Connections (\mhc)~\citep{xie2025mhc}, recently proposed for Transformers, to graph neural networks. Our method, mHC-GNN, expands node representations across $n$ parallel streams and constrains stream-mixing matrices to the Birkhoff polytope via Sinkhorn-Knopp normalization. We prove that mHC-GNN exhibits exponentially slower over-smoothing (rate $(1-γ)^{L/n}$ vs.\ $(1-γ)^L$) and can distinguish graphs beyond 1-WL. Experiments on 10 datasets with 4 GNN architectures show consistent improvements. Depth experiments from 2 to 128 layers reveal that standard GNNs collapse to near-random performance beyond 16 layers, while mHC-GNN maintains over 74\% accuracy even at 128 layers, with improvements exceeding 50 percentage points at extreme depths. Ablations confirm that the manifold constraint is essential: removing it causes up to 82\% performance degradation. Code is available at \href{https://github.com/smlab-niser/mhc-gnn}{https://github.com/smlab-niser/mhc-gnn}&lt;/p&gt;</description></item><item><guid>2601.02457v1</guid><title>PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding</title><link>http://arxiv.org/abs/2601.02457v1</link><author>Souhail Hadgi, Bingchen Gong, Ramana Sundararaman, Emery Pierson, Lei Li, Peter Wonka, Maks Ovsjanikov</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种仅使用编码器的3D模型，能够直接从点云生成与语言对齐的补丁级特征，实现零样本3D部件分割，并在多个基准上显著优于以往基于渲染或前馈的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有3D形状基础模型在全局任务上表现优异，但在局部部件推理方面转移效果差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补现有方法在多视角渲染和语言模型提示工程上的高成本和对3D几何的忽视，提出一种更高效、直接利用点云的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用已有的数据引擎生成带部件标注的3D形状，将多视角SAM区域与VLM字幕配对；随后训练点云变压器编码器，分两步：先将2D视觉编码器的稠密特征蒸馏到3D补丁；再通过多正对比目标将补丁嵌入与部件级文本嵌入对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该3D编码器在零样本部件分割任务中实现一次性快速推理，无需测试时多视角渲染，并在多个基准上显著优于之前的渲染式和前馈式方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过仅编码器的设计和对齐训练，能够高效、准确地完成3D部件分割，证明了直接从点云学习语言对齐特征的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 当前针对3D形状的基础模型在全局任务（检索、分类）上表现出色，但在局部部件级推理方面迁移效果不佳。近期的方法利用视觉与语言基础模型，通过多视角渲染和文本查询直接解决密集任务。虽然前景可观，但这些流程需要在多视角渲染上进行昂贵的推理，且高度依赖大型语言模型的提示工程来生成字幕，且未能利用形状的内在三维几何。我们通过引入一种仅使用编码器的3D模型来填补这一空白，该模型能够直接从点云生成与语言对齐的补丁级特征。我们的预训练方法基于现有的数据引擎，该引擎通过将多视角SAM区域与VLM字幕配对来生成带部件标注的3D形状。利用这些数据，我们在两阶段训练点云变压器编码器：（1）将来自视觉编码器（如DINOv2）的稠密2D特征蒸馏到3D补丁；（2）通过多正对比目标将这些补丁嵌入与部件级文本嵌入对齐。我们的3D编码器实现了零样本3D部件分割，能够在单次推理中快速完成，无需测试时多视角渲染，并在多个3D部件分割基准上显著优于之前的渲染式和前馈式方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在三维形状之间建立密集对应关系的问题。密集对应对于形状编辑、变形、语义分割等任务至关重要，因为它允许在不同模型之间精确对齐细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先提出只关注局部区域而非整体形状的思路，并借鉴了现有的局部特征提取与对齐技术（如 PointNetLK、OpenShape 等）。他们设计了一套基于局部补丁的特征学习与对齐网络，利用已有方法的优势并在此基础上进行改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对局部补丁进行特征学习并对齐来实现全局密集对应。实现流程包括：① 在每个形状上采样点并提取局部补丁；② 用学习的网络生成补丁特征；③ 通过对齐模块预测补丁之间的变换；④ 迭代细化对应关系并聚合得到完整的密集映射。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 引入可微分的局部补丁对齐模块；② 通过局部特征而非全局特征实现更鲁棒的对应；③ 在非等距形状上保持高精度。与之前的工作相比，它减少了对全局配准的依赖，并在多样化形状上表现出更好的适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PatchAlign3D 提出了基于局部补丁对齐的学习框架，能够在三维形状之间生成高精度的密集对应关系。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/&lt;/p&gt;</description></item><item><guid>2601.02509v1</guid><title>hdlib 2.0: Extending Machine Learning Capabilities of Vector-Symbolic Architectures</title><link>http://arxiv.org/abs/2601.02509v1</link><author>Fabio Cumbo, Kabir Dhillon, Daniel Blankenberg</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了hdlib的重大扩展，显著提升了其在向量符号体系中的机器学习功能，包括监督分类、回归、聚类、图学习以及首次实现量子超维计算和量子机器学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; hdlib最初发布用于构建向量符号体系（VSA），VSA是一种利用高维向量表示和处理信息的计算范式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 扩展hdlib的机器学习能力，以满足更先进的数据驱动建模需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 新增四种模型：改进的监督分类模型并支持特征选择；新的回归模型；聚类模型；基于图的学习模型；并首次实现量子超维计算及量子监督学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通过这些扩展，hdlib在监督学习、回归、无监督学习和图学习方面都有显著提升，并首次实现了量子超维计算与量子机器学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; hdlib保持开源，可在GitHub获取，并通过pip和conda安装，文档和示例在Wiki提供。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在hdlib首次发布后，hdlib是一个用于设计向量符号体系（VSA）的Python库，我们推出了一个重大扩展，显著提升了其机器学习能力。VSA，也称为超维计算，是一种利用高维向量表示和处理信息的计算范式。虽然hdlib的第一版为创建和操作这些向量奠定了坚实基础，但此次更新满足了对VSA框架内更先进、数据驱动建模的需求。在此，我们提出了四个扩展：对现有监督分类模型的显著增强，并支持特征选择；一个用于预测连续变量的新回归模型；一个无监督学习的聚类模型；以及一个基于图的学习模型。此外，我们提出了首个量子超维计算实现，采用量子算术运算，并提出了一个用于监督学习的量子机器学习模型。hdlib保持开源，可在GitHub（https://github.com/cumbof/hdlib）以MIT许可证获取，并通过Python Package Index（pip install hdlib）和Conda（conda install -c conda-forge hdlib）分发。新功能的文档和示例可在官方Wiki（https://github.com/cumbof/hdlib/wiki）查看。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Following the initial publication of hdlib, a Python library for designing Vector-Symbolic Architectures (VSA), we introduce a major extension that significantly enhances its machine learning capabilities. VSA, also known as Hyperdimensional Computing, is a computing paradigm that represents and processes information using high-dimensional vectors. While the first version of hdlib established a robust foundation for creating and manipulating these vectors, this update addresses the growing need for more advanced, data-driven modeling within the VSA framework. Here, we present four extensions: significant enhancements to the existing supervised classification model also enabling feature selection, and a new regression model for predicting continuous variables, a clustering model for unsupervised learning, and a graph-based learning model. Furthermore, we propose the first implementation ever of Quantum Hyperdimensional Computing with quantum-powered arithmetic operations and a new Quantum Machine Learning model for supervised learning. hdlib remains open-source and available on GitHub at https://github.com/cumbof/hdlib under the MIT license, and distributed through the Python Package Index (pip install hdlib) and Conda (conda install -c conda-forge hdlib). Documentation and examples of these new features are available on the official Wiki at https://github.com/cumbof/hdlib/wiki.&lt;/p&gt;</description></item><item><guid>2601.02531v1</guid><title>Losses that Cook: Topological Optimal Transport for Structured Recipe Generation</title><link>http://arxiv.org/abs/2601.02531v1</link><author>Mattia Ottoborgo, Daniele Rege Cambrin, Paolo Garza</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在烹饪配方生成中使用多目标损失函数，以提升配方的准确性和可读性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的配方生成训练主要依赖交叉熵损失，侧重文本流畅性，而忽视了时间、温度、配料等关键因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索多目标损失和新的拓扑损失，以更好地捕捉配料列表的结构并提升整体生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在RECIPE-NLG框架下，提出将配料列表视为嵌入空间中的点云，使用拓扑损失最小化预测与真实配料的差异；同时引入Dice损失优化时间/温度精度，并设计混合损失以兼顾数量与时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 拓扑损失显著提升配料和动作层面的指标；Dice损失在时间/温度精度上表现最佳；混合损失在数量和时间上实现了良好折衷；人类偏好实验显示模型在62%的案例中更受青睐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多目标损失，尤其是拓扑损失，能够显著提升配方生成的准确性和可读性；Dice和混合损失提供了互补优势，整体模型优于传统方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 烹饪配方是复杂的程序，不仅需要流畅且准确的文本，还需要精确的时间、温度和程序连贯性，以及正确的配料组合。标准训练程序主要基于交叉熵，且仅关注流畅性。基于RECIPE-NLG，我们研究了使用多种复合目标，并提出了一种新的拓扑损失，将配料列表表示为嵌入空间中的点云，最小化预测配料与真实配料之间的差异。使用标准NLG指标和配方特定指标，我们发现我们的损失显著提升了配料和动作层面的指标。与此同时，Dice损失在时间/温度精度上表现出色，混合损失在数量和时间上实现了竞争性的折衷，并产生协同收益。人类偏好分析支持我们的发现，显示我们的模型在62%的案例中更受青睐。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Cooking recipes are complex procedures that require not only a fluent and factual text, but also accurate timing, temperature, and procedural coherence, as well as the correct composition of ingredients. Standard training procedures are primarily based on cross-entropy and focus solely on fluency. Building on RECIPE-NLG, we investigate the use of several composite objectives and present a new topological loss that represents ingredient lists as point clouds in embedding space, minimizing the divergence between predicted and gold ingredients. Using both standard NLG metrics and recipe-specific metrics, we find that our loss significantly improves ingredient- and action-level metrics. Meanwhile, the Dice loss excels in time/temperature precision, and the mixed loss yields competitive trade-offs with synergistic gains in quantity and time. A human preference analysis supports our finding, showing our model is preferred in 62% of the cases.&lt;/p&gt;</description></item><item><guid>2601.02566v1</guid><title>Shallow- and Deep-fake Image Manipulation Localization Using Vision Mamba and Guided Graph Neural Network</title><link>http://arxiv.org/abs/2601.02566v1</link><author>Junbin Zhang, Hamid Reza Tohidypour, Yixiao Wang, Panos Nasiopoulos</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了利用深度学习网络对浅层伪造图像和深度伪造图像进行操纵定位的可行性，并提出了一种结合Vision Mamba网络和引导图神经网络的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 伪造图像在社会中可能产生重大影响，现有研究多聚焦于浅层伪造或深度伪造视频的定位，缺乏同时处理两类图像的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索一种能够同时定位浅层伪造和深度伪造图像中被篡改像素的深度学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Vision Mamba网络提取特征图以清晰描述篡改与未篡改区域的边界，并引入新颖的引导图神经网络（G-GNN）模块来增强被篡改与真实像素的区分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，该方法在推理准确率上优于其他先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提出的结合Vision Mamba和G-GNN的方案在同时处理浅层伪造和深度伪造图像的操纵定位任务中表现出更高的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图像操纵定位是一个重要的研究任务，因为伪造图像可能在各个方面产生显著的社会影响。这些图像操纵可以使用传统图像编辑工具（称为“浅层伪造”）或先进的人工智能技术（称为“深度伪造”）生成。虽然已有许多研究聚焦于浅层伪造图像或深度伪造视频的图像操纵定位，但很少有方法同时处理这两种情况。本文探讨了使用深度学习网络对浅层和深度伪造图像进行操纵定位的可行性，并提出了相应的解决方案。为精确区分真实像素和被篡改像素，我们利用Vision Mamba网络提取清晰描述篡改与未篡改区域边界的特征图。为进一步增强这种区分，我们提出了一种新颖的引导图神经网络（G-GNN）模块，放大被篡改与真实像素之间的差异。我们的评估结果表明，所提出的方法在推理准确率上优于其他最先进的方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Image manipulation localization is a critical research task, given that forged images may have a significant societal impact of various aspects. Such image manipulations can be produced using traditional image editing tools (known as &amp;quot;shallowfakes&amp;quot;) or advanced artificial intelligence techniques (&amp;quot;deepfakes&amp;quot;). While numerous studies have focused on image manipulation localization on either shallowfake images or deepfake videos, few approaches address both cases. In this paper, we explore the feasibility of using a deep learning network to localize manipulations in both shallow- and deep-fake images, and proposed a solution for such purpose. To precisely differentiate between authentic and manipulated pixels, we leverage the Vision Mamba network to extract feature maps that clearly describe the boundaries between tampered and untouched regions. To further enhance this separation, we propose a novel Guided Graph Neural Network (G-GNN) module that amplifies the distinction between manipulated and authentic pixels. Our evaluation results show that our proposed method achieved higher inference accuracy compared to other state-of-the-art methods.&lt;/p&gt;</description></item><item><guid>2601.02601v1</guid><title>State of the Quantum Software Engineering Ecosystem</title><link>http://arxiv.org/abs/2601.02601v1</link><author>Nazanin Siavash, Armin Moin</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究量子软件工程（QSE）生态系统的现状，重点关注学术界和工业界的成就、活动与合作，尤其是成功的创业案例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 量子软件工程正成为新兴技术领域，需了解其发展动态与关键参与者。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 识别在QSE领域活跃且取得显著成果的机构和公司，这些成果体现在同行评议的出版物或在风险投资市场筹集的资本上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用先进的人工智能技术——大型语言模型（LLM），特别是OpenAI的GPT‑5，通过ChatGPT工具进行研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们研究了量子软件工程（QSE）生态系统的现状，重点关注学术界和工业界的成就、活动和合作，特别关注该领域的成功创业。我们的研究方法是一种新颖的方法，采用人工智能（AI）的最新技术，即大型语言模型（LLM），尤其是生成预训练变换器（GPT）。我们使用其中一种模型，即OpenAI GPT‑5模型，通过ChatGPT工具。目标是识别在QSE领域高度活跃并取得卓越成果的机构和公司，这些成果通过同行评议的出版物或在风险资本市场筹集的资本得到证明。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We study the current state of the Quantum Software Engineering (QSE) ecosystem, focusing on the achievements, activities, and engagements from academia and industry, with a special focus on successful entrepreneurial endeavors in this arena. Our research methodology is a novel one, featuring the state-of-the-art in Artificial Intelligence (AI), namely Large Language Models (LLMs), especially Generative Pretrained Transformers (GPT). We use one of such models, namely the OpenAI GPT-5 model, through the ChatGPT tool. The goal is to identify institutions and companies that are highly active and have achieved distinguished results in QSE, evidenced by peer-reviewed publications or raised capital in the venture capital market.&lt;/p&gt;</description></item><item><guid>2601.02629v1</guid><title>Listen to the Unexpected: Self-Supervised Surprise Detection for Efficient Viewport Prediction</title><link>http://arxiv.org/abs/2601.02629v1</link><author>Arman Nik Khah, Ravi Prakash</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自学习框架，用于检测音频中的“惊奇”事件，并将其用于360度视频的视口预测，从而提高带宽利用率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的自适应流媒体方法主要依赖视觉显著性或历史注视模式，忽略了空间音频对用户注意力的引导作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究音频惊奇事件在视口预测中的作用，并构建能够自动识别这些事件的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用等价图神经网络与递归时间建模相结合的架构，并通过双重自监督目标进行训练，天然地模拟时间注意衰减。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在AVTrack360数据集上，结合音频惊奇与视觉线索的视口预测能将比特率浪费降低多达18%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 音频惊奇事件是视口预测的重要补充，能够显著提升带宽利用效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The paper presents a self‑learning framework for detecting surprising audio events and demonstrates their usefulness for viewport prediction, showing that incorporating audio surprise with visual cues can reduce bitrate waste by up to 18 % compared to visual‑only methods.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 360° 视频中视口预测的问题，目标是通过提前推测用户将要观看的区域来降低带宽消耗并提升观看体验。视口预测在 VR/AR 等沉浸式应用中至关重要，因为它能显著减少数据传输量和延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者提出了自监督的惊喜检测框架，利用视频中的“惊喜”事件来推断视口变化。该方法借鉴了现有的视口预测技术和自监督学习的思想，避免了对手工标注的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先检测视频帧中的惊喜程度，再根据高惊喜分数触发视口更新。实现流程包括：1) 用神经网络为每帧预测惊喜分数；2) 当分数超过阈值时，使用轻量级模型输出下一个视口位置；3) 将预测的视口用于预取和渲染。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 自监督惊喜检测，无需标注视线数据；2) 轻量级视口预测模型，计算开销低；3) 通过惊喜驱动的预测提升了准确率。与以往依赖手工标注或仅基于运动特征的视口预测方法不同，该工作在保持高精度的同时显著降低了数据需求和计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种自监督惊喜检测方法，实现了高效、准确的 360° 视口预测，显著降低了对标注数据和计算资源的需求。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Adaptive streaming of 360-degree video relies on viewport prediction to allocate bandwidth efficiently. Current approaches predominantly use visual saliency or historical gaze patterns, neglecting the role of spatial audio in guiding user attention. This paper presents a self-learning framework for detecting &amp;quot;surprising&amp;quot; auditory events -- moments that deviate from learned temporal expectations -- and demonstrates their utility for viewport prediction. The proposed architecture combines $SE(3)$-equivariant graph neural networks with recurrent temporal modeling, trained via a dual self-supervised objective. A key feature is the natural modeling of temporal attention decay: surprise is high at event onset but diminishes as the listener adapts. Experiments on the AVTrack360 dataset show that integrating audio surprise with visual cues reduces bitrate waste by up to 18% compared to visual-only methods.&lt;/p&gt;</description></item><item><guid>2601.02631v1</guid><title>Copyright Laundering Through the AI Ouroboros: Adapting the 'Fruit of the Poisonous Tree' Doctrine to Recursive AI Training</title><link>http://arxiv.org/abs/2601.02631v1</link><author>Anirban Mukherjee, Hannah Hanwen Chang</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 文章提出一种新的版权执法标准，称为 AI‑FOPT，旨在解决多代 AI 训练链导致的证据盲区问题。通过将“有毒树果”原则应用于 AI 训练，若基础模型的训练被认定侵权，则后续模型被视为有潜在污染，需由开发者证明其独立来源或已清除侵权内容，否则其商业使用将面临法律责任。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统版权执法依赖被告对作品的接触和作品与输出的显著相似性，但在 AI 通过递归合成数据的多代训练管道中，这种证据关系被削弱，导致难以追踪侵权链条。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过制定 AI‑FOPT 标准，恢复证据交易，明确责任归属，并为后续 AI 开发者提供可操作的证明路径，以防止版权洗钱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用“有毒树果”原则，设定触发条件（基础模型训练侵权），建立可逆推定（后续模型有污染假设），并提供可驳回路径（独立来源或可验证的去学习），同时保留对初始摄取阶段的公平使用分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该标准能够在多代 AI 训练链中识别并追溯侵权来源，提供可执行的法律框架，且在行政上可行；同时对创新的抑制和公平使用的担忧已被充分讨论并得到回应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 以来源链为核心的 AI‑FOPT 标准既能维护版权，又能在行政上可操作，是解决 AI 版权执法难题的必要手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 版权执法依赖于证据交易：原告必须证明被告接触作品并且被争议输出与作品具有实质性相似性。当 AI 系统通过多代递归合成数据管道训练时，这种交易受到压力。后续模型在其前辈输出上进行微调，早期模型吸收的受版权保护材料会扩散到更深层的统计抽象中，导致出现证据盲区，重叠看似偶然，来源链过于稀薄难以追踪。这种情况为“版权洗钱”提供了条件——利用多代合成管道（“AI 乌罗波罗斯”）使传统侵权证明变得不切实际。本文将“有毒树果”原则适用于 AI，提出 AI‑FOPT 标准：若基础 AI 模型的训练被认定侵权（无论是非法来源还是未能通过公平使用的非变形摄取），则随后主要基于该基础模型输出或蒸馏权重的 AI 模型承担可逆推定的污染责任。责任转移到下游开发者——他们控制来源证据——以积极证明可验证的独立且合法来源的链条或可修复的重建，而不放弃对初始摄取阶段的公平使用分析。若无此证明，商业部署受污染模型及其输出将构成可诉行为。本文通过阐明触发条件、推定及具体可驳回路径（如独立来源或可验证的去学习），回应对创新抑制和公平使用的反对意见，并展示为何以来源链为焦点的方法既可管理又必要。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Copyright enforcement rests on an evidentiary bargain: a plaintiff must show both the defendant&amp;#x27;s access to the work and substantial similarity in the challenged output. That bargain comes under strain when AI systems are trained through multi-generational pipelines with recursive synthetic data. As successive models are tuned on the outputs of its predecessors, any copyrighted material absorbed by an early model is diffused into deeper statistical abstractions. The result is an evidentiary blind spot where overlaps that emerge look coincidental, while the chain of provenance is too attenuated to trace. These conditions are ripe for &amp;quot;copyright laundering&amp;quot;--the use of multi-generational synthetic pipelines, an &amp;quot;AI Ouroboros,&amp;quot; to render traditional proof of infringement impracticable. This Article adapts the &amp;quot;fruit of the poisonous tree&amp;quot; (FOPT) principle to propose a AI-FOPT standard: if a foundational AI model&amp;#x27;s training is adjudged infringing (either for unlawful sourcing or for non-transformative ingestion that fails fair-use), then subsequent AI models principally derived from the foundational model&amp;#x27;s outputs or distilled weights carry a rebuttable presumption of taint. The burden shifts to downstream developers--those who control the evidence of provenance--to restore the evidentiary bargain by affirmatively demonstrating a verifiably independent and lawfully sourced lineage or a curative rebuild, without displacing fair-use analysis at the initial ingestion stage. Absent such proof, commercial deployment of tainted models and their outputs is actionable. This Article develops the standard by specifying its trigger, presumption, and concrete rebuttal paths (e.g., independent lineage or verifiable unlearning); addresses counterarguments concerning chilling innovation and fair use; and demonstrates why this lineage-focused approach is both administrable and essential.&lt;/p&gt;</description></item><item><guid>2601.02661v1</guid><title>Efficient Screening of Organic Singlet Fission Molecules Using Graph Neural Networks</title><link>http://arxiv.org/abs/2601.02661v1</link><author>Li Fu, Longfei Lv, Fan Zhang, Si Zhou, Weiwei Gao, Jijun Zhao</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出一种高通量筛选框架，结合图神经网络和多级验证，快速发现单重态分裂活性分子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单重态分裂技术有望突破光伏器件的能量转换极限，但有效材料的筛选受限于候选分子数量少和计算成本高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种高通量筛选框架，利用图神经网络预测激发态性质，并通过多级验证加速发现单重态分裂活性分子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在FORMED数据库上训练图神经网络，预测S1、T1、T2激发能，随后在OE62和QO2Mol数据库中筛选超过2000万分子，使用TD-DFT、GW和Bethe‑Salpeter方程进行多级验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架将TD-DFT验证的计算量降低四个数量级，识别出180种潜在单重态分裂分子和1000多种构象，并进一步筛选出可合成且实验可行的候选物。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法为加速发现具有光电功能的分子提供了有效策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Singlet fission (SF) provides a promising strategy for surpassing the Shockley-Queisser limit in photovoltaics. However, the identification of efficient SF materials is hindered by the limited availability of suitable molecular candidates and the high computational costs associated with conventional quantum-chemical methods for excited states. In this study, we introduce a high-throughput screening framework that integrates a graph neural network (GNN) with multi-level validation to accelerate the discovery of SF-active molecules. Trained on a previously reported FORMED database, the GNN achieves state-of-the-art accuracy in predicting SF-relevant excited-state properties, demonstrating a mean absolute error of about 0.1 eV for S1, T1, and T2 excitation energies. This capability facilitates the efficient screening of over 20 million molecular structures from both OE62 and QO2Mol databases. Our framework significantly reduces the computational demand associated with Time-Dependent Density Functional Theory validation by four orders of magnitude and identifies 180 potential SF molecules along with more than 1000 conformers. Subsequent assessments regarding synthetic accessibility, GW approximation and Bethe-Salpeter equation calculations further highlight a subset of experimentally feasible candidates among these SF candidates. The approach presented herein exemplifies an effective strategy for accelerating the discovery of functional molecules with optoelectronic applications.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Singlet fission (SF) provides a promising strategy for surpassing the Shockley-Queisser limit in photovoltaics. However, the identification of efficient SF materials is hindered by the limited availability of suitable molecular candidates and the high computational costs associated with conventional quantum-chemical methods for excited states. In this study, we introduce a high-throughput screening framework that integrates a graph neural network (GNN) with multi-level validation to accelerate the discovery of SF-active molecules. Trained on a previously reported FORMED database, the GNN achieves state-of-the-art accuracy in predicting SF-relevant excited-state properties, demonstrating a mean absolute error of about 0.1 eV for S1, T1, and T2 excitation energies. This capability facilitates the efficient screening of over 20 million molecular structures from both OE62 and QO2Mol databases. Our framework significantly reduces the computational demand associated with Time-Dependent Density Functional Theory validation by four orders of magnitude and identifies 180 potential SF molecules along with more than 1000 conformers. Subsequent assessments regarding synthetic accessibility, GW approximation and Bethe-Salpeter equation calculations further highlight a subset of experimentally feasible candidates among these SF candidates. The approach presented herein exemplifies an effective strategy for accelerating the discovery of functional molecules with optoelectronic applications.&lt;/p&gt;</description></item><item><guid>2601.02662v1</guid><title>When Prompting Meets Spiking: Graph Sparse Prompting via Spiking Graph Prompt Learning</title><link>http://arxiv.org/abs/2601.02662v1</link><author>Bo Jiang, Weijun Zhao, Beibei Wang, Jin Tang</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种稀疏图提示特征学习方法SpikingGPF，利用脉冲神经元实现对图节点特征的选择性提示，提升模型在下游任务中的适应性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的图提示特征（GPF）在所有特征维度上进行提示，导致冗余且对噪声敏感。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统GPF在所有特征维度提示导致的冗余和噪声敏感问题，提出稀疏提示方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SpikingGPF通过脉冲神经元架构学习每个节点的稀疏提示向量，并采用稀疏表示理论将提示表示为提示原子稀疏组合，从而实现选择性提示和高效计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上实验表明，SpikingGPF在效果和鲁棒性方面优于传统方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 稀疏图提示特征学习能够显著提升预训练GNN在下游任务中的性能，并对噪声具有更强的抵抗力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node&amp;#x27;s all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node&amp;#x27;s all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.&lt;/p&gt;</description></item><item><guid>2601.02680v1</guid><title>Adversarial Contrastive Learning for LLM Quantization Attacks</title><link>http://arxiv.org/abs/2601.02680v1</link><author>Dinghong Song, Zhiwei Xu, Hai Wan, Xibin Zhao, Pengfei Su, Dong Li</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在资源受限硬件上部署大型语言模型时的量化问题，并提出了一种新的对抗性对比学习攻击方法ACL，能够显著提升量化后模型的恶意行为成功率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 量化是实现大语言模型在有限资源设备上运行的关键技术，但近期研究表明，完整精度的LLM在量化后可能出现恶意行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种基于梯度的量化攻击方法ACL，以最大化正常与恶意响应概率之间的差距，从而提高攻击效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ACL将攻击目标表述为三元组对比损失，并结合投影梯度下降的两阶段分布式微调策略，确保优化过程稳定高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明ACL在过度拒绝、越狱和广告注入任务上的攻击成功率分别为86.00%、97.69%和92.40%，比现有方法提升44.67%、18.84%和50.80%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ACL在量化攻击中表现出显著优势，能够有效诱导LLM产生恶意行为，提示量化过程需加强安全防护。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 模型量化对于在资源受限硬件上部署大型语言模型至关重要，但近期研究表明，完整精度的良性LLM在量化后可能出现严重安全风险，表现为恶意行为。本文提出了对抗性对比学习（ACL），这是一种新颖的基于梯度的量化攻击方法，通过显式最大化正常与有害响应概率之间的差距，显著提升攻击效果。ACL将攻击目标表述为三元组对比损失，并结合投影梯度下降的两阶段分布式微调策略，以确保优化过程稳定高效。大量实验表明，ACL在过度拒绝、越狱和广告注入任务上的攻击成功率分别为86.00%、97.69%和92.40%，在这些任务上分别比最先进方法提升44.67%、18.84%和50.80%。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL&amp;#x27;s remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.&lt;/p&gt;</description></item><item><guid>2601.02700v1</guid><title>Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study</title><link>http://arxiv.org/abs/2601.02700v1</link><author>Agniv Roy Choudhury, Vignesh Ponselvan Rajasingh</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究通过系统实验评估Transformer模型在AddSent对抗数据集上的鲁棒性，并提出针对性缓解策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 问答系统在标准基准上表现优异，但易受对抗样本攻击。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究Transformer模型在AddSent数据集上的对抗鲁棒性，并寻找最佳微调比例与模型规模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用五种错误分类方案进行多层级错误分析；评估不同对抗微调比例；对小模型进行数据增强实验；将ELECTRA-small扩展至base；实现三种针对性缓解策略，其中包括实体感知对比学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 主要错误模式为否定混淆和实体替换；最优微调比例为80%干净+20%对抗数据；小模型存在容量瓶颈；模型规模扩大后消除了鲁棒性与准确率的权衡；实体感知对比学习在AddSent和SQuAD上分别达到89.89%和90.73%的EM，几乎完全弥补了对抗差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 针对性缓解策略能够实现干净与对抗数据性能的近乎一致，本研究首次将语言错误分析与NER引导的对比学习结合用于对抗问答。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 问答系统在标准基准（如SQuAD）上取得了令人印象深刻的表现，但仍易受到对抗样本的攻击。本项目通过系统实验研究Transformer模型在AddSent对抗数据集上的鲁棒性，涵盖不同模型规模和针对性缓解策略。我们使用五种互补的分类方案进行全面的多层级错误分析，发现否定混淆和实体替换是主要的失败模式。通过系统评估对抗微调比例，确定80%干净数据加20%对抗数据为最佳比例。数据增强实验揭示小模型存在容量瓶颈。将ELECTRA-small（1400万参数）扩展到ELECTRA-base（1.1亿参数）后，消除了鲁棒性与准确率之间的权衡，在干净和对抗数据上均取得显著提升。我们实现了三种针对性缓解策略，其中实体感知对比学习表现最佳：在AddSent上取得89.89%的Exact Match，在SQuAD上取得90.73%的Exact Match，几乎完全弥补了对抗差距。到目前为止，这是首个将全面的语言错误分析与命名实体识别引导的对比学习相结合用于对抗问答的工作，证明了针对性缓解可以实现干净与对抗性能的近乎一致。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Question answering (QA) systems achieve impressive performance on standard benchmarks like SQuAD, but remain vulnerable to adversarial examples. This project investigates the adversarial robustness of transformer models on the AddSent adversarial dataset through systematic experimentation across model scales and targeted mitigation strategies. We perform comprehensive multi-level error analysis using five complementary categorization schemes, identifying negation confusion and entity substitution as the primary failure modes. Through systematic evaluation of adversarial fine-tuning ratios, we identify 80% clean + 20% adversarial data as optimal. Data augmentation experiments reveal a capacity bottleneck in small models. Scaling from ELECTRA-small (14M parameters) to ELECTRA-base (110M parameters) eliminates the robustness-accuracy trade-off, achieving substantial improvements on both clean and adversarial data. We implement three targeted mitigation strategies, with Entity-Aware contrastive learning achieving best performance: 89.89% AddSent Exact Match (EM) and 90.73% SQuAD EM, representing 94.9% closure of the adversarial gap. To our knowledge, this is the first work integrating comprehensive linguistic error analysis with Named Entity Recognition (NER)-guided contrastive learning for adversarial QA, demonstrating that targeted mitigation can achieve near-parity between clean and adversarial performance.&lt;/p&gt;</description></item><item><guid>2601.02704v1</guid><title>Analysis of Various Manipulator Configurations Based on Multi-Objective Black-Box Optimization</title><link>http://arxiv.org/abs/2601.02704v1</link><author>Kento Kawaharazuka, Keita Yoneda, Takahiro Hattori, Shintaro Inoue, Kei Okada</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文综述了目前已开发的六自由度和七自由度机械臂，指出它们的关节排列和连杆比例是通过经验确定的。随着机器人基础模型的兴起，出现了许多新的机械臂设计，但它们的结构并不完全相同。作者通过考虑末端执行器的可达性和关节扭矩两方面进行多目标优化，分析了现有机械臂在优化结果中的位置，并为未来的机械臂设计提供了见解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在长期的研究中，六自由度和七自由度机械臂的关节配置和连杆比例主要依赖经验。近年来，机器人基础模型的快速发展促使人们不断提出新的机械臂方案，以支持这些模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨机械臂的最优结构，评估不同设计在末端执行器可达性和关节扭矩方面的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用多目标优化方法，从末端执行器可达性和关节扭矩两个角度出发，对机械臂结构进行优化，并对采样结果进行分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 现有的机械臂在关节顺序和连杆比例上各不相同，未出现完全相同的结构。通过优化分析，作者定位了现有设计在可达性和扭矩平衡中的位置，并指出了改进的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 没有一种机械臂结构能够在所有指标上最优，未来的设计需要在可达性和扭矩之间进行权衡。多目标优化为机械臂结构的改进提供了有价值的参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Various 6-degree-of-freedom and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Various 6-degree-of-freedom (DOF) and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design.&lt;/p&gt;</description></item><item><guid>2601.02714v1</guid><title>Time-Scaling Is What Agents Need Now</title><link>http://arxiv.org/abs/2601.02714v1</link><author>Zhi Liu, Guangzhi Wang</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文讨论了人工智能从早期分离的认知功能向统一的闭环感知-决策-行动智能体的演进，并提出了“时间尺度化”作为提升深度推理与问题解决能力的关键方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 早期神经网络关注感知表示，强化学习关注决策行为，符号人工智能关注知识推理；随着Transformer和世界模型的发展，这些范式正在融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索通过扩展时间路径实现更完整、更高效的推理过程，以弥补现有链式思维等方法在搜索完整性和效率上的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过分析现有链式思维、树式思维以及DeepSeek-R1等模型的推理轨迹，提出时间尺度化的架构设计，强调延长时间路径、动态策略调整和元认知控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 时间尺度化能够在不显著增加模型参数的情况下，深化问题空间探索、提升策略适应性，并改善元认知控制，从而显著提升深度推理与问题解决性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将时间尺度化原则置于智能体发展的核心，可为实现更强大、资源高效的推理能力提供基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 早期人工智能范式表现出分离的认知功能：神经网络侧重“感知-表示”，强化学习侧重“决策-行为”，符号人工智能侧重“知识-推理”。随着基于Transformer的大模型和世界模型的发展，这些范式正趋向于闭环“感知-决策-行动”能力的认知代理。人类在有限认知资源下通过时间化的顺序推理解决复杂问题。语言依赖于问题空间搜索以实现深层语义推理。早期的大型语言模型能够生成流畅文本，但缺乏稳健的语义推理能力。提示技术如链式思维和树式思维通过显式中间步骤扩展推理路径。最近的模型如DeepSeek-R1通过显式推理轨迹提升性能。然而，这些方法在搜索完整性和效率上存在局限。由此凸显了“时间尺度化”的必要性——系统地扩展和优化代理随时间展开推理的能力。时间尺度化指利用延伸的时间路径进行架构设计，能够实现更深的问题空间探索、动态策略调整和增强的元认知控制，类似于人类在认知约束下的顺序推理。它代表了在不成比例增加静态模型参数的前提下提升深度推理和问题解决的关键前沿。推进智能代理能力需要将时间尺度化原则置于前沿，将显式时间推理管理作为基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on &amp;quot;perception-representation,&amp;quot; Reinforcement Learning on &amp;quot;decision-making-behavior,&amp;quot; and Symbolic AI on &amp;quot;knowledge-reasoning.&amp;quot; With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop &amp;quot;perception-decision-action&amp;quot; capabilities.   Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.   This highlights the need for &amp;quot;Time-Scaling&amp;quot;--the systematic extension and optimization of an agent&amp;#x27;s ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.&lt;/p&gt;</description></item><item><guid>2601.02757v1</guid><title>LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery</title><link>http://arxiv.org/abs/2601.02757v1</link><author>Zixuan Xiao, Jun Ma</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个通用代理框架 ChangeGPT，将大型语言模型与视觉基础模型结合，利用层级结构降低幻觉，并在多场景、多类型问题上表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的变化检测方法在处理多样化的真实世界查询时缺乏灵活性，且缺乏全面分析的智能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够智能、适应性强、支持多类型变化分析的解决方案，以提升遥感决策支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过将大型语言模型与视觉基础模型集成，形成 ChangeGPT，并采用层级结构来减少幻觉；在包含 140 个真实场景问题的数据集上评估其工具选择和查询准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 GPT-4-turbo 后端下，ChangeGPT 的匹配率达到 90.71%，在需要多步推理和强大工具选择的变化相关查询中表现尤为突出，并在深圳前海湾的城市变化监测案例中验证了其实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ChangeGPT 通过提供智能、适应性和多类型变化分析，为遥感应用中的决策制定提供了强有力的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的变化检测方法往往缺乏处理多样化真实世界查询的通用性，也缺乏全面分析的智能。本文提出了一个通用代理框架，将大型语言模型与视觉基础模型集成，形成 ChangeGPT。采用层级结构来降低幻觉。该代理在一个包含 140 个按真实场景分类的问题的数据集上进行评估，涵盖了多种问题类型（如尺寸、类别、数量）和复杂度。评估考察了代理的工具选择能力（精确率/召回率）和整体查询准确率（匹配率）。ChangeGPT，尤其是使用 GPT-4-turbo 后端时，表现优异，匹配率达到 90.71%。其优势特别体现在处理需要多步推理和强大工具选择的变化相关查询。通过在深圳前海湾的真实城市变化监测案例中验证了其实用性。通过提供智能、适应性和多类型变化分析，ChangeGPT 为遥感应用中的决策制定提供了强有力的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent&amp;#x27;s tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.&lt;/p&gt;</description></item><item><guid>2601.02759v1</guid><title>Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups</title><link>http://arxiv.org/abs/2601.02759v1</link><author>Hyungtae Lim, Minkyun Seo, Luca Carlone, Jaesik Park</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个名为 BUFFER-X 的无训练点云配准框架，能够在零样本情况下实现跨域泛化。通过几何自举、分布感知的最远点采样以及补丁级坐标归一化，解决了传统方法在参数固定、关键点检测迁移性差以及尺度不匹配等方面的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度学习点云配准方法往往需要针对每个数据集手动调参或重新训练，难以在新环境中零样本泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种训练自由、零样本可泛化的点云配准方法，克服固定参数、关键点检测迁移性差和尺度不匹配等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 采用几何自举自动估计超参数；2. 用分布感知的最远点采样替代学习的关键点检测器；3. 在补丁级别对坐标进行归一化；4. 采用层次多尺度匹配提取局部、中间和全局对应关系；5. 为效率需求推出 BUFFER-X-Lite，利用早停策略和快速位姿求解器将计算时间降低 43%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在包含 12 个不同规模、室内外和跨传感器 LiDAR 数据集的基准上，BUFFER-X 在不需要手动调参或先验知识的情况下实现了有效泛化，且 BUFFER-X-Lite 在保持精度的同时显著降低了计算时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BUFFER-X 成功实现了无训练、零样本的点云配准，并在多种环境下表现出鲁棒性和高效性，为实际应用提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 一些基于深度学习的点云配准方法在零样本泛化方面存在困难，往往需要针对每个数据集进行超参数调优或在新环境中重新训练。我们识别出三个关键限制：(a) 固定的用户定义参数（如体素大小、搜索半径）无法在不同尺度下泛化；(b) 学习得到的关键点检测器在跨域迁移时表现不佳；(c) 绝对坐标放大了数据集之间的尺度不匹配。为解决这三个问题，我们提出了 BUFFER-X，一种无训练的配准框架，通过：(a) 几何自举实现自动超参数估计；(b) 分布感知的最远点采样替代学习的检测器；(c) 补丁级坐标归一化确保尺度一致性。我们的方案采用层次多尺度匹配，在局部、中间和全局感受野中提取对应关系，从而在多样化环境中实现稳健配准。针对对效率要求高的应用，我们推出 BUFFER-X-Lite，通过早停策略和快速位姿求解器将总计算时间相较 BUFFER-X 降低 43%，同时保持精度。我们在包含 12 个数据集的综合基准上进行评估，涵盖对象尺度、室内和室外场景，并包括不同 LiDAR 配置之间的跨传感器配准。结果表明，我们的方法在无需手动调参或测试域先验知识的情况下实现了有效泛化。代码：https://github.com/MIT-SPARK/BUFFER-X&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现点云配准的零样本（zero‑shot）方法，能够在不同尺度、不同场景以及不同传感器设置下直接对齐点云。此类问题在机器人导航、3D重建、AR/VR 等应用中极为重要，因为这些系统往往需要在未知或多变的环境中快速融合来自多种传感器的数据，而传统方法往往需要针对每种场景或传感器进行专门训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计时参考了现有的点云配准与特征学习工作（如 PREDATOR、BUFFER、FastMAC 等），并意识到这些方法在尺度、传感器差异上易失效。为此他们提出了一个通用、尺度不变的特征提取器，并结合鲁棒匹配与变换估计流程，力求在未见过的域上直接使用。该思路借鉴了自监督/对比学习的思想，但在零样本场景下进行了改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一种对尺度、场景和传感器不敏感的点云特征表示，并在此基础上进行匹配与变换估计。实现流程大致为：①使用预训练的特征提取网络对两组点云进行编码；②基于特征相似度生成对应关系；③利用 RANSAC 或类似的鲁棒估计器求解刚性变换；④对齐后可直接得到配准结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 提出了零样本点云配准框架，能够在未见过的尺度、场景和传感器上直接工作；2) 设计了尺度不变的特征提取器，显著提升跨域泛化能力；3) 通过自监督/对比学习在大规模合成数据上预训练，减少对标注数据的依赖。与以往需要针对特定域或传感器进行微调的配准方法不同，该工作实现了真正的跨域零样本配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种零样本点云配准框架，利用尺度不变的特征表示实现了在不同尺度、场景和传感器设置下的跨域配准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.&lt;/p&gt;</description></item><item><guid>2601.02762v1</guid><title>Unified Meta-Representation and Feedback Calibration for General Disturbance Estimation</title><link>http://arxiv.org/abs/2601.02762v1</link><author>Zihan Yang, Jindou Jia, Meng Wang, Yuhang Liu, Kexin Guo, Xiang Yu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于元学习和反馈校准的在线自适应框架，用于估计机器人控制中的非结构扰动，并在四旋翼实验中验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 机器人控制面临未知时间变化扰动，现有元学习方法依赖共享环境结构，缺乏对非结构扰动的灵活性，且表示误差和分布偏移会降低预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种可推广的扰动估计框架，克服现有方法的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用有限时间窗口的观测特征学习统一表示，结合元学习和状态反馈校准的在线自适应，减小学习残差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 理论证明在线学习误差和扰动估计误差可同时收敛；实验表明框架能有效估计多种快速变化扰动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在不需要预定义结构假设的情况下，提供了对非结构扰动的鲁棒估计，提升了机器人控制的精确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在现代机器人应用中，精确控制始终是一个开放问题，因为存在未知的随时间变化的扰动。现有的基于元学习的方法需要共享的环境结构表示，这在现实的非结构扰动中缺乏灵活性。此外，表示误差和分布偏移会导致预测精度严重下降。本研究提出了一个可推广的扰动估计框架，基于元学习和反馈校准的在线自适应。通过从有限时间窗口的过去观测中提取特征，可以学习到一种统一的表示，有效捕捉一般的非结构扰动，而不需要预定义结构假设。随后，在线自适应过程通过状态反馈机制进行校准，以减弱由表示和可推广性限制产生的学习残差。理论分析表明，在线学习误差和扰动估计误差可以同时收敛。通过统一的元表示，我们的框架能够有效估计多种快速变化的扰动，已在四旋翼飞行实验中得到验证。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Precise control in modern robotic applications is always an open issue due to unknown time-varying disturbances. Existing meta-learning-based approaches require a shared representation of environmental structures, which lack flexibility for realistic non-structural disturbances. Besides, representation error and the distribution shifts can lead to heavy degradation in prediction accuracy. This work presents a generalizable disturbance estimation framework that builds on meta-learning and feedback-calibrated online adaptation. By extracting features from a finite time window of past observations, a unified representation that effectively captures general non-structural disturbances can be learned without predefined structural assumptions. The online adaptation process is subsequently calibrated by a state-feedback mechanism to attenuate the learning residual originating from the representation and generalizability limitations. Theoretical analysis shows that simultaneous convergence of both the online learning error and the disturbance estimation error can be achieved. Through the unified meta-representation, our framework effectively estimates multiple rapidly changing disturbances, as demonstrated by quadrotor flight experiments. See the project page for video, supplementary material and code: https://nonstructural-metalearn.github.io.&lt;/p&gt;</description></item><item><guid>2601.02780v1</guid><title>MiMo-V2-Flash Technical Report</title><link>http://arxiv.org/abs/2601.02780v1</link><author>Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, Gang Xie, Hailin Zhang, Hanglong Lv, Hanyu Li, Heyu Chen, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Lei Li, Liang Zhao, Linghao Zhang, Peidian Li, Qianli Chen, Shaohui Liu, Shihua Yu, Shijie Cao, Shimao Chen, Shouqiu Yu, Shuo Liu, Tianling Zhou, Weijiang Su, Weikun Wang, Wenhan Ma, Xiangwei Deng, Bohan Mao, Bowen Ye, Can Cai, Chenghua Wang, Chengxuan Zhu, Chong Ma, Chun Chen, Chunan Li, Dawei Zhu, Deshan Xiao, Dong Zhang, Duo Zhang, Fangyue Liu, Feiyu Yang, Fengyuan Shi, Guoan Wang, Hao Tian, Hao Wu, Heng Qu, Hongfei Yi, Hongxu An, Hongyi Guan, Xing Zhang, Yifan Song, Yihan Yan, Yihao Zhao, Yingchun Lai, Yizhao Gao, Yu Cheng, Yuanyuan Tian, Yudong Wang, Zhen Tang, Zhengju Tang, Zhengtao Wen, Zhichao Song, Zhixian Zheng, Zihan Jiang, Jian Wen, Jiarui Sun, Jiawei Li, Jinlong Xue, Jun Xia, Kai Fang, Menghang Zhu, Nuo Chen, Qian Tu, Qihao Zhang, Qiying Wang, Rang Li, Rui Ma, Shaolei Zhang, Shengfan Wang, Shicheng Li, Shuhao Gu, Shuhuai Ren, Sirui Deng, Tao Guo, Tianyang Lu, Weiji Zhuang, Weikang Zhang, Weimin Xiong, Wenshan Huang, Wenyu Yang, Xin Zhang, Xing Yong, Xu Wang, Xueyang Xie, Yilin Jiang, Yixin Yang, Yongzhe He, Yu Tu, Yuanliang Dong, Yuchen Liu, Yue Ma, Yue Yu, Yuxing Xiang, Zhaojun Huang, Zhenru Lin, Zhipeng Xu, Zhiyang Chen, Zhonghua Deng, Zihan Zhang, Zihao Yue</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; MiMo-V2-Flash 是一种混合专家模型，拥有 309B 总参数和 15B 活跃参数，旨在实现快速且强大的推理与代理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种高效、可扩展的语言模型，兼具强推理和代理功能，并在参数规模上优于现有顶尖开源模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用混合注意力架构，将滑动窗口注意力与全局注意力交错使用；使用多标记预测预训练，支持 32k 及 256k 上下文；引入多教师 On-Policy 迁移学习，利用专门领域教师的密集奖励提升学生模型；在推理阶段将多标记预测作为草稿模型实现投机解码，显著提升速度和接受长度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MiMo-V2-Flash 在参数量仅为 DeepSeek-V3.2 的一半、Kimi-K2 的三分之一的情况下，性能与它们相当；推理时可获得 3.6 倍的接受长度和 2.6 倍的解码速度提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型通过创新的架构和迁移学习方法，在保持高性能的同时实现了参数和计算效率的显著提升，并已开源模型权重以促进社区研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 MiMo-V2-Flash，一种拥有 309B 总参数和 15B 活跃参数的混合专家模型，旨在实现快速、强大的推理和代理能力。MiMo-V2-Flash 采用混合注意力架构，在滑动窗口注意力与全局注意力之间交错，使用 128-token 滑动窗口并保持 5:1 的混合比例。该模型在 27 万亿个标记上使用多标记预测进行预训练，支持 32k 上下文长度，并随后扩展到 256k。为高效扩展后训练计算，MiMo-V2-Flash 引入了多教师 On-Policy 迁移学习范式。在此框架中，领域专门化教师（例如通过大规模强化学习训练）提供密集且基于标记的奖励，使学生模型能够完美掌握教师专业知识。MiMo-V2-Flash 与 DeepSeek-V3.2 和 Kimi-K2 等顶级开源模型相媲美，尽管其总参数仅为前者的一半、后者的三分之一。推理时，通过将多标记预测重新用于投机解码，MiMo-V2-Flash 在使用三层多标记预测时实现了最高 3.6 倍的接受长度和 2.6 倍的解码速度提升。我们开源了模型权重和三层多标记预测权重，以促进开放研究和社区合作。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.&lt;/p&gt;</description></item><item><guid>2601.02783v1</guid><title>EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework</title><link>http://arxiv.org/abs/2601.02783v1</link><author>Junjue Wang, Yanfei Zhong, Zihang Chen, Zhuo Zheng, Ailong Ma, Liangpei Zhang</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 地球视觉在地理空间对象识别方面已取得进展，但缺乏对象关系推理，限制了对场景的全面理解。为此提出了一个逐步的地球视觉-语言理解与生成框架，包括多任务数据集EarthVLSet和语义引导网络EarthVLNet。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地球视觉在地理空间对象识别已取得里程碑，但在对象关系推理方面缺乏探索，限制了对场景的全面理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个多任务数据集和语义引导网络，以实现从语义分割到关系推理的逐步地球视觉-语言理解与生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; EarthVLSet包含10.9k子米分辨率遥感图像、土地覆盖掩模和761.5k文本对，涵盖多项选择和开放式视觉问答任务；EarthVLNet通过先进行土地覆盖分割生成对象语义，再由像素级语义引导的大语言模型进行关系推理和知识总结，使用数值差异损失进行优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; EarthVLNet在语义分割、多项选择和开放式VQA基准上表现优异；分割特征在跨数据集场景下仍能提升VQA性能；多项选择任务对视觉编码器更敏感；开放式任务需要先进的视觉编码器和语言解码器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该数据集和方法为连接“图像-掩模-文本”提供了有益的基准，推动地球视觉在地理应用中的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 地球视觉在地理空间对象识别方面已取得里程碑，但在对象关系推理方面缺乏探索，限制了对场景的全面理解。为此，提出了一个逐步的地球视觉-语言理解与生成框架，包括多任务数据集（EarthVLSet）和语义引导网络（EarthVLNet）。聚焦城市规划应用，EarthVLSet包含10.9k子米分辨率遥感图像、土地覆盖掩模以及761.5k文本对，涉及多项选择和开放式视觉问答（VQA）任务。以对象为中心，EarthVLNet逐步实现语义分割、关系推理和全面理解。第一阶段进行土地覆盖分割，生成对象语义以指导VQA。基于像素级语义，面向对象意识的大语言模型（LLM）执行关系推理和知识总结，生成所需答案。优化方面，提出数值差异损失，动态添加差异惩罚，解决不同对象统计问题。三个基准（语义分割、多项选择、开放式VQA）展示了EarthVLNet的优势，并提出三个未来方向：1）分割特征在跨数据集场景下也能持续提升VQA性能；2）多项选择任务对视觉编码器的敏感度高于语言解码器；3）开放式任务需要先进的视觉编码器和语言解码器以获得最佳性能。我们相信该数据集和方法将提供一个有益的基准，将“图像-掩模-文本”连接起来，推动地球视觉在地理应用中的发展。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects&amp;#x27; statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects &amp;#x27;&amp;#x27;image-mask-text&amp;#x27;&amp;#x27;, advancing geographical applications for Earth vision.&lt;/p&gt;</description></item><item><guid>2601.02785v1</guid><title>DreamStyle: A Unified Framework for Video Stylization</title><link>http://arxiv.org/abs/2601.02785v1</link><author>Mengtian Li, Jinshu Chen, Songtao Zhao, Wanquan Feng, Pengqi Tu, Qian He</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 视频风格化是视频生成模型的重要下游任务，但尚未得到充分研究。 2. 输入的风格条件包括文本、风格图像和已风格化的首帧，每种条件都有其优势：文本更灵活，风格图像提供更准确的视觉锚点，首帧使长视频风格化成为可能。 3. 现有方法大多仅支持单一风格条件，限制了应用范围。 4. 缺乏高质量数据集导致风格不一致和时间抖动。 5. 本文提出 DreamStyle，统一框架支持文本引导、风格图像引导和首帧引导，并配备精心设计的数据采集管线以获取高质量配对视频数据。 6. DreamStyle 基于普通的图像到视频模型，并使用低秩适配（LoRA）和特定令牌的上采样矩阵进行训练，以减少不同条件令牌之间的混淆。 7. 通过定性和定量评估，DreamStyle 在三种视频风格化任务中均表现出色，且在风格一致性和视频质量上优于竞争方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频风格化是视频生成模型的重要下游任务，但尚未得到充分研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有方法仅支持单一风格条件、缺乏高质量数据集导致风格不一致和时间抖动的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 DreamStyle 统一框架，支持文本、风格图像和首帧三种风格条件；配备高质量数据采集管线；基于普通图像到视频模型，使用低秩适配（LoRA）和特定令牌的上采样矩阵进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DreamStyle 在三种视频风格化任务中均表现出色，且在风格一致性和视频质量上优于竞争方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DreamStyle 为视频风格化提供了统一且高质量的解决方案，显著提升了风格一致性和视频质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视频风格化是视频生成模型的重要下游任务，但尚未得到充分研究。其输入风格条件通常包括文本、风格图像和已风格化的首帧。每种条件都有其优势：文本更灵活，风格图像提供更准确的视觉锚点，首帧使长视频风格化成为可能。然而，现有方法大多仅支持单一风格条件，限制了应用范围。此外，缺乏高质量数据集导致风格不一致和时间抖动。为了解决这些限制，我们提出了 DreamStyle，一种统一的视频风格化框架，支持文本引导、风格图像引导和首帧引导，并配备精心设计的数据采集管线以获取高质量配对视频数据。DreamStyle 基于普通的图像到视频模型，并使用低秩适配（LoRA）和特定令牌的上采样矩阵进行训练，以减少不同条件令牌之间的混淆。定性和定量评估表明，DreamStyle 在三种视频风格化任务中均表现出色，并在风格一致性和视频质量上优于竞争方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.&lt;/p&gt;</description></item><item><guid>2601.02806v1</guid><title>Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining</title><link>http://arxiv.org/abs/2601.02806v1</link><author>Mingzhou Jiang, Jiaying Zhou, Nan Zeng, Mickael Li, Qijie Tang, Chao He, Huazhu Fu, Honghui He</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种拓扑感知框架，用于将常规的 H&amp;amp;E 组织切片图像转换为免疫组化（IHC）图像，从而实现成本更低、效率更高的虚拟染色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; IHC 染色在癌症诊断中具有重要意义，但其复杂、耗时且昂贵的流程限制了临床应用。相比之下，H&amp;amp;E 染色更为常用。虚拟染色技术通过将 H&amp;amp;E 图像转化为 IHC 图像来提供一种经济替代方案，但使用相邻切片作为真值时常出现空间错位和局部变形，导致监督学习效果不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决空间错位和局部变形导致的弱配对数据问题，提出一种拓扑感知的 H&amp;amp;E‑to‑IHC 虚拟染色方法，以提升生成图像的结构和病理一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入拓扑一致性匹配机制（TACM），利用图对比学习和拓扑扰动学习鲁棒匹配模式；再提出拓扑约束病理匹配机制（TCPM），根据节点重要性对病理阳性区域进行对齐，从而增强病理一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在两个基准数据集、四个染色任务上进行的大量实验表明，该方法在生成质量和临床相关性方面均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 拓扑感知框架能够有效克服空间错位和局部变形的挑战，显著提升 H&amp;amp;E‑to‑IHC 虚拟染色的质量和临床价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Immunohistochemical staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&amp;amp;E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&amp;amp;E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&amp;amp;E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&amp;amp;E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&amp;amp;E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.&lt;/p&gt;</description></item><item><guid>2601.02884v1</guid><title>Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction</title><link>http://arxiv.org/abs/2601.02884v1</link><author>Hana Yahia, Bruno Figliuzzi, Florent Di Meglio, Laurent Gerbaud, Stephane Menand, Mohamed Mahjoub</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这篇论文比较了在钻井环境下用于时间序列数据的域泛化技术，重点预测连续的 Stick‑Slip 指数（SSI），评估钻头扭转振动。研究通过在 60 秒标记的 1Hz 地面钻井数据上训练回归模型，测试在未见井中，并通过网格搜索调优超参数。比较了对抗域泛化（ADG）、不变风险最小化（IRM）和基线模型，并评估迁移学习的效果。ADG 和 IRM 分别比基线提升 10% 和 8%，严重事件检测率从 20% 提升到 60%。结果表明 ADG 最优，迁移学习进一步提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在钻井作业中，Stick‑Slip 指数是评估钻头扭转振动的重要指标，传统方法难以在不同井区泛化。域泛化技术可提升模型在新井中的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在不同井区泛化的稳健回归模型，用于预测连续的 SSI。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用 60 秒 1Hz 标记序列训练回归模型，使用网格搜索优化关键超参数；对抗域泛化（ADG）、不变风险最小化（IRM）和基线模型进行比较；在未见井中测试；评估迁移学习对预训练模型的提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ADG 和 IRM 分别比基线提升 10% 和 8%；严重事件检测率从 20% 提升到 60%；ADG 在性能上略优于 IRM；迁移学习进一步提升模型表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ADG 与 IRM 均优于基线，ADG 更为有效；域泛化方法在钻井应用中具有潜力，迁移学习可进一步提升效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 这篇论文提供了在钻井背景下应用于时间序列数据的域泛化技术的全面比较，重点预测连续的 Stick‑Slip 指数（SSI），这是评估钻头扭转下井振动的关键指标。研究旨在开发一种能够在不同域中泛化的稳健回归模型，通过在 60 秒标记的 1Hz 地面钻井数据上训练来预测 SSI，并在与训练时不同的井中进行测试。为微调模型架构，采用网格搜索方法优化关键超参数。本文呈现了对抗域泛化（ADG）、不变风险最小化（IRM）和基线模型的比较分析，并评估了迁移学习（TL）在提升模型性能方面的有效性。ADG 和 IRM 模型分别比基线模型提升 10% 和 8%，最重要的是，严重事件的检测率从基线的 20% 提升到 60%。总体而言，结果表明 ADG 和 IRM 模型均优于基线，ADG 模型略优于 IRM 模型。此外，对预训练模型应用迁移学习进一步提升了性能。我们的发现展示了域泛化方法在钻井应用中的潜力，ADG 成为最有效的方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.&lt;/p&gt;</description></item><item><guid>2601.02905v1</guid><title>LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments</title><link>http://arxiv.org/abs/2601.02905v1</link><author>Sara Micol Ferraina, Michele Brienza, Francesco Argenziano, Emanuele Musumeci, Vincenzo Suriani, Domenico D. Bloisi, Daniele Nardi</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 追踪动态环境中的移动物体是机器人学中的核心挑战。 2. 现有方法往往依赖重量级基础模型，效率低下。 3. 本文提出 LOST-3DSG，一种轻量化、开放词汇的 3D 场景图，用于实时跟踪动态物体。 4. 采用基于 word2vec 与句子嵌入的语义跟踪方法，避免存储密集的 CLIP 视觉特征。 5. 实验表明 LOST-3DSG 在性能上优于依赖高维视觉嵌入的方法，并在 TIAGo 机器人真实环境中验证了其有效性与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 动态环境中物体跟踪是机器人领域的关键难题，传统方法受限于高成本的基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种轻量化、开放词汇的 3D 场景图，以提高动态物体跟踪的效率和性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用 word2vec 和句子嵌入的语义方法进行实体跟踪，避免存储密集的 CLIP 视觉特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LOST-3DSG 在实验中表现出比高维视觉嵌入方法更优的性能，并在真实 3D 环境中验证了其有效性与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LOST-3DSG 是一种高效、轻量化的动态物体跟踪方案，适用于真实世界环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在动态环境中跟踪移动物体是机器人学中的核心挑战。最近的研究在这一领域取得了显著进展，但许多现有方法仍因依赖重量级基础模型而效率低下。为解决这一限制，我们提出了 LOST-3DSG，一种轻量化的开放词汇 3D 场景图，旨在实时跟踪真实世界环境中的动态物体。我们的方法采用基于 word2vec 和句子嵌入的语义实体跟踪策略，既实现了开放词汇表示，又避免了存储密集 CLIP 视觉特征的需求。结果表明，LOST-3DSG 在性能上优于依赖高维视觉嵌入的方法。我们通过在 TIAGo 机器人真实 3D 环境中进行定性和定量实验验证了该方法的有效性和效率。代码及补充材料已公开发布在项目网站 https://lab-rococo-sapienza.github.io/lost-3dsg/。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在构建能够在动态环境中实时生成并跟踪3D场景图的系统，并且支持开放词汇表的对象识别。此类技术对机器人导航、增强现实和智能监控等应用至关重要，因为它们需要在不断变化的场景中准确理解和追踪多种物体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计时参考了现有的3D场景图和开放词汇模型（如CLIP）以及动态场景跟踪技术，并在此基础上提出了轻量化的实现方案。文中引用了多篇相关工作，表明他们借鉴并改进了前人方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将轻量级的3D感知模块与开放词汇分类器和时间序列跟踪器结合，形成一个端到端的动态场景图生成管线。实现流程大致为：采集点云/深度数据 → 语义分割与对象检测 → 开放词汇分类 → 关系推理与图构建 → 对图节点进行时间跟踪。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 轻量化网络架构，显著降低计算开销；2) 支持开放词汇表，能够识别未见过的物体类别；3) 在动态环境中实现语义跟踪，保持图结构的时序一致性。与以往静态或词汇受限的3D场景图方法相比，LOST-3DSG 在实时性、通用性和动态适应性上都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了 LOST-3DSG，一种轻量化、开放词汇、可在动态环境中实时跟踪的3D场景图框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.&lt;/p&gt;</description></item><item><guid>2601.02906v1</guid><title>Linear Script Representations in Speech Foundation Models Enable Zero-Shot Transliteration</title><link>http://arxiv.org/abs/2601.02906v1</link><author>Ryan Soh-Eun Shim, Kwanghee Choi, Kalvin Chang, Ming-Hao Hsu, Florian Eichin, Zhizheng Wu, Alane Suhr, Michael A. Hedderich, David Harwath, David R. Mortensen, Barbara Plank</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了多语言语音基础模型在不同书写系统下的输出不确定性，并提出通过在推理时修改激活向量来直接控制输出脚本的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多语言语音基础模型（如 Whisper）在大规模网络数据上训练，数据包含同一语言的多种地区变体，而这些变体往往使用不同的书写系统，导致语音识别输出的脚本也不确定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决语音识别输出脚本不确定性的问题，使得能够在推理时对输出脚本进行直接控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 证明脚本信息在多语言语音模型的激活空间中呈线性编码；在推理时向激活向量添加脚本向量，从而实现对输出脚本的控制；验证该方法在非传统语言-脚本组合（如意大利语使用西里尔字母、日语使用拉丁字母）中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 添加脚本向量可以在测试时诱导脚本变化，即使在非传统语言-脚本配对中也能实现；该方法在 Whisper 的所有模型尺寸上均表现出竞争性的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过在推理时修改激活向量实现脚本控制是一种有效且可行的后置方法，可用于提升多语言语音识别的脚本一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多语言语音基础模型如 Whisper 在大规模网络数据上训练，其中每种语言的数据包含众多地区变体。然而，不同地区变体往往使用不同的脚本来书写同一种语言，导致语音识别输出也会受到输出脚本非确定性的影响。为缓解这一问题，我们展示了脚本在多语言语音模型的激活空间中是线性编码的，并且在推理时修改激活可以直接控制输出脚本。我们发现，在测试时向激活添加脚本向量可以诱导脚本变化，即使在非传统的语言-脚本配对（例如意大利语使用西里尔字母，日语使用拉丁字母）中也能实现。我们将此方法应用于对语音识别输出脚本的后置控制，并观察到在 Whisper 的所有模型尺寸上都具有竞争性的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multilingual speech foundation models such as Whisper are trained on web-scale data, where data for each language consists of a myriad of regional varieties. However, different regional varieties often employ different scripts to write the same language, rendering speech recognition output also subject to non-determinism in the output script. To mitigate this problem, we show that script is linearly encoded in the activation space of multilingual speech models, and that modifying activations at inference time enables direct control over output script. We find the addition of such script vectors to activations at test time can induce script change even in unconventional language-script pairings (e.g. Italian in Cyrillic and Japanese in Latin script). We apply this approach to inducing post-hoc control over the script of speech recognition output, where we observe competitive performance across all model sizes of Whisper.&lt;/p&gt;</description></item><item><guid>2601.02911v1</guid><title>Image, Word and Thought: A More Challenging Language Task for the Iterated Learning Model</title><link>http://arxiv.org/abs/2601.02911v1</link><author>Hyoyeon Lee, Seth Bullock, Conor Houghton</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 迭代学习模型通过代际语言传递模拟语言结构的出现，最近的半监督模型在更大意义-信号空间中成功应用于七段显示图像的语言学习，结果显示学习到的语言既表达丰富、可组合又稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统迭代学习模型显示，受限的输入量会促使语言从无歧义、符合语法规则并在代际间保持一致的结构出现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索更可计算、生态有效的半监督迭代学习模型在更复杂意义传递任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 结合监督与无监督学习的自编码器架构，限制学习者接触的发声数量，模拟语言传递过程，并在七段显示图像任务中进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 代理能够学习到表达所有128个字符的独特代码，信号的组成部分与意义的组成部分保持一致，且语言在代际间保持不变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 半监督迭代学习模型能够在更大意义空间中产生既表达丰富、可组合又稳定的语言结构，验证了语言传递瓶颈对语言结构的促进作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 迭代学习模型模拟语言从一代传到下一代的过程，以探究语言传递所施加的约束如何促进语言结构的出现。尽管每个模型化的语言学习者从空白开始，但限制学习者接触的发声数量的瓶颈会导致出现缺乏歧义、受语法规则支配且在连续代际中保持一致的语言，即一种表达丰富、可组合且稳定的语言。最近引入的更易计算且生态有效的半监督迭代学习模型，将监督学习和无监督学习结合在自编码器架构中，使得能够在更大的意义-信号空间中探索语言传递动力学。这里首次将该模型成功应用于涉及更复杂意义的语言学习任务：七段显示图像。模型中的代理能够学习并传递一种表达丰富的语言：为所有128个字符使用不同的代码；可组合：信号的组成部分始终映射到意义的组成部分；稳定：语言在代际间不发生变化。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The iterated learning model simulates the transmission of language from generation to generation in order to explore how the constraints imposed by language transmission facilitate the emergence of language structure. Despite each modelled language learner starting from a blank slate, the presence of a bottleneck limiting the number of utterances to which the learner is exposed can lead to the emergence of language that lacks ambiguity, is governed by grammatical rules, and is consistent over successive generations, that is, one that is expressive, compositional and stable. The recent introduction of a more computationally tractable and ecologically valid semi supervised iterated learning model, combining supervised and unsupervised learning within an autoencoder architecture, has enabled exploration of language transmission dynamics for much larger meaning-signal spaces. Here, for the first time, the model has been successfully applied to a language learning task involving the communication of much more complex meanings: seven-segment display images. Agents in this model are able to learn and transmit a language that is expressive: distinct codes are employed for all 128 glyphs; compositional: signal components consistently map to meaning components, and stable: the language does not change from generation to generation.&lt;/p&gt;</description></item><item><guid>2601.02954v1</guid><title>The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models</title><link>http://arxiv.org/abs/2601.02954v1</link><author>Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种层次化的听觉场景分析框架，旨在让大型音频语言模型从单一音频流转向具备空间感知的全景理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的大型音频语言模型只关注音频的语义信息，忽略了空间维度（“在哪里”），这限制了它们在普适声学场景分析中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补这一缺口，使模型能够理解并推理复杂的声学世界，从而实现空间智能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 构建大规模合成双耳音频数据集，提供丰富的空间线索；2. 设计混合特征投影器，使用并行的语义编码器和空间编码器提取解耦表示，并通过密集融合机制整合；3. 采用渐进式训练课程，从监督微调到基于组相对策略优化的强化学习，逐步提升模型推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在综合基准测试中，模型表现出相对较强的空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过实现空间感知，本文为利用大型模型强大的推理能力开展全景声学场景分析提供了明确路径，将从单一语义识别提升到空间智能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的大型音频语言模型把世界视为“单声道”——单一音频流，忽略了普适声学场景分析所需的关键空间维度（“在哪里”）。为弥补这一差距，我们首先提出了听觉场景分析（ASA）的层次化框架。在此框架的指导下，我们引入了一个系统，使像 Qwen2‑Audio 这样的模型能够理解并推理复杂的声学世界。我们的框架通过三个核心贡献实现这一目标：首先，我们构建了一个大规模的合成双耳音频数据集，以提供丰富的空间线索；其次，我们设计了一个混合特征投影器，利用并行的语义和空间编码器提取解耦表示，并通过密集融合机制将这些不同的流整合，确保模型获得声学场景的整体视角；最后，我们采用渐进式训练课程，从监督微调（SFT）到通过组相对策略优化（GRPO）的强化学习，明确地将模型的能力演进至推理。通过我们的综合基准，模型展示了相对较强的空间理解能力。通过实现空间感知，我们的工作为利用大型模型强大的推理能力开展全景声学场景分析提供了清晰路径，从“单声道”语义识别迈向空间智能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing large audio-language models perceive the world as &amp;quot;mono&amp;quot; -- a single stream of audio that ignores the critical spatial dimension (&amp;quot;where&amp;quot;) required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model&amp;#x27;s capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from &amp;quot;mono&amp;quot; semantic recognition to spatial intelligence.&lt;/p&gt;</description></item><item><guid>2601.02971v1</guid><title>Few-shot learning for security bug report identification</title><link>http://arxiv.org/abs/2601.02971v1</link><author>Muhammad Laiq</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于少样本学习的技术，用于在标注数据稀缺的情况下快速识别安全漏洞报告。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统机器学习方法需要大量标注数据，但安全漏洞报告的数据集往往很少，导致模型效果差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索少样本学习框架 SetFit 在安全漏洞报告分类中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用 SetFit 结合句子变换器、对比学习和参数高效微调，在少量标注数据上训练模型，并评估其将报告分为安全相关与非安全相关的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在所有评估的数据集上，SetFit 的 AUC 达到 0.865，优于传统机器学习基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SetFit 基于少样本学习能有效识别安全漏洞报告，且只需极少的标注工作，适用于标注数据稀缺的场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 安全漏洞报告需要及时识别，以最小化软件系统中的漏洞窗口。传统机器学习技术在分类安全漏洞报告时高度依赖大量标注数据。然而，安全漏洞报告的数据集在实践中往往稀缺，导致模型性能差，且在真实环境中的适用性有限。在本研究中，我们提出了一种基于少样本学习的技术，利用有限的标注数据有效识别安全漏洞报告。我们采用 SetFit，这是一种最先进的少样本学习框架，结合句子变换器、对比学习和参数高效微调。该模型在少量标注的漏洞报告数据集上进行训练，并评估其将报告分类为安全相关或非安全相关的能力。我们的方案在所有评估的数据集上，AUC 最多达到 0.865，优于传统机器学习技术（基线）。这突显了 SetFit 在有效识别安全漏洞报告方面的潜力。基于 SetFit 的少样本学习为识别安全漏洞报告提供了一种有前景的替代方案。该方法能够在最小化标注工作量的前提下高效开发模型，适用于标注数据稀缺的场景。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.&lt;/p&gt;</description></item><item><guid>2601.03032v1</guid><title>Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning</title><link>http://arxiv.org/abs/2601.03032v1</link><author>Vidhi Rathore</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了Causal Manifold Fairness框架，利用因果推断与几何深度学习结合，学习在敏感属性干预下保持不变的潜在空间几何结构，从而实现公平性与任务效用的平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统公平性方法将数据视为高维空间中的静态点，忽略了生成结构和敏感属性对数据流形几何的因果影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 证明敏感属性不仅仅改变分布，而是因果地扭曲数据流形几何，并开发一种方法在潜在空间中保持几何不变，以提升公平性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过约束解码器的雅可比矩阵和Hessian，学习一个潜在表示，使得局部黎曼几何在对敏感属性的反事实干预下保持不变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成结构因果模型上验证，CMF能够有效分离敏感属性导致的几何扭曲，同时保持任务性能，并通过几何指标量化公平-效用权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Causal Manifold Fairness为公平机器学习提供了新的几何视角，能够在保持模型效用的同时实现更稳健的公平性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 公平性在机器学习中变得越来越重要，但传统方法往往将数据视为高维空间中的静态点，忽略了潜在的生成结构。我们认为敏感属性（如种族、性别）不仅仅是分布的偏移，而是因果地扭曲数据流形的几何。为了解决这个问题，我们提出了Causal Manifold Fairness（CMF）框架，结合因果推断和几何深度学习。CMF学习一个潜在表示，使得局部黎曼几何（由度量张量和曲率定义）在对敏感属性进行反事实干预时保持不变。通过对解码器的雅可比矩阵和Hessian施加约束，CMF确保潜在空间的距离和形状规则在不同人群之间保持一致。我们在合成结构因果模型上验证了CMF，证明它能够有效分离敏感属性导致的几何扭曲，同时保持任务效用，并通过几何指标严格量化公平-效用权衡。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Fairness in machine learning is increasingly critical, yet standard approaches often treat data as static points in a high-dimensional space, ignoring the underlying generative structure. We posit that sensitive attributes (e.g., race, gender) do not merely shift data distributions but causally warp the geometry of the data manifold itself. To address this, we introduce Causal Manifold Fairness (CMF), a novel framework that bridges causal inference and geometric deep learning. CMF learns a latent representation where the local Riemannian geometry, defined by the metric tensor and curvature, remains invariant under counterfactual interventions on sensitive attributes. By enforcing constraints on the Jacobian and Hessian of the decoder, CMF ensures that the rules of the latent space (distances and shapes) are preserved across demographic groups. We validate CMF on synthetic Structural Causal Models (SCMs), demonstrating that it effectively disentangles sensitive geometric warping while preserving task utility, offering a rigorous quantification of the fairness-utility trade-off via geometric metrics.&lt;/p&gt;</description></item><item><guid>2601.03048v1</guid><title>On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning</title><link>http://arxiv.org/abs/2601.03048v1</link><author>Siyi Lyu, Quan Liu, Feng Yan</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文指出视觉变换器（ViT）在语义识别方面表现优异，但在空间推理任务（如心理旋转）中系统性失效。作者认为这不是单纯的数据规模问题，而是架构本身的电路复杂度限制。通过将空间理解形式化为学习群同态，作者证明对非可解群（如三维旋转群）保持结构映射在计算上受限于 NC1 完整问题；而常数深度 ViT 的计算能力被 TC0 限制。基于 TC0 小于 NC1 的猜想，作者确立了一个复杂度边界，表明常数深度 ViT 无法有效捕捉非可解空间结构。实验通过潜在空间探测验证了这一复杂度差距，显示 ViT 表示在非可解任务上随着组合深度增加会出现结构崩塌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视觉变换器在语义识别任务中表现卓越，但在需要空间推理的任务（如心理旋转）中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究导致 ViT 在空间推理任务中失效的根本原因，并将其归因于架构的电路复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将空间理解定义为学习群同态，分析非可解群的结构保持问题的计算复杂度，并与常数深度 ViT 的 TC0 复杂度进行比较；随后通过潜在空间探测实验验证复杂度差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 非可解群的结构保持问题在计算上受限于 NC1 完整问题；常数深度 ViT 的计算能力被 TC0 限制，无法满足非可解空间结构的需求；实验验证了 ViT 表示在非可解任务上随着组合深度增加会出现结构崩塌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 常数深度 ViT 缺乏足够的逻辑深度，无法高效捕捉非可解空间结构，形成了一个明确的复杂度边界。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉变换器（ViT）在语义识别方面表现卓越，但在空间推理任务（如心理旋转）中系统性失效。虽然常被归因于数据规模，但我们认为这一局限源于架构的内在电路复杂度。我们将空间理解形式化为学习群同态：将图像序列映射到潜在空间，同时保持底层变换群的代数结构。我们证明，对于非可解群（例如三维旋转群），保持这种结构保持的嵌入在计算上受限于 NC1 完整问题。相反，我们证明常数深度 ViT 在多项式精度下严格受 TC0 限制。基于 TC0 小于 NC1 的猜想，我们确立了一个复杂度边界：常数深度 ViT 本质上缺乏足够的逻辑深度，无法高效捕捉非可解空间结构。我们通过潜在空间探测验证了这一复杂度差距，证明 ViT 表示在非可解任务上随着组合深度增加会出现结构崩塌。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Vision Transformers (ViTs) excel in semantic recognition but exhibit systematic failures in spatial reasoning tasks such as mental rotation. While often attributed to data scale, we propose that this limitation arises from the intrinsic circuit complexity of the architecture. We formalize spatial understanding as learning a Group Homomorphism: mapping image sequences to a latent space that preserves the algebraic structure of the underlying transformation group. We demonstrate that for non-solvable groups (e.g., the 3D rotation group $\mathrm{SO}(3)$), maintaining such a structure-preserving embedding is computationally lower-bounded by the Word Problem, which is $\mathsf{NC^1}$-complete. In contrast, we prove that constant-depth ViTs with polynomial precision are strictly bounded by $\mathsf{TC^0}$. Under the conjecture $\mathsf{TC^0} \subsetneq \mathsf{NC^1}$, we establish a complexity boundary: constant-depth ViTs fundamentally lack the logical depth to efficiently capture non-solvable spatial structures. We validate this complexity gap via latent-space probing, demonstrating that ViT representations suffer a structural collapse on non-solvable tasks as compositional depth increases.&lt;/p&gt;</description></item><item><guid>2601.03062v1</guid><title>Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks</title><link>http://arxiv.org/abs/2601.03062v1</link><author>Qusai Khaled, Pasquale De Marinis, Moez Louati, David Ferras, Laura Genga, Uzay Kaymak</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种可解释的图神经网络框架，用于水分配网络中的泄漏检测与定位，结合互信息和模糊逻辑，既保持较高准确率，又提供易于理解的规则解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 水分配网络泄漏检测及时性对资源节约和运营效率至关重要，但传统图神经网络缺乏可解释性，限制了其实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种既能捕捉空间时间依赖，又能提供可解释规则的图神经网络，以提升泄漏检测的可接受性和实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在多种图神经网络架构中进行基准测试，选择GENConv作为基础模型，并在其上加入模糊逻辑层，形成FGENConv；同时利用互信息识别关键网络区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; FGENConv在泄漏检测和定位上的F1分数分别为0.889和0.814，略低于纯GENConv的0.938和0.858，但提供了空间局部化的模糊规则解释，帮助工程师验证结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过在精度与可解释性之间取得平衡，模糊网络能够帮助水利工程师验证泄漏位置、节约人力并优化维护策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.&lt;/p&gt;</description></item><item><guid>2601.03086v1</guid><title>Pretrain Finite Element Method: A Pretraining and Warm-start Framework for PDEs via Physics-Informed Neural Operators</title><link>http://arxiv.org/abs/2601.03086v1</link><author>Yizheng Wang, Zhongkai Hao, Mohammad Sadegh Eshaghi, Cosmin Anitescu, Xiaoying Zhuang, Timon Rabczuk, Yinghua Liu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 提出一种预训练有限元方法（PFEM），将神经算子学习的高效性与传统有限元方法的高精度和鲁棒性相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统有限元方法在精度和稳定性上表现优异，但计算成本高；神经算子学习速度快，但缺乏物理约束。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种物理驱动的框架，既能快速生成初始解，又能保持有限元方法的准确性和收敛保证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用基于Transolver架构的神经算子在无标注数据的偏微分方程约束下进行预训练，直接处理无结构点云，编码几何、材料和边界信息；随后可选地将预训练结果作为传统有限元求解器的初始猜测，进行微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 预训练阶段能在多种基准问题上实现约1%的相对误差，微调阶段相较于无初始猜测的有限元求解器可提升速度多达十倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PFEM在保持有限元方法精度与收敛性的同时，显著降低迭代次数和计算时间，展示了良好的泛化和加速效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出一种预训练有限元方法（PFEM），一种物理驱动的框架，结合了神经算子学习的效率与传统有限元方法（FEM）的精度和鲁棒性。PFEM包括一个物理信息预训练阶段和一个可选的微调阶段。在预训练阶段，基于Transolver架构的神经算子仅从治理偏微分方程中训练，而不依赖标注的解数据。该模型直接在无结构点云上操作，联合编码几何信息、材料属性和边界条件，并产生物理一致的初始解，计算效率极高。通过显式有限元微分来强制执行PDE约束，避免了自动微分的开销。在微调阶段，预训练的预测被用作传统FEM求解器的初始猜测，保留了它们的精度、收敛保证和外推能力，同时显著减少了达到规定容差所需的迭代次数。PFEM在包括线性弹性和非线性超弹性在内的广泛基准问题上得到验证，涵盖复杂几何、异质材料和任意边界条件。数值结果表明预训练阶段具有强大的泛化能力，相关误差约为1%，而微调阶段相较于无初始猜测的FEM可实现最高一阶数量级的加速。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose a Pretrained Finite Element Method (PFEM),a physics driven framework that bridges the efficiency of neural operator learning with the accuracy and robustness of classical finite element methods (FEM). PFEM consists of a physics informed pretraining stage and an optional finetuning stage. In the pretraining stage, a neural operator based on the Transolver architecture is trained solely from governing partial differential equations, without relying on labeled solution data. The model operates directly on unstructured point clouds, jointly encoding geometric information, material properties, and boundary conditions, and produces physically consistent initial solutions with extremely high computational efficiency. PDE constraints are enforced through explicit finite element, based differentiation, avoiding the overhead associated with automatic differentiation. In the fine-tuning stage, the pretrained prediction is used as an initial guess for conventional FEM solvers, preserving their accuracy, convergence guarantees, and extrapolation capability while substantially reducing the number of iterations required to reach a prescribed tolerance. PFEM is validated on a broad range of benchmark problems, including linear elasticity and nonlinear hyperelasticity with complex geometries, heterogeneous materials, and arbitrary boundary conditions. Numerical results demonstrate strong generalization in the pretraining stage with relative errors on the order of 1\%, and speedups of up to one order of magnitude in the fine-tuning stage compared to FEM with zero initial guesses.&lt;/p&gt;</description></item><item><guid>2601.03111v1</guid><title>One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling</title><link>http://arxiv.org/abs/2601.03111v1</link><author>Yiyuan Li, Zhen Huang, Yanan Wu, Weixun Wang, Xuefeng Li, Yijia Luo, Wenbo Su, Bo Zheng, Pengfei Liu</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种一次性学习框架——多学科学习（polymath learning），通过设计单个高质量样本实现跨学科推理性能提升，挑战传统强化学习对海量数据的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统强化学习在大型语言模型中的成功通常需要数千甚至更多高质量样本，缺乏对数据需求的深入探讨。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 证明一次性学习样本即可显著提升推理性能，从而挑战对数据量的传统假设。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计单个数学推理样本，结合多学科元素进行强化学习训练，并与传统多样本训练进行对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1. 单个精心挑选的数学推理样本可在物理、化学、生物等领域显著提升性能；2. 关键数学技能揭示了最佳多学科样本的特征；3. 经过工程化的合成样本优于自然出现的单独样本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 样本质量与设计比样本数量更能解锁语言模型的推理能力，建议转向样本工程而非单纯增加数据量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型（LLM）的推理能力可以通过强化学习（RL）得到释放（OpenAI，2024；DeepSeek-AI 等，2025a；Zeng 等，2025）。现有在 LLM 上的 RL 成功通常依赖于数千甚至更多高质量样本。在本文中，我们通过展示一次性学习的显著效果，挑战了关于 LLM 强化学习数据需求的基本假设。具体而言，我们提出了多学科学习（polymath learning）框架，用于设计一个能够产生多学科影响的训练样本。我们提出了三个关键发现：（1）一个经过策略性选择的数学推理样本能够在物理、化学、生物等多个领域通过 RL 显著提升性能；（2）与推理相关的数学技能揭示了最佳多学科样本的特征；（3）一个集成多学科元素的工程化合成样本优于自然出现的单独样本。我们的方法在各种推理基准上实现了优于使用更大数据集训练的性能，证明样本质量和设计，而非数量，可能是解锁语言模型增强推理能力的关键。我们的结果提示一种被称为样本工程的转变，即精确设计训练样本，而不是简单地增加数据量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.&lt;/p&gt;</description></item><item><guid>2601.03153v1</guid><title>Parallel Latent Reasoning for Sequential Recommendation</title><link>http://arxiv.org/abs/2601.03153v1</link><author>Jiakai Tang, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang, Bo Zheng</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的并行潜在推理框架 PLR，旨在通过宽度级别的计算扩展来捕捉稀疏行为序列中的复杂用户偏好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在序列推荐中，从稀疏行为序列中捕捉复杂用户偏好仍是核心挑战。最近的潜在推理方法通过多步推理扩展测试时计算，但仅依赖单一路径的深度扩展，随着推理深度增加收益递减。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为克服深度扩展的局限，提出并行潜在推理（PLR），通过同时探索多条多样化推理轨迹实现宽度级别的计算扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PLR 通过可学习的触发词在连续潜在空间构建并行推理流，利用全局推理正则化保持流之间的多样性，并通过混合推理流聚合自适应合成多流输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个真实数据集上的实验表明，PLR 在保持实时推理效率的同时，显著优于现有最先进基线；理论分析进一步验证了并行推理在提升泛化能力方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该工作为在序列推荐中提升推理能力提供了新的宽度级别思路，超越了传统的深度扩展方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从稀疏行为序列中捕捉复杂用户偏好仍是序列推荐中的根本挑战。最近的潜在推理方法通过在测试时进行多步推理来扩展计算，显示出前景，但它们仅依赖单一路径的深度扩展，随着推理深度增加收益递减。为解决这一限制，我们提出了并行潜在推理（PLR），这是一种新框架，开创了通过探索多条多样化推理轨迹同时进行宽度级别计算扩展的思路。PLR 通过连续潜在空间中的可学习触发词构建并行推理流，通过全局推理正则化保持流之间的多样性，并通过混合推理流聚合自适应合成多流输出。对三个真实数据集的广泛实验表明，PLR 在保持实时推理效率的同时，显著优于最先进的基线；理论分析进一步验证了并行推理在提升泛化能力方面的有效性。我们的工作为在序列推荐中提升推理能力开辟了新的途径，超越了现有的深度扩展。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose \textbf{Parallel Latent Reasoning (PLR)}, a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.&lt;/p&gt;</description></item><item><guid>2601.03181v1</guid><title>Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey</title><link>http://arxiv.org/abs/2601.03181v1</link><author>Han Zhang, Mohammad Farzanullah, Mohammad Ghassemi, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了基础模型（FM）在无线网络中的应用，重点关注多模态FM在网络管理中的预测和控制任务，并提出了无线专用FM的开发路径及未来挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 基础模型被视为人工智能领域的突破性进展，正在重塑学术与工业界。将FM引入无线网络有望实现通用AI代理，处理多模态数据的复杂网络管理任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 讨论多模态FM在无线网络中的利用，聚焦预测与控制两类网络管理任务，并阐述FM在这些任务中的作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先阐述FM支持的多模态上下文信息理解；随后说明FM如何分别应用于预测和控制任务；接着从数据集和方法论两方面介绍无线专用FM的开发。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 总结了FM增强无线网络的挑战与未来研究方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基础模型（FM）被认为是一次变革性的突破，已开始重塑学术界和工业界的人工智能未来。将FM集成到无线网络中，预计将实现通用AI代理的开发，能够处理多样化的网络管理请求以及涉及多模态数据的高度复杂无线相关任务。受此启发，本文讨论了FM，尤其是多模态FM在无线网络中的应用。我们聚焦于无线网络管理中的两类重要任务：预测任务和控制任务。具体而言，首先讨论FM支持的多模态上下文信息理解在无线网络中的应用。随后，分别说明FM如何应用于预测和控制任务。接着，从可用数据集和方法论两方面介绍无线专用FM的发展。最后，讨论FM增强无线网络的挑战与未来方向。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Foundation models (FMs) are recognized as a transformative breakthrough that has started to reshape the future of artificial intelligence (AI) across both academia and industry. The integration of FMs into wireless networks is expected to enable the development of general-purpose AI agents capable of handling diverse network management requests and highly complex wireless-related tasks involving multi-modal data. Inspired by these ideas, this work discusses the utilization of FMs, especially multi-modal FMs in wireless networks. We focus on two important types of tasks in wireless network management: prediction tasks and control tasks. In particular, we first discuss FMs-enabled multi-modal contextual information understanding in wireless networks. Then, we explain how FMs can be applied to prediction and control tasks, respectively. Following this, we introduce the development of wireless-specific FMs from two perspectives: available datasets for development and the methodologies used. Finally, we conclude with a discussion of the challenges and future directions for FM-enhanced wireless networks.&lt;/p&gt;</description></item><item><guid>2601.03193v1</guid><title>UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</title><link>http://arxiv.org/abs/2601.03193v1</link><author>Ruiyan Han, Zhen Fang, XinYu Sun, Yuchen Ma, Ziheng Wang, Yu Zeng, Zehui Chen, Lin Chen, Wenxuan Huang, Wei-Jie Xu, Yi Cao, Feng Zhao</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 统一多模态模型在跨模态理解方面表现出色，但在高质量生成方面仍存在显著差距。2. 该差距被定义为“传导失语”，即模型能准确解读多模态输入，却难以将理解转化为可信且可控的生成。3. 提出 UniCorn 框架，通过将单一模型拆分为提议者、求解者和评判者三角色，利用自我对弈和认知模式重构实现自我提升。4. 通过 UniCycle 循环一致性基准验证多模态连贯性的恢复。5. 在六个通用图像生成基准上，UniCorn 显著提升性能，并在 TIIF、DPG、CompBench、UniCycle 上实现最先进水平，同时在 WISE 和 OneIG 上分别提升 5.0 和 6.5 分。6. 结果表明，完全自监督的细化方法能显著提升文本到图像生成，同时保持强大的理解能力，展示了统一多模态智能的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 统一多模态模型在跨模态理解任务中取得了显著成功，但在将内部知识转化为高质量生成方面仍存在明显不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将模型在理解与生成之间的差距正式化为“传导失语”，并提出一种无需外部数据或教师监督的自我提升框架 UniCorn，以弥补这一缺口。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 将单一统一多模态模型拆分为提议者、求解者和评判者三角色，形成协作机制。2. 通过自我对弈生成高质量交互。3. 运用认知模式重构，将潜在理解转化为显式生成信号。4. 设计 UniCycle 循环一致性基准（文本→图像→文本重构）验证多模态连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1. UniCorn 在六个通用图像生成基准上实现全面且显著的提升。2. 在 TIIF、DPG、CompBench 和 UniCycle 上达到最先进水平。3. 在 WISE 和 OneIG 上分别提升 5.0 分和 6.5 分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 完全自监督的细化方法显著提升文本到图像生成质量，同时保持强大的跨模态理解能力，证明了统一多模态智能的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在统一多模态模型（UMMs）在跨模态理解方面取得显著成功的同时，它们在利用内部知识进行高质量生成方面仍存在显著差距。我们将这种差距形式化为传导失语，即模型能够准确解释多模态输入，但难以将这种理解转化为真实且可控的合成。为了解决这个问题，我们提出了 UniCorn，一种简单而优雅的自我提升框架，消除了对外部数据或教师监督的需求。通过将单个 UMM 分成提议者、求解者和评判者三种协作角色，UniCorn 通过自我对弈生成高质量交互，并利用认知模式重构将潜在理解提炼为显式生成信号。为验证多模态连贯性的恢复，我们引入了基于文本→图像→文本重构循环的 UniCycle 循环一致性基准。大量实验表明，UniCorn 在六个通用图像生成基准上相较于基线模型实现了全面且显著的提升。值得注意的是，它在 TIIF（73.8）、DPG（86.8）、CompBench（88.5）和 UniCycle 上实现了最先进的性能，并在 WISE 上进一步获得了 +5.0 的显著提升，在 OneIG 上获得了 +6.5 的显著提升。这些结果表明，我们的方法显著提升了文本到图像生成，同时保持了稳健的理解能力，展示了完全自监督细化在统一多模态智能中的可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.&lt;/p&gt;</description></item><item><guid>2601.03194v1</guid><title>X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework</title><link>http://arxiv.org/abs/2601.03194v1</link><author>Mohammad Zia Ur Rehman, Sai Kartheek Reddy Kasu, Shashivardhan Reddy Koppula, Sai Rithwik Reddy Chirra, Shwetank Shekhar Singh, Nagendra Kumar</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种可解释的多语言仇恨言论检测框架 X-MuTeST，结合大型语言模型的语义推理和注意力增强技术，并在印地语、泰卢固语和英语上提供词级人类注释理由，提升分类准确性和可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 社交媒体上的仇恨言论检测在准确性和可解释性方面面临挑战，尤其是对资源匮乏的印度语言。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种可解释的训练框架，提升多语言仇恨言论检测的性能，并为低资源语言提供人类注释理由。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; X-MuTeST 通过比较原文与单词、双词、三词的预测概率差异生成可解释性，并将大型语言模型的解释与此结果合并；在训练中加入人类词级理由，进一步细化模型注意力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 使用人类理由训练显著提升了分类效果和可解释性；将人类理由与 X-MuTeST 解释结合可进一步改进模型注意力；在三种语言上通过 Token-F1、IOU-F1、Comprehensiveness、Sufficiency 等指标验证了可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在低资源语言上有效推进仇恨言论检测，证明了人类理由与可解释性方法的协同作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在社交媒体上检测仇恨言论面临准确性和可解释性双重挑战，尤其是对未充分研究的印度语言。我们提出了一种新颖的可解释性引导训练框架 X-MuTeST（可解释多语言仇恨言论检测），该框架将大型语言模型的高级语义推理与传统的注意力增强技术相结合。我们将此研究扩展到印地语、泰卢固语和英语，并为每个词提供基准人类注释理由以证明所分配的类别标签。X-MuTeST 的可解释性方法通过比较原始文本与单词、双词和三词的预测概率差异来计算。最终解释是大型语言模型解释与 X-MuTeST 解释的并集。我们证明，在训练过程中利用人类理由可提升分类性能和可解释性。进一步地，将人类理由与我们的可解释性方法结合以细化模型注意力，可带来更大的改进。我们使用可行性指标（Token-F1、IOU-F1）和可信度指标（Comprehensiveness、Sufficiency）评估可解释性。通过关注低资源语言，我们的工作推动了多语言环境下仇恨言论检测的发展。我们的数据集包含 6,004 条印地语、4,492 条泰卢固语和 6,334 条英语样本的词级理由注释。数据和代码可在 https://github.com/ziarehman30/X-MuTeST 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST&lt;/p&gt;</description></item><item><guid>2601.03227v1</guid><title>The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization</title><link>http://arxiv.org/abs/2601.03227v1</link><author>Ruixing Zhang, Zihan Liu, Leilei Sun, Tongyu Zhu, Weifeng Lv</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 研究目标是通过声音信息推断信号的地理来源。2. 目前音频地理定位缺乏高质量音频-位置配对数据。3. 提出了AGL1K基准，涵盖72个国家和地区，包含1,444条精选音频片段。4. 通过音频可定位度指标筛选可靠样本。5. 对16种音频语言模型进行评估，发现它们已具备地理定位能力。6. 结果显示闭源模型优于开源模型，语言线索在预测中占主导。7. 进一步分析了模型推理轨迹、区域偏差、错误原因及可定位度指标的可解释性。8. AGL1K为音频地理定位提供了新的基准，并可能推动音频语言模型的空间推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 音频地理定位在计算机视觉中已成为重要的组合推理基准，并与公共安全相关，但在音频领域因缺乏高质量数据而进展缓慢。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个音频地理定位基准AGL1K，并评估音频语言模型在该任务上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 设计音频可定位度指标，用于量化录音的地理信息丰富度。2. 在众包平台上筛选并整理1,444条可定位音频。3. 对16种音频语言模型进行实验评估。4. 对模型的推理轨迹、区域偏差、错误原因及指标可解释性进行深入分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1. 音频语言模型已具备地理定位能力。2. 闭源模型显著优于开源模型。3. 语言线索往往是预测的主要支撑。4. 可定位度指标具有可解释性，可用于筛选高质量样本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AGL1K为音频地理定位提供了可靠的基准，能够推动音频语言模型在地理推理方面的进一步发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs&amp;#x27; reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs&amp;#x27; reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.&lt;/p&gt;</description></item><item><guid>2601.03233v1</guid><title>LTX-2: Efficient Joint Audio-Visual Foundation Model</title><link>http://arxiv.org/abs/2601.03233v1</link><author>Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LTX-2 是一种开源的基础模型，能够统一生成高质量、时间同步的视听内容。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的文本到视频扩散模型能生成引人注目的视频序列，但缺乏音频所提供的语义、情感和氛围线索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 LTX-2，解决视听同步生成问题，并提升音频质量与可控性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用不对称双流 Transformer，视频流约 14 亿参数，音频流约 5 亿参数，通过双向音视频交叉注意力层、时间位置嵌入和跨模态自适应层归一化实现跨模态耦合；使用多语言文本编码器和模态感知无分类器引导来增强对齐与可控性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在开源系统中，LTX-2 在视听质量和提示遵循度上达到了最先进水平，并且在计算成本和推理时间上与专有模型相当但更低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LTX-2 证明了在统一模型中高效生成同步视听内容的可行性，并为开源社区提供了高质量、可控的视听生成工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.&lt;/p&gt;</description></item><item><guid>2601.03237v1</guid><title>PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters</title><link>http://arxiv.org/abs/2601.03237v1</link><author>Javier Salazar Cavazos</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一种改进的无监督深度聚类算法 PET-TURTLE，旨在解决传统 TURTLE 在数据不平衡时的聚类误差问题，并通过稀疏对数提升平衡数据集的准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来，深度学习在无监督学习数据分组结构方面受到关注，TURTLE 作为领先的深度聚类方法通过交替更新标签和超平面实现无监督标注，但假设聚类平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 改进 TURTLE 的成本函数，使其能够处理不平衡数据分布，并通过稀疏对数优化搜索空间以提升聚类性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PET-TURTLE 采用幂律先验扩展成本函数以适应不平衡分布，并在标记过程中引入稀疏对数，简化搜索空间并提高准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 PET-TURTLE 在合成和真实数据上提升了不平衡数据源的准确率，避免了对少数类的过度预测，并整体提升了聚类效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PET-TURTLE 在处理不平衡数据时表现优异，能够有效减少聚类误差并提升整体聚类质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基础视觉、音频和语言模型通过其潜在表示实现零样本下游任务的性能。最近，使用深度学习方法进行无监督学习数据组结构已变得流行。TURTLE 是一种最先进的深度聚类算法，通过交替更新标签和超平面，在类似支持向量机的方式下最大化超平面间距，从而在无监督情况下发现数据标签。然而，TURTLE 假设聚类是平衡的；当数据不平衡时，它会产生非理想的超平面，导致更高的聚类误差。我们提出了 PET-TURTLE，通过幂律先验将成本函数推广以处理不平衡数据分布。此外，通过在标记过程中引入稀疏对数，PET-TURTLE 优化了更简单的搜索空间，从而提高了平衡数据集的准确率。对合成和真实数据的实验表明，PET-TURTLE 在不平衡源上提高了准确率，防止了对少数类的过度预测，并增强了整体聚类。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.&lt;/p&gt;</description></item><item><guid>2601.03244v1</guid><title>Self-Supervised Learning from Noisy and Incomplete Data</title><link>http://arxiv.org/abs/2601.03244v1</link><author>Julián Tachella, Mike Davies</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文综述了自监督学习在逆问题中的应用，重点讨论其理论基础和成像领域的实践。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 逆问题需要从噪声或不完整观测中恢复信号，传统方法依赖手工正则化，数据驱动方法需要真实标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提供自监督方法的全面总结，强调理论和成像应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 回顾了多种仅使用测量数据学习求解器的自监督技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 自监督学习能够在缺乏真实标签的情况下有效解决逆问题，并在成像任务中表现良好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自监督方法为逆问题提供了有前景的解决方案，尤其适用于成像领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 许多科学和工程中的重要问题涉及从噪声和/或不完整的观测中推断信号，其中观测过程是已知的。历史上，这个问题通常通过手工设计的正则化（例如稀疏性、全变差）来获得有意义的估计。最近的数据驱动方法往往通过直接从真实信号及其观测的示例中学习求解器来提供更好的解决方案。然而，在许多实际应用中，获取用于训练的真实参考数据既昂贵又不可能。自监督学习方法提供了一种有前景的替代方案，它仅使用测量数据来学习求解器，避免了对真实参考的需求。本文提供了对逆问题中不同自监督方法的全面综述，特别强调其理论基础，并展示了在成像逆问题中的实际应用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Many important problems in science and engineering involve inferring a signal from noisy and/or incomplete observations, where the observation process is known. Historically, this problem has been tackled using hand-crafted regularization (e.g., sparsity, total-variation) to obtain meaningful estimates. Recent data-driven methods often offer better solutions by directly learning a solver from examples of ground-truth signals and associated observations. However, in many real-world applications, obtaining ground-truth references for training is expensive or impossible. Self-supervised learning methods offer a promising alternative by learning a solver from measurement data alone, bypassing the need for ground-truth references. This manuscript provides a comprehensive summary of different self-supervised methods for inverse problems, with a special emphasis on their theoretical underpinnings, and presents practical applications in imaging inverse problems.&lt;/p&gt;</description></item><item><guid>2601.03248v1</guid><title>STReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning</title><link>http://arxiv.org/abs/2601.03248v1</link><author>Juntong Ni, Shiyu Wang, Ming Jin, Qi He, Wei Jin</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文关注时间序列中的时空推理，提出了 ST-Bench 基准和 STReasoner 方法，并通过 S-GRPO 强化学习提升空间信息利用，实验表明显著提升准确率且成本低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 时空推理在交通、能源和疾病传播等高风险决策系统中至关重要，但现有研究多侧重预测准确性，缺乏对推理的关注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决时空推理研究空白，构建四项核心任务的基准，并开发能够整合时间序列、图结构和文本的推理模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过网络 SDE 多智能体数据合成管道构建 ST-Bench；提出 STReasoner 让大语言模型结合时间序列、图结构和文本进行推理；引入 S-GRPO 强化学习算法，专门奖励利用空间信息的性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; STReasoner 在四项任务上平均提升 17%–135% 的准确率，仅为专有模型成本的 0.004 倍，并能稳健泛化到真实数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 时空推理方法 STReasoner 与 S-GRPO 能显著提高推理准确率并降低成本，为高风险决策系统提供了有效工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时间序列中的时空推理涉及对时间动态、空间依赖和文本上下文的显式综合。这一能力对于交通网络、电力网和疾病传播等高风险决策系统至关重要。然而，该领域仍未成熟，因为大多数现有工作更关注预测准确性而非推理。为填补这一空白，我们提出了 ST-Bench 基准，它由四个核心任务组成，包括病因推理、实体识别、相关性推理和上下文预测，并通过基于网络 SDE 的多智能体数据合成管道开发。随后，我们提出了 STReasoner，使大型语言模型能够整合时间序列、图结构和文本进行显式推理。为促进基于空间的逻辑，我们引入了 S-GRPO，一种强化学习算法，专门奖励那些因利用空间信息而产生的性能提升。实验表明，STReasoner 在仅为专有模型成本的 0.004 倍时，在平均 17% 至 135% 的准确率提升，并能稳健泛化到真实世界数据。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address the gap, we introduce ST-Bench, a benchmark consisting of four core tasks, including etiological reasoning, entity identification, correlation reasoning, and in-context forecasting, developed via a network SDE-based multi-agent data synthesis pipeline. We then propose STReasoner, which empowers LLM to integrate time series, graph structure, and text for explicit reasoning. To promote spatially grounded logic, we introduce S-GRPO, a reinforcement learning algorithm that rewards performance gains specifically attributable to spatial information. Experiments show that STReasoner achieves average accuracy gains between 17% and 135% at only 0.004X the cost of proprietary models and generalizes robustly to real-world data.&lt;/p&gt;</description></item><item><guid>2601.03252v1</guid><title>InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</title><link>http://arxiv.org/abs/2601.03252v1</link><author>Hao Yu, Haotong Lin, Jiawei Wang, Jiaxin Li, Yida Wang, Xueyang Zhang, Yue Wang, Xiaowei Zhou, Ruizhen Hu, Sida Peng</author><pubDate>Wed, 07 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出InfiniDepth，通过将深度表示为神经隐式场，并使用局部隐式解码器实现连续坐标查询，从而实现任意分辨率和细粒度的深度估计；在新构建的4K合成基准以及真实世界数据上，InfiniDepth在相对和度量深度估计任务中表现出色，尤其在细节区域，并且在大视角变化下的视角合成任务中产生更少的空洞和伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有深度估计方法受限于离散图像网格，无法支持任意输出分辨率，且难以恢复几何细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服离散网格限制，提供可扩展、细粒度的深度估计方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出InfiniDepth，将深度建模为神经隐式场；使用简单的局部隐式解码器在连续二维坐标上查询深度；构建包含五款游戏的4K合成基准进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; InfiniDepth在合成与真实世界基准上实现了最先进的相对与度量深度估计性能，尤其在细节区域表现突出；在大视角变化下的视角合成任务中生成更高质量、缺陷更少的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; InfiniDepth通过神经隐式场实现了可扩展且细粒度的深度估计，并显著提升了视角合成质量，证明了其在多种深度估计任务中的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有深度估计方法本质上受限于在离散图像网格上预测深度。这种表示方式限制了它们对任意输出分辨率的可扩展性，并阻碍了几何细节的恢复。本文提出InfiniDepth，将深度表示为神经隐式场。通过一个简单而有效的局部隐式解码器，我们可以在连续二维坐标上查询深度，从而实现任意分辨率和细粒度的深度估计。为了更好地评估我们方法的能力，我们从五个不同的游戏中策划了一个高质量的4K合成基准，涵盖了具有丰富几何和外观细节的多样场景。大量实验表明，InfiniDepth在合成和真实世界基准上的相对和度量深度估计任务中均实现了最先进的性能，尤其在细节区域表现突出。它还在大视角变化下的视角合成任务中受益，产生高质量结果，缺陷和空洞更少。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method&amp;#x27;s capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.&lt;/p&gt;</description></item><item><guid>2512.10386v2</guid><title>Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</title><link>http://arxiv.org/abs/2512.10386v2</link><author>Ge Zhang, Chunyang Wang, Bin Liu, Guan Xi</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自适应双权重重力模型点云去噪方法，结合八叉树分区、体素占据统计和k最近邻密度估计，先快速剔除明显噪声，再用密度与距离加权的重力评分细致区分噪声与物体点。实验表明该方法在多种噪声条件下相较现有方法在F1、PSNR和Chamfer距离上均有提升，并且单帧处理时间更短，证明了其高精度、鲁棒性和实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高质量点云是自动驾驶和三维重建等任务的基础，但基于激光雷达的点云采集常受多种干扰，导致大量噪声点，影响后续目标检测和识别的准确性。现有去噪方法往往在追求精度时牺牲计算效率，或在提升速度时失去边界和细节保留，难以同时实现高精度、强边缘保留和实时性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有点云去噪方法在精度、边缘保留和实时性之间的权衡不足，提出一种兼顾三者的自适应双权重重力模型去噪方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先使用八叉树对全局点云进行空间分区，实现并行加速；在每个叶节点内采用自适应体素占据统计和k最近邻密度估计快速剔除明显孤立且低密度的噪声点，缩小候选集；随后构造结合密度权重与自适应距离权重的重力评分函数，对剩余点进行细粒度噪声判别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在斯坦福3D扫描库、加拿大不良驾驶条件数据集以及实验室自制RUBY PLUS激光雷达点云上实验显示，该方法在各种噪声条件下相较现有方法在F1、PSNR和Chamfer距离上均有一致提升，同时单帧处理时间更短，验证了其高精度、鲁棒性和实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应双权重重力模型点云去噪方法能够在保持高去噪精度和边缘细节的同时实现实时处理，适用于多噪声场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dualweight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house RUBY PLUS LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dualweight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house RUBY PLUS LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.&lt;/p&gt;</description></item><item><guid>2512.20578v1</guid><title>Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits</title><link>http://arxiv.org/abs/2512.20578v2</link><author>Amirhosein Ghasemabadi, Di Niu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Gnosis 是一种轻量级自我意识机制，允许冻结的 LLM 在推理时通过解码隐藏状态和注意力模式来预测自身错误。它被动观察内部轨迹，将其压缩为固定预算的描述符，并以几乎无额外推理成本预测正确性，仅增加约 5M 参数，且与序列长度无关。实验表明，Gnosis 在数学推理、开放域问答和学术知识基准上，且在 1.7B–20B 参数的冻结模型上，始终优于强内部基线和大型外部评判器，且在准确性和校准度上均表现更佳。它还能零样本泛化到部分生成，支持早期错误检测和计算感知控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; LLM 能生成流畅且复杂的输出，但常常无法识别自身错误和幻觉。现有方法多依赖外部评判器、多样本一致性或基于文本的自我批评，导致额外计算开销或与真实正确性相关性弱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究 LLM 是否能通过检查推理过程中的内部状态来预测自身失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 Gnosis，利用冻结 LLM 的内部状态和注意力模式，压缩为固定预算的描述符，随后进行内在自我验证，预测生成结果的正确性。该机制几乎不增加推理成本，仅增 5M 参数，且与序列长度无关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Gnosis 在数学推理、开放域问答和学术知识基准上，且在 1.7B–20B 参数的冻结模型上，始终优于强内部基线和大型外部评判器，准确性和校准度均更好；它还能零样本泛化到部分生成，实现早期错误检测和计算感知控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 生成过程本身蕴含可靠的正确性线索，可在无需外部监督的情况下高效提取，从而实现更可靠的自我验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型（LLM）能够生成流畅且复杂的输出，但往往无法识别自身错误和幻觉。现有方法通常依赖外部评判器、多样本一致性或基于文本的自我批评，这些方法会增加额外计算成本或与真实正确性相关性弱。我们提出了 Gnosis，一种轻量级自我意识机制，使冻结的 LLM 能通过解码隐藏状态和注意力模式来进行内在自我验证。Gnosis 被动观察内部轨迹，将其压缩为固定预算的描述符，并以几乎无额外推理成本预测正确性，仅增加约 5M 参数，且与序列长度无关。在数学推理、开放域问答和学术知识基准上，且在 1.7B–20B 参数的冻结模型上，Gnosis 一直优于强内部基线和大型外部评判器，在准确性和校准度上均表现更佳。它还能零样本泛化到部分生成，支持早期错误检测和计算感知控制。这些结果表明，生成过程本身蕴含可靠的正确性线索，可在无需外部监督的情况下高效提取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.&lt;/p&gt;</description></item><item><guid>2512.21472v1</guid><title>IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset</title><link>http://arxiv.org/abs/2512.21472v1</link><author>Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一个新的多注释者皮肤病变分割数据集ISIC MultiAnnot++，该数据集包含近两万份分割掩码，覆盖近1.5万张皮肤镜图像，其中约2,394张图像拥有2-5份分割结果。数据集还提供了注释者的技能水平和使用工具等元数据，可用于研究注释者偏好建模和元数据分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多注释者医学图像分割是重要研究课题，但需要昂贵的标注数据。皮肤镜图像能让专家和AI观察到常规临床照片无法辨别的形态结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建并公开一个大规模、多注释者的皮肤病变分割数据集，以填补目前缺乏此类公开数据的空白。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 从ISIC Archive收集皮肤镜图像，生成17,684份分割掩码，覆盖14,967张图像，其中2,394张图像有2-5份分割。为每份分割记录注释者的技能水平和使用工具等元数据，并对数据集特征、划分和一致性分割进行分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ISIC MultiAnnot++是目前最大的公开多注释者皮肤病变分割数据集，提供丰富的元数据，支持注释者偏好建模和元数据分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该数据集为多注释者分割研究提供了宝贵资源，可推动注释者特定建模、共识分割等方向的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多注释者医学图像分割是一个重要的研究问题，但需要昂贵的标注数据集。皮肤镜皮肤病变成像使人类专家和AI系统能够观察到常规临床照片无法辨别的形态结构。然而，目前没有公开的大规模多注释者皮肤病变分割（SLS）数据集，包含皮肤镜图像的注释者标签。我们引入ISIC MultiAnnot++，这是一个来自ISIC Archive的公开多注释者皮肤病变分割数据集。最终数据集包含17,684份分割掩码，覆盖14,967张皮肤镜图像，其中2,394张图像每张有2-5份分割，使其成为最大的公开SLS数据集。此外，数据集还包含关于分割的元数据，包括注释者的技能水平和分割工具，支持关于注释者特定偏好建模和注释者元数据分析等主题的研究。我们提供了对该数据集特征的分析、策划的数据划分以及共识分割掩码。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators&amp;#x27; skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.&lt;/p&gt;</description></item><item><guid>2512.23035v1</guid><title>Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion</title><link>http://arxiv.org/abs/2512.23035v1</link><author>Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为Co2S的半监督遥感图像语义分割框架，通过融合视觉-语言模型和自监督模型的先验知识，解决伪标签漂移问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 半监督遥感图像分割需要大量标注，伪标签漂移导致错误累积，影响模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个稳定的半监督分割框架，降低伪标签漂移，提高分割精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用异构双学生架构，分别使用预训练的CLIP和DINOv3 ViT模型；引入显式-隐式语义共引导机制，利用文本嵌入和可学习查询提供类别级指导；设计全局-局部特征协同融合策略，将CLIP的全局上下文与DINOv3的局部细节结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在六个流行数据集上实验表明，Co2S在多种划分协议和场景下均表现出领先性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Co2S通过融合多源先验知识，有效缓解伪标签漂移，显著提升遥感图像语义分割效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 半监督遥感图像语义分割提供了一种减轻繁重标注负担的有前景的解决方案，但它在根本上面临伪标签漂移的问题，即确认偏差导致训练过程中错误累积。在本工作中，我们提出了Co2S，一种稳定的半监督遥感分割框架，协同融合视觉-语言模型和自监督模型的先验知识。具体而言，我们构建了一个异构双学生架构，包含两个不同的基于ViT的视觉基础模型，分别以预训练的CLIP和DINOv3为初始化，以减轻错误累积和伪标签漂移。为有效整合这些不同的先验知识，引入了显式-隐式语义共引导机制，利用文本嵌入和可学习查询分别提供显式和隐式的类别级指导，从而共同提升语义一致性。此外，开发了全局-局部特征协同融合策略，有效融合CLIP捕获的全局上下文信息与DINOv3产生的局部细节，使模型能够生成高度精确的分割结果。对六个流行数据集的广泛实验表明，所提出的方法在各种划分协议和多样场景下始终表现出领先性能。项目页面可在 https://xavierjiezou.github.io/Co2S/ 查看。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.&lt;/p&gt;</description></item><item><guid>2512.23413v2</guid><title>Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment</title><link>http://arxiv.org/abs/2512.23413v2</link><author>Henglin Liu, Nisha Huang, Chang Liu, Jiangpeng Yan, Huijuan Huang, Jixuan Ying, Tong-Yee Lee, Pengfei Wan, Xiangyang Ji</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一个大规模多维度结构化数据集和一个新的美学评估框架，解决了数据稀缺和模型碎片化问题，并在多个数据集上实现了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 美学质量评估是构建面向人类的 AIGC 量化评价系统的关键，但其涉及视觉感知、认知和情感，数据不足且模型分散。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建 RAD 数据集和 ArtQuant 框架，克服数据稀缺与模型碎片化的挑战，提高艺术图像美学评估的准确性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 采用迭代生成流程构建 RAD 数据集，包含约七万条多维度描述；2) 设计 ArtQuant 框架，将独立美学维度通过联合描述生成耦合，并利用大型语言模型解码器处理长文本语义；3) 通过理论分析证明数据与模型协同降低预测熵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上，所提方法实现了最先进的评估效果，仅需传统训练周期的 33%，显著缩小了艺术图像与美学判断之间的认知差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RAD 数据集与 ArtQuant 框架有效解决了美学评估中的数据与模型瓶颈，提供了可扩展且高效的解决方案，未来将促进相关研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 美学质量评估任务对于构建面向人类的 AIGC 定量评价系统至关重要。然而，其固有的复杂性涵盖视觉感知、认知和情感，带来了根本性的挑战。虽然美学描述能够代表这种复杂性，但仍存在两个关键难题：（1）数据稀缺与不平衡：现有数据集过度关注视觉感知，因昂贵的人工标注而忽视更深层维度；（2）模型碎片化：当前视觉网络通过多分支编码器孤立美学属性，而以对比学习为代表的多模态方法难以有效处理长文本描述。为解决（1）问题，我们首先提出了 Refined Aesthetic Description（RAD）数据集，这是一个大规模（约七万条）多维度结构化数据集，采用迭代流程生成，既不需要繁重的标注成本，又易于扩展。为解决（2）问题，我们提出了 ArtQuant，一种面向艺术图像的美学评估框架，它不仅通过联合描述生成耦合孤立的美学维度，还借助大型语言模型解码器更好地建模长文本语义。此外，理论分析证实了这种协同：RAD 的语义充足性（数据）与生成范式（模型）共同最小化预测熵，为框架提供了数学基础。我们的方案在多个数据集上实现了最先进的性能，仅需传统训练周期的 33%，显著缩小了艺术图像与美学判断之间的认知差距。我们将发布代码和数据集，以支持未来研究。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD&amp;#x27;s semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.&lt;/p&gt;</description></item><item><guid>2512.23489v2</guid><title>The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction</title><link>http://arxiv.org/abs/2512.23489v2</link><author>Haoyu Pei, Zhongyang Liu, Xiangyi Xiao, Xiaocong Du, Suting Hong, Kunpeng Zhang, Haipeng Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为MIRAGE-VC的多视角检索增强生成框架，用于预测风险投资项目的成功率。该框架通过迭代选择高价值的图路径并使用多代理结构融合不同证据，显著提升了预测准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 风险投资投资失败率高，传统机器学习和图神经网络缺乏显式推理能力，难以整合公司披露、投资者记录和网络结构等复杂关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在图外预测任务中，如何选择最优路径并实现逐步推理，以提高对创业公司成功率的预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1）信息增益驱动的路径检索器，迭代挑选高价值邻居，将网络压缩为可管理的链条；2）多代理架构，整合三条证据流，并通过可学习的门控机制根据公司属性进行加权。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在严格防泄漏控制下，MIRAGE-VC相较基线提升了5.0%的F1分数和16.6%的PrecisionAt5，并为推荐和风险评估等图外预测任务提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过显式路径选择和多证据融合，MIRAGE-VC在风险投资成功预测上取得显著进展，为未来图外预测研究提供了可行的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种名为MIRAGE-VC的多视角检索增强生成框架，用于预测风险投资项目的成功率。该框架通过迭代选择高价值的图路径并使用多代理结构融合不同证据，显著提升了预测准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.&lt;/p&gt;</description></item><item><guid>2512.23649v3</guid><title>RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</title><link>http://arxiv.org/abs/2512.23649v3</link><author>Zhe Li, Cheng Chi, Boan Zhu, Yangyang Wei, Shuanghao Bai, Yuheng Ji, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, S. -H. Gary Chan, Chang Xu, Shanghang Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了RoboMirror框架，利用视觉语言模型将原始第一/第三人称视频转换为运动意图，直接驱动扩散策略生成符合物理和语义的步态，无需姿态重定向或显式重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类通过视觉观察学习运动，但现有机器人步态系统多依赖人工标注的动作捕捉轨迹或稀疏文本指令，导致视觉理解与控制之间存在显著鸿沟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥合视觉理解与动作控制之间的差距，构建一个“先理解后模仿”的无重定向视频到步态的系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用视觉语言模型提取视频中的视觉运动意图，作为条件输入给扩散式策略，直接生成物理可行且语义一致的步态，无需显式姿态重建或重定向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，RoboMirror支持通过第一人称视频实现远程存在感，显著降低第三人称控制延迟80%，并比基线模型高出3.7%的任务成功率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过围绕视频理解重新定义机器人控制，RoboMirror成功填补了视觉理解与动作执行之间的空白。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类通过视觉观察学习运动，先解释视觉内容再模仿动作。然而，最先进的人形机器人步态系统依赖于精心策划的动作捕捉轨迹或稀疏文本命令，导致视觉理解与控制之间存在关键缺口。文本到动作的方法存在语义稀疏和分阶段管道错误，而基于视频的方法仅执行机械姿态模仿，缺乏真正的视觉理解。我们提出RoboMirror，这是首个无重定向的视频到步态框架，体现了“先理解后模仿”的理念。利用视觉语言模型，它将原始第一/第三人称视频提炼为视觉运动意图，直接作为扩散策略的条件，生成物理可行、语义对齐的步态，无需显式姿态重建或重定向。大量实验验证了RoboMirror的有效性，它通过第一人称视频实现远程存在感，显著降低第三人称控制延迟80%，并比基线模型高出3.7%的任务成功率。通过将人形控制围绕视频理解重新构架，我们弥合了视觉理解与动作之间的差距。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying &amp;quot;understand before you imitate&amp;quot;. Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.&lt;/p&gt;</description></item><item><guid>2512.23914v2</guid><title>Hardware Acceleration for Neural Networks: A Comprehensive Survey</title><link>http://arxiv.org/abs/2512.23914v2</link><author>Bin Xu, Ayan Banerjee, Sandeep Gupta</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本综述系统回顾了深度学习硬件加速技术，指出内存移动、通信和不规则算子已成为主要瓶颈，并通过统一的分类法组织了工作负载、执行环境和优化杠杆，综合了同步阵列、向量引擎、注意力核、量化感知路径和高带宽内存等关键架构思想，讨论了软件栈与编译器的桥接作用，最后提出了长上下文LLM推理、动态稀疏工作负载支持、能量与安全感知部署以及公平基准测试等开放挑战与未来方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 神经网络已成为云端和边缘平台的主导计算工作负载，模型规模和部署多样性的快速增长暴露了内存移动、通信和不规则算子主导的硬件瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 综述并组织深度学习硬件加速器的技术景观，识别关键架构概念，并突出未来研究的开放挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 按工作负载、执行环境和优化杠杆进行统一分类，综合同步阵列、向量/ SIMD 引擎、专用注意力与 softmax 核、量化感知数据通路和高带宽内存等架构思想，讨论软件栈与编译器如何将模型语义映射到硬件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 内存移动、通信和不规则算子是主要瓶颈；关键架构包括同步阵列、向量/ SIMD 引擎、注意力核、量化感知路径和高带宽内存；软件编译器在桥接模型与硬件方面起关键作用；开放挑战包括长上下文 LLM 推理、动态稀疏工作负载支持、能量与安全感知部署以及公平基准测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 下一代神经加速器需解决内存与通信瓶颈，支持动态稀疏工作负载，并实现能量与安全感知部署，未来方向已明确。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 神经网络已成为云端和边缘平台的主导计算工作负载，但模型规模和部署多样性的快速增长暴露了越来越多由内存移动、通信和不规则算子主导的硬件瓶颈，而非峰值算术吞吐量。本综述回顾了深度学习硬件加速技术的全景，涵盖GPU和张量核心架构、专用加速器（如TPU/ NPU）、基于FPGA的设计、ASIC推理引擎以及新兴的大语言模型服务加速器（如LPU），并涉及内存内/近内存计算和神经形态/模拟方法。我们使用统一的分类法组织空间，涵盖（i）工作负载（CNN、RNN、GNN、Transformer/LLM）、（ii）执行环境（训练与推理；数据中心与边缘）和（iii）优化杠杆（低精度、稀疏与剪枝、算子融合、编译与调度、内存系统/互连设计）。我们综合了关键架构思想，包括同步阵列、向量和SIMD引擎、专用注意力和softmax核、量化感知数据通路以及高带宽内存，并讨论了软件栈和编译器如何将模型语义映射到硬件。最后，我们强调了开放挑战——高效长上下文LLM推理（KV缓存管理）、对动态和稀疏工作负载的鲁棒支持、能量与安全感知部署以及公平基准测试——并指出了下一代神经加速的有前景方向。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Neural networks have become a dominant computational workload across cloud and edge platforms, but rapid growth in model size and deployment diversity has exposed hardware bottlenecks increasingly dominated by memory movement, communication, and irregular operators rather than peak arithmetic throughput. This survey reviews the technology landscape for hardware acceleration of deep learning, spanning GPUs and tensor-core architectures; domain-specific accelerators (e.g., TPUs/NPUs); FPGA-based designs; ASIC inference engines; and emerging LLM-serving accelerators such as LPUs (language processing units), alongside in-/near-memory computing and neuromorphic/analog approaches. We organize the space using a unified taxonomy across (i) workloads (CNNs, RNNs, GNNs, and Transformers/LLMs), (ii) execution settings (training vs.\ inference; datacenter vs.\ edge), and (iii) optimization levers (reduced precision, sparsity and pruning, operator fusion, compilation and scheduling, and memory-system/interconnect design). We synthesize key architectural ideas including systolic arrays, vector and SIMD engines, specialized attention and softmax kernels, quantization-aware datapaths, and high-bandwidth memory, and we discuss how software stacks and compilers bridge model semantics to hardware. Finally, we highlight open challenges -- including efficient long-context LLM inference (KV-cache management), robust support for dynamic and sparse workloads, energy- and security-aware deployment, and fair benchmarking -- and point to promising directions for the next generation of neural acceleration.&lt;/p&gt;</description></item><item><guid>2512.24052v2</guid><title>AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives</title><link>http://arxiv.org/abs/2512.24052v2</link><author>Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究大型音频-语言模型在生成文本时出现的幻觉问题，提出了四类错误类型，并通过AHA框架和AHA-Eval基准进行对抗性训练和评估，最终在多项公开基准上取得显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型音频-语言模型已达到最先进水平，但常因生成与音频不匹配的文本而产生幻觉。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 分析幻觉的具体表现，构建针对性的数据集和评测基准，以提升模型的时间推理和对音频证据的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用反事实硬负样本挖掘构建高质量偏好数据集，训练模型区分真实音频证据与语言可行的虚构；同时创建AHA-Eval评测集检验细粒度时间推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在AHA-Eval上提升13.7%，在MMAU-Test提升1.3%，在MMAR提升1.6%，并在公开基准上超越最新SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AHA框架和对应数据集能有效减少音频-语言模型的幻觉，并提升其在时间推理和公开基准上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管大型音频-语言模型（LALMs）已实现最先进的性能，但它们经常出现幻觉现象，例如生成与音频输入不相符的文本。我们对这些定位失败进行了分析，并提出了四类错误的分类：事件省略、错误事件身份、时间关系错误和定量时间错误。为了解决这一问题，我们引入了 AHA（Audio Hallucination Alignment）框架。通过利用反事实硬负样本挖掘，我们的流程构建了一个高质量的偏好数据集，迫使模型区分严格的声学证据与语言上可行的虚构。此外，我们建立了 AHA-Eval，一个旨在严格测试这些细粒度时间推理能力的诊断基准。我们将该数据用于对齐 Qwen2.5-Omni。得到的模型 Qwen-Audio-AHA 在 AHA-Eval 上实现了 13.7% 的提升。更重要的是，这一优势在我们的诊断集之外也得到了验证。我们的模型在公开基准上也取得了显著收益，包括在 MMAU-Test 上提升 1.3%，在 MMAR 上提升 1.6%，并超过了最新的最先进方法。模型和数据集已在 https://github.com/LLM-VLM-GSL/AHA 上开源。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although Large Audio-Language Models (LALMs) deliver state-of-the-art (SOTA) performance, they frequently suffer from hallucinations, e.g. generating text not grounded in the audio input. We analyze these grounding failures and identify a distinct taxonomy: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. To address this, we introduce the AHA (Audio Hallucination Alignment) framework. By leveraging counterfactual hard negative mining, our pipeline constructs a high-quality preference dataset that forces models to distinguish strict acoustic evidence from linguistically plausible fabrications. Additionally, we establish AHA-Eval, a diagnostic benchmark designed to rigorously test these fine-grained temporal reasoning capabilities. We apply this data to align Qwen2.5-Omni. The resulting model, Qwen-Audio-AHA, achieves a 13.7% improvement on AHA-Eval. Crucially, this benefit generalizes beyond our diagnostic set. Our model shows substantial gains on public benchmarks, including 1.3% on MMAU-Test and 1.6% on MMAR, outperforming latest SOTA methods. The model and dataset are open-sourced at https://github.com/LLM-VLM-GSL/AHA.&lt;/p&gt;</description></item><item><guid>2512.24470v2</guid><title>Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</title><link>http://arxiv.org/abs/2512.24470v2</link><author>Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavone, Martin Steinert</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于视觉-语言模型的回退机动选择器，旨在帮助自主海事船舶满足IMO MASS代码的安全与人机交互要求。通过快速-慢速异常管道与短期可被人类覆盖的机动，该方法在港口场景中实现了语义感知与安全距离提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; IMO MASS代码要求自主船舶在偏离操作设计域时能检测、进入回退模式、通知操作员、允许人类立即覆盖，并避免未经批准的航行计划更改。传统自主系统在需要语义理解的决策（如潜水员下水旗、附近火灾）上表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证视觉-语言模型能为分布外情况提供语义意识，并证明快速-慢速异常管道与短期可覆盖回退机动能在实际延迟预算内满足IMO MASS代码的要求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出Semantic Lookout——一种仅使用摄像头、受候选约束的VLM回退机动选择器，在持续人类授权下从水域有效轨迹中挑选谨慎动作或停泊。对40个港口场景进行评估，测量场景理解、延迟、与人类共识对齐、火灾场景风险缓解以及端到端警报→回退→接管流程，并与几何基线进行对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 10秒以下模型保留了大部分慢速最先进模型的语义意识；回退机动选择器优于几何基线，在火灾场景中显著增加安全距离；现场运行验证了端到端操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 视觉-语言模型可作为符合IMO MASS代码的语义回退机动选择器，满足实际延迟需求，并为未来将基础模型语义与多传感器感知和短期重规划相结合的混合自主系统提供方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 草案IMO MASS代码要求自主和远程监督的海事船舶检测其操作设计域的偏离，进入预定义的回退模式并通知操作员，允许立即的人类覆盖，并避免在未获批准的情况下更改航行计划。为满足警报到接管间隙的这些义务，需要一种短期、可被人类覆盖的回退机动。传统的海事自主堆栈在正确行动取决于意义时（例如，潜水员下水旗表示水中有人，附近有火表示危险）会遇到困难。我们认为（i）视觉-语言模型（VLM）为此类分布外情况提供语义意识；（ii）快速-慢速异常管道与短期、可被人类覆盖的回退机动相结合，使其在交接窗口内可行。我们提出Semantic Lookout，一种仅使用摄像头、受候选约束的VLM回退机动选择器，在持续的人类授权下从水域有效、世界锚定的轨迹中选择一个谨慎动作（或停泊）。在40个港口场景中，我们测量了每次调用的场景理解和延迟、与人类共识的对齐（模型三人投票多数）、火灾场景的短期风险缓解以及水上警报→回退机动→操作员接管。10秒以下的模型保留了大部分慢速最先进模型的意识。回退机动选择器优于仅基于几何的基线，并在火灾场景中增加了安全距离。现场运行验证了端到端操作。这些结果支持VLM作为与草案IMO MASS代码兼容的语义回退机动选择器，在实际延迟预算内，并激励未来在域适应、混合自主系统中将基础模型语义与多传感器鸟瞰感知和短期重规划相结合。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained VLM fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert-&amp;gt;fallback maneuver-&amp;gt;operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird&amp;#x27;s-eye-view perception and short-horizon replanning. Website: kimachristensen.github.io/bridge_policy&lt;/p&gt;</description></item><item><guid>2512.24556v2</guid><title>Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs</title><link>http://arxiv.org/abs/2512.24556v2</link><author>Muhammad Abdullahi Said, Muhammad Sammani Sani</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究系统评估了三款先进语言模型在西非威胁情境下的安全性，发现语言与时间框架的交互会导致安全性出现复杂的干扰，而非简单的低资源劣势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大型语言模型被广泛应用于关键基础设施，假设其安全性能零样本迁移至非英语语言被认为是一个危险的盲点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证语言迁移假设的可靠性，并探究语言与时间框架对模型安全性的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用名为HausaSafety的对抗性数据集，对GPT‑5.1、Gemini 3 Pro和Claude 4.5 Opus进行1,440次评估，采用2×4因子设计，考察英语与豪萨语以及过去时与未来时的交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在豪萨语下表现出逆向语言脆弱性，Claude 4.5 Opus在豪萨语下更安全；但在时间推理上出现严重失效，过去时情境安全性低，未来时情境安全性高；安全性在不同配置间差距显著，表明安全性是情境依赖的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 当前模型依赖表面启发式，缺乏深层语义理解，导致在全球南方用户中形成安全漏洞；需要引入不变对齐的范式以实现跨语言和时间的安全稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着大型语言模型被整合到关键全球基础设施中，假设安全对齐能从英语零样本迁移到其他语言仍是一个危险的盲点。本研究使用基于西非威胁情境的对抗性数据集HausaSafety，对GPT‑5.1、Gemini 3 Pro和Claude 4.5 Opus进行系统审计。通过2×4因子设计共1,440次评估，检验语言（英语与豪萨语）与时间框架之间的非线性交互。结果挑战了多语言安全差距的叙事，发现安全性由变量交集决定，呈现复杂干扰机制。模型在豪萨语下表现出逆向语言脆弱性，Claude 4.5 Opus在豪萨语下更安全，但在时间推理上出现灾难性失败。报告了显著的时间不对称：过去时情境安全性低，未来时情境安全性高。安全性在不同配置间差距巨大，表明安全性不是固定属性，而是情境依赖的状态。结论是当前模型依赖表面启发式而非稳健语义理解，形成安全空洞，使全球南方用户面临本地化危害。提出不变对齐作为确保跨语言和时间安全稳定的必要范式转变。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the narrative of the multilingual safety gap. Instead of a simple degradation in low-resource settings, we identified a complex interference mechanism in which safety is determined by the intersection of variables. Although the models exhibited a reverse linguistic vulnerability with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal, they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.&lt;/p&gt;</description></item><item><guid>2512.25061v2</guid><title>Melting curve of correlated iron at Earth's core conditions from machine-learned DFT+DMFT</title><link>http://arxiv.org/abs/2512.25061v2</link><author>Rishi Rao, Li Zhu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文开发了一种机器学习加速器，利用E(3)-等变图神经网络预测局部自能和费米能级，从而显著减少DFT+DMFT的迭代次数，并应用该方法在地球核心压力下计算铁的相关能量和力，训练神经网络原子势，最终通过两相共存模拟得到铁在330 GPa下的熔点温度为6225 K。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确的有限温度电子相关性是确定铁在地球内核边界熔点曲线的关键，但传统的DFT+DMFT计算成本过高，难以进行大规模热力学采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过机器学习方法加速DFT+DMFT计算，使其能够在大规模采样中使用，从而更可靠地预测铁的熔点曲线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 训练E(3)-等变图神经网络预测局部自能和费米能级，提供DFT+DMFT自洽循环的高效预热启动；使用Fe、FeO、NiO的高通量数据减少迭代次数；在核心压力下生成铁的相关能量和力；训练神经网络原子势；利用两相共存模拟确定熔点曲线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通过机器学习加速器，DFT+DMFT迭代次数减少了2-4倍；在330 GPa下预测的铁熔点温度为6225 K。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 机器学习加速器显著提高了DFT+DMFT的计算效率，使得在高压条件下能够准确预测铁的熔点曲线，为地球内部研究提供了可靠的数据支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可靠的铁在地球内核边界的熔点曲线需要准确的有限温度电子相关性，但DFT+DMFT计算仍然过于昂贵，无法进行大规模热力学采样。本文通过训练E(3)-等变图神经网络，预测局部自能和费米能级，从而为自洽的DFT+DMFT提供高效的预热启动，开发了一种机器学习加速器。利用Fe、FeO和NiO的高通量数据，我们将DMFT迭代次数减少了2-4倍。借助这一改进，我们在核心压力下生成了铁的相关能量和力，训练了神经网络原子势，并通过两相共存模拟确定熔点曲线。预测的熔点温度为330 GPa下的6225 K。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable constraints on iron&amp;#x27;s melting curve at Earth&amp;#x27;s inner-core boundary require accurate finite-temperature electronic correlations, yet DFT+DMFT calculations remain too costly for large-scale thermodynamic sampling. Here, we develop a machine-learning accelerator for charge self-consistent DFT+DMFT by training E(3)-equivariant graph neural networks to predict the local self-energy and Fermi level from atomic environments, providing an efficient warm start to the DMFT self-consistency loop. Using high-throughput data for Fe, FeO, and NiO, we obtain a 2-4 times reuduction in DMFT iterations. Leveraging this improvement, we generate correlated energies and forces for Fe at core pressures, train a neural-network interatomic potential, and determine the melting curve via two-phase coexistence simulations. We obtain a predicted melting temperature of 6225 K at 330 GPa.&lt;/p&gt;</description></item><item><guid>2601.00787v1</guid><title>Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries</title><link>http://arxiv.org/abs/2601.00787v1</link><author>Jonathan Simkin, Lovedeep Gondara, Zeeshan Rizvi, Gregory Doyle, Jeff Dowden, Dan Bond, Desmond Martin, Raymond Ng</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文评估了两种Transformer模型在加拿大不同省份癌症病理报告中的适用性，并通过模型集成显著提高了癌症检测的召回率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人口基准癌症登记依赖病理报告，但人工提取耗时且导致数据延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证在不同省份间迁移预训练Transformer模型的可行性，并探索集成方法提升检测敏感度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用新斯科舍和拉布拉多省的约104,000份和22,000份病理报告进行微调，采用同步和诊断重点输入管道，并构建保守的OR集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 微调后的模型在测试集上保持高性能；集成模型召回率接近99%，将漏检癌症从48/54降至24；在报告可报类任务中，漏检从54/46降至33。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 跨省迁移的Transformer模型可通过少量微调实现本地化，集成方法显著减少漏检，且仅共享模型权重的隐私保护流程为全国统一模型奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人口基准癌症登记依赖病理报告作为主要诊断来源，但人工抽取耗费资源并导致癌症数据延迟。虽然基于Transformer的自然语言处理系统已改善登记工作流程，但其在不同报告规范的司法辖区之间的泛化能力尚不清楚。本文首次对英国哥伦比亚癌症登记处开发的领域适应Transformer模型BCCRTron与生物医学Transformer模型GatorTron在加拿大的跨省评估进行了研究。我们的训练数据集包含约104,000份和22,000份来自纽芬兰与拉布拉多癌症登记处的匿名病理报告，分别用于第1层（癌症与非癌症）和第2层（可报与不可报）任务。两种模型均使用同步和诊断重点报告段落的输入管道进行微调。在纽芬兰与拉布拉多的测试集上，微调后的模型保持了高性能，证明在一个司法辖区预训练的Transformer可以通过适度微调本地化。为提高敏感度，我们将两种模型结合使用保守的OR集成，获得第1层召回率接近99%，将漏检癌症从48和54例降至24例。对于第2层，集成模型召回率同样接近99%，将漏检可报癌症从54和46例降至33例。这些发现表明，结合互补文本表示的集成显著减少漏检癌症并提升错误覆盖率。我们实现了一种隐私保护工作流程，仅在省份之间共享模型权重，支持可互操作的NLP基础设施，并为未来的全加拿大癌症病理与登记工作流程基础模型奠定基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland &amp;amp; Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.&lt;/p&gt;</description></item><item><guid>2601.00837v1</guid><title>Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs</title><link>http://arxiv.org/abs/2601.00837v1</link><author>Agniv Roy Choudhury</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了使用迁移学习与微调的卷积神经网络在儿童肺炎检测中的效果，比较了从零开始训练的自定义CNN与预训练模型的表现，并评估了冻结骨干网络与微调两种策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 肺炎是5岁以下儿童死亡的主要原因，每年导致超过70万死亡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 比较从零开始训练的自定义CNN与使用ResNet50、DenseNet121、EfficientNet-B0的迁移学习模型在儿童肺炎检测中的表现，并评估冻结骨干网络与微调两种策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用5,216张儿童胸片，按80/10/10划分为训练、验证和测试集，训练七个模型，使用准确率、F1分数和AUC评估，并用Grad-CAM提供可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 微调的ResNet50取得最佳效果，准确率99.43%，F1分数99.61%，AUC99.93%，仅有3例误判；微调平均比冻结骨干高5.5个百分点；Grad-CAM显示模型关注肺部相关区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 迁移学习与微调显著优于从头训练的CNN，几乎完美的准确率显示其在资源有限环境下作为筛查工具的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 肺炎是5岁以下儿童死亡的主要原因，每年导致超过70万死亡。准确诊断胸片受限于放射科医生的可用性和变异性。本研究比较了从零开始训练的自定义卷积神经网络与使用ResNet50、DenseNet121、EfficientNet-B0的迁移学习模型在儿童肺炎检测中的表现，并评估了冻结骨干网络和微调两种策略。使用5,216张儿童胸片数据集，按80/10/10划分为训练、验证和测试集。共训练七个模型，并使用准确率、F1分数和AUC进行评估，Grad-CAM可视化提供可解释性。结果显示，微调的ResNet50取得最佳表现：准确率99.43%，F1分数99.61%，AUC99.93%，仅有3例误判。微调平均比冻结骨干高5.5个百分点。Grad-CAM确认模型关注临床相关的肺部区域。结论是，迁移学习与微调显著优于从头训练的CNN，显示出近乎完美的准确率，具有在资源有限环境下作为筛查工具的强大潜力。未来工作应在多中心和成人数据集上验证这些发现。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.   Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.   Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.   Results: Fine-tuned ResNet50 achieved the best performance: 99.43\% accuracy, 99.61\% F1-score, and 99.93\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.   Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.   Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.&lt;/p&gt;</description></item><item><guid>2601.00845v1</guid><title>Enhancing Temporal Awareness in LLMs for Temporal Point Processes</title><link>http://arxiv.org/abs/2601.00845v1</link><author>Lili Chen, Wensheng Gan, Shuang Liang, Philip S. Yu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的框架TPP-TAL，用于提升大型语言模型在时间点过程中的时间推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 时间点过程在金融、医疗和社交系统等领域广泛应用，能够捕捉事件随时间的非规则性和相互依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有LLM在时间点过程建模中难以有效捕捉时间信息与语义上下文交互的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; TPP-TAL通过在输入LLM前先将时间动态与上下文语义对齐，而不是简单拼接时间和事件类型嵌入，从而增强模型对时间依赖和长程交互的感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上实验表明，TPP-TAL在时间似然估计和事件预测准确率上显著优于传统方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提升LLM的时间意识对连续时间事件建模至关重要，TPP-TAL为此提供了有效的插件式解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL&lt;/p&gt;</description></item><item><guid>2601.00862v1</guid><title>Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions</title><link>http://arxiv.org/abs/2601.00862v1</link><author>Joey Chan, Huan Wang, Haoyu Pan, Wei Wu, Zirong Wang, Zhen Chen, Ershun Pan, Min Xie, Lifeng Xi</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种统一的电池容量衰减预测框架，利用大规模公开老化数据集训练时间序列基础模型，并结合低秩适配和物理引导的对比学习，能够在不同化学成分、形状和运行条件下保持稳健性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确预测电池容量衰减对于能源存储系统的安全、可靠性和长期效率至关重要，但不同电池化学成分、形状和运行条件的高度异质性使得单一模型难以泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在多种化学成分和使用场景下保持稳健性能的统一容量预测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 整理20个公开老化数据集，覆盖1704个电池和3961195个充放电循环段，温度范围-5至45摄氏度，多种C倍率以及快速充电和部分循环配置；在此语料库上采用时间序列基础模型骨干，结合参数高效的低秩适配和基于物理的对比表示学习，以捕捉共享的衰减模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 单一统一模型在已知和留出的未知数据集上与每数据集基线模型相比，准确性相当或更优，并在训练之外的化学成分、容量尺度和运行条件上保持稳定性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于时间序列基础模型的架构是可扩展且可迁移的电池容量衰减预测解决方案，适用于真实电池管理系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确预测电池容量衰减对于能源存储系统的安全、可靠性和长期效率至关重要。然而，电池化学成分、形状因素和运行条件之间的高度异质性使得构建一个能够在训练域之外泛化的单一模型变得困难。本研究提出了一种统一的容量预测框架，能够在多种化学成分和使用场景下保持稳健性能。我们将20个公开老化数据集整理成一个大规模语料库，涵盖1704个电池和3961195个充放电循环段，温度范围从-5摄氏度到45摄氏度，涵盖多种C倍率以及面向应用的快速充电和部分循环配置。在此语料库上，我们采用时间序列基础模型（TSFM）骨干，并结合参数高效的低秩适配（LoRA）以及基于物理的对比表示学习，以捕捉共享的衰减模式。对已知和故意留出的未知数据集进行实验表明，单一统一模型在准确性上与强大的每数据集基线相当或更优，同时在训练之外的化学成分、容量尺度和运行条件上保持稳定性能。这些结果展示了基于TSFM的架构作为可扩展且可迁移的容量衰减预测解决方案在真实电池管理系统中的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\,^{\circ}\mathrm{C}$ to $45\,^{\circ}\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.&lt;/p&gt;</description></item><item><guid>2601.00877v1</guid><title>LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification</title><link>http://arxiv.org/abs/2601.00877v1</link><author>Thomas Andrews, Mark Law, Sara Ahmadi-Abhari, Alessandra Russo</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LearnAD 是一种神经符号方法，用于从脑磁共振成像数据预测阿尔茨海默病，并学习完全可解释的规则。它结合统计模型、决策树、随机森林或图神经网络识别脑连接，然后使用 FastLAS 学习全局规则。实验表明，LearnAD 的最佳实例在准确率上优于决策树，接近支持向量机，并仅略低于随机森林和图神经网络，同时保持完全可解释性。消融实验显示，神经符号方法在保持性能的同时提升了解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 阿尔茨海默病的早期诊断需要从脑影像中提取有意义的特征，而传统机器学习模型往往缺乏可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在保持高预测性能的同时提供完全可解释规则的神经符号方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用统计模型（决策树、随机森林或图神经网络）识别与疾病相关的脑连接，再利用 FastLAS 学习全局可解释规则；随后通过消融实验评估方法的解释性和性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LearnAD 的最佳实例在准确率上优于决策树，接近支持向量机，仅略低于随机森林和图神经网络；神经符号方法在保持性能的同时显著提升了解释性；符号学习有助于深入理解图神经网络在临床神经科学中的行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LearnAD 展示了符号学习在提升神经网络可解释性和理解其行为方面的潜力，为临床神经科学提供了新的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出 LearnAD，一种神经符号方法，用于从脑磁共振成像数据预测阿尔茨海默病，并学习完全可解释的规则。LearnAD 使用统计模型、决策树、随机森林或图神经网络识别相关脑连接，然后利用 FastLAS 学习全局规则。我们的最佳实例在准确率上优于决策树，接近支持向量机，并仅略低于使用所有特征训练的随机森林和图神经网络，同时保持完全可解释性。消融研究表明，我们的神经符号方法在保持与纯统计模型相当的性能的同时提升了解释性。LearnAD 展示了符号学习如何深化我们对图神经网络在临床神经科学中行为的理解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer&amp;#x27;s disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.&lt;/p&gt;</description></item><item><guid>2601.00887v1</guid><title>VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition</title><link>http://arxiv.org/abs/2601.00887v1</link><author>Hongbo Jin, Kuanwei Lin, Wenhao Zhang, Yichen Jin, Ge Li</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 强化学习是视频大语言模型复杂时空推理的关键；2. 现有方法主要采用随机打乱或基于标量难度的简单课程策略；3. 标量难度无法区分视觉时空感知负荷与认知推理深度两大挑战；4. 提出 VideoCuRL 框架，将难度拆分为两条轴；5. 采用光流和关键帧熵评估视觉复杂度，使用校准惊讶度评估认知复杂度；6. 将样本映射到二维课程网格；7. 采用能力感知的对角波前策略从基础对齐到复杂推理；8. 引入动态稀疏 KL 与结构化复习以稳定训练，防止奖励崩溃和灾难性遗忘；9. 实验表明 VideoCuRL 在推理与感知任务上均优于强基线；10. 消除了基于生成的课程的高推理开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 强化学习在视频大语言模型中用于复杂时空推理，但现有方法主要依赖随机打乱或基于标量难度指标的简单课程策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 VideoCuRL 框架，将难度拆分为视觉时空感知负荷和认知推理深度两条轴，以更精准地指导训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用光流和关键帧熵评估视觉复杂度，使用校准惊讶度评估认知复杂度，将样本映射到二维课程网格；采用能力感知的对角波前策略从基础对齐到复杂推理；引入动态稀疏 KL 和结构化复习以稳定训练，防止奖励崩溃和灾难性遗忘。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; VideoCuRL 在 VSI-Bench 推理任务上提升 2.5 分，在 VideoMME 感知任务上提升 2.9 分，显著优于强基线；并消除了基于生成的课程的高推理开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; VideoCuRL 为视频后训练提供了可扩展、稳健的解决方案，提升了推理与感知性能并降低了推理成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 强化学习（RL）对于赋能视频大语言模型（VideoLLMs）进行复杂时空推理至关重要。然而，当前的 RL 框架主要依赖随机数据打乱或基于标量难度指标的简单课程策略。我们认为，标量指标无法分离视频理解中的两个正交挑战：视觉时空感知负荷和认知推理深度。为此，我们提出了 VideoCuRL，一种将难度拆分为这两条轴的新框架。我们使用高效、无训练的代理——光流和关键帧熵评估视觉复杂度，校准惊讶度评估认知复杂度，将数据映射到二维课程网格。随后，采用能力感知的对角波前策略，从基础对齐逐步过渡到复杂推理。我们还引入动态稀疏 KL 和结构化复习，以稳定训练，防止奖励崩溃和灾难性遗忘。大量实验表明，VideoCuRL 在推理（VSI-Bench +2.5）和感知（VideoMME +2.9）任务上均优于强基线。值得注意的是，VideoCuRL 消除了基于生成的课程所带来的高推理开销，为稳健的视频后训练提供了可扩展的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.&lt;/p&gt;</description></item><item><guid>2601.00963v1</guid><title>Deep Clustering with Associative Memories</title><link>http://arxiv.org/abs/2601.00963v1</link><author>Bishwajit Saha, Dmitry Krotov, Mohammed J. Zaki, Parikshit Ram</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的深度聚类方法DCAM，利用关联记忆的能量动力学构造损失函数，将表示学习与聚类紧密结合在单一目标中；实验表明该方法在卷积、残差或全连接网络以及图像和文本数据上均能提升聚类质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度聚类是计算机视觉和文本处理中的热门研究方向，表示学习可微分，但聚类本质上是离散优化，需要多种近似和正则化，导致表示学习与聚类过程相对分离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新的损失函数，利用关联记忆的能量动力学，将表示学习与聚类更紧密地结合在一起。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计DCAM方法，使用能量基动力学与关联记忆相结合的损失函数，使表示学习与聚类在同一目标下共同优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果显示DCAM在不同网络架构（卷积、残差、全连接）和不同数据模态（图像、文本）上均优于传统方法，提升了聚类质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DCAM通过将表示学习与聚类整合到单一目标，显著改善了深度聚类性能，为未来研究提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 深度聚类——联合表示学习和潜在空间聚类——是一个在计算机视觉和文本处理领域下深度学习框架中得到充分研究的问题。虽然表示学习通常是可微分的，但聚类本质上是一个离散优化任务，需要各种近似和正则化才能适应标准可微分流程。这导致表示学习和聚类在某种程度上是分离的。在这项工作中，我们提出了一种利用关联记忆的能量动力学来构造新的深度聚类方法DCAM的新损失函数，它将表示学习和聚类方面更紧密地结合在单一目标中。我们的实验展示了DCAM的优势，在各种架构选择（卷积、残差或全连接）和数据模态（图像或文本）上产生了改进的聚类质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).&lt;/p&gt;</description></item><item><guid>2601.00987v1</guid><title>Tessellation Localized Transfer learning for nonparametric regression</title><link>http://arxiv.org/abs/2601.00987v1</link><author>Hélène Halconruy, Benjamin Bobbia, Paul Lejamtel</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种非参数回归迁移学习框架，显式建模源任务与目标任务之间的异质性，并通过局部迁移假设实现有效迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 迁移学习旨在利用相关源任务信息提升目标任务性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建能够捕捉源-目标关系异质性的非参数回归迁移学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将协变量空间划分为有限个单元，在每个单元内目标回归函数可视为源回归函数的低复杂度变换；同时设计联合学习局部迁移函数与目标回归的估计器，并采用完全数据驱动的程序自适应未知划分结构和迁移强度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 证明了局部迁移可通过降低函数复杂度缓解维数灾难，给出了尖锐的极小极大收敛速率，并提供了将过度风险分解为估计误差与近似误差的预言机不等式，保证对模型误设的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 数值实验验证了所提方法的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 迁移学习旨在通过利用相关源任务的信息来提升目标任务的性能。我们提出了一种非参数回归迁移学习框架，显式建模源-目标关系中的异质性。我们的方法基于局部迁移假设：协变量空间被划分为有限个单元，在每个单元内，目标回归函数可以表示为源回归函数的低复杂度变换。这种局部结构使得在相似性存在的地方实现有效迁移，同时限制了负迁移。我们引入了联合学习局部迁移函数和目标回归的估计器，并提供了完全数据驱动的程序，能够自适应未知的划分结构和迁移强度。我们证明了目标回归估计的尖锐极小极大收敛速率，表明局部迁移可以通过利用降低的函数复杂度来缓解维数灾难。我们的理论保证以预言机不等式的形式出现，将过度风险分解为估计误差和近似误差，确保对模型误设的鲁棒性。数值实验展示了所提方法的优势。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Transfer learning aims to improve performance on a target task by leveraging information from related source tasks. We propose a nonparametric regression transfer learning framework that explicitly models heterogeneity in the source-target relationship. Our approach relies on a local transfer assumption: the covariate space is partitioned into finitely many cells such that, within each cell, the target regression function can be expressed as a low-complexity transformation of the source regression function. This localized structure enables effective transfer where similarity is present while limiting negative transfer elsewhere. We introduce estimators that jointly learn the local transfer functions and the target regression, together with fully data-driven procedures that adapt to unknown partition structure and transfer strength. We establish sharp minimax rates for target regression estimation, showing that local transfer can mitigate the curse of dimensionality by exploiting reduced functional complexity. Our theoretical guarantees take the form of oracle inequalities that decompose excess risk into estimation and approximation terms, ensuring robustness to model misspecification. Numerical experiments illustrate the benefits of the proposed approach.&lt;/p&gt;</description></item><item><guid>2601.00988v1</guid><title>Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss</title><link>http://arxiv.org/abs/2601.00988v1</link><author>Lin Xi, Yingliang Ma, Xiahai Zhuang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的 FSVOS 模型，利用局部匹配策略将搜索空间限制在最相关的邻域像素，并通过方向采样视角实现非参数化动态采样，提升多帧特征一致性，并在 X 光血管造影视频多目标分割任务上取得优异表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的空间卷积、深度卷积和特征平移等实现方式效率低下，且在非 CUDA 设备上的可移植性差；现有的可变形和邻域注意力等硬件特定实现也存在局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种高效、可移植且无需重新训练的局部采样方法，并通过监督时空对比学习提升帧间特征一致性，最终在多目标分割任务中实现更高精度和更好的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用方向基采样的非参数化采样机制，动态调整采样区域；使用监督时空对比学习约束特征一致性；构建公开的 MOSXAV 数据集并在 CADICA、XACV、MOSXAV 上进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 FSVOS 在 CADICA、XACV、MOSXAV 数据集上相较现有最先进方法在分割精度和对已知/未知类别的泛化能力上均有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该工作提供了更灵活的局部采样方案，具有广泛的临床应用潜力，并在多目标分割任务中实现了优异的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种新型 FSVOS 模型，采用局部匹配策略将搜索空间限制在最相关的邻域像素。与依赖低效的标准 im2col 类实现（如空间卷积、深度卷积和特征平移机制）或硬件特定的 CUDA 核心（如可变形和邻域注意力）不同，这些实现往往在非 CUDA 设备上可移植性有限，我们通过方向基采样视角重新组织局部采样过程。具体而言，我们实现了一个非参数化采样机制，支持动态变化的采样区域。这种方法提供了适应多样空间结构的灵活性，而不需要参数化层的计算成本和模型重新训练。为进一步提升帧间特征一致性，我们设计了一个监督的时空对比学习方案，强制特征表示的一致性。此外，我们引入了一个公开可用的多目标分割 X 光血管造影视频基准数据集 MOSXAV，包含详细的人工标注分割真值。对 CADICA、XACV 和 MOSXAV 数据集的广泛实验表明，我们提出的 FSVOS 方法在分割精度和泛化能力（即已知和未知类别）方面优于当前最先进的视频分割方法。本工作提供了更高的灵活性和广泛的临床应用潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.&lt;/p&gt;</description></item><item><guid>2601.01015v1</guid><title>HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery</title><link>http://arxiv.org/abs/2601.01015v1</link><author>Shiyuan Liu, Jianwei Wang, Xuemin Lin, Lu Qin, Wenjie Zhang, Ying Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 论文提出 HyperJoin，一个基于大型语言模型的超图框架，用于发现可连接表。通过构建包含表内超边和语言模型增强的表间超边的超图，将可连接表发现转化为超图链接预测，并设计 HIN 进行层次交互学习，在线排名采用一致性感知的 top‑k 选择并用最大生成树重排序，实验表明相较最佳基线提升 21.4% 精度和 17.2% 召回。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在数据湖管理中，发现可连接表是关键任务。现有基于语言模型的方法通过离线列表示学习和在线排序实现高性能，但忽视了表间和表内的结构交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有方法在离线建模和在线排序中对结构交互考虑不足的问题，提升可连接表发现的准确性和一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①构建超图，包含表内超边和 LLM 增强的表间超边；②将任务转化为超图链接预测；③设计 HIN 通过双向消息传递学习列表示；④在线排序视为一致性感知的 top‑k 选择，并使用最大生成树算法进行重排序。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; HyperJoin 在公开基准上平均提升 21.4% 的 Precision@15 和 17.2% 的 Recall@15，显著优于现有最佳方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过超图和层次交互网络结合 LLM，HyperJoin 能更充分捕捉表间表内结构信息，提供更准确、更一致的可连接表发现结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 作为数据湖管理中的关键任务，可连接表发现已受到广泛关注。现有基于语言模型的方法通过离线列表示学习与在线排序相结合，取得了显著性能，但其设计在充分考虑底层结构交互方面不足：（1）在离线阶段，它们直接将表建模为孤立或成对的列，难以捕捉丰富的表间和表内结构信息；（2）在在线阶段，它们仅基于查询与候选列的相似度进行排序，忽略了候选列之间的相互作用，导致结果集不连贯。为解决这些局限，我们提出 HyperJoin，一种基于大型语言模型增强的超图框架，用于可连接表发现。具体而言，我们首先构建一个超图，使用表内超边和 LLM 增强的表间超边来建模表。因此，可连接表发现任务被表述为在该超图上的链接预测。随后，我们设计了 HIN，即层次交互网络，通过在列和超边之间进行双向消息传递来学习表达性列表示。为增强结果列的连贯性和内部一致性，我们将在线排序视为一致性感知的 top‑k 列选择问题。随后，我们引入了一个重排序模块，利用最大生成树算法剔除噪声连接并最大化连贯性。实验表明，HyperJoin 的优势显著，平均提升 21.4%（Precision@15）和 17.2%（Recall@15），超过最佳基线。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.&lt;/p&gt;</description></item><item><guid>2601.01027v1</guid><title>A Platform for Interactive AI Character Experiences</title><link>http://arxiv.org/abs/2601.01027v1</link><author>Rafael Wampfler, Chen Yang, Dillon Elste, Nikola Kovacevic, Philine Witzig, Markus Gross</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 通过将角色融入互动、情节驱动的对话，激发跨世代的想象力。2. 实现此目标面临多重 AI 挑战，包括对话、角色完整性、个性与情感、知识与记忆、语音合成、动画生成、现实交互以及与物理环境的集成。3. 现有基础模型、提示工程和下游任务微调技术已能解决单一挑战，但将它们组合用于交互式角色仍是未解难题。4. 本文提出一种便捷的系统与平台，可设计可信的数字角色，实现对话与情节体验，并解决上述所有技术难点。5. 以“数字爱因斯坦”为概念验证，用户可与数字化的爱因斯坦就其生平、研究和人格进行对话。6. 该系统具备通用性，可推广至任何情节驱动或对话角色。7. 通过统一多种 AI 组件为单一易适配平台，为沉浸式角色体验铺平道路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着电影角色和现代科幻的普及，创造可与人类进行情节驱动对话的角色已成为热门研究方向。实现这一目标需要超越传统语言模型，涉及多项复杂 AI 技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个统一的平台，能够方便地设计可信的数字角色，并在对话与情节驱动体验中解决所有技术挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 开发一个集成多种 AI 组件（对话、角色完整性、情感管理、知识记忆、语音合成、动画生成、现实交互等）的系统平台，并以数字爱因斯坦为示例进行验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该平台能够成功实现数字爱因斯坦与用户的对话，展示了系统在处理多重 AI 任务时的可行性和灵活性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过统一多种 AI 技术为单一易适配平台，本文为实现逼真、情节驱动的角色交互奠定了基础，推动了沉浸式角色体验的实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从电影角色到现代科幻——将角色融入互动、情节驱动的对话已捕捉跨世代的想象力。实现这一愿景极具挑战，远不止语言建模。它涉及众多复杂的 AI 挑战，如对话 AI、保持角色完整性、管理个性与情感、处理知识与记忆、语音合成、动画生成、实现现实交互以及与物理环境的集成。近期在基础模型、提示工程和下游任务微调方面的进展，使研究人员能够解决这些单独的挑战。然而，将这些技术组合用于交互式角色仍是一个开放问题。我们提出了一个系统和平台，方便地设计可信的数字角色，提供对话和情节驱动体验，并解决所有技术挑战。作为概念验证，我们介绍了 Digital Einstein，它允许用户与数字化的阿尔伯特·爱因斯坦就他的生活、研究和人格进行对话。虽然 Digital Einstein 展示了我们针对特定角色的方法，但我们的系统灵活且可推广到任何情节驱动或对话角色。通过将这些多样的 AI 组件统一为一个易于适配的平台，我们的工作为沉浸式角色体验铺平了道路，使逼真、基于故事的交互成为现实。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.&lt;/p&gt;</description></item><item><guid>2601.01036v1</guid><title>Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising</title><link>http://arxiv.org/abs/2601.01036v1</link><author>Kiet Dang Vu, Trung Thai Tran, Kien Nguyen Do Trung, Duc Dung Nguyen</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 Mono3DV 框架，针对单目 3D 检测中 DETR 结构因忽略 3D 属性导致匹配失效的问题，提出三项创新方法以提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; DETR 类结构在单目 3D 检测中表现良好，但由于 3D 估计的不确定性，传统的双向匹配仅使用 2D 信息，导致高质量 3D 预测被误判并抑制，训练过程不稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决 3D 属性被排除导致的匹配误差和训练不稳定问题，从而提升单目 3D 检测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 3D‑Aware 双向匹配：将 3D 几何信息直接加入匹配成本；2) 3D‑DeNoising：在训练阶段加入去噪机制以稳定匹配；3) Variational Query DeNoising：采用变分查询去噪，缓解梯度消失并进一步提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 KITTI 3D 检测基准上，Mono3DV 在不使用任何外部数据的情况下取得了最先进的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过将 3D 信息融入匹配成本并采用去噪策略，Mono3DV 有效解决了传统方法的局限，显著提升了单目 3D 检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然 DETR 类架构在单目 3D 目标检测中表现出显著潜力，但它们往往受到一个关键限制的阻碍：在双向匹配过程中排除了 3D 属性。这种排除源于从单目图像进行 3D 估计的固有不确定性，导致训练过程不稳定。因此，高质量的 3D 预测可能被仅基于 2D 的匹配准则错误抑制，导致次优结果。为了解决这一问题，我们提出了 Mono3DV，一种新颖的 Transformer 框架。我们的方法引入了三项关键创新。首先，我们开发了 3D‑Aware 双向匹配策略，直接将 3D 几何信息纳入匹配成本，解决了纯 2D 标准导致的误差。其次，为了稳定双向匹配并解决在整合 3D 属性时出现的不稳定性，我们在训练阶段提出了 3D‑DeNoising 方案。最后，鉴于传统去噪技术存在梯度消失问题，我们提出了一种新颖的变分查询去噪机制，以克服这一限制，显著提升模型性能。无需利用任何外部数据，我们的方法在 KITTI 3D 目标检测基准上实现了最先进的结果。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文旨在解决从单张RGB图像中检测三维物体（即单目三维目标检测）的问题。该问题在自动驾驶、机器人导航和增强现实等场景中至关重要，因为单目摄像头成本低、部署方便，而获取精确的三维位置信息是实现安全感知的关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计时借鉴了基于Transformer的查询检测框架（如DETR）以及现有的单目三维检测方法，进一步引入了对三维几何的匹配约束和查询去噪技术。通过分析传统匹配策略在三维空间中的不足，他们提出了3D感知的二分匹配，并结合变分推断来改进查询初始化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将三维几何信息融入查询匹配过程，并通过变分查询去噪提升训练稳定性。实现流程包括：①使用CNN提取图像特征；②生成初始查询向量；③利用Transformer解码器预测三维框；④采用3D感知的二分匹配计算损失；⑤通过变分推断对查询进行去噪并更新；⑥最终输出三维检测结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）3D感知的二分匹配机制，能够更准确地将预测框与真实框对应；2）变分查询去噪方法，显著提升了训练过程的收敛速度和检测精度；3）将上述两项技术集成到单目三维检测框架中，形成端到端的查询式网络。与以往仅使用二维匹配或无去噪的查询检测器不同，Mono3DV在匹配和查询初始化上做了显著改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Mono3DV提出了一种结合3D感知二分匹配和变分查询去噪的单目三维检测框架，在保持端到端训练的同时显著提升了检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.&lt;/p&gt;</description></item><item><guid>2601.01046v1</guid><title>KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs</title><link>http://arxiv.org/abs/2601.01046v1</link><author>Yixuan Tang, Yi Yang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 KV-Embedding 的框架，利用冻结的大语言模型内部的键值状态来提升无训练环境下的表示能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在无训练设置中，LLM 的因果注意力限制了早期 token 访问后续上下文，且下一个 token 的预测目标导致表示偏向生成而非语义压缩。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述两大结构性挑战，激活冻结 LLM 的潜在表示能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 观察到每层最终 token 的键值状态包含序列的压缩视图，作者将这些状态重新路由为前缀，使所有 token 在一次前向传播中即可获取全序列上下文，并通过基于内在维度的自动层选择策略实现模型无关性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 MTEB 评测上，KV-Embedding 在 Qwen、Mistral、Llama 等骨干模型上比现有无训练基线提升多达 10%，且在长度达 4,096 token 的序列上保持稳健性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 内部状态的操纵为输入修改提供了高效替代方案，鼓励进一步探索 LLM 内部机制用于表示学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然大型语言模型（LLM）是强大的嵌入骨干，但在无训练环境中的应用面临两个结构性挑战：因果注意力限制了早期 token 访问后续上下文，且下一个 token 的预测目标使表示偏向生成而非语义压缩。为了解决这些限制，我们提出了 KV-Embedding，一种激活冻结 LLM 隐含表示能力的框架。我们的方法利用观察到的事实，即每层最终 token 的键值（KV）状态编码了序列的压缩视图。通过将这些状态重新路由为预置前缀，我们使所有 token 能在一次前向传播中访问序列级上下文。为确保模型无关性，我们引入了基于内在维度的自动层选择策略。在 Qwen、Mistral 和 Llama 等骨干模型上对 MTEB 进行评估显示，KV-Embedding 在现有无训练基线基础上提升了多达 10%，同时在长度达 4,096 token 的序列上保持稳健性能。这些结果表明，内部状态操纵为输入修改提供了高效替代方案，我们希望这项工作能鼓励进一步探索 LLM 内部机制用于表示学习。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.&lt;/p&gt;</description></item><item><guid>2601.01089v1</guid><title>Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding</title><link>http://arxiv.org/abs/2601.01089v1</link><author>Nobuyuki Ota</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了中央遗传学变换器（Central Dogma Transformer，CDT），通过将 DNA、RNA 和蛋白质的预训练语言模型按中央遗传学的方向性逻辑整合，利用 DNA‑to‑RNA 和 RNA‑to‑Protein 的交叉注意力机制，生成统一的虚拟细胞嵌入。该模型在 CRISPRi 诱导增强子扰动数据上验证，取得了相当可观的预测相关性，并通过注意力与梯度分析提供了对基因组不同区域的互补解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 理解细胞机制需要整合 DNA、RNA 和蛋白质三大分子系统的信息，但现有的领域特定基础模型仅在单一模态上取得成功，彼此孤立，限制了对整合细胞过程的建模能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够沿着中央遗传学的方向性流动，将 DNA、RNA 和蛋白质三种模态信息整合的统一架构，以提升对细胞过程的预测与解释能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用预训练的 DNA、RNA 和蛋白质语言模型，采用 DNA‑to‑RNA 的注意力模拟转录调控，RNA‑to‑Protein 的注意力模拟翻译关系，生成统一的虚拟细胞嵌入。通过在 K562 细胞的 CRISPRi 增强子扰动数据上进行验证，并结合注意力图和梯度分析来解释模型关注的基因组区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在验证实验中，CDT v1 取得了 0.503 的皮尔逊相关系数，约占理论上限的 63%。注意力和梯度分析揭示了不同的基因组区域，其中梯度分析识别出一个 CTCF 结合位点，该位点在 Hi‑C 数据中被证明与增强子和靶基因存在物理接触。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 与生物信息流对齐的 AI 架构能够同时实现高预测准确性和机制可解释性，为细胞机制研究提供了新的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解细胞机制需要整合 DNA、RNA 和蛋白质——这三大分子系统通过分子生物学的中央遗传学相互关联。虽然针对每种模态的领域特定基础模型已取得成功，但它们仍然是孤立的，限制了我们对整合细胞过程的建模能力。本文提出了中央遗传学变换器（CDT），一种将预训练的 DNA、RNA 和蛋白质语言模型按中央遗传学的方向性逻辑整合的架构。CDT 采用方向性交叉注意力机制——DNA‑to‑RNA 注意力模拟转录调控，RNA‑to‑Protein 注意力模拟翻译关系——生成统一的虚拟细胞嵌入，整合了三种模态。我们在 K562 细胞的 CRISPRi 增强子扰动数据上验证了 CDT v1——一种使用固定（非细胞特异）RNA 和蛋白质嵌入的概念验证实现——并取得了 0.503 的皮尔逊相关系数，代表了由跨实验变异性设定的理论上限的 63%。注意力和梯度分析提供了互补的解释窗口：在详细案例研究中，这些方法突出显示了大不相同的基因组区域，梯度分析识别出一个 CTCF 结合位点，Hi‑C 数据显示该位点与增强子和靶基因存在物理接触。这些结果表明，与生物信息流对齐的 AI 架构能够实现预测准确性和机制可解释性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.&lt;/p&gt;</description></item><item><guid>2601.01095v1</guid><title>NarrativeTrack: Evaluating Video Language Models Beyond the Frame</title><link>http://arxiv.org/abs/2601.01095v1</link><author>Hyeonjeong Ha, Jinjin Ge, Bo Feng, Kaixin Ma, Gargi Chakraborty</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 NarrativeTrack 基准，用于评估多模态大语言模型在视频叙事理解方面的能力。通过细粒度的实体中心推理，构建了一个分层的评估框架，逐步增加叙事复杂度。实验表明现有模型在跟踪实体、保持时间连贯性方面存在不足，揭示了感知定位与时间推理之间的权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大语言模型在视觉-语言推理上已取得显著进展，但对视频中随时间展开的叙事理解仍未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个针对多模态大语言模型的叙事理解基准，评估其在实体持续性、变化和歧义等维度上的推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将视频拆分为实体，使用自动化实体中心管道提取时间定位的实体表示；设计了 Compositional Reasoning Progression (CRP) 框架，按实体存在、变化、歧义三维度递进增加难度；对现有模型进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 现有模型在跨视觉转场和时间动态中难以稳健跟踪实体，常在上下文变化时产生身份幻觉。通用 MLLM 在感知定位上表现强劲，但时间连贯性弱；视频专用 MLLM 捕捉时间上下文但会幻觉实体上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 感知定位与时间推理之间存在根本权衡，叙事理解需要二者的整合。NarrativeTrack 为诊断和提升多模态大语言模型的时间叙事理解提供了系统框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity&amp;#x27;s contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity&amp;#x27;s contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.&lt;/p&gt;</description></item><item><guid>2601.01099v1</guid><title>Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks</title><link>http://arxiv.org/abs/2601.01099v1</link><author>Mahmudul Hasan, Mabsur Fatin Bin Hossain</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文通过对比自定义卷积神经网络与常用预训练及迁移学习模型，在五个真实图像数据集上进行实验，探讨网络深度、残差连接和特征提取等架构因素对分类与定位性能的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 研究聚焦于不同任务（二分类、细粒度多分类、目标检测）下的网络架构选择，比较自定义CNN与预训练/迁移学习模型的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估自定义CNN在不同任务中的优势与局限，并为根据任务复杂度和资源约束选择合适网络提供实用指导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在五个数据集上进行系统比较，分析网络深度、残差连接、特征提取策略对性能的影响，并将自定义架构扩展到目标检测任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 更深的CNN在细粒度多分类任务中显著提升性能；轻量级预训练/迁移学习模型在简单二分类任务中仍表现优异；自定义架构可灵活适用于检测未授权三轮车的场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 根据任务复杂度和资源限制，选择更深或更轻量级的网络可获得最佳性能，本文为网络设计提供了实用建议。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文呈现了对自定义卷积神经网络（CNN）架构与广泛使用的预训练和迁移学习CNN模型在五个真实图像数据集上的比较研究。数据集涵盖二分类、细粒度多分类识别和目标检测场景。我们分析了网络深度、残差连接和特征提取策略等架构因素如何影响分类和定位性能。结果表明，较深的CNN架构在细粒度多分类数据集上提供了显著的性能提升，而轻量级的预训练和迁移学习模型在更简单的二分类任务中仍保持高度有效。此外，我们将所提出的架构扩展到目标检测设置，展示了其在识别真实交通场景中未授权三轮车方面的适应性。基于对自定义CNN架构与预训练和迁移学习模型的系统分析，本研究为根据任务复杂度和资源约束选择合适的网络设计提供了实用指导。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.&lt;/p&gt;</description></item><item><guid>2601.01123v1</guid><title>Learning from Historical Activations in Graph Neural Networks</title><link>http://arxiv.org/abs/2601.01123v1</link><author>Yaniv Galron, Hadar Sinai, Haggai Maron, Moshe Eliasof</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的两阶段注意力聚合层HISTOGRAPH，用于改进图神经网络的池化过程，利用各层历史激活信息和图结构来生成更具信息量的最终特征，从而提升图分类任务的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 图神经网络在社交网络、分子化学等领域表现出色，池化是其关键环节，但传统方法仅使用最后一层特征，忽略了前层重要激活。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补传统池化方案对历史激活的低利用，尤其在深层网络中节点表示随层显著变化时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; HISTOGRAPH先对中间激活进行统一的层级注意力，再对节点进行注意力聚合，建模节点表示随层演化的过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个图分类基准上，HISTOGRAPH相较传统技术表现更好，尤其在深层GNN中表现出更强的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 利用历史激活和图结构的两阶段注意力聚合能显著提升图分类性能，尤其适用于深层图神经网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图神经网络（GNN）在社交网络、分子化学等多个领域取得了显著成功。GNN 的关键组成部分是池化过程，在该过程中模型计算得到的节点特征被组合成一个信息丰富的最终描述符，用于下游任务。然而，之前的图池化方案依赖于最后一层 GNN 的特征作为池化或分类层的输入，可能低效利用了模型前向传播过程中产生的重要前层激活，我们将其视为历史图激活。这个缺口在节点表示在多层图神经网络中显著变化的情况下尤为突出，并且在深层架构中因过度平滑等图特定挑战而加剧。为弥补这一缺口，我们提出了 HISTOGRAPH，一种新颖的两阶段基于注意力的最终聚合层，首先对中间激活应用统一的层级注意力，然后进行节点级注意力。通过建模节点表示在各层之间的演化，HISTOGRAPH 利用节点的激活历史和图结构来细化用于最终预测的特征。对多个图分类基准的实证结果表明，HISTOGRAPH 在性能上始终优于传统技术，并在深层 GNN 中表现出特别强的鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node&amp;#x27;s representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.&lt;/p&gt;</description></item><item><guid>2601.01144v1</guid><title>VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction</title><link>http://arxiv.org/abs/2601.01144v1</link><author>Shu Pan, Simon Archieri, Ahmet Cinar, Jonatan Scharff Willners, Ignacio Carlucho, Yvan Petillot</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; VISO是一套稳健的水下SLAM系统，融合立体相机、惯性测量单元和三维声纳，实现精确的六自由度定位和高光度保真度的高效稠密三维重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 水下环境中的视觉挑战严重影响基于视觉的定位精度和高保真稠密重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够克服水下视觉挑战、实现精确定位和高效稠密三维重建的稳健水下SLAM系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过立体相机、IMU和三维声纳的融合，采用粗到细的在线标定方法估计声纳与相机之间的外参，并提出光度渲染策略为声纳点云增添视觉信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在实验室水箱和开放湖泊的广泛实验中，VISO在定位鲁棒性和精度方面超过了当前最先进的水下和视觉SLAM算法，并且实时稠密重建性能与离线稠密映射方法相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; VISO在水下环境中提供了稳健的定位和高质量的稠密重建，展示了实时性能和优越的精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉挑战在水下环境中显著阻碍了基于视觉的定位精度和高保真稠密重建。本文提出了VISO，一套稳健的水下SLAM系统，融合立体相机、惯性测量单元和三维声纳，实现精确的六自由度定位，并支持高光度保真度的高效稠密三维重建。我们引入了粗到细的在线标定方法，用于估计三维声纳与相机之间的外参。此外，提出了一种光度渲染策略，为三维声纳点云增添视觉信息。实验在实验室水箱和开放湖泊中进行，结果表明VISO在定位鲁棒性和精度方面超过了当前最先进的水下和视觉SLAM算法，并且实时稠密重建性能与离线稠密映射方法相当。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本论文旨在解决在水下环境中实现鲁棒的视觉-惯性-声纳SLAM，并通过光度渲染实现稠密三维重建。水下导航与地图构建对海洋勘探、海底机器人和救援任务至关重要，但低能见度和光照变化使传统视觉SLAM难以稳定工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有视觉-惯性SLAM（如VINS-Mono、ORB‑SLAM）和声纳SLAM的基础上，提出将声纳深度信息与视觉-惯性前端融合，并引入光度渲染以提升稠密重建质量。该设计借鉴了现有多传感器融合框架，并针对水下特性进行了改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视觉、惯性和声纳数据联合建模，并利用光度渲染对三维场景进行稠密重建。实现流程包括：①传感器同步采集视觉、IMU和声纳数据；②视觉-惯性前端估计相机轨迹；③声纳前端提供深度约束；④光度渲染模块将声纳深度投影到图像，形成稠密深度图；⑤后端优化（束束调整/图优化）融合所有约束，输出精确位姿和稠密点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1)首次将声纳深度与视觉-惯性SLAM联合使用；2)引入光度渲染实现稠密三维重建；3)提出针对水下低能见度的鲁棒优化策略；4)在多传感器融合中使用统一的光度一致性约束。与以往仅使用视觉或声纳的单一传感器方法相比，VISO在低光照、雾霾等环境下表现更稳定，重建精度更高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VISO通过将视觉、惯性和声纳数据与光度渲染相结合，提供了一种在水下环境中鲁棒且稠密的SLAM系统，显著提升了低能见度场景下的定位与三维重建性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.&lt;/p&gt;</description></item><item><guid>2601.01200v1</guid><title>MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity</title><link>http://arxiv.org/abs/2601.01200v1</link><author>Zhang Chen, Shuai Wan, Yuezhe Zhang, Siyu Ren, Fuzheng Yang, Junhui Hou</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对点云不规则性导致的质量评估难题，提出多尺度隐式结构相似度测量方法，并设计了 ResGrouped-MLP 网络进行感知评分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云数据结构不规则，传统点对点匹配难以准确对应感知特征，影响客观质量评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在不依赖精确匹配的情况下，准确测量点云失真并映射到感知质量分数的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用径向基函数连续表示局部特征，将失真测量转化为隐式函数系数比较；构建 ResGrouped-MLP 网络，采用分组编码、残差块和通道注意机制，分尺度提取亮度、色度和几何信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 MS-ISSM 在多个基准上比现有指标更可靠、更具泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法通过避免匹配错误并利用多尺度特征，显著提升点云质量评估性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.&lt;/p&gt;</description></item><item><guid>2601.01207v1</guid><title>Sparse Bayesian Message Passing under Structural Uncertainty</title><link>http://arxiv.org/abs/2601.01207v1</link><author>Yoonhyuk Choi, Jiho Choi, Chanran Kim, Yumin Lee, Hawon Shin, Yeowon Jeon, Minjeong Kim, Jiwoo Kang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对半监督学习在异质图中的挑战，提出通过后验分布建模有符号邻接矩阵来捕捉结构不确定性，并设计稀疏有符号消息传递网络，结合后验边缘化和稀疏聚合，显著提升在合成与真实噪声下的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 半监督学习在真实图中常受异质性和结构噪声影响，现有 GNN 依赖固定邻接或正则化处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 显式捕捉结构不确定性，构建对边噪声和异质性鲁棒的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用后验分布对有符号邻接矩阵建模，允许边为正、负或缺失；提出稀疏有符号消息传递网络，结合后验边缘化和稀疏消息聚合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在合成和真实结构噪声下的异质性基准上优于强基线模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过后验分布和稀疏有符号消息传递，可有效处理边噪声和异质性，提升半监督学习性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在真实世界图上进行半监督学习时，异质性（即观测图不可靠或标签不相似）经常构成挑战。许多现有的图神经网络要么依赖固定的邻接结构，要么尝试通过正则化来处理结构噪声。本文通过对有符号邻接矩阵建模后验分布，显式捕捉结构不确定性，使每条边可以是正、负或缺失。我们提出了一种稀疏有符号消息传递网络，自然对边噪声和异质性具有鲁棒性，并可从贝叶斯角度解释。通过结合对有符号图结构的后验边缘化和稀疏有符号消息聚合，我们的方法为同时处理边噪声和异质性提供了一种原则性途径。实验结果表明，在合成和真实结构噪声下的异质性基准上，我们的方法优于强基线模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.&lt;/p&gt;</description></item><item><guid>2601.01210v1</guid><title>Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission</title><link>http://arxiv.org/abs/2601.01210v1</link><author>Kazuhiko Murasaki, Shunsuke Konagai, Masakatsu Aoki, Taiga Yoshida, Ryuichi Tanida</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出一种高速 LiDAR 点云稠密化方法，结合多源 LiDAR 数据和高分辨率彩色图像，通过卷积神经网络实现联合双边滤波，能够在保持实时性能的同时生成全高清稠密深度图，速度比现有训练方法快 15 倍以上，且生成的点云几何精确，无多视角不一致或幽灵伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 低延迟空间传输系统是沉浸式远程呈现的关键，但存在两大难点：动态 3D 场景的高密度捕获和实时处理。LiDAR 能实时捕获 3D，但点云稀疏。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决实时深度补全需求，生成稠密 3D 场景，保持极低延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 结合多源 LiDAR 输入与高分辨率彩色图像，使用卷积神经网络实现联合双边滤波进行点云稠密化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 方法在 30 帧每秒下生成全高清稠密深度图，速度比最近的训练式深度补全方法快 15 倍以上，生成的稠密点云几何准确，无多视角不一致或幽灵伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 高速 LiDAR 点云稠密化方法可在实时条件下提供高质量稠密 3D 场景，显著提升低延迟空间传输系统的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 为了实现低延迟的空间传输系统以支持沉浸式远程呈现，存在两个主要问题：以高密度捕获动态 3D 场景以及实时处理它们。LiDAR 传感器能够实时捕获 3D，但产生的点云稀疏。因此，本文提出了一种高速 LiDAR 点云稠密化方法，能够在保持实时性能的前提下，以最小延迟生成稠密 3D 场景，满足即时深度补全的需求。我们的方法将多源 LiDAR 输入与高分辨率彩色图像相结合，并通过卷积神经网络实现联合双边滤波策略。实验表明，所提出的方法能够在实时（30 帧/秒）下生成全高清分辨率的稠密深度图，速度比最近的基于训练的深度补全方法快 15 倍以上。生成的稠密点云在几何上准确，并且没有多视角不一致或幽灵伪影。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决实时 LiDAR 点云稠密化以实现低延迟空间数据传输的问题。LiDAR 数据体积大，实时传输对自动驾驶、机器人和增强现实等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.&lt;/p&gt;</description></item><item><guid>2601.01213v1</guid><title>Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation</title><link>http://arxiv.org/abs/2601.01213v1</link><author>Riccardo Gelato, Carlo Sgaravatti, Jakob Grahn, Giacomo Boracchi, Filippo Maria Bianchi</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种利用并改进Segment Anything Model（SAM）来加速Sentinel-1 SAR影像中雪崩标注的方法。通过域适配器、多通道编码器、提示工程和高效训练算法，解决了域差异、输入通道、提示不精确和训练成本等挑战，并将改进后的模型集成到标注工具中，实验表明显著提升了标注速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在山地地区，遥感技术对雪崩分割与制图至关重要。Sentinel-1的合成孔径雷达（SAR）影像可有效用于此任务，但训练高效检测模型需要大量高质量的专家标注数据，耗时过长。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 促进并加速SAR图像的雪崩标注工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 基于SAM模型，使用域适配器弥补与卫星/SAR图像的域差距；采用多通道编码器处理SAR多通道输入；通过提示工程提升雪崩定位精度；设计限制编码器训练时间的高效训练算法。将改进后的模型集成到标注工具中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，集成改进后的模型的标注工具能够显著加速SAR图像的雪崩标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法成功实现了对SAR数据的分割基础模型的定制，显著提升了雪崩标注效率，为山地地区的风险预测与减缓提供了实用工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 遥感技术在山地地区的雪崩分割与制图中起着关键作用，支持风险预测与减缓。Sentinel-1 的合成孔径雷达（SAR）影像可有效用于此任务，但训练高效检测模型需要收集大量高质量的专家标注数据，耗时过长。本文旨在促进并加速 SAR 图像的雪崩标注。我们基于在自然图像上训练的分割基础模型 Segment Anything Model（SAM），并将其定制为 Sentinel-1 SAR 数据。为适配我们的用例，需要解决若干领域特定挑战：①域不匹配，SAM 未在卫星/SAR 图像上训练；②输入适配，SAR 产品通常提供超过三通道，而 SAM 仅限 RGB；③对不精确提示的鲁棒性，提示误差会影响目标识别并降低分割质量，尤其在小型、低对比度雪崩中更为突出；④训练效率，标准微调对 SAM 计算量大。我们通过使用适配器弥补域差距、使用多编码器处理多通道 SAR 输入、提示工程提升雪崩定位精度，以及限制编码器训练时间的算法来解决这些问题。将得到的模型集成到标注工具中，并通过实验表明它能加速 SAR 图像的标注。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.&lt;/p&gt;</description></item><item><guid>2601.01222v1</guid><title>UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass</title><link>http://arxiv.org/abs/2601.01222v1</link><author>Mengfei Li, Peng Li, Zheng Zhang, Jiahao Lu, Chengfeng Zhao, Wei Xue, Qifeng Liu, Sida Peng, Wenxiao Zhang, Wenhan Luo, Yuan Liu, Yike Guo</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; UniSH 提出了一种统一的前向框架，能够同时恢复场景几何、人体点云、相机参数以及一致的度量尺度 SMPL 体。通过利用无标签的真实视频数据，解决了合成数据与真实数据之间的差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在该领域，缺乏大规模标注的真实数据，导致研究者依赖合成数据，进而产生显著的仿真到真实的域差距，导致泛化能力差、人体几何低保真以及与真实视频对齐不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种创新的训练范式，充分利用无标签的真实视频数据，以提升模型在真实场景中的表现和人体几何质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; UniSH 通过融合场景重建与人体模型 HMR 的强大先验，采用两大核心组件：一是稳健的蒸馏策略，从专家深度模型中提取高频细节以细化人体表面；二是两阶段监督方案，先在合成数据上学习粗定位，再在真实数据上直接优化 SMPL 网格与人体点云的几何对应关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，UniSH 在以人为中心的场景重建任务上达到了最先进的性能，并在全局人体运动估计上与基于优化的框架和仅使用 HMR 的方法相比具有高度竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UniSH 能在一次前向传播中同时恢复高保真场景几何、人体点云、相机参数以及一致的度量尺度 SMPL 体，显著提升了在真实视频中的重建质量和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出 UniSH，一种统一的前向框架，用于联合度量尺度的 3D 场景和人体重建。该领域的一个关键挑战是缺乏大规模、标注的真实世界数据，迫使人们依赖合成数据集。这种依赖导致显著的仿真到真实的域差距，导致泛化能力差、低保真人体几何以及在真实视频中的对齐不佳。为了解决这个问题，我们提出了一种创新的训练范式，有效利用无标签的真实视频数据。我们的框架桥接了场景重建和 HMR 的强大、不同的先验，并通过两个核心组件进行训练：1）一种稳健的蒸馏策略，通过从专家深度模型中提取高频细节来细化人体表面细节；2）一种两阶段监督方案，首先在合成数据上学习粗定位，然后通过直接优化 SMPL 网格与人体点云之间的几何对应关系，在真实数据上进行微调。这种方法使我们的前向模型能够在一次前向传播中联合恢复高保真场景几何、人体点云、相机参数以及一致的度量尺度 SMPL 体。广泛的实验表明，我们的模型在以人为中心的场景重建上实现了最先进的性能，并在全局人体运动估计上提供了高度竞争的结果，优于基于优化的框架和仅使用 HMR 的方法。项目页面：https://murphylmf.github.io/UniSH/&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/&lt;/p&gt;</description></item><item><guid>2601.01229v1</guid><title>NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis</title><link>http://arxiv.org/abs/2601.01229v1</link><author>Furkan Genç, Boran İsmet Macun, Sait Sarper Özaslan, Emine U. Saritas, Tolga Çukur</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的选择性状态空间模型 NeuroSSM，用于端到端分析 fMRI 原始 BOLD 信号，能够同时捕捉快速瞬态动态和慢速大尺度波动，提高对多尺度时间结构的敏感性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统深度学习方法难以在长时间序列中同时捕捉快速与慢速动态；Transformer 虽能处理长程依赖但计算成本高；选择性状态空间模型能高效传播长程依赖，但现有方法多基于功能连接且单尺度处理，限制了对多尺度动态的联合表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有 fMRI 深度学习模型在多尺度时间建模上的不足，提出一种能够同时捕捉快速瞬态和慢速趋势的端到端模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; NeuroSSM 采用多尺度状态空间骨干同时学习快慢动态，并加入并行差分分支增强对瞬态状态变化的敏感性；模型直接处理原始 BOLD 信号，无需先验功能连接。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在临床和非临床数据集上实验表明，NeuroSSM 在性能和效率上与最先进的 fMRI 分析方法相竞争，甚至优于它们。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多尺度状态空间架构结合差分分支能够有效捕捉 fMRI 信号中的多尺度时间结构，为精准脑功能分析提供了新的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确的 fMRI 分析需要对跨多个尺度的时间结构保持敏感，因为 BOLD 信号编码的认知过程从快速瞬态动力学到慢速大尺度波动都有体现。现有的深度学习（DL）方法在联合捕捉这些动态方面面临挑战，尤其是在长时间 fMRI 序列上。当前的 DL 模型中，Transformer 通过显式建模注意力中的成对交互来解决长程依赖，但其二次计算成本限制了在长 fMRI 序列中有效整合时间依赖。选择性状态空间模型（SSM）则通过动力系统中的潜在状态演化隐式建模长程时间依赖，使得依赖关系能够高效传播。然而，最近针对 fMRI 的基于 SSM 的方法通常在派生的功能连接表示上操作，并采用单尺度时间处理。这些设计选择限制了在单一模型中同时表示快速瞬态动态和慢速全局趋势的能力。我们提出 NeuroSSM，一种为端到端分析原始 BOLD 信号的选择性状态空间架构。NeuroSSM 通过两个互补的设计组件解决上述限制：一个多尺度状态空间骨干同时捕捉快速和慢速动态，以及一个并行差分分支提高对瞬态状态变化的敏感性。在临床和非临床数据集上的实验表明，NeuroSSM 在性能和效率上与最先进的 fMRI 分析方法具有竞争力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods.&lt;/p&gt;</description></item><item><guid>2601.01260v1</guid><title>MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance</title><link>http://arxiv.org/abs/2601.01260v1</link><author>Hamad Khan, Saddam Hussain Khan</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; • 论文提出一种基于大型语言模型的 MambaFormer 混合专家框架，用于高效的医学问答和临床辅助。• 该框架通过轻量级门控机制在每个 token 级别动态路由，短而复杂的查询使用定制的 Transformer 专家 ET5，长且高吞吐量的序列使用状态空间模型专家 EMamba。• 专家模型根据输入序列维度、嵌入结构、序列长度和目标特定输出头进行定制，并在新设计的 DentalQA 数据集上通过迁移学习进行微调。• 路由决策基于 token 嵌入的上下文复杂度、归一化序列长度和领域感知特征，确保推理延迟与预测准确性之间的帕累托最优平衡。• 引入了以效用为导向的多目标损失，联合优化决策、路由参数、路由行为、专家利用率和计算成本，动态调节 token 级别专家激活。• 在 DentalQA 和 PubMedQA 数据集上进行留出交叉验证，与最先进方法比较，MambaFormer 在 BERTScore 0.9180、0.077 秒延迟、相较 T5-Large 提升 24.4 倍的速度上表现优异。• 该方法为资源受限的临床部署提供了可扩展的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型在真实临床应用中的部署受限于计算成本与线性时间模型效率之间的基本权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种基于 LLM 的 MambaFormer 混合专家框架，以实现高效的医学问答和临床辅助。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 采用轻量级门控机制在 token 级别进行动态路由；2. 定制 Transformer 专家 ET5 处理短复杂查询；3. 定制状态空间模型专家 EMamba 处理长高吞吐量序列；4. 专家模型根据输入维度、嵌入结构、序列长度和目标输出头进行定制，并在 DentalQA 数据集上迁移学习微调；5. 路由决策基于 token 嵌入上下文复杂度、归一化序列长度和领域特征；6. 引入以效用为导向的多目标损失，联合优化决策、路由参数、路由行为、专家利用率和计算成本；7. 在 DentalQA 和 PubMedQA 数据集上进行留出交叉验证，并与最先进方法进行比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MambaFormer 在 BERTScore 0.9180、0.077 秒延迟、相较 T5-Large 提升 24.4 倍的速度上表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MambaFormer 为资源受限的临床部署提供了可扩展且高效的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 论文提出了一种基于大型语言模型的 MambaFormer 混合专家框架，用于高效的医学问答和临床辅助。该框架通过轻量级门控机制在每个 token 级别动态路由，短而复杂的查询使用定制的 Transformer 专家 ET5，长且高吞吐量的序列使用状态空间模型专家 EMamba。专家模型根据输入序列维度、嵌入结构、序列长度和目标特定输出头进行定制，并在新设计的 DentalQA 数据集上通过迁移学习进行微调。路由决策基于 token 嵌入的上下文复杂度、归一化序列长度和领域感知特征，确保推理延迟与预测准确性之间的帕累托最优平衡。引入了以效用为导向的多目标损失，联合优化决策、路由参数、路由行为、专家利用率和计算成本，动态调节 token 级别专家激活。在 DentalQA 和 PubMedQA 数据集上进行留出交叉验证，与最先进方法比较，MambaFormer 在 BERTScore 0.9180、0.077 秒延迟、相较 T5-Large 提升 24.4 倍的速度上表现优异。该方法为资源受限的临床部署提供了可扩展的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.&lt;/p&gt;</description></item><item><guid>2601.01268v1</guid><title>Accelerated Full Waveform Inversion by Deep Compressed Learning</title><link>http://arxiv.org/abs/2601.01268v1</link><author>Maayan Gelboim, Amir Adler, Mauricio Araya-Polo</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出一种利用深度神经网络进行压缩学习的全波形反演输入降维方法，显著降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现代地震采集系统产生的海量数据使得工业级全波形反演难以处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过减少输入数据维度，降低全波形反演的计算负担。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用带二值感知层的深度网络先选取数据子集，随后用自编码器提取潜在表示，再通过 K‑均值聚类挑选最具代表性的数据进行反演。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 与随机采样相比，该层级选择方法在仅使用 10% 数据时仍能保持甚至提升 2D 全波形反演效果，为大规模 3D 反演加速奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在降维和性能方面均优于传统随机采样，可显著加速大规模全波形反演。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出并测试了一种通过降维全波形反演（FWI）输入来降低计算成本的方法。鉴于现代地震采集系统，工业级案例所需的数据量达到太拉波级存储，因此使用 FWI 解决复杂地下结构或探索多种情景变得不可行。该方法利用带二值感知层的深度神经网络，通过压缩学习从大量地下模型中学习出简洁但重要的地震采集布局。因此，在需要反演的大规模地震数据集中，训练好的网络首先选择一小部分数据，然后通过表示学习，使用自编码器计算数据的潜在表示，随后对潜在表示进行 K‑均值聚类，以进一步挑选最相关的数据用于 FWI。该方法可视为层级选择。实验表明，即使仅使用 10% 的数据进行二维 FWI，该方法也始终优于随机采样，这些结果为在大规模三维反演中加速 FWI 打开了道路。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.&lt;/p&gt;</description></item><item><guid>2601.01298v1</guid><title>Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware</title><link>http://arxiv.org/abs/2601.01298v1</link><author>Jorge L. Ruiz Williams</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了Warp Cortex异步架构，旨在解决多代理大型语言模型在消费者硬件上因线性内存扩展导致的并行推理不可行的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 当前多代理LLM框架的内存随代理数量线性增长，导致在普通硬件上实现系统二级并行推理变得不切实际。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 实现百万级代理的认知扩展，降低内存占用，使多代理LLM在消费级显卡上可行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过将代理逻辑与物理内存解耦，使用单例权重共享和基于拓扑数据分析的拓扑突触技术，将权重内存复杂度降至常数级；将KV缓存视为潜在空间中的点云，采用见证复合稀疏化保留上下文流形的持久同调特征；引入参考注入机制，使异步子代理能够在不打断主流的情况下更新KV缓存。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在单块NVIDIA RTX 4090显卡上，实验显示可同时运行100个代理，仅占用2.2GB总显存；理论上在计算延迟成为瓶颈前，可支持超过1000个代理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Warp Cortex通过显著降低内存复杂度和引入高效的缓存更新机制，证明了在消费级硬件上实现大规模多代理LLM的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 当前多代理大型语言模型（LLM）框架存在线性内存扩展问题，使得在消费级硬件上实现“系统二”并行推理变得不切实际。我们提出了Warp Cortex，一种异步架构，理论上通过将代理逻辑与物理内存解耦，能够实现百万级代理的认知扩展。通过单例权重共享和一种新颖的拓扑突触（受拓扑数据分析中混合地标技术启发），我们将权重的内存复杂度从与代理数和长度成正比降低到常数级别，并将上下文的复杂度从与代理数和长度成正比降低到与代理数和较小参数k成正比，其中k远小于长度。将KV缓存视为潜在空间中的点云后，我们采用见证复合启发的稀疏化方法，保留上下文流形的持久同调特征。在单块NVIDIA RTX 4090显卡上，我们实证演示了100个并发代理仅占用2.2GB总显存，理论上在计算延迟成为瓶颈前，容量可超过1000个代理。我们还引入了参考注入，一种非侵入式KV缓存更新机制，使异步子代理能够在不打断主流的情况下影响主生成。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering &amp;quot;System 2&amp;quot; parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k &amp;lt;&amp;lt; L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.&lt;/p&gt;</description></item><item><guid>2601.01315v1</guid><title>Quantifying Local Strain Field and Deformation in Active Contraction of Bladder Using a Pretrained Transformer Model: A Speckle-Free Approach</title><link>http://arxiv.org/abs/2601.01315v1</link><author>Alireza Asadbeygi, Anne M. Robertson, Yasutaka Tobe, Masoud Zamani, Sean D. Stocker, Paul Watton, Naoki Yoshimura, Simon C Watkins</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种无人工斑点的框架，利用零样本变压器模型CoTracker3和多光子显微镜，准确测量膀胱收缩过程中的局部应变场，验证了其高精度和低误差，并在实验中发现膀胱收缩呈纵向高于环向的各向异性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 膀胱排尿的生物力学研究需要精确的局部应变测量，传统数字图像相关法需要人工斑点，可能改变组织特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种无斑点、非侵入性的应变测量方法，以获得更符合生理的膀胱应变数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用定制的可携式等张双向装置与多光子显微镜配合，采用CoTracker3变压器模型对自然膀胱腔纹理进行跟踪，进行基准测试验证精度，并在四只大鼠体外膀胱收缩实验中应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 方法在基准测试中显示像素精度高、应变误差低；能够捕捉复杂折叠和弯曲的异质变形；在四只大鼠实验中发现纵向收缩显著高于环向，且多光子显微镜显示收缩时出现大折叠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该无斑点框架消除了斑点引起的伪影，提供更生理相关的应变测量，可广泛应用于其他生物和工程材料的力学测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在膀胱收缩过程中准确量化局部应变场对于理解膀胱排尿的生物力学，无论在健康还是疾病状态下，都是至关重要的。传统的数字图像相关（DIC）方法已成功应用于多种生物组织；然而，该方法需要人工斑点，可能改变组织的被动和主动特性。本文提出了一种无斑点的框架，利用先进的零样本变压器模型CoTracker3来量化局部应变场。我们使用定制的、可携式等张双向装置与多光子显微镜（MPM）兼容，演示了该方法，成功跟踪了自然膀胱腔纹理，无需人工标记。基准测试验证了该方法在像素精度和应变误差方面的高性能。该框架能够有效捕捉异质变形模式，即使在复杂的折叠和弯曲情况下，传统DIC往往难以跟踪。将该方法应用于四只大鼠体外活性膀胱收缩实验（n=4），发现存在显著的各向异性，纵向收缩高于环向收缩。多光子显微镜进一步展示并证实了异质形态变化，例如在活性收缩期间形成的大折叠。该非侵入性方法消除了斑点引起的伪影，能够实现更符合生理的测量，并具有广泛的适用性，可用于其他生物和工程系统的材料测试。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate quantification of local strain fields during bladder contraction is essential for understanding the biomechanics of bladder micturition, in both health and disease. Conventional digital image correlation (DIC) methods have been successfully applied to various biological tissues; however, this approach requires artificial speckling, which can alter both passive and active properties of the tissue. In this study, we introduce a speckle-free framework for quantifying local strain fields using a state-of-the-art, zero-shot transformer model, CoTracker3. We utilized a custom-designed, portable isotonic biaxial apparatus compatible with multiphoton microscopy (MPM) to demonstrate this approach, successfully tracking natural bladder lumen textures without artificial markers. Benchmark tests validated the method&amp;#x27;s high pixel accuracy and low strain errors. Our framework effectively captured heterogeneous deformation patterns, despite complex folding and buckling, which conventional DIC often fails to track. Application to in vitro active bladder contractions in four rat specimens (n=4) revealed statistically significant anisotropy (p&amp;lt;0.01), with higher contraction longitudinally compared to circumferentially. Multiphoton microscopy further illustrated and confirmed heterogeneous morphological changes, such as large fold formation during active contraction. This non-invasive approach eliminates speckle-induced artifacts, enabling more physiologically relevant measurements, and has broad applicability for material testing of other biological and engineered systems.&lt;/p&gt;</description></item><item><guid>2601.01321v1</guid><title>Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models</title><link>http://arxiv.org/abs/2601.01321v1</link><author>Rong Zhou, Dongping Chen, Zihan Jia, Yao Su, Yixin Liu, Yiwen Lu, Dongwei Shi, Yue Huang, Tianyang Xu, Yi Pan, Xinliang Li, Yohannes Abate, Qingyu Chen, Zhengzhong Tu, Yu Yang, Yu Zhang, Qingsong Wen, Gengchen Mai, Sunyang Fu, Jiachen Li, Xuyu Wang, Ziran Wang, Jing Huang, Tianming Liu, Yong Chen, Lichao Sun, Lifang He</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个统一的四阶段框架，系统描述了人工智能在数字孪生生命周期中的整合，包括建模、镜像、干预和自治管理，并探讨了物理建模与数据驱动学习的协同、生成式人工智能的作用以及跨领域应用中的挑战与责任方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 数字孪生已从被动仿真工具发展为通过人工智能技术实现的智能自治实体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 系统化阐述人工智能如何嵌入数字孪生生命周期，并提供一个统一的四阶段框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 综合现有技术与实践，构建四阶段框架，分析物理建模与数据驱动学习的协同，回顾十一大应用领域的案例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 框架显示从传统物理建模到物理信息化和基础模型的演进，生成式人工智能使数字孪生具备主动推理和自我改进能力，且普遍面临可扩展性、可解释性和可信度挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AI驱动的数字孪生可实现主动、自我改进的认知系统，但需通过负责任的人工智能实践解决可扩展性、可解释性和可信度问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 数字孪生作为物理系统的精确数字表示，已从被动仿真工具演变为通过人工智能技术集成的智能自治实体。本文提出了一个统一的四阶段框架，系统地描述了人工智能在数字孪生生命周期中的整合，涵盖建模、镜像、干预和自治管理。通过综合现有技术和实践，我们提炼出一个统一的四阶段框架，系统地描述了人工智能方法如何嵌入数字孪生生命周期：1）通过基于物理和物理信息化的人工智能方法对物理孪生进行建模；2）通过实时同步将物理系统镜像为数字孪生；3）通过预测建模、异常检测和优化策略对物理孪生进行干预；4）通过大型语言模型、基础模型和智能代理实现自治管理。我们分析了基于物理建模与数据驱动学习的协同，强调从传统数值求解器向物理信息化和基础模型的转变。进一步，我们探讨了包括大型语言模型和生成世界模型在内的生成式人工智能技术如何将数字孪生转变为主动且自我改进的认知系统，具备推理、沟通和创造性情景生成能力。通过跨领域综述，涵盖医疗、航空航天、智能制造、机器人和智慧城市等十一大应用领域，我们识别了与可扩展性、可解释性和可信度相关的共同挑战，并概述了负责任的人工智能驱动数字孪生系统的发展方向。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.&lt;/p&gt;</description></item><item><guid>2601.01339v1</guid><title>Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning</title><link>http://arxiv.org/abs/2601.01339v1</link><author>Weihang You, Hanqi Jiang, Yi Pan, Junhao Chen, Tianming Liu, Fei Dou</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 NeuroAlign 框架，通过双阶段机制实现 fMRI 与视频的细粒度对齐，显著提升跨模态检索性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 神经对视觉刺激的响应难以理解，现有方法无法捕捉层级和时间过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有方法的局限，提供更精细的 fMRI-视频对齐方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; NeuroAlign 采用两阶段机制：NTCL 进行全局语义对齐，增强向量量化进行细粒度匹配，并使用 DynaSyncMM-EMA 进行动态多模态融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 NeuroAlign 在跨模态检索任务中显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; NeuroAlign 为理解视觉认知机制提供了新的范式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解神经对视觉刺激的反应仍然具有挑战性，因为大脑表征的内在复杂性以及神经数据与视觉输入之间的模态差距。现有方法主要基于将神经解码简化为生成任务或简单相关性，未能反映大脑视觉处理的层级和时间过程。为解决这些局限，我们提出了 NeuroAlign，一种受人类视觉系统层级组织启发的细粒度 fMRI-视频对齐新框架。该框架实现了两阶段机制，模仿生物视觉通路：通过 Neural-Temporal Contrastive Learning (NTCL) 进行全局语义理解，并通过增强向量量化进行细粒度模式匹配。NTCL 通过双向预测两种模态来显式建模时间动态，而我们的 DynaSyncMM-EMA 方法实现了动态多模态融合与自适应加权。实验表明，NeuroAlign 在跨模态检索任务中显著优于现有方法，确立了理解视觉认知机制的新范式。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.&lt;/p&gt;</description></item><item><guid>2601.01356v1</guid><title>Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance</title><link>http://arxiv.org/abs/2601.01356v1</link><author>Dang H. Pham, Tu N. Nguyen, Hoa N. Nguyen</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本论文提出了三种先进方法，分别针对监督学习、无监督域适应和完全无监督的行人重识别任务，显著提升了模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 行人重识别在智能监控系统中至关重要，但面临外观差异、域漂移和标注数据稀缺等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过创新的学习策略和域适应技术，克服上述限制，提升跨摄像头、跨域的识别准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) SCM‑ReID：结合监督对比学习与多重损失（分类、中心、三元组、质心三元组），实现更具判别力的特征表示。2) IQAGA 与 DAPRH：利用 GAN 图像增强、域不变映射和伪标签细化，降低域差异并提升跨域泛化。3) ViTC‑UReID：采用 Vision Transformer 编码与摄像头感知代理学习，融合全局与局部注意力及摄像头身份约束，提升无监督重识别效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SCM‑ReID 在 Market‑1501 与 CUHK03 数据集上取得了最先进的准确率；IQAGA 与 DAPRH 在域适应实验中 mAP 与 Rank‑1 提升可达 12%；ViTC‑UReID 在 CUHK03、Market‑1501、DukeMTMC‑reID 与 MSMT17 等大规模基准上显著优于现有无监督方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 三种方法分别解决了特征学习、域适应和标签噪声问题，为行人重识别研究提供了新的思路，并为在真实监控环境中的稳健部署奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 行人重识别（ReID）在智能监控系统中起着关键作用，通过在复杂环境中跨多摄像头关联身份来实现。然而，ReID 面临外观变化、域漂移和标注数据有限等重大挑战。本论文提出了三种先进方法，以提升监督、无监督域适应（UDA）和完全无监督设置下的 ReID 性能。首先，SCM‑ReID 将监督对比学习与混合损失优化（分类、中心、三元组和质心三元组损失）相结合，提升判别特征表示，并在 Market‑1501 与 CUHK03 数据集上实现了最先进的准确率。其次，对于 UDA，IQAGA 与 DAPRH 结合基于 GAN 的图像增强、域不变映射和伪标签细化，以缓解域差异并提升跨域泛化。实验表明，在具有挑战性的迁移场景中，mAP 与 Rank‑1 的提升可达 12%。最后，ViTC‑UReID 利用 Vision Transformer‑based 特征编码和摄像头感知代理学习来提升无监督 ReID。通过整合全局与局部注意力以及摄像头身份约束，该方法在大型基准上显著优于现有无监督方法。对 CUHK03、Market‑1501、DukeMTMC‑reID 与 MSMT17 的全面评估确认了所提方法的有效性。本文的贡献通过解决特征学习、域适应和标签噪声处理等关键限制，推动了 ReID 研究的发展，为在真实监控系统中的稳健部署铺平了道路。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.&lt;/p&gt;</description></item><item><guid>2601.01362v1</guid><title>Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning</title><link>http://arxiv.org/abs/2601.01362v1</link><author>Jerry Huang, Peng Lu, Qiuhao Zeng, Yusuke Iwasawa, Yutaka Matsuo, Sarath Chandar, Edison Marrese-Taylor, Irene Li</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了大型语言模型在多语言环境下的预测不确定性校准问题，指出数据稀缺会导致校准失衡，并评估了常用技术的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度学习模型的可信度与可靠性取决于其预测不确定性的良好校准。尽管基础模型研究不断进步，但大型语言模型与校准之间的关系仍是未解之谜，尤其在多语言场景中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究多语言设置下大型语言模型的校准缺口，了解数据稀缺如何影响校准效果，并检验常用技术在此环境中的适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在两个多语言基准（分别覆盖29种和42种语言）上进行分析，使用高资源语言的指令微调（SFT）数据进行指令调优，并尝试标签平滑（label smoothing）方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 即使在低资源语言中，使用高资源语言的SFT数据进行指令调优也能显著提升模型置信度，但准确率提升有限或不存在，导致校准失衡；而标签平滑在不需要低资源SFT数据的情况下，能够在所有语言中保持更好的校准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多语言考虑在训练和调优大型语言模型时至关重要，可提升其在下游任务中的可靠性和公平性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 确保深度学习模型在预测不确定性方面得到良好校准对于维护其可信度和可靠性至关重要，然而尽管基础模型研究不断进步，关于大型语言模型（LLMs）与其校准之间的关系仍是一个开放的研究领域。在本研究中，我们关注多语言环境下LLMs校准的关键空白，试图更好地理解数据稀缺如何可能导致不同的校准效应，以及常用技术在这些环境中的适用性。我们在两个多语言基准上进行分析，分别覆盖29种和42种语言，结果表明，即使在低资源语言中，模型置信度在使用高资源语言SFT数据进行指令调优后也会显著提升。然而，准确率的提升是微不足道的或不存在的，导致误校准，凸显了标准SFT在多语言环境中的一个关键缺陷。此外，我们观察到使用标签平滑是一种合理的方法来缓解这一问题，再次无需低资源SFT数据，能够在所有语言中保持更好的校准。总体而言，这强调了在训练和调优LLMs时考虑多语言因素的重要性，以提升其在下游使用中的可靠性和公平性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.&lt;/p&gt;</description></item><item><guid>2601.01364v1</guid><title>Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography</title><link>http://arxiv.org/abs/2601.01364v1</link><author>Mostofa Rafid Uddin, Mahek Vora, Qifeng Wu, Muyuan Chen, Min Xu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种深度学习框架，能够将冷冻电子断层扫描数据中的空间变换与宏分子形态内容在表示空间中解耦，从而更准确地推断宏分子的形态并发现新的结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 冷冻电子断层扫描可以直接三维可视化细胞内的大分子，但由于噪声大和空间变换多样，提取其原位形态十分困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种无需大量手动调参、能够同时估计宏分子形态及其空间变换且不遗漏稀有形态的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建一个解耦表示学习框架，包含一个多选学习模块，用于在高度噪声的数据中将 SE(3) 变换与形态内容分离，并利用学习到的形态内容生成模板形态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在模拟和真实数据集上的实验表明，该方法相较于传统期望最大化方法有显著提升，并发现了此前未识别的宏分子形态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架有效解耦空间变换与形态内容，提升了形态推断精度，并促进了新结构的发现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 冷冻电子断层扫描（cryo-ET）能够直接三维可视化细胞内的大分子，使其原位形态得以分析。该形态可视为从断层图像中提取的子体积的 SE(3) 不变、去噪体积表示。推断形态实际上是一个逆问题，需要同时估计模板形态及其 SE(3) 变换。现有基于期望最大化的解决方案往往忽略稀有但重要的形态，并且需要大量手动调参。为解决此问题，我们提出了一个解耦深度表示学习框架，将 SE(3) 变换与形态内容在表示空间中分离。该框架包含一个新颖的多选学习模块，能够在高度噪声的 cryo-ET 数据中实现此解耦，并利用学习到的形态内容生成模板形态。对模拟和真实 cryo-ET 数据集的实验表明，该方法相较于以往方法有明显提升，并发现了此前未识别的宏分子形态。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.&lt;/p&gt;</description></item><item><guid>2601.01373v1</guid><title>UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models</title><link>http://arxiv.org/abs/2601.01373v1</link><author>Qundong Shi, Jie Zhou, Biyuan Lin, Junbo Cui, Guoyang Zeng, Yixuan Zhou, Ziyang Wang, Xin Liu, Zhen Luo, Yudong Wang, Zhiyuan Liu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 UltraEval-Audio，一个统一的音频基础模型评估框架，解决评估碎片化、音频编解码器评估缺失以及中文评测不足等问题，并提供多语言、多任务、模型与基准整合以及实时排行榜。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着 GPT‑4o 的出现，音频基础模型发展迅速，但缺乏全面评估成为进一步进步的瓶颈，尤其在音频生成领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决音频评估缺乏统一框架、编解码器评估方法不完善以及现有评测过度依赖英语等三大挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①构建 UltraEval‑Audio，模块化架构支持10种语言、14类核心任务，集成24主流模型与36权威基准；②提供一键评测与实时排行榜；③为编解码器设计语义准确度、音色保真度、声学质量三维评估方案；④推出 SpeechCMMLU 与 SpeechHSK 两个中文评测基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; UltraEval‑Audio 成功整合多语言、多任务与多模型评测资源，提出全面的编解码器评估维度，并补充了中文知识与流利度评测基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UltraEval‑Audio 为学术界与工业界提供了透明、高效、公平的音频模型比较平台，有望推动音频基础模型的进一步发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 音频基础模型的发展自 GPT‑4o 出现以来迅速加速。然而，缺乏全面评估已成为该领域进一步进步的关键瓶颈，尤其是在音频生成方面。目前的音频评估面临三大挑战：（1）缺乏统一框架，数据集和代码分散在各个来源，阻碍了公平高效的跨模型比较；（2）音频编解码器作为音频基础模型的关键组件，缺乏广泛接受的整体评估方法；（3）现有语音基准过度依赖英语，使得客观评估模型在中文上的表现变得困难。为解决第一个问题，我们推出 UltraEval‑Audio，这是一个专为音频理解和生成任务设计的统一评估框架。UltraEval‑Audio 采用模块化架构，支持10种语言和14个核心任务类别，并无缝集成24个主流模型和36个权威基准。为提升研究效率，该框架提供一键评测功能，并配有实时公开排行榜。针对第二个挑战，UltraEval‑Audio 采用一种新颖的全面评估方案，对音频编解码器在语义准确度、音色保真度和声学质量三维度进行评估。为解决第三个问题，我们提出了两个新的中文基准，SpeechCMMLU 和 SpeechHSK，用于评估中文知识熟练度和语言流利度。我们希望 UltraEval‑Audio 能为学术界和工业界提供一个透明、高效、公平的音频模型比较平台。我们的代码、基准和排行榜可在 https://github.com/OpenBMB/UltraEval‑Audio 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The development of audio foundation models has accelerated rapidly since the emergence of GPT-4o. However, the lack of comprehensive evaluation has become a critical bottleneck for further progress in the field, particularly in audio generation. Current audio evaluation faces three major challenges: (1) audio evaluation lacks a unified framework, with datasets and code scattered across various sources, hindering fair and efficient cross-model comparison;(2) audio codecs, as a key component of audio foundation models, lack a widely accepted and holistic evaluation methodology; (3) existing speech benchmarks are heavily reliant on English, making it challenging to objectively assess models&amp;#x27; performance on Chinese. To address the first issue, we introduce UltraEval-Audio, a unified evaluation framework for audio foundation models, specifically designed for both audio understanding and generation tasks. UltraEval-Audio features a modular architecture, supporting 10 languages and 14 core task categories, while seamlessly integrating 24 mainstream models and 36 authoritative benchmarks. To enhance research efficiency, the framework provides a one-command evaluation feature, accompanied by real-time public leaderboards. For the second challenge, UltraEval-Audio adopts a novel comprehensive evaluation scheme for audio codecs, evaluating performance across three key dimensions: semantic accuracy, timbre fidelity, and acoustic quality. To address the third issue, we propose two new Chinese benchmarks, SpeechCMMLU and SpeechHSK, designed to assess Chinese knowledge proficiency and language fluency. We wish that UltraEval-Audio will provide both academia and industry with a transparent, efficient, and fair platform for comparison of audio models. Our code, benchmarks, and leaderboards are available at https://github.com/OpenBMB/UltraEval-Audio.&lt;/p&gt;</description></item><item><guid>2601.01376v1</guid><title>Classifying Core-Collapse Supernova Gravitational Waves using Supervised Contrastive Learning</title><link>http://arxiv.org/abs/2601.01376v1</link><author>Ao-Bo Wang, Yong Yuan, Hao Cai, Xi-Long Fan</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究利用对比学习预训练的 ResNet-50 编码器，构建深度学习框架以检测和重建核心坍缩超新星产生的引力波。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 核心坍缩超新星的引力波信号高度随机且噪声复杂，传统方法难以有效识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过对比学习优化特征空间，提高信号与噪声的区分度，从而提升检测效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用四台探测器的模拟网络，注入磁旋转和中微子驱动波形，采用监督对比学习训练 ResNet-50 编码器，最大化同类紧凑度和异类可分性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在误报率为百分之0.01的条件下，方法在10到200千秒差距范围内对旋转和中微子驱动信号的真正率接近百分之百，在1200千秒差距时约为百分之八十；传统端到端方法在距离大于等于200千秒差距时对旋转信号的真正率低于百分之二十，即使在10千秒差距也无法超过百分之六十。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 对比学习框架显著提升了核心坍缩超新星引力波的检测性能，尤其在远距离和低信噪比场景下优于传统方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文探讨了利用对比学习预训练的 ResNet-50 编码器来检测和重建核心坍缩超新星产生的引力波。该方法通过优化特征空间，增强了信号与噪声的区分度。使用四台探测器的模拟网络，注入磁旋转和中微子驱动波形，实验表明在误报率为百分之0.01的条件下，方法在10到200千秒差距范围内对旋转和中微子驱动信号的真正率接近百分之百，在1200千秒差距时约为百分之八十。相比之下，传统端到端方法在距离大于等于200千秒差距时对旋转信号的真正率低于百分之二十，即使在10千秒差距也无法超过百分之六十。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The detection and reconstruction of gravitational waves from core-collapse supernovae (CCSN) present significant challenges due to the highly stochastic nature of the signals and the complexity of detector noise. In this work, we introduce a deep learning framework utilizing a ResNet-50 encoder pre-trained via supervised contrastive learning to classify CCSN signals and distinguish them from instrumental noise artifacts. Our approach explicitly optimizes the feature space to maximize intra-class compactness and inter-class separability. Using a simulated four-detector network (LIGO Hanford, LIGO Livingston, Virgo, and KAGRA) and realistic datasets injecting magnetorotational and neutrino-driven waveforms, we demonstrate that the contrastive learning paradigm establishes a superior metric structure within the embedding space, significantly enhancing detection efficiency. At a false positive rate of $10^{-4}$, our method achieves a true positive rate (TPR) of nearly $100\%$ for both rotational and neutrino-driven signals within a distance range of $10$--$200$~kpc, while maintaining a TPR of approximately $80\%$ at $1200$~kpc. In contrast, traditional end-to-end methods yield a TPR below $20\%$ for rotational signals at distances $\geq 200$~kpc, and fail to exceed $60\%$ for neutrino-driven signals even at a close proximity of $10$~kpc.&lt;/p&gt;</description></item><item><guid>2601.01399v1</guid><title>Quaternion optical computing chip for parallel high-dimensional data processing</title><link>http://arxiv.org/abs/2601.01399v1</link><author>Songyue Liu, Qi Lu, Yuan Zhong, Yuru Li, Meng Xiang, Zhaohui Li, Chao Lu, Yikai Su, Lu Sun</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文展示了首个四元数光学计算芯片，并在三维点云处理、RGB色彩转换和四元数卷积神经网络等应用中验证其性能，显示出更高的计算精度和更低的计算负载。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的实数和复数光学计算芯片在高维数据表示上存在局限，限制了其在现代信号处理中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发并验证一种能够直接处理三维和四维数据的四元数光学计算芯片，以提升高维数据处理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用波分复用实现光的高并行性，通过多波长通道同时处理高维数据，构建四元数光学计算芯片，并在三维点云、RGB色彩转换和四元数卷积神经网络等场景中进行基准测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; QOCC 在计算精度上优于电子计算对应器件（均方根误差小于0.035），并将计算负载降低约三分之二。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 四元数光学计算芯片为下一代光学计算奠定基础，克服了传统系统在高维数据处理方面的限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Optical computing chips have emerged as a transformative computing technology due to their high computational density, low energy consumption, and compact footprint. While real- and complex-valued computing chips have been well developed, their fundamental limitations in representing high-dimensional data significantly constrain their applicability in modern signal processing. Quaternions enable direct operations on three- and four-dimensional data, powering high-dimensional processing in data analytics and artificial intelligence. Here we demonstrate a quaternion optical computing chip (QOCC) for the first time and benchmark its performance in several typical application scenarios: three-dimensional point cloud processing, RGB chromatic transformation, and quaternion convolutional neural network for color image recognition. The QOCC harnesses high parallelism of light by wavelength-division multiplexing, processing high-dimensional data simultaneously through multiple optical wavelength channels. Compared to the electronic computing counterpart, our QOCC achieves higher computational fidelity (root mean square error less than 0.035) and substantially reduced computational load (two-thirds lower). It paves the way towards next-generation optical computing, overcoming the limitations of traditional computing systems in high-dimensional data processing.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Optical computing chips have emerged as a transformative computing technology due to their high computational density, low energy consumption, and compact footprint. While real- and complex-valued computing chips have been well developed, their fundamental limitations in representing high-dimensional data significantly constrain their applicability in modern signal processing. Quaternions enable direct operations on three- and four-dimensional data, powering high-dimensional processing in data analytics and artificial intelligence. Here we demonstrate a quaternion optical computing chip (QOCC) for the first time and benchmark its performance in several typical application scenarios: three-dimensional point cloud processing, RGB chromatic transformation, and quaternion convolutional neural network for color image recognition. The QOCC harnesses high parallelism of light by wavelength-division multiplexing, processing high-dimensional data simultaneously through multiple optical wavelength channels. Compared to the electronic computing counterpart, our QOCC achieves higher computational fidelity (root mean square error &amp;lt; 0.035) and substantially reduced computational load (2/3 lower). It paves the way towards next-generation optical computing, overcoming the limitations of traditional computing systems in high-dimensional data processing.&lt;/p&gt;</description></item><item><guid>2601.01416v1</guid><title>AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval</title><link>http://arxiv.org/abs/2601.01416v1</link><author>Yue Zhou, Ran Ding, Xue Yang, Xue Jiang, Xingzhao Liu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个针对无人机拍摄车辆图像的空间感知数据集 AirSpatial，并基于此开发了能够进行空间定位和空间问答的视觉语言模型。通过两阶段训练，模型在空间理解上得到显著提升，并进一步构建了 AirSpatialBot 代理，能够完成细粒度车辆属性识别与检索。实验验证了该方法的有效性，并揭示了现有视觉语言模型在空间理解方面的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 远程感知视觉语言模型在实际应用中常因缺乏空间理解能力而受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 推动远程感知视觉语言模型在空间任务上的发展，解决车辆图像的空间定位与问答问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建 AirSpatial 数据集，包含 206K 条指令，提供 3D 边界框；提出空间定位和空间问答两项新任务；采用两阶段训练：图像理解预训练和空间理解微调；基于训练好的模型开发 AirSpatialBot，集成任务规划、图像理解、空间理解和执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明现有 VLM 在空间理解上存在明显不足，而经过空间感知训练的模型和 AirSpatialBot 在车辆属性识别与检索任务上表现更好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 空间感知数据集和两阶段训练策略显著提升了远程感知视觉语言模型的空间能力，为无人机车辆监测等应用提供了有效工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从航拍图像中进行细粒度车辆属性识别和检索的问题。该问题在交通监控、公共安全和智能交通系统中具有重要意义，因为准确识别车辆品牌、型号、颜色等属性有助于事件追踪、违法行为检测和交通流量分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计时考虑到航拍图像中车辆的空间关系和多尺度特征，借鉴了现有的目标检测、属性识别和图像检索技术，并在此基础上引入了空间感知机制，使模型能够利用车辆之间的相对位置信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个空间感知的航空代理（AirSpatialBot），通过先检测车辆、提取多尺度特征、应用空间注意力机制来增强特征表示，然后进行细粒度属性分类并计算相似度实现检索。实现流程包括图像预处理、车辆检测、特征提取、空间关系建模、属性识别与检索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入空间感知机制以捕捉航拍图像中车辆的相对位置；2) 将属性识别与检索任务统一到同一框架；3) 在细粒度车辆属性上取得新的性能基准。与以往工作相比，本文更强调空间上下文的利用，并提供了端到端的检索系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AirSpatialBot提出了一种空间感知的航空代理，能够同时完成细粒度车辆属性识别和检索，并在航拍数据集上实现了新的性能记录。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot&lt;/p&gt;</description></item><item><guid>2601.01425v1</guid><title>DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer</title><link>http://arxiv.org/abs/2601.01425v1</link><author>Xu Guo, Fulong Ye, Xinghui Li, Pengqi Tu, Pengze Zhang, Qichao Sun, Songtao Zhao, Xiangwang Hou, Qian He</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个完整的框架，将图像人脸交换技术迁移到视频领域，并通过新数据管道、Diffusion Transformer、课程学习和强化学习等方法提升视觉真实性和身份一致性，构建了新的基准数据集，实验表明该方法优于现有技术并具有广泛适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频人脸交换需要在保持姿态、表情、光照、背景和动态信息的同时注入源身份，但现有方法难以同时保持身份相似性、属性保留和时间一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决视频人脸交换中身份相似性、属性保留和时间一致性难题，将图像人脸交换的优势迁移到视频域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 SyncID-Pipe 数据管道预训练身份锚定视频合成器并与 IFS 模型结合生成双向 ID 四元组；基于配对数据构建 DreamID-V Diffusion Transformer 框架，使用模态感知条件模块注入多模态条件；引入 Synthetic-to-Real 课程机制和身份一致性强化学习策略；创建 IDBench-V 基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DreamID-V 在实验中优于现有方法，显示出卓越的视觉真实性、身份一致性和适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架有效提升视频人脸交换质量，可无缝适用于多种交换相关任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视频人脸交换（VFS）需要无缝地将源身份注入目标视频，同时精细地保留原始姿态、表情、光照、背景和动态信息。现有方法在保持身份相似性和属性保留的同时维持时间一致性方面存在困难。为解决这一挑战，我们提出了一个全面的框架，将图像人脸交换（IFS）的优势无缝迁移到视频领域。我们首先引入了新颖的数据管道 SyncID-Pipe，预训练身份锚定视频合成器，并与 IFS 模型结合，构建双向 ID 四元组进行显式监督。基于配对数据，我们提出了首个基于扩散 Transformer 的框架 DreamID-V，采用核心模态感知条件模块来区分性地注入多模态条件。同时，我们提出了 Synthetic-to-Real 课程机制和身份一致性强化学习策略，以在挑战场景下提升视觉真实性和身份一致性。为了解决基准有限的问题，我们引入了 IDBench-V，一个涵盖多样场景的综合基准。大量实验表明 DreamID-V 超越了最先进的方法，并进一步展示了卓越的多功能性，可无缝适应各种交换相关任务。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.&lt;/p&gt;</description></item><item><guid>2601.01450v1</guid><title>Topology-Informed Jet Tagging using Persistent Homology</title><link>http://arxiv.org/abs/2601.01450v1</link><author>Saurav Mittal</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种基于拓扑学的粒子喷射分类方法，利用持久同调分析点云结构，并通过卷积神经网络实现夸克-胶子喷射的区分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 粒子喷射是质子-质子碰撞产生的粒子级联，传统方法未充分利用拓扑信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索拓扑特征（尤其是环状结构）在喷射子结构识别中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将喷射成分映射到三维特征空间，使用 Vietoris-Rips 过滤得到持久图，再转换为持久图像，分别对应零维和一维同调群，作为卷积网络输入进行分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 零维和一维同调图像各自具有相当的判别能力，说明环状拓扑特征携带重要信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 环状拓扑特征为喷射分类提供了新的有效信息，拓扑信息的利用是一个有前景的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种基于拓扑学的方法，用持久同调来对粒子喷射进行分类，持久同调是一种捕捉点云结构属性的框架。质子-质子碰撞产生的粒子喷射由一系列源自共同硬相互作用的粒子级联组成。每个喷射成分被表示为三维特征空间中的一个点，该空间由相对横向动量和相对于喷射轴的角坐标定义，从而得到每个喷射的点云描述。使用 Vietoris-Rips 过滤计算持久同调，得到持久图，然后将其转换为持久图像，分别对应零维和一维同调群，分别对应连通分量和环状结构。这些持久图像被用作卷积神经网络的输入，用于夸克-胶子喷射分类。使用公开的 HLS4ML 喷射数据集，我们发现来自零维和一维的持久图像各自具有可比的判别能力。这些结果表明，环状拓扑特征——传统上未被利用的喷射标记信息——编码了关于喷射子结构的有意义信息，并为基于拓扑的喷射分类提供了有前景的方向。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a topology-informed approach for classifying particle jets using persistent homology, a framework that captures the structural properties of point clouds. Particle jets produced in proton-proton collisions consist of cascades of particles originating from a common hard interaction. Each jet constituent is represented as a point in a three-dimensional feature space defined by the relative transverse momentum and angular coordinates with respect to the jet axis, yielding a point cloud description of each jet. Persistent homology is computed using a Vietoris-Rips filtration to obtain persistence diagrams, which are subsequently converted into persistence images for the H0 and H1 homology groups, corresponding to connected components and loop-like structures, respectively. These persistence images are used as inputs to a convolutional neural network for quark-gluon jet classification. Using the publicly available HLS4ML jet dataset, we find that persistence images derived from H0 and H1 individually exhibit comparable discriminative power. These results demonstrate that loop-like topological features, which are not conventionally exploited in jet tagging, encode meaningful information about jet substructure and provide a promising direction for topology-informed jet classification.&lt;/p&gt;</description></item><item><guid>2601.01456v1</guid><title>Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration</title><link>http://arxiv.org/abs/2601.01456v1</link><author>Wentao Bian, Fenglei Xu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种解耦专家仲裁的少样本分割网络，解决了多模态点云分割中的可塑性-稳定性冲突和语义盲区问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态少样本3D点云语义分割常采用先融合后细化方法，但存在可塑性与稳定性之间的矛盾，并且CLIP模型的类间混淆会导致语义识别失效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述冲突和语义盲区，提升模型的泛化能力和模态利用率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计DA-FSS模型，采用并行专家细化模块生成模态关联，堆叠仲裁模块进行卷积融合并仲裁关联，解耦专家分别维护几何可塑性和语义稳定性，并通过解耦对齐模块传递知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在S3DIS和ScanNet数据集上，DA-FSS优于MM-FSS，且在几何边界、完整性和纹理区分方面均优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 解耦专家仲裁策略有效解决了可塑性-稳定性困境，提升了多模态少样本点云分割的性能和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文重新审视多模态少样本3D点云语义分割（FS-PCS），发现“先融合后细化”范式中的冲突：可塑性-稳定性困境。同时，CLIP的类间混淆可能导致语义盲区。为解决这些问题，我们提出了解耦专家仲裁少样本分割网络（DA-FSS），该模型能够有效区分语义路径和几何路径，并相互正则化梯度以实现更好的泛化。DA-FSS采用与MM-FSS相同的骨干网络和预训练文本编码器生成文本嵌入，可提升自由模态的利用率，更好地利用每种模态的信息空间。为此，我们提出了并行专家细化模块来生成每个模态的关联。我们还提出了堆叠仲裁模块（SAM），用于卷积融合并仲裁每个模态路径的关联。并行专家解耦两条路径：几何专家保持可塑性，语义专家确保稳定性。它们通过解耦对齐模块（DAM）协调，传递知识而不传播混淆。对流行数据集（S3DIS、ScanNet）的实验表明，DA-FSS优于MM-FSS。同时，几何边界、完整性和纹理区分均优于基线。代码可在 https://github.com/MoWenQAQ/DA-FSS 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in &amp;quot;Fuse-then-Refine&amp;quot; paradigms: the &amp;quot;Plasticity-Stability Dilemma.&amp;quot; In addition, CLIP&amp;#x27;s inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities&amp;#x27; utilization rate and better leverage each modality&amp;#x27;s information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.&lt;/p&gt;</description></item><item><guid>2601.01457v1</guid><title>Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2601.01457v1</link><author>Mingxing Zhan, Li Zhang, Beibei Wang, Yingjie Wang, Zenglin Shi</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种在冻结基础模型的前提下，通过图像特定的仿射变换恢复度量深度，并利用语言不确定性包围来预测可行的标定参数，最终在多尺度视觉特征中选择最佳标定，从而在多数据集上提升单目深度估计的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 相对深度基础模型迁移效果好，但单目度量深度因全局尺度不可辨识和域迁移敏感性而难以确定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在冻结基础模型的情况下，恢复度量深度并提高跨域鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用图像特定的逆深度仿射变换，训练轻量级标定头，保持相对深度骨干和CLIP文本编码器不变；利用语言预测不确定性包围，限定可行标定参数；在多尺度冻结视觉特征中选取图像特定标定；训练时使用逆深度的闭式最小二乘监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在NYUv2和KITTI上实现了更高的域内准确率；在SUN-RGBD和DDAD的零样本迁移中相较于仅使用语言的基线表现出更好的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过冻结骨干并结合语言不确定性包围和多尺度视觉特征选择，能够有效恢复度量深度并提升跨域性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 相对深度基础模型迁移效果良好，但单目度量深度由于全局尺度无法辨识和域迁移敏感性而仍然是一个不确定的问题。在冻结骨干的标定设置下，我们通过在逆深度上进行图像特定的仿射变换来恢复度量深度，并仅训练轻量级的标定头，同时保持相对深度骨干和CLIP文本编码器不变。由于字幕提供的尺度线索粗略且噪声大，且会因措辞和缺失对象而变化，我们使用语言来预测一个不确定性感知的包围，限定可行的标定参数在一个无约束空间内，而不是仅给出文本的点估计。随后，我们利用池化的多尺度冻结视觉特征在该包围内选择图像特定的标定。在训练过程中，逆深度的闭式最小二乘解为每张图像提供监督，用于学习包围和所选标定。对NYUv2和KITTI的实验表明在域内准确率得到提升，而在SUN-RGBD和DDAD的零样本迁移中相较于强大的仅语言基线显示出更好的鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在解决单目深度估计中的度量尺度恢复问题。单目深度只能得到相对深度，缺乏绝对尺度，而绝对尺度在机器人导航、增强现实、三维重建等应用中至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.&lt;/p&gt;</description></item><item><guid>2601.01473v1</guid><title>Accelerating Storage-Based Training for Graph Neural Networks</title><link>http://arxiv.org/abs/2601.01473v1</link><author>Myung-Hwan Jang, Jeong-Min Park, Yunyong Ko, Sang-Wook Kim</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 AGNES 的存储式图神经网络训练框架，利用块级存储 I/O 处理和超批量处理技术，显著提升了大规模图数据的训练效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着真实世界图规模不断扩大，传统 GNN 训练难以在单机上处理海量数据，研究者开始探索利用外部存储（如 NVMe SSD）进行存储式训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有存储式 GNN 训练方法在数据准备阶段因大量小 I/O 操作导致的瓶颈问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; AGNES 通过块级存储 I/O 处理充分利用高性能存储带宽，并结合基于图特征的超批量处理策略，提升每次 I/O 的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在五个真实世界图数据集上实验表明，AGNES 的训练速度比四种最先进方法快最多 4.1 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 块级 I/O 与超批量处理的组合能显著提升存储式 GNN 训练性能，为大规模图学习提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图神经网络（GNN）因其强大的表达能力在各种真实世界下游任务中取得突破。随着真实世界图规模的持续增长，研究者开始探索基于存储的 GNN 训练方法，该方法利用外部存储（如 NVMe SSD）在单台机器上处理网络规模图。虽然这些基于存储的 GNN 训练方法在大规模 GNN 训练中显示出良好潜力，但我们观察到它们在数据准备阶段存在严重瓶颈，因为它们忽视了一个关键挑战：如何处理大量小的存储 I/O。为解决这一挑战，本文提出了一种新型基于存储的 GNN 训练框架，名为 AGNES，该框架采用块级存储 I/O 处理方法，充分利用高性能存储设备的 I/O 带宽。此外，为进一步提升每次存储 I/O 的效率，AGNES 采用了基于真实世界图特征的超批量处理策略。对五个真实世界图的综合实验表明，AGNES 一直优于四种最先进方法，最快可比最佳竞争者快 4.1 倍。我们的代码可在 https://github.com/Bigdasgit/agnes-kdd26 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \textsf{AGNES}, that employs a method of \textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \textsf{AGNES} employs a simple yet effective strategy, \textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.&lt;/p&gt;</description></item><item><guid>2601.01487v1</guid><title>DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion</title><link>http://arxiv.org/abs/2601.01487v1</link><author>Ziyue Zhang, Luxi Lin, Xiaolin Hu, Chao Chang, HuaiXi Wang, Yiyi Zhou, Rongrong Ji</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自监督的扩散反演方法DeepInv，通过自监督目标和数据增强生成伪噪声，并采用迭代多尺度训练训练可参数化的反演求解器，实现快速准确的图像到噪声映射。实验表明DeepInv在COCO数据集上相较于EasyInv和ReNoise在SSIM和速度上分别提升约40.435%和9887.5%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 扩散反演是恢复图像噪声的任务，对可控扩散图像编辑至关重要，但由于缺乏可行的监督信号，仍是一个挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种不依赖真实噪声标注的自监督扩散反演方法，以提高性能和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用自监督目标和数据增强生成高质量伪噪声，结合迭代多尺度训练训练参数化的反演求解器，实现逐步预测噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DeepInv在COCO数据集上相较于EasyInv和ReNoise在SSIM和速度上分别提升约40.435%和9887.5%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DeepInv提供了一种可训练的反演求解器，显著提升了扩散反演的性能和速度，为社区提供了有价值的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; DeepInv is a self-supervised diffusion inversion approach that generates pseudo-noise from real images using a self-supervised objective and data augmentation, and trains a parameterized inversion solver with an iterative multi-scale regime to achieve fast and accurate image-to-noise mapping. Experiments show significant improvements in SSIM and speed over existing methods on the COCO dataset.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.&lt;/p&gt;</description></item><item><guid>2601.01500v1</guid><title>DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster</title><link>http://arxiv.org/abs/2601.01500v1</link><author>Jinxiao Zhang, Yunpu Xu, Xiyong Wu, Runmin Dong, Shenggan Cheng, Yi Zhao, Mengxuan Chen, Qinrui Zheng, Jianting Liu, Haohuan Fu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了DiT-HC系统，该系统在下一代高性能CPU集群上训练并扩展了生成模型DiT，并通过三项关键技术实现显著加速。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成基础模型已成为科学计算中数据重建和仿真的重要工具，并与传统数值模拟紧密结合。随着矩阵加速单元和高带宽内存等新硬件特性的出现，基于CPU的集群为加速和扩展这些模型提供了有前景的机会。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 展示DiT-HC作为首个在高性能CPU集群上训练和扩展DiT的系统，并验证其可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用通信自由张量并行（CFTP）与AutoMem实现自动化内存感知数据流；使用HCOps提供针对向量和矩阵加速单元优化的GEMM和算子核；以及自定义MPI后端实现计算、通信和内存移动的重叠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明相较于本地或公开的CPU库，DiT-HC实现了8.2至87.7倍的加速，并在256节点上达到90.6%的弱规模效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 这些结果证明了在CPU集群上进行大规模生成模型训练的可行性，并为未来的HPC-AI协同设计提供了新见解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成基础模型已成为科学计算中数据重建和仿真的重要工具，并与传统数值模拟紧密结合。同时，随着矩阵加速单元和高带宽内存等新硬件特性的出现，基于CPU的集群为加速和扩展这些模型提供了有前景的机会，促进了人工智能与科学计算的统一。我们提出了DiT-HC，这是首个在下一代高性能CPU集群上训练和扩展生成模型DiT的系统。DiT-HC 引入了三项关键技术：(1) 采用 AutoMem 的通信自由张量并行（CFTP），实现自动化的内存感知数据流；(2) HCOps，一套利用向量和矩阵加速单元优化的 GEMM 与算子核；以及 (3) 自定义 MPI 后端，重叠计算、通信和内存移动。实验结果显示，相较于本地或公开的 CPU 库，DiT-HC 在速度上实现了 8.2 至 87.7 倍的加速，并在 256 节点上达到了 90.6% 的弱规模效率。这些结果证明了在 CPU 集群上进行大规模生成模型训练的可行性，并为未来 HPC-AI 协同设计提供了新的见解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.&lt;/p&gt;</description></item><item><guid>2601.01501v1</guid><title>Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE</title><link>http://arxiv.org/abs/2601.01501v1</link><author>Fan Xu, Wei Gong, Hao Wu, Lilan Peng, Nan Wang, Qingsong Wen, Xian Wu, Kun Wang, Xibin Zhao</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的层次图ODE框架HiGO，用于学习野火的多尺度连续时间动力学，并在全球野火预测中取得显著优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 野火是地球系统的重要组成部分，受大气、海洋和陆地过程的复杂相互作用影响，全球尺度的长期预测具有重要意义，但目前仍面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 重新定义全球野火预测问题，开发HiGO框架以捕捉多尺度连续时间动力学。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将地球系统建模为多层图层次结构，设计自适应滤波消息传递机制实现层内层间信息流动，并在各层引入GNN参数化的神经ODE模块学习连续动力学。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在SeasFire Cube数据集上，HiGO在长时段野火预测任务中明显优于现有基准，并且其连续时间预测与观测结果高度一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; HiGO展示了在全球野火预测中的强大潜力，可为实际应用提供可靠的连续时间预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 野火作为地球系统的重要组成部分，其行为受大气、海洋和陆地过程的复杂相互作用影响，跨越广泛的时空尺度。对其在大尺度时间范围内的全球活动进行建模因此成为一项关键但具有挑战性的任务。虽然深度学习在全球天气预报方面已取得显著突破，但其在全球野火行为预测方面的潜力尚未得到充分探索。在本研究中，我们重新定义了这一问题，并提出了层次图ODE（HiGO），一种旨在学习野火多尺度连续时间动力学的新框架。具体而言，我们将地球系统表示为多层图层次结构，并提出了一种自适应滤波消息传递机制，用于层内和层间信息流动，从而实现更有效的特征提取与融合。此外，我们在多个层级引入了由图神经网络参数化的神经ODE模块，以显式学习每个尺度固有的连续动力学。通过在SeasFire Cube数据集上的广泛实验，我们证明了HiGO在长时段野火预报上显著优于最先进的基准，其连续时间预测表现出强烈的观测一致性，凸显了其在实际应用中的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.&lt;/p&gt;</description></item><item><guid>2601.01535v1</guid><title>Improving Flexible Image Tokenizers for Autoregressive Image Generation</title><link>http://arxiv.org/abs/2601.01535v1</link><author>Zixuan Fu, Lanqing Guo, Chong Wang, Binbin Song, Ding Liu, Bihan Wen</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的可变长度图像分词器ReToK，利用冗余填充和层次语义正则化，解决传统尾部截断导致信息过度集中在前置分词的问题，并在ImageNet数据集上实现了更优的生成效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的可变长度图像分词器通过嵌套丢弃实现尾部分词随机截断，导致图像信息主要集中在前面分词，限制了自回归生成模型的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服尾部截断策略的局限，使所有分词都能充分参与潜在建模，从而提升下游图像生成的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 冗余分词填充：频繁激活尾部分词，减轻前置分词信息过度集中；2) 层次语义正则化：将早期分词的解码特征与预训练视觉基础模型对齐，并逐渐减弱正则化强度，以便更细致地重建低层细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ImageNet 256 by 256数据集上，ReToK在生成性能上优于现有的可变长度和固定长度分词器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ReToK通过冗余填充和层次正则化有效利用所有分词，显著提升了图像生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可变长度图像分词器旨在使用有序的一维可变长度分词序列来表示图像。该灵活分词通常通过嵌套丢弃实现，即在训练期间随机截断尾部分词，并使用剩余的前置序列重建图像。然而，这种尾部截断策略本质上将图像信息集中在早期分词中，限制了随着分词长度增加的下游自回归（AR）图像生成的效果。为克服这些限制，我们提出了ReToK，一种具有冗余分词填充和层次语义正则化的灵活分词器，旨在充分利用所有分词以增强潜在建模。具体而言，我们引入冗余分词填充，以更频繁地激活尾部分词，从而减轻早期分词信息过度集中。除此之外，我们应用层次语义正则化，将早期分词的解码特征与预训练视觉基础模型对齐，并逐步降低尾部的正则化强度，以允许更细致的低层细节重建。大量实验表明ReTok的有效性：在ImageNet 256 by 256上，我们的方法在生成性能上优于可变长度和固定长度分词器。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \textbf{ReToK}, a flexible tokenizer with \underline{Re}dundant \underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}&lt;/p&gt;</description></item><item><guid>2601.01558v1</guid><title>Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings</title><link>http://arxiv.org/abs/2601.01558v1</link><author>Pengfei Qu, Wenyu Ouyang, Chi Zhang, Yikai Chai, Shuolong Xu, Lei Ye, Yongri Piao, Miao Zhang, Huchuan Lu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 研究利用AlphaEarth Foundation嵌入来描述流域特征，并评估其在无流量记录地区预测河流流量的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 河流流量预测在缺乏流量记录的地区困难，因为流域对气候、地形、植被和土壤的响应不同，传统属性无法完全捕捉自然环境的复杂性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨AlphaEarth嵌入是否比传统属性更能有效描述流域特征，从而提高预测准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用从大量卫星图像学习得到的嵌入来训练模型，并比较其在未用于训练的流域上的预测准确性；同时研究基于嵌入相似度选择捐赠流域对预测的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 嵌入模型在未训练流域上的预测准确性更高，表明其捕捉了关键物理差异；基于嵌入相似度选择相似流域可提升性能，加入过多不相似流域会降低准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于卫星图像的环境表示能增强水文预测，并支持更易适应不同景观的模型开发。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在没有流量记录的地区预测河流流量是一项挑战，因为流域对气候、地形、植被和土壤的响应各不相同。传统的流域属性描述了一些差异，但无法完全代表自然环境的复杂性。本研究探讨了AlphaEarth Foundation 嵌入是否能提供更具信息量的流域特征描述，这些嵌入是从大量卫星图像学习得到的，而非由专家设计。它们总结了植被、地表属性和长期环境动态的模式。我们发现，使用这些嵌入的模型在未用于训练的流域上预测流量的准确性更高，表明它们比传统属性更有效地捕捉关键物理差异。我们进一步研究了选择合适的捐赠流域如何影响无测流区域的预测。基于嵌入的相似度有助于识别具有相似环境和水文行为的流域，从而提升性能，而加入许多不相似的流域可能降低准确性。结果表明，基于卫星信息的环境表示可以加强水文预测，并支持更易适应不同景观的模型开发。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.&lt;/p&gt;</description></item><item><guid>2601.01576v1</guid><title>OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment</title><link>http://arxiv.org/abs/2601.01576v1</link><author>Ming Zhang, Kexin Tan, Yueyuan Huang, Yujiong Shen, Chunchun Ma, Li Ju, Xinran Zhang, Yuhui Wang, Wenqing Jing, Jingyi Deng, Huayu Sha, Binze Hu, Jingqi Tong, Changhao Jiang, Yage Geng, Yuankai Ying, Yue Zhang, Zhangyue Yin, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一个名为OpenNovelty的LLM驱动的系统，用于在同行评审中进行透明、基于证据的新颖性分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 评估论文的新颖性是同行评审中的关键但困难任务，因为评审者需要与庞大且快速演变的文献进行对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一个可验证、可扩展的工具，帮助研究社区实现公平、一致且有证据支持的同行评审。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 系统分四个阶段：提取核心任务和贡献声明生成检索查询；使用语义检索引擎检索相关先前工作；构建核心任务相关工作的层级分类，并对每个贡献进行全文比较；将所有分析合成结构化的新颖性报告，包含引用和证据片段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在对500多份ICLR 2026提交的论文进行部署后，初步分析表明OpenNovelty能够识别相关先前工作，包括作者可能忽视的密切相关论文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenNovelty通过将评估根植于真实论文，提供可验证的判断，促进了更公平、可持续的同行评审流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 评估新颖性在同行评审中至关重要但具有挑战性，因为评审者必须将提交的工作与庞大且快速演变的文献进行比较。本文报告了OpenNovelty，一个基于大型语言模型的代理系统，用于透明、基于证据的新颖性分析。该系统通过四个阶段运作：（1）提取核心任务和贡献声明以生成检索查询；（2）通过语义搜索引擎根据提取的查询检索相关先前工作；（3）构建核心任务相关工作的层级分类，并对每个贡献进行全文比较；（4）将所有分析合成结构化的新颖性报告，包含明确的引用和证据片段。与简单的LLM方法不同，OpenNovelty将所有评估根植于检索到的真实论文，确保可验证的判断。我们在500多份ICLR 2026提交的论文上部署了该系统，并公开了所有报告，初步分析表明它能够识别相关先前工作，包括作者可能忽视的密切相关论文。OpenNovelty旨在为研究社区提供一个可扩展的工具，促进公平、一致且有证据支持的同行评审。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, \textsc{OpenNovelty} grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.&lt;/p&gt;</description></item><item><guid>2601.01653v1</guid><title>Learning Resilient Elections with Adversarial GNNs</title><link>http://arxiv.org/abs/2601.01653v1</link><author>Hao Xiang Li, Yash Shah, Lorenzo Giusti</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究探讨如何利用机器学习改进选举规则，以提高其在现实环境中的鲁棒性和社会福利。通过将选举建模为二分图并使用图神经网络，结合对抗训练，作者提出了一种更具表达力且更具抵抗策略投票的学习型投票规则，并在合成与真实数据集上验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 选举是现代民主的核心机制，自17世纪以来一直是治理市场、推荐系统和点对点网络的主要手段。然而，设计能够满足所有假设情景的通用投票规则仍是机制设计的前沿挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过扩展学习投票规则的表达能力，并结合神经网络架构改进与对抗训练，提升投票规则的鲁棒性和社会福利，以实现更可靠的自动化机制设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用二分图表示选举，使用图神经网络学习投票规则，并在训练过程中加入对抗样本以增强对策略投票的抵抗力；同时对网络架构进行改进以提升表达能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法克服了以往工作在学习投票规则时的关键局限，能够在合成和真实数据集上实现更高的鲁棒性和社会福利，证明了图神经网络在选举建模中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过将机器学习与对抗训练相结合，研究为在现实选举中应用机器学习提供了新的前景，展示了更强大、更鲁棒的学习型投票规则的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在面对不良动机时，达成共识是不可或缺的。选举自17世纪以来一直是现代民主的典范方式。如今，它们调节市场，为现代推荐系统或点对点网络提供引擎，并仍然是代表民主的主要方法。然而，满足所有假设情景的理想通用投票规则仍是一个具有挑战性的话题，设计这些系统是机制设计研究的前沿。自动化机制设计是一种有前景的方法，最近的工作表明，集合不变架构非常适合建模选举系统。然而，诸如对策略投票的鲁棒性等各种担忧阻碍了其直接应用于现实世界。本论文将学习投票规则的表达能力进行推广，并结合神经网络架构的改进与对抗训练，以提高投票规则的韧性，同时最大化社会福利。我们在合成和真实数据集上评估了我们方法的有效性。我们的方法通过使用二分图表示选举并使用图神经网络学习投票规则，解决了以往工作在学习投票规则方面的关键局限。我们认为这为将机器学习应用于现实选举开辟了新的前沿。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.&lt;/p&gt;</description></item><item><guid>2601.01675v1</guid><title>VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data</title><link>http://arxiv.org/abs/2601.01675v1</link><author>Snehal s. Dikhale, Karankumar Patel, Daksh Dhingra, Itoshi Naramura, Akinobu Hayashi, Soshi Iba, Nawid Jamali</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种结合触觉和视觉数据的机器人手中物体6维姿态估计方法，并通过合成数据训练网络，验证了触觉数据能提升估计精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统仅靠视觉的姿态估计受机器人抓手遮挡影响，难以准确定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用触觉传感器补充视觉信息，改进手中物体姿态估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 用点云表示触觉接触表面，构建像素级密集融合网络；扩展NVIDIA Deep Learning Dataset Synthesizer生成合成照片级真实视觉数据和对应触觉点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 触觉数据与视觉数据结合能显著提升6维姿态估计精度，网络从合成训练成功迁移到真实机器人。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 触觉与视觉融合方法在手中物体姿态估计中有效，且具有良好的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 对物体的6维姿态的了解可以提升手中物体的操作。手中6维物体姿态估计因机器人抓手产生的严重遮挡而具有挑战性，这会对仅依赖视觉数据的方法产生不利影响。许多机器人在其指尖配备了触觉传感器，可用于补充视觉数据。本文提出了一种利用触觉和视觉数据估计机器人手中抓取物体姿态的方法。为了解决触觉数据缺乏标准表示和传感器融合等挑战，我们提出使用点云来表示与触觉传感器接触的物体表面，并提出基于像素级密集融合的网络架构。我们还扩展了NVIDIA的深度学习数据集合成器，以生成合成的逼真视觉数据和相应的触觉点云。结果表明，使用触觉数据与视觉数据相结合可以改善6维姿态估计，并且我们的网络能够成功地从合成训练迁移到真实物理机器人。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决机器人手中物体的 6D 姿态估计问题，尤其在手爪造成严重遮挡的情况下。准确的姿态信息对抓取、操作规划以及虚拟现实等任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别了缺乏标准触觉数据表示、异构传感器融合以及大规模数据集的挑战，并借鉴了现有的 RGB‑Depth 视觉姿态估计方法（如 PoseCNN、像素级融合）。随后他们提出将触觉信息转换为物体表面点云，并在 NVIDIA 的深度学习数据集合成器基础上生成同步的视觉与触觉数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两通道网络（视觉通道和触觉通道）对颜色、深度和触觉点云进行像素/点级融合，随后利用融合特征和全局特征估计每个采样点的平移、旋转和置信度，最终选取置信度最高的估计作为物体的 6D 姿态。实现流程包括语义分割、深度点云生成、随机采样、两阶段 CNN 处理、像素/点级融合、全局特征提取、姿态估计网络和置信度筛选。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 触觉传感器不变的物体表面点云表示；2) 视觉与触觉的像素/点级密集融合网络架构；3) 针对 11 个 YCB 物体的合成视觉-触觉数据集。与以往仅使用视觉或触觉的姿态估计方法不同，该工作首次将两种传感器信息结合，并提供了可迁移到真实机器人的训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种融合视觉与触觉点云的深度学习框架，能够在手爪遮挡严重的情况下准确估计握持物体的 6D 姿态，并通过创新的触觉表示和合成数据集实现了从仿真到真实机器人的迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot&amp;#x27;s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot&amp;#x27;s hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA&amp;#x27;s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.&lt;/p&gt;</description></item><item><guid>2601.01676v1</guid><title>LabelAny3D: Label Any Object 3D in the Wild</title><link>http://arxiv.org/abs/2601.01676v1</link><author>Jin Yao, Radowan Mahmud Redoy, Sebastian Elbaum, Matthew B. Dwyer, Zezhou Cheng</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文将单目图像中的3D目标检测问题放在现实世界场景中进行研究，提出一种基于分析-合成的框架LabelAny3D，用于从二维图像重建完整的三维场景并生成高质量的3D边界框标注。基于该框架，构建了COCO3D基准数据集，覆盖了MS-COCO中大量缺失的物体类别。实验表明，LabelAny3D生成的标注能显著提升多种基准上的单目3D检测性能，优于以往的自动标注方法，验证了基于基础模型的标注方法在开放世界3D识别中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单目3D检测在机器人和场景理解等领域具有重要意义，但现有模型在室内和自动驾驶场景表现良好，却难以处理野外图像，主要原因是缺乏野外3D数据集和3D标注难度大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建高质量的3D标注方法和开放词汇的基准数据集，提升单目3D检测在现实开放世界中的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出LabelAny3D框架，采用分析-合成技术从二维图像重建完整三维场景，自动生成高质量3D边界框标注；基于该框架创建COCO3D基准，利用MS-COCO图像并覆盖更多物体类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LabelAny3D生成的标注在多个基准上显著提升单目3D检测效果，优于之前的自动标注方案，证明了基础模型驱动的标注方法可扩展到真实开放世界。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于分析-合成的LabelAny3D和COCO3D基准为单目3D检测在开放世界中的发展提供了有效工具，展示了基础模型在大规模3D识别中的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 检测单目输入中的三维空间目标对于从机器人到场景理解等应用至关重要。尽管在室内和自动驾驶领域已有先进表现，但现有单目三维检测模型因缺乏野外三维数据集和三维标注难题，在野外图像上表现不佳。我们提出LabelAny3D，一种分析-合成框架，可从二维图像重建整体三维场景，快速生成高质量三维边界框标注。在此管线基础上，我们推出COCO3D，一个新的开放词汇单目三维检测基准，来源于MS-COCO数据集，涵盖了现有三维数据集中缺失的广泛物体类别。实验表明，LabelAny3D生成的标注在多个基准上提升了单目三维检测性能，优于以往自动标注方法的质量。这些结果展示了基于基础模型的标注在扩展真实开放世界三维识别中的前景。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.&lt;/p&gt;</description></item><item><guid>2601.01677v1</guid><title>Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada</title><link>http://arxiv.org/abs/2601.01677v1</link><author>Zhengsen Xu, Lanying Wang, Sibo Cheng, Xue Rui, Kyle Gao, Yimin Zhu, Mabel Heffring, Zack Dewis, Saeid Taleghanidoozdoozan, Megan Greenwood, Motasem Alkayid, Quinn Ledingham, Hongjie He, Jonathan Li, Lincoln Linlin Xu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种可信的数据驱动森林火灾风险预测框架，利用长序列多尺度时间建模，整合多种驱动因素，显式量化预测不确定性并实现过程级解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近几十年来，加拿大西部森林火灾频发，造成社会经济和环境损失。传统数据驱动模型受火灾点火和蔓延的随机性以及燃料、气象、气候、地形和人类活动等非线性交互影响，难以可靠解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个可信、可解释的森林火灾风险预测模型，克服纯数据驱动方法的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用长序列多尺度时间建模，融合异质驱动因素，使用不确定性感知分析和SHAP解释，评估2023-2024年西部加拿大火季。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在2023-2024年火季的F1分数为0.90，PR-AUC为0.98，计算成本低。预测不确定性显示空间和季节性模式，温度驱动在两年中占主导，2024年湿度约束对空间和土地覆盖差异影响更大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在准确性、低成本和可解释性方面优于现有时间序列方法，为森林火灾管理提供可靠决策支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.&lt;/p&gt;</description></item><item><guid>2601.01695v1</guid><title>Learnability-Driven Submodular Optimization for Active Roadside 3D Detection</title><link>http://arxiv.org/abs/2601.01695v1</link><author>Ruiyu Mao, Baoming Zhang, Nicholas Ruozzi, Yunhui Guo</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在仅有路侧单目图像的情况下进行三维目标检测的难点，并提出了一种基于可学习性主动学习的框架，以减少对不可辨识样本的标注成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的路侧感知数据集需要车辆与路侧同步帧的协同标注，但实际部署受硬件和隐私限制，往往只能获取路侧单独数据。没有车辆侧信息，专家标注难度大，且许多场景中的目标远、模糊或被遮挡，单视角难以确定其三维属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 降低在不可辨识样本上的标注浪费，同时获得高性能的路侧单目三维检测模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出了一个学习性驱动的主动学习框架 LH3D，选择既具信息量又易于可靠标注的场景，抑制不可辨识样本，保证覆盖率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 DAIR-V2X-I 数据集上，LH3D 仅使用 25% 的标注预算即可达到车辆、行人和骑行者分别 86.06%、67.32% 和 78.67% 的完整性能，显著优于基于不确定性的基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可学习性比不确定性更能决定路侧三维感知的效果，主动学习框架能有效提升标注效率和模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.&lt;/p&gt;</description></item><item><guid>2601.01701v1</guid><title>Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT</title><link>http://arxiv.org/abs/2601.01701v1</link><author>Mohammed Ayalew Belay, Adil Rasheed, Pierluigi Salvo Rossi</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出将数字孪生与联邦学习结合的多种方法，以提高工业系统异常检测的准确性和通信效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 工业系统异常检测对安全可靠性至关重要，传统方法受限于真实传感器数据、标签稀缺、误报率高和隐私问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过数字孪生集成联邦学习，提升全局模型性能，同时保持数据隐私和通信效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出五种方法：DTML、FPF、LPE、CWA、DTKD，每种方法通过合成与真实知识结合，平衡泛化与通信开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示CWA在33轮内达到80%准确率，FPF 41轮，LPE 48轮，DTML 87轮；FedAvg和DTKD未在100轮内达标；CWA比DTML少62%轮次，LPE少31%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将数字孪生知识融入联邦学习显著加速收敛，提升工业物联网异常检测的通信效率和实用准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 异常检测在维护工业系统的安全、可靠性和效率方面变得越来越重要。随着数字孪生和数据驱动决策的出现，已经提出了多种统计和机器学习方法。然而，这些方法面临着依赖仅真实传感器数据、标注数据有限、误报率高以及隐私问题等挑战。为了解决这些问题，我们提出了一套集成数字孪生的联邦学习（DTFL）方法，既能提升全局模型性能，又能保持数据隐私和通信效率。具体而言，我们提出了五种新方法：基于数字孪生的元学习（DTML）、联邦参数融合（FPF）、层级参数交换（LPE）、循环权重适配（CWA）以及数字孪生知识蒸馏（DTKD）。每种方法都引入了独特的机制，将合成知识与真实世界知识相结合，在泛化能力与通信开销之间取得平衡。我们使用公开的网络物理异常检测数据集进行了广泛实验。针对80%的目标准确率，CWA在33轮内达到目标，FPF在41轮，LPE在48轮，DTML在87轮，而标准的FedAvg基线和DTKD在100轮内未能达到目标。结果显示，通信效率显著提升（与DTML相比减少多达62%轮次，与LPE相比减少31%轮次），并证明将数字孪生知识融入联邦学习能加速收敛到工业物联网异常检测的实用准确率阈值。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.&lt;/p&gt;</description></item><item><guid>2601.01703v1</guid><title>Beyond Homophily: Community Search on Heterophilic Graphs</title><link>http://arxiv.org/abs/2601.01703v1</link><author>Qing Sima, Xiaoyang Wang, Wenjie Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 AdaptCS 的统一框架，用于在异质图上进行社区搜索，克服传统方法在异质图上的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 社区搜索旨在从图中识别与查询最相关的节点集合，广泛应用于欺诈检测、推荐等任务。传统图大多为同质图，但现实网络往往是异质图，边主要连接不同类型节点，导致结构信号呈现高频对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在异质图上有效进行社区搜索的问题，避免传统算法返回混合标签社区以及 GNN 在同质假设下模糊社区边界的缺陷。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; AdaptCS 由三部分组成：1) AdaptCS Encoder，分离多跳和多频信号，既捕捉同质平滑关系，又捕捉异质对比关系；2) 低秩优化，减少计算瓶颈，提升可扩展性；3) Adaptive Community Score，在线搜索时平衡嵌入相似度与拓扑关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多种异质和同质基准上，AdaptCS 的 F1 分数平均比最佳基线高 11%，在不同异质程度下保持鲁棒性，并实现高达两位数的速度提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AdaptCS 在异质图社区搜索中表现优异，兼顾同质与异质信号，具备高效可扩展的特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 社区搜索旨在识别与给定查询最相关的一组节点，支持从欺诈检测到推荐等多种任务。与同质图不同，许多真实网络是异质图，边主要连接不同类型的节点。因此，曾经反映平滑低频相似性的结构信号现在表现为尖锐的高频对比。然而，传统算法（如 k‑core、k‑truss）和最近的基于机器学习的模型在异质图上难以实现有效的社区搜索，边的符号或语义通常未知。基于算法的方法往往返回标签混杂的社区，而基于图神经网络的模型在同质假设下会平滑掉有意义的信号，模糊社区边界。因此，我们提出了 Adaptive Community Search（AdaptCS），一个统一框架，包含三个关键设计：（i）AdaptCS Encoder 分离多跳和多频信号，使模型能够捕捉平滑（同质）和对比（异质）关系；（ii）一种内存高效的低秩优化，消除主要计算瓶颈并确保模型可扩展性；（iii）Adaptive Community Score（ACS），通过平衡嵌入相似度和拓扑关系来指导在线搜索。对异质和同质基准的广泛实验表明，AdaptCS 在 F1 分数上平均比最佳基线高 11%，在不同异质程度下保持鲁棒性，并实现高达两位数的速度提升。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Community search aims to identify a refined set of nodes that are most relevant to a given query, supporting tasks ranging from fraud detection to recommendation. Unlike homophilic graphs, many real-world networks are heterophilic, where edges predominantly connect dissimilar nodes. Therefore, structural signals that once reflected smooth, low-frequency similarity now appear as sharp, high-frequency contrasts. However, both classical algorithms (e.g., k-core, k-truss) and recent ML-based models struggle to achieve effective community search on heterophilic graphs, where edge signs or semantics are generally unknown. Algorithm-based methods often return communities with mixed class labels, while GNNs, built on homophily, smooth away meaningful signals and blur community boundaries. Therefore, we propose Adaptive Community Search (AdaptCS), a unified framework featuring three key designs: (i) an AdaptCS Encoder that disentangles multi-hop and multi-frequency signals, enabling the model to capture both smooth (homophilic) and contrastive (heterophilic) relations; (ii) a memory-efficient low-rank optimization that removes the main computational bottleneck and ensures model scalability; and (iii) an Adaptive Community Score (ACS) that guides online search by balancing embedding similarity and topological relations. Extensive experiments on both heterophilic and homophilic benchmarks demonstrate that AdaptCS outperforms the best-performing baseline by an average of 11% in F1-score, retains robustness across heterophily levels, and achieves up to 2 orders of magnitude speedup.&lt;/p&gt;</description></item><item><guid>2601.01739v1</guid><title>K-EXAONE Technical Report</title><link>http://arxiv.org/abs/2601.01739v1</link><author>Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen, Hwan Chang, Stanley Jungkyu Choi, Yejin Choi, Jiwon Ham, Kijeong Jeon, Geunyeong Jeong, Gerrard Jeongwon Jo, Yonghwan Jo, Jiyeon Jung, Naeun Kang, Dohoon Kim, Euisoon Kim, Hayeon Kim, Hyosang Kim, Hyunseo Kim, Jieun Kim, Minu Kim, Myoungshin Kim, Unsol Kim, Youchul Kim, YoungJin Kim, Chaeeun Lee, Chaeyoon Lee, Changhun Lee, Dahm Lee, Edward Hwayoung Lee, Honglak Lee, Jinsang Lee, Jiyoung Lee, Sangeun Lee, Seungwon Lim, Solji Lim, Woohyung Lim, Chanwoo Moon, Jaewoo Park, Jinho Park, Yongmin Park, Hyerin Seo, Wooseok Seo, Yongwoo Song, Sejong Yang, Sihoon Yang, Chang En Yea, Sihyuk Yi, Chansik Yoon, Dongkeun Yoon, Sangyeon Yoon, Hyeongu Yun</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了LG AI Research开发的大型多语言语言模型K-EXAONE，并对其性能进行了评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; LG AI Research致力于开发大规模多语言语言模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 推出K-EXAONE，作为面向工业和科研的专有AI基础模型，以推动更好的人工智能应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用专家混合架构，总参数约二百三十六亿，推理时激活约二十三亿；支持二十五六万词上下文窗口，覆盖韩语、英语、西班牙语、德语、日语和越南语六种语言；在涵盖推理、代理、通用、韩语和多语言能力的综合基准上进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在所有评估中，K-EXAONE的表现与同等规模的开放权重模型相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; K-EXAONE是一款功能强大的专有AI基础模型，适用于广泛的工业和科研应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本技术报告介绍了LG AI Research开发的大型多语言语言模型K-EXAONE。K-EXAONE基于专家混合架构，总参数约二百三十六亿，在推理时激活约二十三亿参数。它支持二十五六万词上下文窗口，并覆盖韩语、英语、西班牙语、德语、日语和越南语六种语言。我们在涵盖推理、代理、通用、韩语和多语言能力的综合基准套件上评估了K-EXAONE。在所有评估中，K-EXAONE的表现与同等规模的开放权重模型相当。K-EXAONE旨在推动更好的人工智能生活，被定位为适用于广泛工业和科研应用的强大专有AI基础模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.&lt;/p&gt;</description></item><item><guid>2601.01741v1</guid><title>Latent Space Element Method</title><link>http://arxiv.org/abs/2601.01741v1</link><author>Seung Whan Chung, Youngsoo Choi, Christopher Miller, H. Keo Springer, Kyle T. Sullivan</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LSEM是一种基于潜在空间的元件方法，利用局部子域模型拼接构建大尺度仿真器，保持预测精度且无需直接访问PDE算子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的代理求解器需要在大域上训练或访问PDE算子，限制了可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够在小域训练、但可扩展到更大域的代理求解器，且不需要侵入式PDE算子访问。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用DD-FEM框架，构建每个子域为LaSDI潜在ODE代理模型，通过学习的方向性交互项在潜在空间耦合邻域，使用平滑窗口混合重建全局场，形成可扩展的潜在动力学系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在一维Burgers和Korteweg-de Vries方程实验中，LSEM在训练域之外的更大空间域上保持了预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LSEM提供了一种可解释、可扩展的基础模型代理求解器路径，可由可重用的局部模型构建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们如何构建在小域训练但可扩展到更大域且不需要侵入式访问PDE算子的代理求解器？受数据驱动有限元方法（DD-FEM）框架启发，我们提出了潜在空间元件方法（LSEM），这是一种基于元件的潜在代理组装方法，其中学习到的子域（“元件”）模型可以被铺设并耦合以形成更大的计算域。每个元件是从局部补丁的快照训练得到的LaSDI潜在ODE代理，邻近元件通过潜在空间中的学习方向交互项耦合，避免了Schwarz迭代和界面残差评估。基于平滑窗口的混合从重叠的元件预测中重建全局场，得到可扩展的组装潜在动力学系统。在一维Burgers和Korteweg-de Vries方程的实验中，LSEM在保持预测精度的同时，能够扩展到比训练时更大的空间域。LSEM为基于可重用局部模型构建的基础模型代理求解器提供了一条可解释且可扩展的路径。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;How can we build surrogate solvers that train on small domains but scale to larger ones without intrusive access to PDE operators? Inspired by the Data-Driven Finite Element Method (DD-FEM) framework for modular data-driven solvers, we propose the Latent Space Element Method (LSEM), an element-based latent surrogate assembly approach in which a learned subdomain (&amp;quot;element&amp;quot;) model can be tiled and coupled to form a larger computational domain. Each element is a LaSDI latent ODE surrogate trained from snapshots on a local patch, and neighboring elements are coupled through learned directional interaction terms in latent space, avoiding Schwarz iterations and interface residual evaluations. A smooth window-based blending reconstructs a global field from overlapping element predictions, yielding a scalable assembled latent dynamical system. Experiments on the 1D Burgers and Korteweg-de Vries equations show that LSEM maintains predictive accuracy while scaling to spatial domains larger than those seen in training. LSEM offers an interpretable and extensible route toward foundation-model surrogate solvers built from reusable local models.&lt;/p&gt;</description></item><item><guid>2601.01743v1</guid><title>AI Agent Systems: Architectures, Applications, and Evaluation</title><link>http://arxiv.org/abs/2601.01743v1</link><author>Bin Xu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文综述了AI代理系统的最新架构，涵盖推理、规划、记忆和工具使用等核心能力，并提出了统一的分类体系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; AI代理正成为自然语言意图与现实计算之间的实用接口，需整合多种认知与操作功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 系统梳理AI代理的设计范式，构建组件、协同与部署的统一分类，并探讨关键设计权衡与评估难点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对现有研究进行归纳，按推理、规划、工具调用三大维度划分，并进一步细化为核心组件、协同模式和部署场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 提出了包含策略/LLM核心、记忆、世界模型、规划器、工具路由器和评估者的组件框架；区分单体与多体、集中与分散的协同；识别了延迟与精度、自治与可控、能力与可靠性等权衡；指出评估受非确定性、长周期信用分配、工具与环境变异及隐藏成本影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AI代理技术正快速发展，亟需在工具验证、可扩展记忆、决策可解释性和可复现评估等方面开展研究，以实现安全可靠的实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; AI代理——将基础模型与推理、规划、记忆和工具使用相结合的系统——正迅速成为自然语言意图与现实计算之间的实用接口。本综述综合了AI代理架构的最新景观，涵盖：（i）推理与决策（如链式思维分解、自我反思与验证、约束感知决策）；（ii）规划与控制（从反应式策略到分层和多步规划器）；以及（iii）工具调用与环境交互（检索、代码执行、API和多模态感知）。我们将先前工作组织成统一的分类体系，涵盖代理组件（策略/LLM核心、记忆、世界模型、规划器、工具路由器和评估者）、协同模式（单体与多体；集中与分散协调）以及部署场景（离线分析与在线交互式辅助；安全关键与开放式任务）。讨论了关键设计权衡——延迟与精度、自治与可控、能力与可靠性——并强调评估因非确定性、长周期信用分配、工具与环境变异以及隐藏成本（如重试和上下文增长）而复杂。最后总结了测量与基准实践（任务套件、人类偏好与效用指标、受约束成功率、鲁棒性与安全性），并指出开放挑战，包括工具动作的验证与防护、可扩展记忆与上下文管理、代理决策的可解释性，以及在现实工作负载下可复现评估。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.&lt;/p&gt;</description></item><item><guid>2601.01746v1</guid><title>Point-SRA: Self-Representation Alignment for 3D Representation Learning</title><link>http://arxiv.org/abs/2601.01746v1</link><author>Lintong Wei, Jian Lu, Haozhe Cheng, Jihua Zhu, Kaibing Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 Point-SRA，一种通过自蒸馏和概率建模对 3D 点云进行表示学习的方法。它通过为 MAE 设定不同的遮挡比例、使用 MeanFlow Transformer 进行跨模态条件嵌入、双重自表示对齐以及流条件微调，捕获几何与语义信息的互补性，从而显著提升下游任务性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的 MAE 方法采用固定遮挡比例，忽视了多层次表示相关性和几何结构，并且基于逐点重建假设与点云多样性不符。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述局限，提出一种能够更好捕获几何与语义互补信息的 3D 表示学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 为 MAE 设定多种遮挡比例以获取不同层次信息；2) 引入 MeanFlow Transformer，利用跨模态条件嵌入实现多样化概率重建；3) 在 MAE 与 MFT 两层引入双重自表示对齐机制；4) 设计流条件微调架构充分利用 MeanFlow 学到的点云分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ScanObjectNN 上比 Point-MAE 提升 5.37%；在脑动脉瘤分割任务中，动脉平均 IoU 96.07%，动脉瘤 86.87%；在 3D 目标检测中，AP@50 达 47.3%，比 MaskPoint 高 5.12%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Point-SRA 通过多尺度遮挡、概率重建和自表示对齐，有效提升了 3D 点云表示学习的质量，显著改善了多种下游任务的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 掩码自编码器（MAE）已成为 3D 表示学习的主导范式，在多种下游任务中设定了新的性能基准。现有采用固定遮挡比例的方法忽视了多层次表示相关性和内在几何结构，并且依赖逐点重建假设与点云多样性相冲突。为解决这些问题，我们提出了一种名为 Point-SRA 的 3D 表示学习方法，通过自蒸馏和概率建模来对齐表示。具体而言，我们为 MAE 分配不同的遮挡比例，以捕获互补的几何和语义信息；MeanFlow Transformer（MFT）利用跨模态条件嵌入实现多样化的概率重建。我们的分析进一步表明，MFT 在不同时间步的表示也具有互补性。因此，在 MAE 和 MFT 两层都提出了双重自表示对齐机制。最后，我们设计了流条件微调架构，以充分利用 MeanFlow 学到的点云分布。Point-SRA 在 ScanObjectNN 上比 Point-MAE 提升 5.37%；在脑动脉瘤分割任务中，动脉平均 IoU 为 96.07%，动脉瘤为 86.87%；在 3D 目标检测中，Point-SRA 达到 47.3% 的 AP@50，超过 MaskPoint 5.12%。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升3D点云表示学习的质量，解决现有MAE方法固定掩码比例和点对点重建假设导致的几何细节与语义抽象无法充分融合的问题。该问题在点云分割、检测等实际应用中至关重要，因为高质量的表示能显著提升模型的泛化和性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过信息瓶颈理论分析掩码比例的互补性，发现低比例保留几何细节，高比例促成语义抽象。随后借鉴MAE框架、MeanFlow概率建模和自蒸馏技术，设计了教师-学生双掩码结构和时间步对齐机制。该方法在保持MAE核心思想的同时，加入了多比例和概率重建的创新。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双重自表示对齐（MAE‑SRA和MFT‑SRA）将低掩码的几何特征与高掩码的语义特征以及时间步的分布信息融合。实现流程包括：①预训练MAE，教师使用30%掩码，学生使用75%掩码；②使用MeanFlow Transformer对点云分布进行概率建模并在时间步上对齐；③在下游任务中采用流条件微调架构，利用冻结的MFT输出流向指导训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①对掩码比例互补性进行理论阐述；②提出双重自表示对齐机制，跨掩码比例和时间步对齐；③使用MeanFlow实现多解概率重建，克服点对点重建的不确定性；④流条件微调架构充分利用预训练分布信息。与以往固定掩码、确定性重建的MAE方法不同，Point‑SRA通过可变掩码和概率建模显著提升表示质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point‑SRA通过双重自表示对齐和概率流建模，联合几何细节与语义抽象，显著提升3D点云表示学习的效果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.&lt;/p&gt;</description></item><item><guid>2601.01781v1</guid><title>Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery</title><link>http://arxiv.org/abs/2601.01781v1</link><author>Lakshay Sharma, Alex Marin</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督预训练任务——子图像重叠预测，旨在提升遥感图像语义分割的性能，并显著降低对预训练数据的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自监督学习方法已成为构建通用模型的主流，但大多数方法需要海量预训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种在遥感图像分割任务中使用更少预训练数据的自监督方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 从原始图像中提取子图像，训练模型预测该子图像在原图中的位置掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 使用该任务预训练可显著加快收敛速度，并在下游分割任务上实现相等或更好的性能；当标注数据减少时，优势更明显；该方法在多种网络架构和数据集上均表现优异，并且需要的预训练数据远少于其他自监督方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 子图像重叠预测是一种高效、数据需求低的自监督预训练策略，能够提升遥感图像语义分割的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自监督学习（SSL）方法已成为创建通用模型的主导范式，这些模型的能力可以迁移到下游监督学习任务。然而，大多数此类方法依赖大量预训练数据。本研究提出了子图像重叠预测，一种新颖的自监督预训练任务，旨在帮助遥感图像中的语义分割，并使用显著更少的预训练图像。给定一幅图像，提取一个子图像，并训练模型生成该子图像在原始图像中的位置的语义掩码。我们证明，使用此任务进行预训练可显著加快收敛速度，并在下游分割任务上实现相等或更好的性能（通过mIoU衡量）。当标记训练数据减少时，这种收敛和性能差距会扩大。我们在多种架构类型和多种下游数据集上展示了这一点。我们还表明，与其他SSL方法相比，我们的方法在需要显著更少的预训练数据的情况下匹配或超过性能。代码和模型权重已提供在 https://github.com/sharmalakshay93/subimage-overlap-prediction。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.&lt;/p&gt;</description></item><item><guid>2601.01804v1</guid><title>Causality-Aware Temporal Projection for Video Understanding in Video-LLMs</title><link>http://arxiv.org/abs/2601.01804v1</link><author>Zhengjian Kang, Qi Chen, Rui Liu, Kangtong Mo, Xingyu Zhang, Xiaoyu Deng, Ye Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了V-CORE框架，旨在通过显式的时间顺序约束提升视频大语言模型在需要时间一致性和因果连贯性的任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频大语言模型在多模态推理方面表现突出，但在需要严格时间顺序和因果关系的任务中仍面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有参数高效视频大语言模型中双向投影器模糊时间顺序的问题，提出显式时间顺序约束的V-CORE。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; V-CORE由可学习空间聚合器和因果感知时间投影器两部分组成，采用块因果注意力和终端动态摘要标记实现单向信息流；使用4位QLoRA和冻结的LLM骨干，在单卡GPU上高效训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在NExT-QA基准上取得61.2%的准确率，并在MSVD-QA、MSRVTT-QA和TGIF-QA上保持竞争力；在时间和因果推理子类别上分别提升了3.5%和5.2%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 显式时间顺序约束对提升视频理解中的时间和因果推理至关重要，V-CORE在保持参数效率的同时实现了显著性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近的视频大语言模型（Video-LLMs）在多模态推理方面表现出色，但在需要一致时间顺序和因果连贯性的视像理解任务中仍面临挑战。许多参数高效的视频大语言模型依赖于无约束的双向投影器来建模帧间交互，这可能通过允许后续帧影响早期表示来模糊时间顺序，而没有明确的架构机制来尊重视频推理的方向性。为了解决这一限制，我们提出了V-CORE，一个参数高效的框架，引入了显式的时间顺序约束以实现视频理解。V-CORE由两个关键组件组成：（1）可学习空间聚合器（LSA），它自适应地选择显著的空间标记以减少冗余；（2）因果感知时间投影器（CATP），通过块因果注意力和作为因果汇聚点的终端动态摘要标记来强制执行结构化的单向信息流。该设计在保留帧内空间交互的同时，确保时间信息以严格的顺序聚合。使用4位QLoRA和冻结的LLM骨干，V-CORE可以在单个消费级GPU上高效训练。实验表明，V-CORE在具有挑战性的NExT-QA基准上实现了强劲的性能，准确率达到61.2%，并在MSVD-QA、MSRVTT-QA和TGIF-QA上保持竞争力，时间和因果推理子类别的提升分别为3.5%和5.2%，直接验证了显式时间顺序约束的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.&lt;/p&gt;</description></item><item><guid>2601.01813v1</guid><title>Spatio-temporal modeling and forecasting with Fourier neural operators</title><link>http://arxiv.org/abs/2601.01813v1</link><author>Pratik Nag, Andrew Zammit-Mangion, Sumeetpal Singh, Noel Cressie</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出使用傅里叶神经算子构建统计动态时空模型，用于预测空间和时间演化的物理与生物现象，并在模拟与真实数据上验证其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统统计过程模型（如高斯过程）难以捕捉环境异质性和复杂交互的时空现象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用傅里叶神经算子在不需要显式 PDE 知识的情况下，构建高效的时空预测模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过训练 FNO 以映射输入输出样本，比较其预测结果与最先进的统计时空预测方法，并在海表温度和欧洲降水数据上进行实证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; FNO 在非线性 PDE 模拟和真实海表温度、降水数据上均能准确预测，并提供可靠的不确定性量化，优于或与现有方法相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 傅里叶神经算子为时空动态建模提供了一种有效且计算高效的工具，能够捕捉复杂的现实依赖关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时空过程模型常用于模拟在空间和时间上演化的动态物理和生物现象。这些现象可能表现出环境异质性和复杂交互，传统统计过程模型（如高斯过程）难以捕捉。本文提出使用傅里叶神经算子（FNO）构建统计动态时空模型进行预测。FNO 是一种灵活的函数映射，能够近似可能未知的线性或非线性偏微分方程的解算子，计算效率高。它通过输入输出样本进行训练，无需显式了解底层 PDE。通过对已知解的非线性 PDE 进行仿真，本文将 FNO 预测与最先进的统计时空预测方法进行比较。随后，利用大西洋海表温度和欧洲降水数据，展示基于 FNO 的动态时空（DST）统计建模能够捕捉复杂的现实时空依赖关系。通过测试实例集合，证明 FNO-DST 预测准确且不确定性量化有效。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatio-temporal process models are often used for modeling dynamic physical and biological phenomena that evolve across space and time. These phenomena may exhibit environmental heterogeneity and complex interactions that are difficult to capture using traditional statistical process models such as Gaussian processes. This work proposes the use of Fourier neural operators (FNOs) for constructing statistical dynamical spatio-temporal models for forecasting. An FNO is a flexible mapping of functions that approximates the solution operator of possibly unknown linear or non-linear partial differential equations (PDEs) in a computationally efficient manner. It does so using samples of inputs and their respective outputs, and hence explicit knowledge of the underlying PDE is not required. Through simulations from a nonlinear PDE with known solution, we compare FNO forecasts to those from state-of-the-art statistical spatio-temporal-forecasting methods. Further, using sea surface temperature data over the Atlantic Ocean and precipitation data across Europe, we demonstrate the ability of FNO-based dynamic spatio-temporal (DST) statistical modeling to capture complex real-world spatio-temporal dependencies. Using collections of testing instances, we show that the FNO-DST forecasts are accurate with valid uncertainty quantification.&lt;/p&gt;</description></item><item><guid>2601.01822v1</guid><title>DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization</title><link>http://arxiv.org/abs/2601.01822v1</link><author>Shiyong Meng, Tao Zou, Bolei Chen, Chaoxu Mu, Jianxin Wang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新的视觉楼层平面定位方法DisCo-FLoc，利用双层视觉-几何对比来消除基于深度的定位歧义，无需额外语义标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 楼层平面数据易获取、持久且对视觉变化鲁棒，视觉楼层平面定位受到广泛关注。现有方法通过匹配几何先验或稀疏语义来降低不确定性，但仍受重复结构导致的歧义影响，且昂贵的语义标注限制了应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有方法在简约楼层平面中因重复结构导致的定位歧义，并消除对昂贵语义标注的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先使用射线回归预测器为基于射线投射的定位生成一系列候选位置；随后提出位置级和方向级约束的对比学习方法，将深度感知的视觉特征与楼层平面中的几何结构严格匹配，从而消除歧义并选取最佳相机姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在两个标准视觉楼层平面定位基准上，DisCo-FLoc 在鲁棒性和准确性上均显著优于最先进的基于语义的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 双层视觉-几何对比学习能够在无需语义标签的情况下，有效消除定位歧义，提升视觉楼层平面定位的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 由于楼层平面数据易于获取、长期持久且对视觉外观变化具有鲁棒性，视觉楼层平面定位（FLoc）受到了广泛关注。现有方法要么巧妙地匹配几何先验，要么利用稀疏语义来降低FLoc的不确定性。然而，它们仍然受到简约楼层平面中重复结构导致的模糊定位的影响，并且昂贵但有限的语义注释限制了其适用性。为了解决这些问题，我们提出了DisCo-FLoc，它利用双层视觉-几何对比来消除基于深度的视觉FLoc歧义，而不需要额外的语义标签。我们的方案首先使用针对射线投射式FLoc的射线回归预测器，利用深度估计专业知识预测一系列FLoc候选。随后，提出了一种具有位置级和方向级约束的新型对比学习方法，严格将基于深度的视觉特征与楼层平面中的相应几何结构匹配。这样的匹配可以有效消除FLoc歧义，并从FLoc候选中选择最佳成像姿态。 在两个标准视觉FLoc基准上进行的全面比较研究表明，我们的方法优于最先进的基于语义的方法，在鲁棒性和准确性方面都有显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决室内视觉平面图定位（FLoc）中因平面图重复结构导致的定位歧义问题。该问题在机器人导航、增强现实等应用中很重要，因为平面图易获取、长期稳定，但传统方法在缺乏语义信息时容易产生多模态错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先借鉴现有基于光束投射的FLoc和深度估计预训练模型，提出深度感知的光束回归预测器（RRP）以生成候选定位。随后引入双层对比学习（位置级和方向级），严格匹配视觉特征与平面图几何结构，消除歧义。该设计参考了光束投射、深度估计、对比学习以及语义辅助定位等已有工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用深度感知的RRP生成深度感知概率图（DAFPM），得到若干高概率候选位姿；再通过视觉-几何对比学习，对每个候选位姿对应的平面图局部结构进行正负样本对比，得到消歧概率图（DPM）。最后将DAFPM与DPM融合，选取概率最高的位姿作为最终定位结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 深度感知的光束回归预测器，利用预训练深度模型提升光束预测精度；2) 双层对比学习（位置级和方向级），无需语义标签即可实现严格的视觉-几何匹配；3) 通过消歧概率图显著降低重复结构导致的定位歧义。与以往单一几何匹配或依赖语义标签的方法不同，DisCo‑FLoc 在不使用额外语义信息的前提下实现了更高的鲁棒性和准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DisCo‑FLoc 通过深度感知的光束回归预测器和双层视觉-几何对比学习，在无需语义标签的条件下，显著消除平面图定位中的歧义，达到领先的定位精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.&lt;/p&gt;</description></item><item><guid>2601.01829v1</guid><title>RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data</title><link>http://arxiv.org/abs/2601.01829v1</link><author>Peiyan Hu, Haodong Feng, Hongyuan Liu, Tongtong Yan, Wenhao Deng, Tianrun Gao, Rong Zheng, Haoren Zheng, Chenglei Yu, Chuanrui Wang, Kaiwen Li, Zhi-Ming Ma, Dezhi Zhou, Xingcai Lu, Dixia Fan, Tailin Wu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; RealPDEBench 是首个将真实测量与数值仿真相结合的科学机器学习基准，包含五个数据集、三个任务、八个评估指标和十个基线。它揭示了仿真数据与真实数据之间的显著差异，并证明预训练能提升模型准确性和收敛速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 科学机器学习在缺乏昂贵真实世界数据的情况下，主要依赖模拟数据进行训练和验证，导致 sim‑to‑real 转移等关键任务受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个集成真实测量与配对仿真的基准，促进科学机器学习在真实世界中的发展与评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提供五个真实测量数据集及其对应的仿真数据，定义三个任务以比较真实与仿真数据，设计八个数据导向和物理导向评估指标，并对十个代表性基线进行基准测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明仿真数据与真实数据存在显著差异，但使用仿真数据预训练能持续提升模型的准确性和收敛速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RealPDEBench 为科学机器学习提供真实数据洞见，推动 sim‑to‑real 桥接和真实世界部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 预测复杂物理系统的演化仍是科学与工程中的核心问题。尽管科学机器学习模型取得了快速进展，但关键瓶颈在于缺乏昂贵的真实世界数据，导致大多数现有模型仅在模拟数据上训练和验证。缺乏真实数据不仅限制了科学机器学习的发展和评估，也阻碍了 sim‑to‑real 转移等关键任务的研究。我们提出 RealPDEBench，这是第一个将真实测量与配对数值仿真相结合的科学机器学习基准。RealPDEBench 包含五个数据集、三个任务、八个评估指标和十个基线。首先，我们提供了五个真实测量数据集，并与不同复杂物理系统的配对仿真数据集一起发布。随后定义了三个任务，便于比较真实与仿真数据，并促进桥接两者的方法研究。我们还设计了八个评估指标，涵盖数据导向和物理导向两类，并对十个代表性基线（包括最先进模型、预训练 PDE 基础模型和传统方法）进行了基准测试。实验结果显示，仿真数据与真实数据存在显著差异，但使用仿真数据预训练能持续提升准确性和收敛速度。我们希望通过 RealPDEBench 为科学机器学习提供真实数据洞见，推动 sim‑to‑real 桥接和真实世界部署。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.&lt;/p&gt;</description></item><item><guid>2601.01836v1</guid><title>COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs</title><link>http://arxiv.org/abs/2601.01836v1</link><author>Dasol Choi, DongGeon Lee, Brigitta Jesica Kartono, Helena Berndt, Taeyoun Kwon, Joonwon Jang, Haon Park, Hwanjo Yu, Minsuk Kahng</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 COMPASS 框架，用于评估大型语言模型是否遵守组织的允许和禁止列表政策，并在八个行业场景中测试 5,920 条查询，发现模型在合法请求上表现良好，但在执行禁止规则时表现差，表明当前模型缺乏必要的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大型语言模型在医疗、金融等高风险企业应用中的部署，确保其遵守组织特定政策变得至关重要，但现有安全评估仅关注普遍危害。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发首个系统化框架 COMPASS，以评估 LLM 是否符合组织的允许和禁止列表政策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在八个行业场景中生成并验证 5,920 条查询，涵盖常规合规和通过设计的边缘案例进行的对抗鲁棒性测试，并评估七个最先进模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在合法请求上准确率超过 95%，但在执行禁止列表时仅拒绝 13-40% 的对抗违规，显示出显著的不对称性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 当前 LLM 缺乏满足政策关键部署所需的鲁棒性，COMPASS 成为组织 AI 安全评估的必要工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着大型语言模型在医疗、金融等高风险企业应用中的部署，确保其遵守组织特定政策已变得至关重要。然而，现有的安全评估仅关注普遍危害。我们提出 COMPASS（公司/组织政策对齐评估），这是第一个系统化框架，用于评估 LLM 是否符合组织的允许列表和禁止列表政策。我们将 COMPASS 应用于八个多样化的行业场景，生成并验证 5,920 条查询，测试常规合规性和通过策略性设计的边缘案例进行的对抗鲁棒性。评估七个最先进模型后，我们发现一个根本的不对称性：模型可靠地处理合法请求（&amp;gt;95% 的准确率），但在执行禁止规则时表现灾难性，仅拒绝 13-40% 的对抗禁止违规。这些结果表明当前 LLM 缺乏满足政策关键部署所需的鲁棒性，确立 COMPASS 为组织 AI 安全评估的必要框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (&amp;gt;95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.&lt;/p&gt;</description></item><item><guid>2601.01891v1</guid><title>Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems</title><link>http://arxiv.org/abs/2601.01891v1</link><author>Niloufar Alipour Talemi, Julia Boone, Fatemeh Afghah</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本综述指出地球观测分析正从静态深度学习模型转向自主代理式人工智能，提出了单代理与多代理系统的统一分类法，分析了规划机制、检索增强生成和记忆结构等架构基础，并评估了从像素级精度向轨迹感知推理正确性转变的新基准，最后指出在定位、可靠性与协同方面的不足，并给出发展路线图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地球观测分析正从传统的静态深度学习模型转向需要序列规划和主动工具协同的自主代理式人工智能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 首次全面综述遥感领域的代理式人工智能，构建统一分类法，分析关键架构，评估新基准，并提出实现稳健自主地理空间情报的战略路线图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过统一分类法区分单代理与多代理系统，分析规划机制、检索增强生成和记忆结构等架构基础，回顾新基准的设计思路，并对定位、安全与协同等局限性进行批判性评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1) 单代理与多代理系统的区别与适用场景；2) 现有视觉基础模型和多模态大语言模型缺乏序列规划与工具协同；3) 新基准从像素级精度转向轨迹感知推理正确性；4) 在定位、可靠性与协同方面仍存在显著不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 需要在定位、可靠性与协同方面进行改进，制定路线图以实现稳健的自主地理空间情报系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 地球观测分析的范式正从静态深度学习模型转向自主代理式人工智能。尽管最近的视觉基础模型和多模态大型语言模型推动了表征学习，但它们往往缺乏复杂地理空间工作流程所需的序列规划和主动工具协同。本综述呈现了遥感领域代理式人工智能的首个全面回顾。我们提出了一个统一的分类法，区分单代理副驾驶和多代理系统，并分析了规划机制、检索增强生成和记忆结构等架构基础。此外，我们回顾了新兴基准，将评估从像素级精度转向轨迹感知推理正确性。通过批判性审视定位、安全和协同方面的局限性，本文概述了发展稳健自主地理空间情报的战略路线图。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.&lt;/p&gt;</description></item><item><guid>2601.01914v1</guid><title>Learning Action Hierarchies via Hybrid Geometric Diffusion</title><link>http://arxiv.org/abs/2601.01914v1</link><author>Arjun Ramesh Kaushik, Nalini K. Ratha, Venu Govindaraju</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的混合欧氏与双曲几何的扩散模型框架 HybridTAS，用于视频中的时间动作分割任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统方法通过迭代细化来为视频帧分配动作标签，但未能充分利用人类动作的层级结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用双曲几何的树状关系，在扩散模型的去噪过程中显式地利用动作的层级结构，从而实现更精细的动作分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; HybridTAS 在扩散模型的去噪阶段融合欧氏和双曲几何，采用从粗到细的指导策略：在较早的扩散时间步受抽象高层动作类别影响，后期时间步则细化为细粒度动作类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 GTEA、50Salads 和 Breakfast 三个基准数据集上，HybridTAS 达到了最先进的性能，验证了双曲引导去噪的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 双曲几何引导的去噪方法能够充分利用动作的层级结构，显著提升时间动作分割的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时间动作分割是视频理解中的关键任务，其目标是为视频中的每一帧分配动作标签。虽然最近的进展利用了迭代细化策略，但它们未能显式利用人类动作的层级特性。在本文中，我们提出了 HybridTAS——一种新颖的框架，它将欧氏几何与双曲几何的混合引入扩散模型的去噪过程，以利用动作的层级结构。双曲几何自然提供了嵌入之间的树状关系，使我们能够以粗到细的方式引导动作标签的去噪过程：较高的扩散时间步受抽象的高层动作类别（根节点）的影响，而较低的时间步则使用细粒度动作类别（叶节点）进行细化。我们在三个基准数据集 GTEA、50Salads 和 Breakfast 上进行了广泛实验，结果表明我们的方法达到了最先进的性能，验证了双曲引导去噪在时间动作分割任务中的有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.&lt;/p&gt;</description></item><item><guid>2601.01924v1</guid><title>Self-Supervised Learning with Noisy Dataset for Rydberg Microwave Sensors Denoising</title><link>http://arxiv.org/abs/2601.01924v1</link><author>Zongkai Liu, Qiming Ren, Wenguang Yang, Yanjie Tong, Huizhen Wang, Yijie Zhang, Ruohao Zhi, Junyao Xie, Mingyong Jing, Hao Zhang, Liantuan Xiao, Suotang Jia, Ke Tang, Linjie Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自监督深度学习框架，用于Rydberg传感器的单次测量噪声抑制，效果与多次测量平均相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在量子传感中，获取干净参考信号困难，传统噪声抑制方法如小波变换和卡尔曼滤波存在计算量大、效果有限的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种无需干净参考信号、计算效率高且能实现高精度噪声抑制的自监督深度学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用两组具有相同统计分布的噪声信号进行训练，构建自监督模型；在Rydberg传感器数据集上评估，比较U-Net和Transformer两种架构的复杂度与性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架在Rydberg传感器数据集上优于小波变换和卡尔曼滤波，等效于一万次平均的去噪效果，同时计算时间缩短三百倍；在不同噪声环境下表现稳健，并量化了模型复杂度与性能的权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自监督深度学习框架为Rydberg传感器提供了高效、精确的噪声抑制方案，可为量子传感器系统的深度学习去噪优化提供实用指导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们报告了一种用于Rydberg传感器的自监督深度学习框架，该框架实现了单次测量噪声抑制，精度与多次测量平均相匹配。该框架通过在两组具有相同统计分布的噪声信号上训练，消除了对干净参考信号的需求（在量子传感中几乎不需要）。在Rydberg传感器数据集上评估时，该框架优于小波变换和卡尔曼滤波，去噪效果相当于一万次平均，同时计算时间缩短三百倍。我们进一步在多种噪声配置下验证了性能，并量化了U-Net和Transformer架构的复杂度-性能权衡，为优化基于深度学习的Rydberg传感器去噪提供了可操作的指导。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We report a self-supervised deep learning framework for Rydberg sensors that enables single-shot noise suppression matching the accuracy of multi-measurement averaging. The framework eliminates the need for clean reference signals (hardly required in quantum sensing) by training on two sets of noisy signals with identical statistical distributions. When evaluated on Rydberg sensing datasets, the framework outperforms wavelet transform and Kalman filtering, achieving a denoising effect equivalent to 10,000-set averaging while reducing computation time by three orders of magnitude. We further validate performance across diverse noise profiles and quantify the complexity-performance trade-off of U-Net and Transformer architectures, providing actionable guidance for optimizing deep learning-based denoising in Rydberg sensor systems.&lt;/p&gt;</description></item><item><guid>2601.01952v1</guid><title>Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration</title><link>http://arxiv.org/abs/2601.01952v1</link><author>Max Unterbusch, Andreas Vogelsang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种人机LLM协作方法，将缺陷预测视为自适应过程，利用链式思维推理和反馈循环，快速提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的自动化需求评估依赖通用模式和规则或机器学习分类器，但缺陷定义因项目、领域和利益相关者而异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将缺陷预测从静态分类转变为可适应的过程，利用人类反馈和LLM的链式思维能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 人机LLM协作（HLC）方法：使用LLM链式思维推理，用户验证预测及其解释，验证示例通过少量样本学习动态引导后续预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; HLC在仅20个验证示例下即可显著提升性能，结合验证解释的方式比标准少量提示和微调BERT模型表现更好，并保持高召回率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LLM的上下文学习和链式思维能力支持自适应分类，超越“一刀切”模型，为持续从利益相关者反馈中学习的工具提供机会。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自动化需求评估传统上依赖通用模式作为缺陷性的代理，通过基于规则的启发式或在大型标注数据集上训练的机器学习分类器实现。然而，什么构成“缺陷”本质上是依赖上下文的，并在项目、领域和利益相关者解释之间有所不同。在本文中，我们提出了一种人机LLM协作（HLC）方法，将缺陷预测视为自适应过程而非静态分类任务。HLC利用LLM链式思维推理在反馈循环中：用户在验证预测及其解释后，这些已验证的示例通过少量样本学习自适应地指导未来的预测。我们使用QuRE基准的弱词气味对1,266条标注的梅赛德斯-奔驰需求评估了该方法。我们的结果表明，HLC能够有效适应已验证示例的提供，并在仅20个已验证示例的情况下实现快速性能提升。结合已验证的解释，而不仅仅是标签，使HLC显著优于标准少量提示和微调BERT模型，同时保持高召回率。这些结果凸显了LLM的上下文和链式思维学习能力如何实现超越“一刀切”模型的自适应分类方法，为能够持续从利益相关者反馈中学习的工具创造机会。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a &amp;quot;defect&amp;quot; is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.&lt;/p&gt;</description></item><item><guid>2601.01954v1</guid><title>Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations</title><link>http://arxiv.org/abs/2601.01954v1</link><author>Alexander Korn, Lea Zaruchas, Chetan Arora, Andreas Metzger, Sven Smolka, Fanyu Wang, Andreas Vogelsang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了大型语言模型在软件工程中的应用，指出提示工程的重要性，并提出了改进提示报告的指南。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型，尤其是仅解码器的生成模型，如 GPT，正被越来越多地用于自动化软件工程任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 系统评估当前论文中提示设计、测试和优化的报告情况，并制定结构化的提示报告指南。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 第一阶段对近300篇自2022年以来顶级软件工程会议的论文进行分析；第二阶段对105位程序委员会成员进行调查，收集他们对提示报告的期望。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 发现当前实践与评审者期望存在显著偏差，尤其在版本披露、提示理由说明和有效性威胁方面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提出的指南可提升 LLM 驱动的软件工程研究的透明度、可重复性和方法学严谨性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型，特别是仅解码器的生成模型，如 GPT，正越来越多地用于自动化软件工程任务。这些模型主要通过自然语言提示进行引导，使提示工程成为系统性能和行为的关键因素。尽管它们在软件工程研究中的作用日益增长，但与提示相关的决策很少以系统化或透明的方式记录，阻碍了研究的可重复性和可比性。为了解决这一差距，我们开展了两阶段实证研究。首先，我们分析了自2022年以来在前三大软件工程会议上发表的近300篇论文，以评估提示设计、测试和优化目前的报告方式。其次，我们对这些会议的105位程序委员会成员进行了调查，以捕捉他们对 LLM 驱动研究中提示报告的期望。基于研究结果，我们制定了一套结构化指南，将必要、可取和卓越的报告要素区分开来。我们的结果显示，当前实践与评审者期望之间存在显著不匹配，特别是在版本披露、提示理由说明和有效性威胁方面。我们将指南视为提升 LLM 基础软件工程研究透明度、可重复性和方法学严谨性的步骤。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.&lt;/p&gt;</description></item><item><guid>2601.02002v1</guid><title>Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models</title><link>http://arxiv.org/abs/2601.02002v1</link><author>Antonio Colacicco, Vito Guida, Dario Di Palma, Fedelucio Narducci, Tommaso Di Noia</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这篇论文研究了大型语言模型在推荐系统中的数据泄露问题，探讨了通过自动化方法提取已记忆的MovieLens-1M数据的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; LLMs因强大的自然语言理解和生成能力被广泛应用于推荐场景，但其训练数据不公开，导致对数据泄露的担忧。已有研究表明LLaMA和OpenAI模型记住了MovieLens-1M数据，但提取方式仅靠人工提示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 论文提出三个问题：能否改进人工提示？是否可以通过非人工提示的方法检测LLM记忆？是否可以自动化检测数据泄露？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 评估三种方法：1）利用越狱提示工程；2）无监督潜在知识发现，使用对比一致搜索（CCS）和Cluster-Norm探测内部激活；3）自动提示工程（APE），将提示发现视为元学习过程，迭代优化提示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明越狱提示未提升记忆检索且不稳定；CCS能可靠区分真实与伪造电影标题，但对用户编号和评分数据失效；APE在检索条目级信息方面取得中等成功，但难以恢复数值交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自动化优化提示是提取已记忆样本的最有前景策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 这篇论文探讨了大型语言模型在推荐系统中的数据泄露问题，并评估了三种自动化方法来提取已记忆的MovieLens-1M数据。实验结果显示，自动提示优化方法在检索已记忆信息方面最为有效。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.&lt;/p&gt;</description></item><item><guid>2601.02020v1</guid><title>Adapting Depth Anything to Adverse Imaging Conditions with Events</title><link>http://arxiv.org/abs/2601.02020v1</link><author>Shihan Peng, Yuyang Xiong, Hanyu Zhou, Zhiwei Shi, Haoyue Liu, Gang Chen, Luxin Yan, Yi Chang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于事件相机的时空融合框架ADAE，用于在光照变化和运动模糊等恶劣条件下提升Depth Anything模型的深度估计性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度估计在机器人系统中至关重要，但现有的深度基础模型在极端光照和运动模糊等不良成像条件下表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在保持深度基础模型开放世界知识和泛化能力的前提下，利用事件相机补偿帧图像的失真，提升在恶劣环境下的深度估计精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出ADAE框架，包含熵感知空间融合和运动引导时间校正两部分：第一部分根据信息熵自适应融合帧图像和事件特征；第二部分利用事件的运动线索校正模糊区域的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，ADAE在极端光照和运动模糊场景中显著优于传统方法，验证了两种融合策略的互补性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ADAE通过事件引导的时空融合有效提升了Depth Anything在恶劣成像条件下的鲁棒性，展示了在保持基础模型优势的同时实现专用融合的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在动态和恶劣照明条件下实现鲁棒深度估计对于机器人系统至关重要。目前，像Depth Anything这样的深度基础模型在理想场景中取得了巨大成功，但在极端照明和运动模糊等不良成像条件下仍面临挑战。这些失真会破坏帧相机的视觉信号，削弱空间和时间维度上基于帧的深度的判别特征。现有方法通常利用事件相机的高动态范围和时间分辨率来补偿受损的帧特征，但此类专用融合模型大多在特定领域数据集上从零开始训练，无法继承基础模型的开放世界知识和鲁棒泛化能力。本文提出ADAE，一种针对降解场景的事件引导时空融合框架，用于Depth Anything。我们的设计基于两个关键洞察：1）熵感知空间融合。我们使用信息熵策略自适应融合帧图像和事件特征，以指示光照诱发的失真。2）运动引导时间校正。我们利用事件的运动线索重新校准模糊区域中模糊的特征。在我们的统一框架下，这两个组件相互补充，共同提升Depth Anything在不良成像条件下的性能。已进行广泛实验以验证所提方法的优越性。代码将在接受后发布。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在解决在低光、运动模糊或高动态范围等恶劣成像条件下的深度估计问题。深度信息对自动驾驶、机器人导航和增强现实等应用至关重要，而传统深度模型在这些条件下性能显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的Depth Anything大规模深度模型基础上，结合事件相机的高时间分辨率和宽动态范围特性，设计了一个融合RGB与事件信息的网络。该方法借鉴了事件驱动深度估计和多模态融合的相关工作，并在此基础上引入了轻量级事件编码器和融合模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将事件数据与预训练的Depth Anything模型相结合，使模型在恶劣条件下保持鲁棒性。实现流程包括：采集RGB与事件序列；使用事件编码器提取时空特征；将事件特征与Depth Anything提取的RGB特征进行融合；通过深度解码器输出深度图；使用深度损失进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 将事件信息融入大规模深度模型；2) 设计轻量级事件编码器以捕捉时空动态；3) 提出融合策略兼顾RGB深度线索与事件线索；4) 通过专门的训练协议将Depth Anything适配恶劣成像。与以往仅使用RGB或小模型的工作相比，该方法在大规模预训练模型和事件数据的双重支持下，显著提升了在恶劣条件下的深度估计性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过将事件相机数据与大规模深度模型相融合，提出了一种在恶劣成像条件下实现高精度深度估计的方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.&lt;/p&gt;</description></item><item><guid>2601.02029v1</guid><title>Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding</title><link>http://arxiv.org/abs/2601.02029v1</link><author>Toshihiko Nishimura, Hirofumi Abe, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文提出一种无需3D标注或RGB图像的训练自由3D语义分割方法。通过虚拟相机将点云投影到二维图像，利用自然语言提示驱动的基础2D模型进行分割，再通过多视角加权投票聚合得到3D分割。该方法在训练自由场景下优于现有方法，且分割精度接近监督式方法，并支持开放词汇识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统3D语义分割依赖大量标注数据和RGB图像，限制了大规模点云处理的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种不需要3D训练数据或配对RGB图像的训练自由3D分割技术，并实现开放词汇识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 用虚拟相机将3D点云投影到多张二维图像；2. 使用自然语言提示驱动的2D分割模型对每张图像进行语义分割；3. 通过加权投票将多视角预测聚合回3D空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在训练自由设置下的性能超过现有方法，且与监督式方法相当；支持任意文本查询的开放词汇识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过投影+2D模型+投票的组合，可实现高效、训练自由且开放词汇的3D语义分割，为大规模点云分析提供新思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决在没有3D标注数据或配对RGB图像的情况下，对大规模户外点云进行语义分割的问题。此问题重要，因为3D数据的标注成本高且难以获取，而在自动驾驶、增强现实等应用中需要快速、可扩展的3D理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了将2D图像与语言模型结合的先行工作，提出使用虚拟相机将点云投影到2D视图，并利用基于自然语言提示的2D视觉‑语言模型（如Grounded SAM）进行分割。随后通过加权投票将多视角结果融合回3D，实现无训练、无标注的分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云投影为多张2D图像，使用2D‑VLM进行分割，然后将分割结果映射回3D并通过加权投票聚合。实现流程包括：①根据轨迹放置虚拟相机并渲染图像；②用Grounded SAM对图像进行语义分割；③将像素标签投影到对应3D点；④对每个3D点收集邻近视角的标签，按置信度和距离加权投票；⑤可选的鸟瞰视角细化以提升难识别对象的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①完全无训练、无3D标注的分割框架；②利用2D视觉‑语言模型实现开放词汇识别；③多视角加权投票融合策略；④针对大型或细长结构的鸟瞰细化模块。与以往仅在室内或小范围户外、需要RGB图像或训练的工作不同，该方法在大规模户外点云上实现了接近监督学习的精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种训练‑免费、开放词汇的3D分割框架，通过将大规模户外点云投影为2D图像、使用视觉‑语言模型分割并加权投票融合，实现在无3D标注的条件下接近监督学习的分割精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.&lt;/p&gt;</description></item><item><guid>2601.02043v1</guid><title>Simulated Reasoning is Reasoning</title><link>http://arxiv.org/abs/2601.02043v1</link><author>Hendrik Kempt, Alon Lavie</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了基础模型（FM）在推理任务中不依赖传统符号推理的能力，指出它们通过“大声思考”式的自我测试和迭代实现推理，但缺乏扎根与常识导致推理脆弱，进而呼吁更新安全与规范框架，并废弃“随机鹦鹉”比喻。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统上，推理被视为连接理解阶段的符号过程；基础模型的出现挑战了这一观点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 对FM推理的哲学解释进行讨论，批判“随机鹦鹉”比喻的适用性，并探讨其安全与适当性问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 分析FM的推理行为，进行哲学讨论，并评估安全规范。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; FM可通过自我测试和迭代实现推理，缺乏扎根与常识导致推理脆弱，这改变了对推理必要条件的认知，并为安全防御提供了新思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; “随机鹦鹉”比喻已过时，需要新的安全与规范框架来应对FM推理的脆弱性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 推理长期以来被视为理解阶段之间的桥梁。适当的推理能导致对特定主题的理解。此类推理被概念化为一种特定方式的理解过程，即“符号推理”。基础模型（FM）表明，这并非许多推理任务的必要条件：它们可以通过模仿“大声思考”的过程、测试生成的路径并自行迭代这些路径来“推理”。这导致了一种能够独立或少量样本学习解决问题的推理形式，但由于缺乏扎根和常识，显得与人类推理根本不同，导致推理过程脆弱。这些见解有望大幅改变我们对推理及其必要条件的评估，同时也为针对FM脆弱性的安全和稳健防御提供信息。本文提出并讨论了对这一现象的若干哲学解释，认为先前贴切的“随机鹦鹉”比喻已失去相关性，应予以废弃，并反思这些推理模型及其日益增长的能力所带来的安全和适当性方面的不同规范要素。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., &amp;quot;symbolic reasoning&amp;quot;. Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can &amp;quot;reason&amp;quot; by way of imitating the process of &amp;quot;thinking out loud&amp;quot;, testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the &amp;quot;stochastic parrot&amp;quot; has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.&lt;/p&gt;</description></item><item><guid>2601.02090v1</guid><title>Flo: A data-driven limited-area storm surge model</title><link>http://arxiv.org/abs/2601.02090v1</link><author>Nils Melsom Kristensen, Mateusz Matuszak, Paulina Tedesco, Ina Kristine Berentsen Kullmann, Johannes Röhrs</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了Flo模型，一种基于图神经网络的数据驱动风暴潮预测模型，覆盖北海、挪威海和巴伦支海，能够以4公里水平分辨率和1小时时间分辨率模拟风暴潮水位，性能与训练所用的数值模型相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的风暴潮预测主要依赖纯物理数值模型，近年来机器学习技术的进步为改进预测方法提供了可能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建并评估Flo模型，以验证其在风暴潮预测中的准确性，并为未来更灵活的观测融合模型奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Anemoi框架构建模型，采用图神经网络；训练数据为43年挪威再分析的气象数据和NORA‑Surge风暴潮后向模拟；通过与欧洲沿岸90余个水位计观测以及NORA‑Surge后向模拟结果进行比较进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Flo模型的后向预测准确度与NORA‑Surge相当；由于训练数据未包含数据同化，Flo在与观测对比时并未表现出优于数值模型的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 虽然Flo在预测精度上并未显著提升，但它标志着风暴潮预测从纯物理模型向机器学习方法转变的关键一步，为后续加入观测信息的更灵活模型提供了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了Flo模型，一种基于图神经网络的数据驱动风暴潮模型，覆盖北海、挪威海和巴伦支海。该模型使用Anemoi框架构建，能够以4公里水平分辨率和1小时时间分辨率模拟风暴潮水位，其质量与训练所用的数值模型相当。模型训练使用了43年的挪威再分析气象数据和NORA‑Surge风暴潮后向模拟数据。通过将Flo模型的后向运行结果与欧洲沿岸90余个水位计的独立观测以及NORA‑Surge后向模拟进行比较进行评估，结果显示Flo模型的准确度与NORA‑Surge相当。由于训练数据未采用数据同化，Flo模型在与观测对比时并未表现出优于数值模型的优势。本文工作标志着风暴潮预测从纯物理数值模型向利用机器学习技术的转变，为未来训练更灵活、能够更好利用观测信息的风暴潮模型奠定了基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present Flo, a data-driven storm surge model, covering the North Sea, Norwegian Sea and Barents Sea. The model is built using the Anemoi framework for creating machine learning weather forecasting systems, developed by the European Centre for Medium-Range Weather Forecasts and partners. The model is based on a graph neural network, and is capable of simulating water level due to storm surge at a horizontal resolution of 4 km and a temporal resolution of 1 hour with a quality comparable to the numerical model on which it was trained. The model was trained using a data set consisting of 43 years of atmospheric data from the 3-km Norwegian Reanalysis hindcast for mean sea level pressure and winds, and the NORA-Surge hindcast for storm surge. Evaluation was done by comparing results from hindcast runs of the Flo model against independent observations of more than 90 water level gauges along the European coast, and against the NORA-Surge hindcast. The evaluation shows that Flo produces hindcasts with accuracy similar to the NORA-Surge hindcast. Since no data assimilation was applied in the NORA-Surge hindcast used as training data, the Flo model is not expected to outperform the numerical model when compared to observations. The current work takes an important step transforming storm surge forecasting from being based purely on numerical physics-based models, to taking advantage of recent advancements in machine learning. This does not represent a large step forward with regards to improving the forecast skill, but forms a foundation for future training of a storm surge model that offers more flexibility for taking observations into account.&lt;/p&gt;</description></item><item><guid>2601.02102v1</guid><title>360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images</title><link>http://arxiv.org/abs/2601.02102v1</link><author>Jiaqi Yao, Zhongmiao Yan, Jingyi Xu, Songpengcheng Xia, Yan Xiang, Ling Pei</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这篇论文提出了一种新的前馈式3D高斯喷射框架，用于360度图像，能够生成几何一致的高斯原语，同时保持高渲染质量，并通过深度-法线几何正则化显著提升几何一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D场景重建是空间智能应用的基础，传统多视角立体方法在稀疏视角或低纹理区域表现不佳，神经渲染虽能产生高质量结果，但需要逐场景优化且实时性不足；显式3D高斯喷射实现了高效渲染，但大多数前馈变体侧重视觉质量，缺乏几何一致性，限制了精确表面重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过前馈式3D高斯喷射实现360度图像的几何一致重建，提升点云和表面精度，同时保持渲染质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用深度-法线几何正则化，将渲染深度梯度与法线信息耦合，监督高斯旋转、尺度和位置，从而改进点云和表面精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在保持高渲染质量的同时，显著提升了几何一致性，为空间感知任务提供了有效的3D重建方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该前馈式3D高斯喷射框架通过几何正则化实现了高质量且几何一致的重建，适用于需要实时性和精确几何的空间感知应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D场景重建是空间智能应用（如AR、机器人技术和数字孪生）的基础。传统多视角立体方法在视角稀疏或低纹理区域表现不佳，而神经渲染方法虽然能产生高质量结果，但需要针对每个场景进行优化，且缺乏实时效率。显式3D高斯喷射（3DGS）实现了高效渲染，但大多数前馈变体侧重视觉质量而非几何一致性，限制了精确表面重建和空间感知任务的整体可靠性。本文提出了一种针对360度图像的新型前馈式3DGS框架，能够生成几何一致的高斯原语，同时保持高渲染质量。引入深度-法线几何正则化，将渲染深度梯度与法线信息耦合，监督高斯的旋转、尺度和位置，以提升点云和表面精度。实验结果表明，所提出的方法在保持高渲染质量的同时，显著提升了几何一致性，为空间感知任务提供了有效的3D重建方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在从360度图像快速生成几何一致的三维场景重建，解决传统多视角立体匹配在稀疏视角或低纹理区域表现不佳，以及神经渲染方法需要逐场景优化且实时性差的问题。准确、实时的三维重建对增强现实、机器人定位和数字孪生等空间感知任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合SphereCNN提取球面特征、构建成本体估计深度，并借鉴360Recon、MVSplat、Splatter-360等方法的特征融合与U‑Net解码框架。通过FiLM实现多尺度特征调制，并引入Depth‑Normal正则化，借鉴NeuSG的几何约束，形成端到端的快速推理网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接在360图像上预测3D高斯原语，并用深度-法线正则化保证几何一致性。流程包括：①用SphereCNN提取特征并构成本体得到初始深度；②提取多尺度特征并通过FiLM调制；③将深度、RGB和特征融合后送入U‑Net+适配器回归高斯参数；④渲染深度和法线，计算Depth‑Normal损失；⑤端到端训练得到几何一致的高斯点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①面向360图像的全流程feed‑forward 3DGS网络；②Depth‑Normal正则化，将深度梯度与法线耦合，联合监督高斯位置、尺度和方向；③通过压平高斯实现更精确的交点深度计算；④在保持渲染质量的同时显著提升几何一致性。与以往仅关注视觉质量或需要逐场景优化的工作不同，该方法兼顾速度、通用性和几何精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种快速、端到端的360图像3D高斯分裂框架，通过深度‑法线正则化实现几何一致的三维重建，同时保持高渲染质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.&lt;/p&gt;</description></item><item><guid>2601.02112v1</guid><title>Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model</title><link>http://arxiv.org/abs/2601.02112v1</link><author>Utkarsh Singh, Absaar Ali, Adarsh Roy</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种轻量级的替代模型，用于快速预测汽车气动阻力系数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的CFD和风洞测试耗时耗力，限制了早期设计的快速迭代。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种计算成本低、可解释且精度高的气动预测模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将车辆3D点云按流向切成2D截面，使用轻量级PointNet2D编码每个截面，再用双向LSTM捕捉截面序列的纵向几何变化，训练于DrivAerNet++数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在预测阻力系数时决定系数超过0.95，平均绝对误差约为0.006，推理时间仅约0.025秒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法实现了快速、准确且可解释的气动反馈，可加速汽车设计探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 汽车行业追求更高燃油经济性和性能，需要高效的气动设计。然而，传统的计算流体动力学（CFD）和风洞测试方法资源消耗大，阻碍了早期设计阶段的快速迭代。基于机器学习的代理模型提供了有前景的替代方案，但许多现有方法存在计算复杂度高、可解释性有限或对细节几何输入精度不足的问题。本文提出一种新型轻量级代理模型，用于预测3D车辆几何的气动阻力系数（Cd）。受医学影像启发，将车辆的3D点云分解为沿流向轴的有序2D截面序列。每个截面由轻量级PointNet2D模块编码，截面嵌入序列随后由双向LSTM处理，以捕捉纵向几何演变。该模型在DrivAerNet++数据集上训练和评估，取得了高决定系数（R^2&amp;gt;0.9528）和低平均绝对误差（MAE≈0.006）的Cd预测结果。推理时间约为0.025秒/样本（在消费级GPU上），我们的方案提供了快速、准确且可解释的气动反馈，促进更敏捷、信息化的汽车设计探索。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The automotive industry&amp;#x27;s pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 &amp;gt; 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.&lt;/p&gt;</description></item><item><guid>2601.02147v1</guid><title>BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models</title><link>http://arxiv.org/abs/2601.02147v1</link><author>Sunny Gupta, Shounak Das, Amit Sethi</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种双向提示优化框架BiPrompt，在测试时同时对视觉和文本两种模态进行去偏，提升模型在分布偏移下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视觉语言基础模型如CLIP在零样本推理上表现优异，但易受视觉与文本模态间的伪相关影响，现有去偏方法多聚焦单一模态，导致鲁棒性不完整。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不需要重新训练或域监督的前提下，降低模型对非因果特征的依赖，使其在测试时能够进行因果、域不变的推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 视觉端使用结构化注意力引导的抹除技术抑制背景激活，并强制因果与伪相关区域的预测保持正交一致；文本端引入平衡提示归一化，学习重心化机制将类别嵌入对齐到等方性语义空间；两端模块共同最小化伪相关与预测之间的条件互信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实与合成偏差基准上，BiPrompt在平均准确率和最差组准确率上均优于现有测试时去偏方法，验证了其轻量且有效的特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BiPrompt为可信且因果基础的视觉语言适配提供了一条轻量化、无需重训练的有效路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.&lt;/p&gt;</description></item><item><guid>2601.02157v1</guid><title>Multi-fidelity graph-based neural networks architectures to learn Navier-Stokes solutions on non-parametrized 2D domains</title><link>http://arxiv.org/abs/2601.02157v1</link><author>Francesco Songia, Raoul Sallé de Chou, Hugues Talbot, Irene Vignon-Clementel</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于图的多保真学习框架，用于预测二维非参数几何中的稳态Navier-Stokes解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在流体动力学中，精确求解Navier-Stokes方程往往计算量大，传统方法难以在复杂几何中高效实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过从简化模型逐步逼近完整Navier-Stokes解，利用物理知识引导学习过程，提高预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 结合图神经网络、Transformer和Mamba架构，使用无监督节点排序适配Mamba；通过加权最小二乘法构造代数算子计算导数；在编码-处理-物理信息解码管道中嵌入物理约束；使用差分算子丰富图卷积以满足质量守恒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Transformer在准确性上表现最佳；Mamba在保持性能的同时显著降低计算成本；物理约束的加入使模型输出更符合方程并提升预测质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架通过物理知识和流体动力学洞察成功引导学习，得到更平滑、准确的Navier-Stokes预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种基于图的多保真学习框架，用于预测非参数二维几何中的稳态Navier-Stokes解。该方法通过从简化的低阶模型和完整的Stokes模型开始，逐步逼近Navier-Stokes解，引导学习过程。为有效捕捉速度和压力场中的局部与长程依赖关系，我们将图神经网络与Transformer和Mamba架构相结合。虽然Transformer在准确性上表现最佳，但我们展示了Mamba可以通过无监督的节点排序策略成功适配图结构数据。Mamba方法在保持性能的同时显著降低了计算成本。物理知识直接嵌入到编码-处理-物理信息解码管道中。导数通过加权最小二乘法构造的代数算子计算。这些算子的灵活性不仅使输出满足控制方程，还能约束选定的隐藏特征满足质量守恒。我们通过使用相同的微分算子丰富图卷积，引入额外的物理偏置。总体而言，我们通过物理知识和流体动力学洞察成功引导学习过程，得到更平滑、更准确的预测。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose a graph-based, multi-fidelity learning framework for the prediction of stationary Navier--Stokes solutions in non-parametrized two-dimensional geometries. The method is designed to guide the learning process through successive approximations, starting from reduced-order and full Stokes models, and progressively approaching the Navier--Stokes solution. To effectively capture both local and long-range dependencies in the velocity and pressure fields, we combine graph neural networks with Transformer and Mamba architectures. While Transformers achieve the highest accuracy, we show that Mamba can be successfully adapted to graph-structured data through an unsupervised node-ordering strategy. The Mamba approach significantly reduces computational cost while maintaining performance. Physical knowledge is embedded directly into the architecture through an encoding -- processing -- physics informed decoding pipeline. Derivatives are computed through algebraic operators constructed via the Weighted Least Squares method. The flexibility of these operators allows us not only to make the output obey the governing equations, but also to constrain selected hidden features to satisfy mass conservation. We introduce additional physical biases through an enriched graph convolution with the same differential operators describing the PDEs. Overall, we successfully guide the learning process by physical knowledge and fluid dynamics insights, leading to more regular and accurate predictions&lt;/p&gt;</description></item><item><guid>2601.02196v1</guid><title>ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense</title><link>http://arxiv.org/abs/2601.02196v1</link><author>Yu Li, Sizhe Tang, Rongqian Chen, Fei Xu Yu, Guangyu Jiang, Mahdi Imani, Nathaniel D. Bastian, Tian Lan</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于蒙特卡洛树搜索（MCTS）和图神经网络（GNN）的规划方法，用于自动化网络防御（ACD），通过学习网络状态的图嵌入来引导搜索，从而在复杂网络环境中实现更高效的样本利用和更强的防御效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的深度强化学习方法在面对大规模决策空间和复杂网络结构时，探索效率低下，需要大量样本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在CAGE-4挑战中构建一种样本高效的防御策略，改进探索-利用权衡，提升ACD性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将ACD建模为基于上下文的部分可观测马尔可夫决策问题；使用MCTS进行规划；利用GNN将网络观测嵌入为属性图；用学习到的图嵌入和图编辑动作先验来引导MCTS；结合无模型泛化、策略蒸馏与前瞻规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多样化的网络结构和对手行为场景中，搜索引导、图嵌入的规划方法相较于最先进的强化学习基线，显著提升了防御奖励和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于MCTS和GNN的规划策略在自动化网络防御中具有高效、鲁棒的优势，为样本高效的ACD提供了可行路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自动化网络防御旨在通过最小或无人工干预来保护计算机网络，本文提出一种基于蒙特卡洛树搜索和图神经网络的规划方法，显著提升了样本效率和防御效果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.&lt;/p&gt;</description></item><item><guid>2601.02198v1</guid><title>Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models</title><link>http://arxiv.org/abs/2601.02198v1</link><author>Alexander Möllers, Julius Hense, Florian Schulz, Timo Milbich, Maximilian Alber, Lukas Ruff</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了病理学基础模型在不同放大率下的表现及训练时的放大率采样策略。作者将放大率采样视为多源域适应问题，构建理论框架揭示采样策略的权衡。实验发现传统的离散均匀采样在中间放大率上会导致性能下降。作者提出连续放大率采样，填补了放大率覆盖的空白，并保持了标准尺度的性能。进一步推导出优化的采样分布，可进一步提升表示质量。通过新建的 TCGA-MS 与 BRACS-MS 基准，实验表明连续采样在中间放大率上比离散采样提升高达四个百分点，优化分布还能进一步提升性能。最后评估现有模型发现放大率是性能变化的主要驱动因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在组织病理学中，病理学家需要在低倍放大率下观察组织结构，在高倍放大率下观察细粒度形态，但目前对不同放大率下模型性能及训练时采样策略的影响了解不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究放大率采样对病理学基础模型性能的影响，提出更优的采样策略，并评估其效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将放大率采样建模为多源域适应问题，构建理论框架；提出连续放大率采样并推导优化采样分布；创建 TCGA-MS 与 BRACS-MS 两个基准并设计相应评估指标；在这些基准上对不同采样策略进行实验评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 离散均匀采样在中间放大率上导致性能下降；连续采样显著提升中间放大率的平衡分类准确率，提升幅度可达四个百分点；进一步的优化采样分布可进一步提升性能；放大率是模型性能变化的主要驱动因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 连续放大率采样及其优化分布能显著提升病理学基础模型在不同放大率下的表现，为构建在多放大率下可靠运行的未来模型奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在组织病理学中，病理学家在低倍放大率下检查组织结构，在高倍放大率下检查细粒度形态。然而，病理学基础模型在不同放大率下的表现以及训练期间放大率采样的影响尚不清楚。我们将放大率采样建模为多源域适应问题，并开发了一个简单的理论框架，揭示了采样策略之间的系统性权衡。我们发现，广泛使用的离散均匀采样（0.25、0.5、1.0、2.0 mpp）会导致中间放大率的性能下降。我们提出了连续放大率采样，消除了放大率覆盖的间隙，同时保持了标准尺度的性能。进一步地，我们推导了在不同放大率下优化表示质量的采样分布。为评估这些策略，我们引入了两个新的基准（TCGA-MS、BRACS-MS）及相应指标。实验表明，连续采样在中间放大率上显著优于离散采样，平衡分类准确率提升高达四个百分点，且优化分布可进一步提升性能。最后，我们评估了当前的组织病理学基础模型，发现放大率是模型性能变化的主要驱动因素。我们的工作为未来能够在不同放大率下可靠运行的病理学基础模型奠定了基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.&lt;/p&gt;</description></item><item><guid>2601.02200v1</guid><title>Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics</title><link>http://arxiv.org/abs/2601.02200v1</link><author>Markus Borg, Nadim Hagatulah, Adam Tornhill, Emma Söderberg</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究探讨了在人工智能与人类开发者共同工作的混合时代中，代码的可读性与可维护性如何影响大型语言模型（LLM）的代码编辑效果。通过对 5,000 个竞赛编程 Python 文件进行 LLM 重构实验，发现人类友好的代码（通过 CodeHealth 指标衡量）与重构后语义保持之间存在显著关联，表明更易读的代码也更适合 AI 工具使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着 AI 编码代理越来越多地参与代码库维护，传统面向人类可读性的代码优化已不足以满足 AI 的需求，需要考虑 AI 对代码的理解与编辑能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证“AI 友好代码”概念，即人类友好的代码是否也能让 LLM 更可靠地进行重构，并探索 CodeHealth 指标在指导 AI 介入中的作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 收集 5,000 个竞赛编程 Python 文件，使用大型语言模型对代码进行重构；通过 CodeHealth 指标评估原始代码质量，并比较重构前后代码的语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 人类友好的代码与 LLM 重构后语义保持之间存在显著正相关；CodeHealth 能帮助识别 AI 介入风险低的代码区域，并提示需要额外人工监督的部分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提升代码可维护性不仅有利于人类开发者，也为大规模 AI 采用奠定基础；组织可利用 CodeHealth 指标来优化 AI 与人类协作的安全性与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们正进入一个混合时代，人类开发者和 AI 编码代理在同一代码库中工作。虽然行业实践长期以来已优化代码以便人类理解，但确保不同能力的 LLM 能可靠编辑代码变得越来越重要。在本研究中，我们通过对来自竞赛编程的 5,000 个 Python 文件进行基于 LLM 的重构，探讨了“AI 友好代码”的概念。我们发现 CodeHealth（为人类理解校准的质量指标）与 AI 重构后的语义保持之间存在有意义的关联。我们的发现确认，人类友好的代码也更兼容 AI 工具。这些结果表明，组织可以使用 CodeHealth 来指导 AI 干预风险较低的区域，以及需要额外人工监督的地方。投资可维护性不仅有助于人类，也为大规模 AI 采用做好准备。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code&amp;#x27;&amp;#x27; via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.&lt;/p&gt;</description></item><item><guid>2601.02203v1</guid><title>Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules</title><link>http://arxiv.org/abs/2601.02203v1</link><author>Oliver Custance, Saad Khan, Simon Parkinson, Quan Z. Sheng</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 提出一种两阶段框架，利用WiFi CSI实现无设备人群计数。2. 通过自监督对比学习预训练模型，获得域不变特征。3. 使用轻量化Adapter模块进行高效微调。4. 采用状态计数机生成稳定的占用人数估计。5. 在WiFlow数据集上，10-shot无监督学习MAE仅0.44，优于监督基线。6. 引入Generalisation Index（GI）评估鲁棒性，模型表现接近完美。7. 在公开WiAR基准上刷新记录，准确率达98.8%。8. 消融实验显示Adapter微调仅损失1%性能，却减少97.2%参数量。9. 该方案为真实IoT部署提供可扩展、稳健的感知系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; WiFi CSI在人群计数领域具有隐私保护优势，但实际部署受限于域迁移问题，即在不同环境下训练的模型难以泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服域迁移挑战，构建一种能够在新环境中快速、准确计数的人群检测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 采用CSI-ResNet-A架构并通过自监督对比学习预训练，学习域不变表示；2) 在预训练模型上插入轻量化Adapter模块，实现高效微调；3) 将微调后的模型输出的事件序列输入状态计数机，得到最终人数估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; - 在10-shot无监督学习场景下，MAE仅0.44，远优于监督基线。- Generalisation Index（GI）指标显示模型几乎完美泛化。- 在WiAR公开基准上取得98.8%准确率，刷新记录。- Adapter微调仅比完整微调低1%性能，却减少97.2%参数量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该两阶段框架通过自监督预训练和Adapter微调，实现了在不同环境下的高鲁棒性和高精度人群计数，为IoT实际部署提供了可行且可扩展的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 设备无关的人群计数利用WiFi信道状态信息（CSI）是新一代隐私保护物联网（IoT）应用的关键技术。然而，实际部署受到域迁移问题的严重阻碍，即在一个环境中训练的模型无法推广到另一个环境。为克服这一挑战，我们提出了一种以CSI-ResNet-A架构为核心的两阶段框架。该模型通过自监督对比学习进行预训练，以学习域不变表示，并利用轻量化Adapter模块实现高效微调。随后，将得到的事件序列输入状态计数机，生成最终稳定的占用人数估计。我们在WiFlow数据集上对该框架进行了广泛验证。在10-shot学习场景下，我们的无监督方法表现卓越，最终平均绝对误差（MAE）仅为0.44——这是监督基线无法达到的水平。为正式量化鲁棒性，我们引入了Generalisation Index（GI），在该指标上我们的模型几乎完美，证明了其泛化能力。此外，我们的框架在公开WiAR基准上刷新了最高记录，准确率达98.8%。消融研究表明，基于Adapter的微调在性能上仅落后完整微调1%（98.84% vs. 99.67%），同时训练参数量减少了97.2%。我们的工作为开发适用于真实世界IoT部署的稳健感知系统提供了实用且可扩展的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\% of a full fine-tune (98.84\% vs. 99.67\%) while training 97.2\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.&lt;/p&gt;</description></item><item><guid>2601.02204v1</guid><title>NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation</title><link>http://arxiv.org/abs/2601.02204v1</link><author>Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, Xinglong Wu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; NextFlow 是一种统一的解码器仅自回归变压器，训练于 6 万亿个交错文本-图像离散标记，能够实现多模态理解与生成，包括图像编辑、交错内容和视频生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的多模态模型往往分别处理文本和图像，缺乏统一的表示和生成框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个统一的自回归模型，天然激活多模态理解与生成能力，提升图像生成速度和质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用统一视觉表示与自回归架构，对文本使用下一词预测，对视觉使用下一尺度预测，避免传统扫描方法；通过稳健训练方案解决多尺度生成不稳定性，并引入前缀调优策略用于强化学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; NextFlow 在统一模型中实现了最先进的性能，图像生成速度仅需 5 秒即可得到 1024x1024 图像，速度比同类自回归模型快数倍；在视觉质量上与专用扩散基线相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; NextFlow 证明了统一自回归架构在多模态生成中的可行性与高效性，兼具速度与质量优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 NextFlow，一种统一的仅解码器自回归变压器，训练于 6 万亿个交错文本-图像离散标记。通过在统一的自回归架构中使用统一的视觉表示，NextFlow 本身激活了多模态理解与生成能力，解锁了图像编辑、交错内容和视频生成的能力。受不同模态特性的启发——文本严格顺序，图像本质上是层次化的——我们对文本保留下一词预测，但对视觉生成采用下一尺度预测。这与传统的栅格扫描方法不同，使得 1024x1024 的图像仅需 5 秒即可生成，速度比可比的自回归模型快数倍。我们通过稳健的训练方案解决了多尺度生成的不稳定性。此外，我们引入了前缀调优策略用于强化学习。实验表明，NextFlow 在统一模型中实现了最先进的性能，并在视觉质量上与专用扩散基线相当。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.&lt;/p&gt;</description></item><item><guid>2601.02213v1</guid><title>Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction</title><link>http://arxiv.org/abs/2601.02213v1</link><author>Haoyu Zhou, Ping Xue, Tianfan Fu, Hao Zhang</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了三种低位量化技术，分别是幅度-方向解耦量化、分支分离量化感知训练以及鲁棒性增强的注意力归一化，旨在压缩并加速SO(3)等变图神经网络，使其能够在边缘设备上高效运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; SO(3)等变图神经网络在化学分子预测中表现优异，但其高计算成本限制了在边缘设备上的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过低位量化方法降低模型尺寸和计算量，同时保持能量和力预测的准确性与物理对称性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 幅度-方向解耦量化：分别量化向量特征的模长和方向；2) 分支分离量化感知训练：在注意力基SO(3)-GNN中分别处理不变与等变通道；3) 鲁棒性增强注意力归一化：稳定低精度注意力计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明8位模型在QM9和rMD17基准上与全精度模型保持相近的能量与力预测准确率，推理速度提升2.37–2.73倍，模型尺寸缩小4倍；消融实验验证了每项技术对准确性和等变性的贡献。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的量化技术使得对称性感知的图神经网络能够在实际化学应用中以更快的推理速度和更小的模型尺寸部署，而不牺牲准确性或物理对称性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在边缘设备上部署对3D旋转群SO(3)等变的3D图神经网络（GNN）具有挑战性，因为它们的计算成本很高。本文通过低位量化技术压缩并加速SO(3)-等变GNN。我们为量化等变变压器提出了三项创新：(1) 幅度-方向解耦量化方案，分别对等变（向量）特征的模长和方向进行量化；(2) 分支分离量化感知训练策略，在基于注意力的SO(3)-GNN中分别处理不变和等变特征通道；(3) 鲁棒性增强的注意力归一化机制，稳定低精度注意力计算。对QM9和rMD17分子基准的实验表明，我们的8位模型在能量和力预测上与全精度基线相当，同时显著提升效率。我们还通过消融研究量化下保持准确性和等变性的每个组件的贡献，使用局部等变误差（LEE）指标。所提出的技术使得在实际化学应用中部署对称性感知的GNN成为可能，推理速度提升2.37–2.73倍，模型尺寸缩小4倍，而不牺牲准确性或物理对称性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.&lt;/p&gt;</description></item><item><guid>2601.02231v1</guid><title>On the Role of Spatial Features in Foundation-Model-Based Speaker Diarization</title><link>http://arxiv.org/abs/2601.02231v1</link><author>Marc Deegen, Tobias Gburrek, Tobias Cord-Landwehr, Thilo von Neumann, Jiangyu Han, Lukáš Burget, Reinhold Haeb-Umbach</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文评估了在单通道说话人分离系统中加入空间信息的效果，发现空间特征能提升性能，但提升幅度不大，说明现有的WavLM特征已能捕捉大部分区分信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来，基于大型预训练模型（如WavLM）的说话人分离技术在多数据集上取得了最先进的表现，但现有系统多为单通道，无法利用多通道录音中的空间线索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 分析将空间信息整合到单通道说话人分离系统中的影响，并评估不同的空间特征条件化策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在会议式数据集上，对多通道空间特征进行多种条件化策略的实验，比较其对说话人分离性能的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 空间信息确实能提升说话人分离效果，但提升幅度低于预期；这表明WavLM层聚合的特征已能捕获大部分必要信息，即使在重叠语音区域也能实现准确区分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 空间线索对基于基础模型的说话人分离有一定帮助，但其潜力有限；研究为进一步利用空间信息提供了洞见。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近在说话人分离领域的进展利用大型预训练基础模型（如WavLM）在多个数据集上实现了最先进的性能。像DiariZen这样的系统利用这些丰富的单通道表示，但仅限于单通道音频，无法使用多通道录音中可用的空间线索。本文通过评估多种将空间特征条件化到最先进单通道分离系统的策略，分析了将空间信息纳入的影响。对会议式数据集的实验表明，空间信息可以提升分离性能，但整体提升小于预期，表明对所有WavLM层聚合的特征已捕获了大部分准确说话人区分所需的信息，即使在重叠语音区域也是如此。这些发现为利用空间线索增强基于基础模型的分离提供了洞见。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in speaker diarization exploit large pretrained foundation models, such as WavLM, to achieve state-of-the-art performance on multiple datasets. Systems like DiariZen leverage these rich single-channel representations, but are limited to single-channel audio, preventing the use of spatial cues available in multi-channel recordings. This work analyzes the impact of incorporating spatial information into a state-of-the-art single-channel diarization system by evaluating several strategies for conditioning the model on multi-channel spatial features. Experiments on meeting-style datasets indicate that spatial information can improve diarization performance, but the overall improvement is smaller than expected for the proposed system, suggesting that the features aggregated over all WavLM layers already capture much of the information needed for accurate speaker discrimination, also in overlapping speech regions. These findings provide insight into the potential and limitations of using spatial cues to enhance foundation model-based diarization.&lt;/p&gt;</description></item><item><guid>2601.02246v1</guid><title>A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets</title><link>http://arxiv.org/abs/2601.02246v1</link><author>Annoor Sharara Akhand</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文比较了三种CNN训练策略在五个实际图像分类任务中的表现，发现迁移学习效果最好，定制CNN在资源受限时效率更高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 卷积神经网络是视觉识别的标准方法，能够从原始像素学习层次化表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估从零开始训练定制CNN、使用预训练CNN作为固定特征提取器以及通过迁移学习微调预训练骨干网络这三种范式在不同数据集上的性能和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在道路表面缺陷识别、农作物品种识别、果叶病害识别、行人通道侵占识别和未授权车辆识别等五个真实世界数据集上进行对照实验，使用准确率、宏F1分数以及训练时间和参数量等指标进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 迁移学习始终获得最强的预测性能；定制CNN在计算和内存受限时提供了良好的效率-准确率折衷。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 在大多数情况下，迁移学习是首选策略；但当资源有限时，训练小型定制CNN是可行且高效的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 卷积神经网络（CNN）因其能够从原始像素学习层次化表示而成为视觉识别的标准方法。在实践中，研究人员通常在以下三种方案中进行选择：（i）从零开始训练一个紧凑的定制CNN；（ii）使用大型预训练CNN作为固定特征提取器；以及（iii）通过对预训练骨干网络进行部分或完整微调来进行迁移学习。本报告对这三种范式在五个真实世界图像分类数据集上进行了受控比较，这些数据集涵盖了道路表面缺陷识别、农作物品种识别、果叶病害识别、行人通道侵占识别和未授权车辆识别。模型使用准确率和宏F1分数进行评估，并辅以训练时间每个epoch和参数量等效率指标。结果表明，迁移学习始终产生最强的预测性能，而定制CNN在计算和内存预算受限时提供了有吸引力的效率-准确率折衷。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.&lt;/p&gt;</description></item><item><guid>2601.02249v1</guid><title>SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection</title><link>http://arxiv.org/abs/2601.02249v1</link><author>Xiantai Xiang, Guangyao Zhou, Zixiao Wen, Wenshuai Li, Ben Niu, Feng Wang, Lijia Huang, Qiantong Wang, Yuhan Liu, Zongxu Pan, Yuxin Hu</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种参数高效的多模态目标检测框架SLGNet，结合层次结构先验和语言引导调制，利用冻结的ViT模型在RGB和红外图像上实现高效且鲁棒的检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态目标检测在全天候场景中至关重要，但现有适配器方法在保持跨模态结构一致性方面存在不足，且静态融合机制缺乏环境感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决结构一致性缺失和环境适应性差的问题，提升在高对比度或夜间环境下的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计结构感知适配器提取双模态层次结构表示并动态注入ViT；提出语言引导调制模块利用VLM生成的结构化字幕动态重校视觉特征，增强环境感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在LLVIP、FLIR、KAIST和DroneVehicle数据集上，SLGNet实现了新的SOTA，LLVIP上mAP 66.1，训练参数减少约87%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SLGNet是一种鲁棒且高效的多模态感知解决方案，兼顾性能与参数效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 利用RGB和红外（IR）图像的多模态目标检测对于在全天气条件下实现稳健感知至关重要。虽然最近基于适配器的方法能够高效地将预训练的RGB基础模型迁移到此任务，但它们往往以牺牲跨模态结构一致性为代价，导致在出现显著域差异（如高对比度或夜间环境）时关键结构线索被丢失。此外，传统的静态多模态融合机制通常缺乏环境感知，导致在复杂动态场景变化下的适配不佳，检测性能受限。为了解决这些局限，我们提出了SLGNet，一种参数高效的框架，融合层次结构先验和语言引导调制，基于冻结的Vision Transformer（ViT）基础模型。具体而言，我们设计了结构感知适配器，从两种模态中提取层次结构表示，并动态注入ViT，以补偿ViT骨干中固有的结构退化。进一步地，我们提出了语言引导调制模块，利用VLM驱动的结构化字幕动态重校视觉特征，从而赋予模型强大的环境感知能力。对LLVIP、FLIR、KAIST和DroneVehicle数据集的广泛实验表明，SLGNet实现了新的SOTA表现。在LLVIP基准上，我们的方法实现了66.1的mAP，同时将可训练参数减少约87%，相较于传统的全微调。结果证实SLGNet是多模态感知的鲁棒且高效的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.&lt;/p&gt;</description></item><item><guid>2601.02256v1</guid><title>VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation</title><link>http://arxiv.org/abs/2601.02256v1</link><author>Shikun Sun, Liao Qu, Huichao Zhang, Yiheng Liu, Yangyang Song, Xian Li, Xu Wang, Yi Jiang, Daniel K. Du, Xinglong Wu, Jia Jia</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究视觉生成模型中的异构输入导致的异步策略冲突问题，提出改进的GRPO框架，通过三种机制提升VAR模型的训练稳定性和生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视觉生成领域主要有自回归、扩散和视觉自回归模型。视觉自回归模型在生成过程中使用不同类型的输入，导致策略冲突，尤其在强化学习场景中表现为训练不稳定和目标对齐不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决视觉自回归模型在强化学习中的异步策略冲突，提升训练稳定性和生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 改进的GRPO框架包含三部分：1) 用于早期生成的稳定中间奖励；2) 用于精确信用分配的动态时间步重加权；3) 基于奖励反馈学习的掩码传播算法，隔离空间和时间上的优化影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在样本质量和目标对齐方面显著优于原始GRPO基线，能够实现对VAR模型的稳健有效优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过显式管理异步策略冲突，改进的GRPO框架显著提升了VAR模型的训练效果，为视觉生成任务提供了更可靠的优化方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉生成领域主要由三种范式主导：自回归（AR）、扩散和视觉自回归（VAR）模型。与AR和扩散不同，VAR在生成过程中使用异构输入结构，导致严重的异步策略冲突。该问题在强化学习场景中尤为突出，导致训练不稳定和目标对齐不足。为了解决这一问题，我们提出了一种新框架，通过显式管理这些冲突来增强组相对策略优化（GRPO）。我们的方法集成了三个协同组件：1）用于引导早期生成的稳定中间奖励；2）用于精确信用分配的动态时间步重加权方案；3）基于奖励反馈学习（ReFL）原理的全新掩码传播算法，旨在在空间和时间上隔离优化影响。我们的方案在样本质量和目标对齐方面相较于原始GRPO基线表现出显著提升，实现了对VAR模型的稳健有效优化。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.&lt;/p&gt;</description></item><item><guid>2601.02289v1</guid><title>Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery</title><link>http://arxiv.org/abs/2601.02289v1</link><author>Tom Burgert, Leonard Hackel, Paolo Rota, Begüm Demir</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一种新的对比式自监督学习正则化方法 GeoRank，并系统研究了多光谱遥感图像中对比式自监督学习的关键适配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自监督学习在计算机视觉领域已成为从大规模无标签数据中学习的强大范式，但将其应用于多光谱遥感图像时会面临地理和时间变异性带来的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 GeoRank 以更好地将地理关系嵌入特征空间，并评估多光谱遥感图像中对比式自监督学习的各种适配策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GeoRank 通过直接优化球面距离来正则化对比式自监督学习；作者还系统评估了数据增强、数据集规模、图像尺寸以及时间视角对模型性能的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; GeoRank 在多种对比式自监督学习算法（如 BYOL、DINO）上均能匹配或优于整合地理元数据的方法；同时研究表明数据增强、数据集规模、图像尺寸和时间视角对性能有显著影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GeoRank 是一种有效的正则化方法，可提升多光谱遥感图像的自监督学习效果；系统实验为未来研究提供了实用的指导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自监督学习（SSL）已成为从大规模无标签数据中学习的强大范式，尤其在计算机视觉（CV）领域。然而，由于数据的地理和时间变异性，将 SSL 应用于多光谱遥感（RS）图像面临独特的挑战和机遇。本文提出了 GeoRank，一种针对对比式 SSL 的新型正则化方法，通过直接优化球面距离，将地理关系嵌入学习到的特征空间，从而改进了以往技术。GeoRank 在多种对比式 SSL 算法（如 BYOL、DINO）上均能匹配或优于整合地理元数据的方法，并持续提升性能。除此之外，我们还系统地研究了多光谱 RS 图像中对比式 SSL 的关键适配，包括数据增强的有效性、数据集规模和图像尺寸对性能的影响，以及时间视角的任务依赖性。代码已公开在 https://github.com/tomburgert/georank。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.&lt;/p&gt;</description></item><item><guid>2601.02306v1</guid><title>Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify</title><link>http://arxiv.org/abs/2601.02306v1</link><author>Shivam Verma, Hannes Karlbom, Yu Zhao, Nick Topping, Vivian Chen, Kieran Stanley, Bharath Rengarajan</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种统一的多目标模型，用于在 Spotify 播客生态系统中同时针对广告和促销进行精准投放。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在播客广告投放中，个性化和冷启动问题尤为突出，尤其是针对新的广告目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过迁移学习和多任务学习框架，构建一个单一的联合模型，能够快速适配低数据或新目标的投放任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用大规模广告与内容交互数据进行迁移学习，采用多任务学习框架，使用共享的用户、内容、上下文和创意特征表示，联合优化流量、点击和关注等指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在线 A/B 测试显示，模型可将有效每次流量成本降低 22%，并使播客流量率提升 18-24%；离线实验表明辅助目标和特征组对冷启动性能有显著贡献。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 统一建模策略提升了可维护性、冷启动表现和覆盖率，打破了传统的孤立投放管道，并在实际广告系统中展示了可行的权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种统一的多目标模型，用于在 Spotify 播客生态系统中同时针对广告和促销进行投放。该方法解决了个性化和冷启动初始化的关键挑战，尤其是针对新的广告目标。通过在多任务学习框架中利用大规模广告与内容交互的迁移学习，单一的联合模型可以被微调或直接应用于新的或低数据量的投放任务，包括应用内促销。该多目标设计通过共享用户、内容、上下文和创意特征的表示，联合优化播客的流量、点击和关注等结果，既支持多样化的业务目标，又提升了用户体验。在线 A/B 测试显示，模型可将有效每次流量成本降低多达 22%，尤其在流量较少的播客中，并使播客流量率提升 18-24%。离线实验和消融实验强调了辅助目标和特征组对冷启动性能的贡献。我们的经验表明，统一建模策略提升了可维护性、冷启动表现和覆盖率，并打破了传统的孤立投放管道。我们讨论了在真实广告系统中使用此类联合模型的实际权衡。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a unified multi-objective model for targeting both advertisements and promotions within the Spotify podcast ecosystem. Our approach addresses key challenges in personalization and cold-start initialization, particularly for new advertising objectives. By leveraging transfer learning from large-scale ad and content interactions within a multi-task learning (MTL) framework, a single joint model can be fine-tuned or directly applied to new or low-data targeting tasks, including in-app promotions. This multi-objective design jointly optimizes podcast outcomes such as streams, clicks, and follows for both ads and promotions using a shared representation over user, content, context, and creative features, effectively supporting diverse business goals while improving user experience. Online A/B tests show up to a 22% reduction in effective Cost-Per-Stream (eCPS), particularly for less-streamed podcasts, and an 18-24% increase in podcast stream rates. Offline experiments and ablations highlight the contribution of ancillary objectives and feature groups to cold-start performance. Our experience shows that a unified modeling strategy improves maintainability, cold-start performance, and coverage, while breaking down historically siloed targeting pipelines. We discuss practical trade-offs of such joint models in a real-world advertising system.&lt;/p&gt;</description></item><item><guid>2601.02307v1</guid><title>Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck</title><link>http://arxiv.org/abs/2601.02307v1</link><author>Dina El Zein, James Henderson</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种通过在Transformer嵌入中加入噪声来实现文本数据隐私保护的方法，并在GLUE基准上验证了其在隐私与准确性之间的平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度模型的隐藏表示可能泄露输入的敏感信息，尤其是Transformer嵌入由每个token对应的向量组成，增加了信息泄露风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种既能共享有用数据又能提供强隐私保护的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用差分隐私思路，在Transformer中加入非参数变分信息瓶颈层（NVIB），向多向量嵌入注入噪声，并用Rényi散度和贝叶斯差分隐私（BDP）评估隐私。噪声水平通过训练调节以匹配实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在GLUE基准上，噪声水平可调，低噪声时模型保持高准确率，同时提供强隐私保证，展示了隐私与实用性的良好折中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; NVDP方法实现了隐私与实用性的平衡，证明了在Transformer嵌入中注入噪声并通过NVIB层调节可有效保护隐私。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种通过共享噪声版本的Transformer嵌入来实现文本数据隐私保护的方法。已知深度模型学习的隐藏表示能够编码输入的敏感信息，使攻击者能够以相当高的准确率恢复输入数据。由于Transformer嵌入由每个token对应的多个向量组成，这一问题更为严重。为降低此风险，我们提出了非参数变分差分隐私（NVDP），它既能实现有用的数据共享，又能提供强有力的隐私保护。我们采用差分隐私思路，在Transformer架构中集成非参数变分信息瓶颈（NVIB）层，以向其多向量嵌入注入噪声，从而隐藏信息，并使用Rényi散度及其对应的贝叶斯差分隐私（BDP）保证来衡量隐私保护。训练NVIB层可根据实用性校准噪声水平。我们在GLUE基准上测试了NVDP，并展示了通过调节噪声水平可以在隐私与准确性之间取得有用的折中。低噪声水平下，我们的模型在保持高准确率的同时提供了强隐私保证，有效平衡了隐私与实用性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.&lt;/p&gt;</description></item><item><guid>2601.02314v1</guid><title>Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents</title><link>http://arxiv.org/abs/2601.02314v1</link><author>Sourena Khanzadeh</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究评估大型语言模型代理在高风险决策中的推理透明度，提出 Project Ariadne 框架来检测推理的因果完整性，并发现推理与输出之间存在显著不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着 LLM 代理承担更重要的自主决策任务，推理过程的可解释性成为安全关键问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证 Chain-of-Thought 推理是否真实驱动模型输出，还是仅为事后解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用结构因果模型和反事实逻辑，对中间推理节点进行硬性干预（逻辑反转、否定前提、反转事实），测量终端答案的因果敏感性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 发现“因果解耦”现象，代理在事实和科学领域的违背密度高达 0.77，推理轨迹与最终结论不一致，说明推理仅是“戏剧化”而非真实决策依据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 当前代理架构易产生不可信解释，提出 Ariadne Score 作为衡量推理与行为一致性的基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着大型语言模型代理越来越多地承担高风险的自主决策任务，其推理过程的透明度已成为关键的安全问题。虽然链式思维提示可以让代理生成可读的推理轨迹，但尚不清楚这些轨迹是否真实驱动模型输出，还是仅仅是事后解释。我们提出了 Project Ariadne，这是一个新的可解释人工智能框架，利用结构因果模型和反事实逻辑来审计代理推理的因果完整性。与现有依赖表面文本相似度的解释方法不同，Project Ariadne 对中间推理节点进行硬性干预——系统地反转逻辑、否定前提、反转事实——以测量最终答案的因果敏感性。我们对最先进模型的实证评估揭示了持续存在的可信度缺口。我们定义并检测到一种广泛的失败模式，称为因果解耦，在事实和科学领域中，代理的违背密度高达 0.77。在这些情况下，代理在内部逻辑相互矛盾的情况下仍能得到相同的结论，证明其推理轨迹仅是“推理剧场”，而决策则由潜在的参数先验驱动。我们的发现表明，当前的代理架构本质上容易产生不可信的解释，并提出 Ariadne Score 作为将声明的逻辑与模型行为对齐的新基准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model&amp;#x27;s output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as &amp;quot;Reasoning Theater&amp;quot; while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.&lt;/p&gt;</description></item><item><guid>2601.02315v1</guid><title>Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping</title><link>http://arxiv.org/abs/2601.02315v1</link><author>Saurabh Kaushik, Lalit Maurya, Beth Tellman</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的融合编码器Prithvi-CAFE，结合Prithvi预训练模型与CNN残差分支，并通过注意力模块实现多尺度特征融合，显著提升洪水映射任务的分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然Geo-Foundation Models在多种下游任务中表现优异，但在Sen1Flood11洪水映射任务中难以超过传统U-Net，表明其在捕捉局部细节方面存在局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够更好捕捉局部细节且保持长程依赖的模型，以提升洪水映射的精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Prithvi-CAFE将Prithvi预训练编码器与并行CNN残差分支相结合，后者通过卷积注意力模块增强；通过适配器实现快速微调，并在多尺度、多层次上融合CNN特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Sen1Flood11和FloodPlanet两个数据集上，Prithvi-CAFE分别取得IoU 83.41和64.70，均超过原始Prithvi、其他GFMs以及U-Net，尤其在保留测试集上表现突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Prithvi-CAFE是一种简单有效的架构，能够在多通道、多模态数据下提升分割任务的性能，尤其适用于需要细粒度局部信息的场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Geo-Foundation Models（GFMs）已在语义分割、分类和回归等多种下游任务中证明其有效性。然而，在使用Sen1Flood11数据集进行洪水映射时，GFMs难以超过基线U-Net，凸显了模型在捕捉关键局部细节方面的局限。为了解决这一问题，我们提出了Prithvi-Complementary Adaptive Fusion Encoder（CAFE），它将Prithvi GFM预训练编码器与通过卷积注意力模块增强的并行CNN残差分支相结合。Prithvi-CAFE通过Prithvi中的适配器实现快速高效的微调，并在多尺度、多层次上融合CNN特征，既捕获关键局部细节，又保持长程依赖。我们在两个全面的洪水映射数据集Sen1Flood11和FloodPlanet上取得了最先进的结果。在Sen1Flood11测试数据上，Prithvi-CAFE（IoU 83.41）优于原始Prithvi（IoU 82.50）以及其他主要GFMs（TerraMind 82.90、DOFA 81.54、spectralGPT 81.02）。在保留测试站点上，Prithvi-CAFE的IoU为81.37，明显高于基线U-Net（70.57）和原始Prithvi（72.42）。在FloodPlanet上，Prithvi-CAFE也超过了基线U-Net和其他GFMs，IoU为64.70，对比U-Net 60.14、Terramind 62.33、DOFA 59.15和Prithvi 2.0 61.91。我们提出的简单而有效的Prithvi-CAFE展示了在多通道和多模态数据提供互补信息且局部细节至关重要的分割任务中提升性能的强大潜力。代码已发布在Prithvi-CAFE Github。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model&amp;#x27;s limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}&lt;/p&gt;</description></item><item><guid>2601.02316v1</guid><title>DatBench: Discriminative, Faithful, and Efficient VLM Evaluations</title><link>http://arxiv.org/abs/2601.02316v1</link><author>Siddharth Joshi, Haoli Yin, Rishabh Adiga, Ricardo Monti, Aldo Carranza, Alex Fang, Alvin Deng, Amro Abbas, Brett Larsen, Cody Blakeney, Darren Teh, David Schwab, Fan Pan, Haakon Mongstad, Jack Urbanek, Jason Lee, Jason Telanoff, Josh Wills, Kaleigh Mentzer, Luke Merrick, Parth Doshi, Paul Burstein, Pratyush Maini, Scott Loftin, Spandan Das, Tony Jiang, Vineeth Dorna, Zhengping Wang, Bogdan Gaza, Ari Morcos, Matthew Leavitt</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对视觉语言模型的评估方法进行研究，提出评估应具备真实性、区分度和计算效率三大要求，并指出现有评估存在多项缺陷。通过对现有基准进行转换和过滤，构建了DatBench-Full和DatBench两套评估套件，显著提升评估质量与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着视觉语言模型的快速发展，训练方法已成熟，但评估手段仍不完善，缺乏系统性和可持续性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出评估的三项基本要求，并通过改进现有基准来提升评估的真实性、区分度和计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对多选题转换为生成式任务，剔除可直接通过文本回答的题目和标注错误的样本，对数据集进行清洗和筛选，形成DatBench-Full和DatBench两套评估套件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 多选题导致模型能力被高估，转换后可见最高35%的性能下降；可直接文本回答的题目占比高达70%；标注错误样本占比高达42%；过滤后既提升了区分度，又降低了计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过对评估基准的改造，可实现更真实、更具区分力且更高效的评估，为视觉语言模型的可持续发展提供了路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.&lt;/p&gt;</description></item><item><guid>2601.02339v1</guid><title>Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding</title><link>http://arxiv.org/abs/2601.02339v1</link><author>Jingming He, Chongyi Li, Shiqi Wang, Sam Kwong</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种联合增强框架，结合语义分割和渲染分支，使用各向异性3D高斯切比雪夫描述符和局部语义/形状信号自适应分配高斯和球谐函数，并通过跨场景知识迁移实现快速收敛。实验表明该方法在多数据集上提升了分割精度和渲染质量，同时保持高帧率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法在3DGS中加入语义特征向量以实现语义分割和图像渲染，但往往将语义分支和渲染分支分离处理，单纯依赖二维监督，忽略了三维高斯几何；且自适应策略仅基于渲染梯度，难以处理细微或纹理缺失区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种联合增强框架，协同语义和渲染分支，提升3D语义高斯建模的精度与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 引入各向异性3D高斯切比雪夫描述符，利用拉普拉斯-贝尔特拉米算子捕捉细粒度3D形状细节；2) 在不单纯依赖渲染梯度的情况下，结合局部语义和形状信号自适应调整高斯分配和球谐函数；3) 采用跨场景知识迁移模块持续更新形状模式，实现快速收敛和稳健表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个数据集上实验表明，该方法在保持高渲染帧率的同时，显著提升了语义分割准确率和渲染质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 联合语义与渲染分支的增强框架能够更好地利用三维几何信息，提升性能并加速收敛，具有良好的实用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在同时提升3D Gaussian模型的语义分割精度和渲染质量，并降低对2D监督的依赖。实现更准确的语义地图对机器人导航、AR/VR等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在3D Gaussian Splatting（3DGS）和语义NeRF的基础上，结合Laplace–Beltrami算子和Chebyshev描述子，提出了局部形状编码；同时参考了先前的高阶SH剪枝和语义辅助的Gaussian增删策略，构建了跨场景知识迁移模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让语义分支和渲染分支相互强化：先用ALBO提取局部形状特征并通过Transformer聚合，更新每个Gaussian的语义向量；随后根据语义和形状信息动态增删Gaussian并调整SH级别；最后利用跨场景知识库加速新场景的收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）使用ALBO的3D Gaussian Chebyshev描述子捕获细粒度形状；2）语义与渲染分支的联合优化；3）基于语义和形状的自适应Gaussian分配与SH剪枝；4）跨场景知识迁移模块。与以往仅依赖2D监督或仅用梯度驱动的自适应策略不同，该方法充分利用3D几何信息并实现更高效渲染。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种联合语义与渲染的3D Gaussian框架，通过局部形状编码、语义驱动的自适应Gaussian与SH调整以及跨场景知识迁移，实现更精准的语义分割和高效渲染。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.&lt;/p&gt;</description></item><item><guid>2601.02346v1</guid><title>Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling</title><link>http://arxiv.org/abs/2601.02346v1</link><author>Falcon LLM Team, Iheb Chaabane, Puneesh Khanna, Suhail Mohmad, Slim Frikha, Shi Hu, Abdalgader Abubaker, Reda Alami, Mikhail Lubinets, Mohamed El Amine Seddik, Hakim Hacid</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Falcon-H1R 是一个 7B 参数的推理优化模型，证明小型语言模型也能实现与更大模型相当的推理性能；它在多项推理密集型基准测试中，参数效率高，持续匹配或超越 2 倍至 7 倍更大模型；通过精心的数据筛选和针对性训练（高效的 SFT 与 RL 扩展），在不增大模型规模的前提下获得显著性能提升；采用混合并行架构实现更快推理、令牌效率更高、准确率更高，成为扩展高级推理系统的实用骨干；结合 DeepConf 方法，Falcon-H1R 在测试时刻的扩展效率达到行业领先，显著提升准确率与计算成本；综上，紧凑模型通过针对性训练与架构选择，可提供稳健且可扩展的推理表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 小型语言模型在推理任务中往往表现不如更大模型，如何在保持参数规模小的同时实现竞争性推理性能是研究热点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证 7B 参数模型 Falcon-H1R 能在推理任务中与更大模型竞争，并探讨其高效性与可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用参数高效设计、精细数据筛选、针对性训练（SFT 与 RL 扩展）、混合并行架构实现快速推理、DeepConf 方法提升测试时刻扩展效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Falcon-H1R 在多项推理基准测试中匹配或超越 2 倍至 7 倍更大模型；实现更快推理、更高令牌效率和更高准确率；在测试时刻的扩展效率上达到行业领先。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 紧凑模型通过针对性训练与架构优化，可实现稳健且可扩展的推理性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究介绍了 Falcon-H1R，一种 7B 参数的推理优化模型，证明了小型语言模型（SLMs）能够实现与更大模型相当的竞争性推理性能。Falcon-H1R 以其参数效率突出，在多种推理密集型基准测试中始终匹配或超越 2 倍至 7 倍更大模型。结果强调了精心的数据筛选和针对性训练策略（通过高效的 SFT 和 RL 扩展）在不增加模型规模的前提下实现显著性能提升的重要性。此外，Falcon-H1R 通过混合并行架构设计实现更快推理、令牌效率更高、准确率更高，突破了推理效率的三维极限。这种独特组合使 Falcon-H1R-7B 成为扩展高级推理系统的实用骨干，特别适用于需要大量链式思考生成和并行测试时刻扩展的场景。利用最近引入的 DeepConf 方法，Falcon-H1R 在测试时刻扩展效率上实现了行业领先，显著提升了准确率和计算成本。结果表明，紧凑模型通过针对性训练和架构选择，能够提供稳健且可扩展的推理性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.&lt;/p&gt;</description></item><item><guid>2601.02353v1</guid><title>Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices</title><link>http://arxiv.org/abs/2601.02353v1</link><author>Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种结合网络剪枝和少样本学习的疾病识别方法，显著压缩模型并保持高准确率，适用于低成本边缘设备。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 农户缺乏实验室和高性能计算资源，传统深度学习模型体积大、计算量大，难以在如树莓派等低成本设备上运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决模型体积大和数据收集成本高的问题，使植物病害识别在边缘设备上实现实时、可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 Disease-Aware Channel Importance Scoring (DACIS) 识别网络中对区分病害最重要的通道，并在三阶段 Prune-then-Meta-Learn-then-Prune (PMP) 管道中应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 PlantVillage 和 PlantDoc 数据集上，压缩后模型大小减少 78%，准确率保持 92.3%，在树莓派 4 上以 7 帧/秒运行，满足现场诊断需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法实现了高效、可部署的植物病害识别，为小农户提供了实用的实时诊断工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 农户在偏远地区需要快速可靠的植物病害识别方法，但往往缺乏实验室或高性能计算资源。深度学习模型可以从叶片图像中高精度检测病害，但这些模型通常过大且计算量大，无法在低成本边缘设备（如树莓派）上运行。此外，收集数千张标注病害图像进行训练既昂贵又耗时。本文通过结合神经网络剪枝（去除模型中不必要的部分）和少样本学习（使模型能从有限样本中学习）来解决这两个挑战。本文提出 Disease-Aware Channel Importance Scoring (DACIS)，一种识别神经网络中最重要的部分以区分不同植物病害的方法，并将其集成到三阶段 Prune-then-Meta-Learn-then-Prune (PMP) 管道中。对 PlantVillage 和 PlantDoc 数据集的实验表明，所提出的方法将模型大小压缩 78%，同时保持 92.3% 的原始准确率，压缩后的模型在树莓派 4 上以 7 帧/秒运行，使小农户现场实时诊断成为可能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\% while maintaining 92.3\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.&lt;/p&gt;</description></item><item><guid>2601.02356v1</guid><title>Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</title><link>http://arxiv.org/abs/2601.02356v1</link><author>Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Talk2Move 是一种基于强化学习的扩散框架，能够根据文本指令对场景中的物体进行空间变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在多模态生成系统中，通过自然语言对场景中的物体进行空间变换是一大挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 实现基于文本指令的物体几何变换，克服对配对数据的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用 Group Relative Policy Optimization 进行动作探索，使用空间奖励引导模型对齐几何变换与语言描述，并通过离策略评估和主动采样提升学习效率；同时设计以物体为中心的空间奖励评估位移、旋转和缩放。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 Talk2Move 在空间精度和场景连贯性上优于现有文本引导编辑方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Talk2Move 成功实现了精确、连贯且语义忠实的物体空间变换，验证了其方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 Talk2Move，一种基于强化学习的扩散框架，用于根据文本指令在场景中对物体进行空间变换。通过自然语言对场景中的物体进行空间操作对多模态生成系统来说是一大挑战。现有的基于文本的操控方法可以调整外观或风格，但由于缺乏配对监督和像素级优化的限制，它们难以执行物体级的几何变换——如平移、旋转或缩放。Talk2Move 采用 Group Relative Policy Optimization（GRPO）通过从输入图像和轻量级文本变体生成多样化的 rollouts 来探索几何动作，消除了对昂贵配对数据的需求。一个空间奖励引导的模型将几何变换与语言描述对齐，同时离策略步骤评估和主动步骤采样通过聚焦信息丰富的变换阶段来提高学习效率。此外，我们设计了以物体为中心的空间奖励，直接评估位移、旋转和缩放行为，实现可解释且连贯的变换。对精选基准的实验表明，Talk2Move 在空间精度和场景连贯性方面均优于现有文本引导编辑方法，能够实现精确、一致且语义忠实的物体变换。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.&lt;/p&gt;</description></item><item><guid>2601.02358v1</guid><title>VINO: A Unified Visual Generator with Interleaved OmniModal Context</title><link>http://arxiv.org/abs/2601.02358v1</link><author>Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</author><pubDate>Tue, 06 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; VINO 是一种统一的视觉生成器，能够在同一框架下完成图像与视频的生成与编辑。它采用共享的扩散骨干网络，并结合视觉语言模型与多模态扩散变换器，通过交错的条件令牌来引导生成过程，从而支持多参考定位、长文本指令跟随以及跨静态与动态内容的一致身份保持。训练采用多阶段管线，将视频生成基础模型逐步扩展为支持图像与视频输入输出的多任务生成器。实验表明，VINO 在多种生成与编辑基准上表现出高质量、准确指令执行、参考与属性保持以及可控多身份编辑的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统方法往往为每个视觉任务使用专门的模型或独立模块，导致资源浪费与难以统一管理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个单一模型，能够同时处理图像与视频的生成与编辑，减少任务专用模型的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用共享的扩散骨干网络，结合视觉语言模型与多模态扩散变换器；将多模态输入编码为交错的条件令牌；采用多阶段训练管线，将视频生成基础模型逐步扩展为支持图像与视频输入输出的统一多任务生成器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; VINO 在多种生成与编辑基准上表现出强大的视觉质量、准确的指令跟随、改进的参考与属性保持以及更可控的多身份编辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; VINO 展示了可扩展的统一视觉生成的实用路径，并证明交错的、上下文计算为通用视觉创作奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 VINO，一种统一的视觉生成器，能够在单一框架内完成图像和视频的生成与编辑。与依赖任务专用模型或每种模态独立模块不同，VINO 使用共享的扩散骨干网络，并对文本、图像和视频进行条件化，从而在一个模型下实现广泛的视觉创作与编辑任务。具体而言，VINO 将视觉语言模型（VLM）与多模态扩散变换器（MMDiT）相结合，多模态输入被编码为交错的条件令牌，然后用于引导扩散过程。该设计支持多参考定位、长文本指令跟随以及跨静态与动态内容的一致身份保持，同时避免了模态特定的架构组件。为训练这样一个统一系统，我们引入了多阶段训练管线，逐步将视频生成基础模型扩展为能够处理图像和视频输入输出的统一多任务生成器。在多种生成与编辑基准上，VINO 展示了出色的视觉质量、准确的指令跟随、改进的参考与属性保持以及更可控的多身份编辑。我们的结果凸显了实现可扩展统一视觉生成的实用路径，并展示了交错、上下文计算作为通用视觉创作基础的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.&lt;/p&gt;</description></item><item><guid>2512.24330v1</guid><title>SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</title><link>http://arxiv.org/abs/2512.24330v1</link><author>Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 SenseNova-MARS，一种多模态代理推理与搜索框架，利用强化学习实现视觉推理与工具使用的交错，显著提升 VLM 在知识密集、视觉复杂任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统 VLM 主要通过文本链式思考或单独调用工具，缺乏人类般的动态工具操控与连续推理能力，尤其在需要搜索和图像裁剪等外部工具的场景中表现不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够在视觉推理过程中动态调用多种工具的 VLM，提升其在细粒度视觉理解和知识密集任务中的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SenseNova-MARS 通过强化学习实现图像搜索、文本搜索和图像裁剪工具的交错调用，并提出 Batch‑Normalized Group Sequence Policy Optimization（BN‑GSPO）算法以提升训练稳定性和工具调用效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在公开的搜索和细粒度图像理解基准上，SenseNova‑MARS‑8B 在 MMSearch 上得分 67.84，在 HR‑MMSearch 上得分 41.64，超过了 Gemini‑3‑Flash、GPT‑5 等专有模型，显示出领先的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SenseNova‑MARS 为实现具备强大工具使用能力的代理式 VLM 提供了有效且稳健的方案，标志着该领域的重要进展，并将进一步推动相关研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然视觉语言模型（VLM）能够通过代理式推理解决复杂任务，但它们的能力主要局限于文本导向的链式思考或孤立的工具调用。它们无法展示人类般的熟练度，无法在知识密集且视觉复杂的场景中无缝交错动态工具操作与连续推理，尤其需要协调外部工具如搜索和图像裁剪的情况。本文提出 SenseNova‑MARS，一种新型多模态代理推理与搜索框架，通过强化学习赋予 VLM 交错的视觉推理和工具使用能力。具体而言，SenseNova‑MARS 动态整合图像搜索、文本搜索和图像裁剪工具，以应对细粒度和知识密集的视觉理解挑战。在强化学习阶段，我们提出 Batch‑Normalized Group Sequence Policy Optimization（BN‑GSPO）算法，以提升训练稳定性并增强模型调用工具和有效推理的能力。为全面评估代理式 VLM 在复杂视觉任务上的表现，我们引入 HR‑MMSearch 基准，这是首个由高分辨率图像和知识密集、搜索驱动问题组成的搜索导向基准。实验表明，SenseNova‑MRS 在公开的搜索和细粒度图像理解基准上实现了最先进的性能。具体而言，在搜索导向基准上，SenseNova‑MARS‑8B 在 MMSearch 上得分 67.84，在 HR‑MMSearch 上得分 41.64，超过了 Gemini‑3‑Flash、GPT‑5 等专有模型。SenseNova‑MARS 代表了朝着代理式 VLM 的有前景的一步，通过提供有效且稳健的工具使用能力。为促进该领域的进一步研究，我们将发布所有代码、模型和数据集。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model&amp;#x27;s ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.&lt;/p&gt;</description></item><item><guid>2512.24445v2</guid><title>Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics</title><link>http://arxiv.org/abs/2512.24445v2</link><author>Akash Samanta, Sheldon Williamson</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种诊断驱动的自适应学习框架，显式建模误差演化，通过偏差、噪声和对齐三部分分解捕捉持续漂移、随机波动和重复方向激励导致的过冲。诊断信息在线从损失或时差误差轨迹的轻量统计量计算，独立于模型架构或任务领域。该框架可统一监督优化、演员-评论家强化学习和学习优化器，并在此基础上提出三种实例：人类启发的监督自适应优化器(HSAO)、混合误差诊断强化学习(HED-RL)以及元学习学习策略(MLLP)。理论分析证明在标准平滑假设下，所有实例均具有有界有效更新和稳定性。诊断示例展示了信号如何根据时差误差结构调节适应，从而提升学习系统在非平稳安全关键环境中的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 学习系统在非平稳且安全关键的环境中往往出现不稳定、收敛慢或适应脆弱的问题。现代优化、强化学习和元学习方法主要关注梯度统计，却忽略了误差信号本身的时间结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提供一种显式建模误差演化的诊断驱动自适应学习框架，以提升学习系统在动态环境中的稳定性和适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建偏差-噪声-对齐三分解的误差演化模型，在线计算轻量统计量；将该模型作为控制骨干，分别实现监督自适应优化器、混合误差诊断强化学习和元学习学习策略；在标准平滑假设下证明有界更新和稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 偏差-噪声-对齐分解能够统一监督优化、演员-评论家强化学习和学习优化器的控制；诊断信号能够根据时差误差结构调节适应；理论证明保证了所有实例的稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 误差演化被提升为自适应学习中的首要对象，提供了可解释、轻量的基础，能够在非平稳动态环境中实现可靠学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种诊断驱动的自适应学习框架，显式建模误差演化，通过偏差、噪声和对齐三部分分解捕捉持续漂移、随机波动和重复方向激励导致的过冲。诊断信息在线从损失或时差误差轨迹的轻量统计量计算，独立于模型架构或任务领域。我们证明该偏差-噪声-对齐分解为监督优化、演员-评论家强化学习和学习优化器提供统一的控制骨干，并在此框架下提出三种诊断驱动实例：人类启发的监督自适应优化器(HSAO)、混合误差诊断强化学习(HED-RL)以及元学习学习策略(MLLP)。在标准平滑假设下，证明所有情况均具有有界有效更新和稳定性。演员-评论家学习中的诊断示例展示了信号如何根据时差误差结构调节适应。总体而言，本文将误差演化提升为自适应学习中的首要对象，并提供可解释、轻量的基础以实现动态环境中的可靠学习。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference (TD) error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Within this framework, we introduce three diagnostic-driven instantiations: the Human-inspired Supervised Adaptive Optimizer (HSAO), Hybrid Error-Diagnostic Reinforcement Learning (HED-RL) for actor-critic methods, and the Meta-Learned Learning Policy (MLLP). Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to TD error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.&lt;/p&gt;</description></item><item><guid>2512.24615v1</guid><title>Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization</title><link>http://arxiv.org/abs/2512.24615v1</link><author>Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Youtu-Agent 是一个模块化框架，旨在自动生成和持续演进大语言模型代理，降低配置成本并提升适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有 LLM 代理框架面临高配置成本和静态能力的挑战，构建高质量代理需要大量手工工具集成和提示工程，且部署后难以在动态环境中适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 Youtu-Agent，解决配置成本高和能力静态化的问题，实现代理的自动化生成和持续演进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Youtu-Agent 采用结构化配置系统，解耦执行环境、工具包和上下文管理；提供 Workflow 模式和 Meta-Agent 模式自动生成工具代码、提示和配置；结合 Agent Practice（无参数更新的上下文优化）和 Agent RL（分布式强化学习）实现混合策略优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 WebWalkerQA 和 GAIA 上使用开源权重模型，Youtu-Agent 达到 71.47% 和 72.8% 的最高成绩；工具合成成功率超过 81%；Practice 模块在 AIME 2024/2025 上分别提升 2.7% 和 5.4%；Agent RL 训练在 7B LLM 上实现 40% 的速度提升，并在数学和多跳问答基准上提升 35% 和 21%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Youtu-Agent 通过自动化生成和混合策略优化，显著降低了代理构建成本，提升了适应性和性能，展示了在多种任务上的领先效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的大型语言模型（LLM）代理框架面临两个主要挑战：高配置成本和静态能力。构建高质量代理通常需要在工具集成和提示工程方面进行大量手工工作，而已部署的代理在没有昂贵微调的情况下难以适应动态环境。为了解决这些问题，我们提出了 Youtu-Agent，一种模块化框架，旨在实现 LLM 代理的自动生成和持续演进。Youtu-Agent 具有结构化的配置系统，解耦执行环境、工具包和上下文管理，支持灵活重用和自动合成。我们引入了两种生成范式：Workflow 模式用于标准任务，Meta-Agent 模式用于复杂、非标准需求，能够自动生成工具代码、提示和配置。此外，Youtu-Agent 建立了混合策略优化系统：1）Agent Practice 模块使代理能够通过上下文优化积累经验并提升性能，而无需参数更新；2）Agent RL 模块与分布式训练框架集成，实现任何 Youtu-Agents 的可扩展、稳定的强化学习，端到端、大规模。实验表明，Youtu-Agent 在 WebWalkerQA（71.47%）和 GAIA（72.8%）上使用开源权重模型实现了最先进的性能。我们的自动生成管道实现了超过 81% 的工具合成成功率，而 Practice 模块在 AIME 2024/2025 上分别提升了 +2.7% 和 +5.4%。此外，Agent RL 训练在 7B LLM 上实现了 40% 的速度提升，并在数学和通用/多跳问答基准上分别提升了 35% 和 21% 的编码/推理和搜索能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.&lt;/p&gt;</description></item><item><guid>2512.24695v1</guid><title>Nested Learning: The Illusion of Deep Learning Architectures</title><link>http://arxiv.org/abs/2512.24695v1</link><author>Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了嵌套学习（Nested Learning, NL）范式，认为机器学习模型可以被视为一组嵌套、多层或并行的优化问题，每个问题都有自己的上下文流。通过NL的视角，现有深度学习方法被解释为通过压缩自身上下文流来学习数据，而大模型中的上下文学习自然出现。作者进一步提出三项核心贡献：1）表达性优化器，指出传统梯度优化器是关联记忆模块，并设计更深层次的记忆和更强学习规则；2）自我修改学习模块，构建序列模型学习自身更新算法；3）连续记忆系统，推广传统短/长期记忆观念，并与自我修改模块结合，形成名为Hope的持续学习模块，在语言建模、知识融合、少样本泛化、持续学习和长上下文推理任务中表现出良好效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来语言模型取得显著进展，但如何实现持续学习、记忆、自动改进以及有效解决问题仍存在根本挑战和未解之问。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出嵌套学习范式，以更高阶的上下文学习和更丰富的优化层级来实现更强的持续学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1）将传统优化器视为关联记忆模块，设计更表达性的优化器；2）构建自我修改的序列学习模块；3）提出连续记忆系统并与自我修改模块结合，形成Hope持续学习模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1）传统梯度优化器本质上是压缩梯度信息的关联记忆模块；2）更深层次的记忆和学习规则可提升优化器表达力；3）自我修改序列模型能学习自身更新算法；4）连续记忆系统扩展了短/长期记忆概念；5）Hope模块在多项任务中表现出有前景的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 嵌套学习为设计更具表达力的学习算法提供了哲学基础，能够实现更高阶的上下文学习，并有望解锁有效的持续学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管最近在开发语言模型方面取得了进展，但关于这些模型如何持续学习/记忆、自我改进以及找到有效解决方案仍存在根本挑战和未解之问。本文提出了一种新的学习范式，称为嵌套学习（NL），它以一种连贯的方式将机器学习模型表示为一组嵌套的、多层级的和/或并行的优化问题，每个问题都有其自身的上下文流。通过NL的视角，现有的深度学习方法通过压缩自身的上下文流来从数据中学习，而在大型模型中，自我上下文学习自然出现。NL提出了一种设计更具表达力的学习算法的哲学，具有更多层级，从而实现更高阶的自我上下文学习，并可能解锁有效的持续学习能力。我们通过提出三项核心贡献来倡导NL：（1）表达性优化器：我们展示了已知的基于梯度的优化器，如Adam、带动量的SGD等，实际上是旨在通过梯度下降压缩梯度信息的关联记忆模块。在此见解的基础上，我们提出了其他更具表达力的优化器，具有更深的记忆和/或更强大的学习规则；（2）自我修改学习模块：利用NL对学习算法的见解，我们提出了一种序列模型，它通过学习自身的更新算法来学习如何修改自身；（3）连续记忆系统：我们提出了一种新的记忆系统公式，推广了传统的长期/短期记忆观点。将我们的自我修改序列模型与连续记忆系统相结合，我们提出了一个持续学习模块，称为Hope，在语言建模、知识整合、少样本泛化任务、持续学习和长上下文推理任务中显示出有前景的结果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients&amp;#x27; information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL&amp;#x27;s insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.&lt;/p&gt;</description></item><item><guid>2601.00204v1</guid><title>MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing</title><link>http://arxiv.org/abs/2601.00204v1</link><author>Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; MorphAny3D 是一种无训练需求的 3D 变形框架，利用结构化潜在表示实现高质量、跨类别的 3D 变形。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D 变形难点在于生成语义一致且时间平滑的形变，尤其在不同类别之间更具挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种训练自由的框架，利用结构化潜在特征实现可行的 3D 变形序列。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过在 3D 生成器的注意力机制中智能融合源与目标的结构化潜在特征，设计了 Morphing Cross-Attention 用于结构一致性，Temporal-Fused Self-Attention 用于时间一致性，并加入姿态校正策略以减少姿态歧义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在跨类别场景下也能生成最先进的变形序列，并支持解耦变形和 3D 风格迁移等高级应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MorphAny3D 在无训练的前提下实现了高质量、跨类别的 3D 变形，可推广到其他基于结构化潜在的生成模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 变形仍然具有挑战性，因为在生成语义一致且时间平滑的变形时存在困难，尤其是在跨类别的情况下。我们提出 MorphAny3D，一种无训练需求的框架，利用结构化潜在（SLAT）表示实现高质量的 3D 变形。我们的关键洞察是，在 3D 生成器的注意力机制中智能地混合源和目标的 SLAT 特征，自然会产生可行的变形序列。为此，我们引入 Morphing Cross-Attention（MCA），将源和目标信息融合以保持结构一致性，并引入 Temporal-Fused Self-Attention（TFSA），通过整合前一帧的特征来增强时间一致性。姿态校正策略进一步减轻了变形步骤中的姿态歧义。大量实验表明，我们的方法生成了最先进的变形序列，即使在具有挑战性的跨类别案例中也是如此。MorphAny3D 还支持高级应用，如解耦变形和 3D 风格迁移，并可推广到其他基于 SLAT 的生成模型。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决在不同类别之间生成平滑、语义一致的 3D 变形序列的问题。3D 变形在动画、电影和游戏等领域广泛应用，但现有方法往往缺乏结构合理性和时间连贯性，尤其是跨类别变形时更为困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现直接插值 SLAT 特征会导致不自然的结果，并研究了 SLAT 在注意力块中的融合模式。借鉴了先前在 2D/3D 变形中使用键值融合的做法，他们设计了 Morphing Cross‑Attention（MCA）和 Temporal‑Fused Self‑Attention（TFSA）模块，以在生成过程中融合源目标信息和时间信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 Trellis 的 Structured Latent 表示，并在注意力机制中融合源目标 SLAT 特征。流程包括：① 将源和目标对象逆向到 SLAT 潜在空间；② 通过球面插值得到每帧的初始噪声潜在；③ 在每个生成步骤中使用 MCA 融合交叉注意力的键值，使用 TFSA 将上一帧特征注入自注意力；④ 通过姿态校正策略消除突兀的姿态变化，最终得到平滑的变形序列。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 无需训练的基于 SLAT 的变形框架；② MCA 与 TFSA 两个注意力模块，分别保证结构连贯性和时间一致性；③ 基于统计姿态分布的姿态校正策略。与传统基于对应或简单插值的方法不同，MorphAny3D 直接在生成模型的注意力层内操作 SLAT 特征，实现了跨类别的高质量变形。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MorphAny3D 通过在生成模型的注意力机制中融合 Structured Latent 特征，实现了无需训练、跨类别的平滑且语义一致的 3D 变形。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.&lt;/p&gt;</description></item><item><guid>2601.00393v1</guid><title>NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos</title><link>http://arxiv.org/abs/2601.00393v1</link><author>Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 NeoVerse，一种可扩展的 4D 世界模型，能够完成 4D 重建、创新轨迹视频生成以及多种下游应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 当前 4D 世界建模方法受限于昂贵的多视角数据或繁琐的训练预处理，难以在多样化的单目视频中实现可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够在野外单目视频上实现可扩展、无姿态依赖的 4D 重建与生成的通用模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; NeoVerse 采用无姿态前馈 4D 重建、在线单目退化模式模拟等技术，形成完整可扩展的处理流水线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; NeoVerse 在标准重建与生成基准测试中实现了最先进的性能，并展现出对不同领域的高度通用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型证明了在单目视频上实现高质量 4D 重建与生成的可行性，为未来多模态应用提供了强大工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了 NeoVerse，一种多功能的 4D 世界模型，能够实现 4D 重建、创新轨迹视频生成以及丰富的下游应用。我们首先识别了当前 4D 世界建模方法在可扩展性方面的共同局限，这些局限要么由昂贵且专业的多视角 4D 数据导致，要么由繁琐的训练预处理造成。相比之下，NeoVerse 基于一种核心理念构建，使完整流程能够扩展到多样化的野外单目视频。具体而言，NeoVerse 具备无姿态前馈 4D 重建、在线单目退化模式模拟以及其他与之高度匹配的技术。这些设计赋予 NeoVerse 在各个领域的多功能性和泛化能力。同时，NeoVerse 在标准重建和生成基准测试中实现了最先进的性能。我们的项目页面可在 https://neoverse-4d.github.io 访问。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; NeoVerse 解决了如何在大规模、未标注的单目视频上构建可扩展的 4D 世界模型的问题。4D 模型能够实现时间连贯、视角可控的渲染与生成，广泛应用于数字内容创作、自动驾驶和机器人等领域，而现有方法往往受限于多视角数据或繁重的离线预处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者意识到现有 4D 方法要么需要多视角数据，要么需要离线预处理，于是改造了 VGGT 的前向高斯重建框架，加入双向运动建模，并将其与生成网络结合。该设计借鉴了 VGGT、4DGT、Stream‑Splat、MoVieS 等前向重建工作，以及 ViewCrafter、TrajectoryCrafter 等基于重建的生成方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个无姿态、双向运动感知的前向 4D 高斯重建模型，从单目视频中一次性得到 4D 场景，然后利用该场景在新相机轨迹下渲染低质量图像作为条件，训练扩散生成模型生成高质量、时间连贯的视角视频。实现流程包括：提取图像特征 → 计算双向运动特征 → 预测 4D 高斯参数 → 重建 4D 场景 → 通过随机轨迹和可见性/平均几何滤波模拟降质渲染 → 训练生成模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; NeoVerse 的创新包括：1) 无姿态、双向运动感知的前向 4D 高斯重建；2) 仅使用少量关键帧在线重建并插值所有帧；3) 轻量级的降质模拟（可见性裁剪、平均几何滤波）直接从单目视频生成训练对；4) 将重建与生成统一训练，支持数百万未标注视频。与以往需要多视角、姿态标注或离线预处理的工作不同，NeoVerse 能在大规模、未标注的单目视频上高效训练并取得最先进的重建与生成效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NeoVerse 提出了一种可扩展、无姿态的 4D 世界模型，利用前向高斯重建与在线降质模拟，能够在海量单目视频上学习并生成高质量、时间连贯的视角视频。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io&lt;/p&gt;</description></item><item><guid>2601.00417v1</guid><title>Deep Delta Learning</title><link>http://arxiv.org/abs/2601.00417v1</link><author>Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了深度 Delta 学习（DDL）架构，改进了传统残差网络的恒等捷径连接，使用可学习的几何变换代替单纯的加法，从而使网络能够在保持训练稳定性的同时，更好地建模复杂的非单调动态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 残差网络通过恒等捷径有效缓解梯度消失问题，但这种机制强加了严格的加性偏置，限制了网络对复杂状态转移的建模能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过在残差连接中引入可学习、数据相关的几何变换，扩展残差网络的表达能力，提升对复杂动态的建模效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入 Delta 操作符，将恒等矩阵做秩一扰动，参数化为反射方向向量和门控标量；对其进行谱分析，证明门控可实现恒等映射、正交投影和几何反射的动态插值；将残差更新重构为同步秩一注入，门控充当动态步长。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 门控标量使网络能够在恒等映射、正交投影和几何反射之间动态切换；网络可显式控制层级转换算子的谱，既能建模复杂动态，又保持训练稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 深度 Delta 学习为残差网络提供了统一的框架，显式谱控制提升了建模能力，同时不牺牲训练稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 深度残差网络的有效性主要基于恒等捷径连接。虽然这种机制有效缓解了梯度消失问题，但它对特征变换施加了严格的加性先验偏置，从而限制了网络建模复杂状态转移的能力。本文提出了深度 Delta 学习（DDL），一种通过用可学习、数据相关的几何变换调制恒等捷径来推广标准残差连接的新架构。该变换称为 Delta 操作符，它是恒等矩阵的秩一扰动，由反射方向向量和门控标量参数化。我们对该操作符进行了谱分析，证明门控标量实现了恒等映射、正交投影和几何反射之间的动态插值。此外，我们将残差更新重构为同步秩一注入，其中门控充当动态步长，控制旧信息的抹除和新特征的写入。该统一方法使网络能够显式控制其层级转换算子的谱，既能建模复杂的非单调动态，又能保持门控残差架构的稳定训练特性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network&amp;#x27;s capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.&lt;/p&gt;</description></item><item><guid>2601.00664v1</guid><title>Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</title><link>http://arxiv.org/abs/2601.00664v1</link><author>Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为Avatar Forcing的新框架，用于生成能够实时互动的头部头像，解决了现有模型缺乏情感互动和实时响应的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 目前的头像生成模型往往只能产生单向、缺乏情感参与的响应，无法实现真正的互动沟通。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 识别实现真正互动头像的两大挑战，并提出相应的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过扩散强制（diffusion forcing）实现实时用户-头像交互，支持多模态输入（音频与动作），并采用直接偏好优化方法，利用构造的无标签样本实现表达式互动学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该框架实现了约500毫秒的低延迟实时交互，比基线快6.8倍，且在80%的评测中优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Avatar Forcing能够生成反应迅速、富有表现力的头像运动，显著提升了交互体验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user&amp;#x27;s audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user&amp;#x27;s audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.&lt;/p&gt;</description></item><item><guid>2601.00671v1</guid><title>Fast-weight Product Key Memory</title><link>http://arxiv.org/abs/2601.00671v1</link><author>Tianyu Zhao, Llion Jones</author><pubDate>Mon, 05 Jan 2026 14:10:01 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的Fast-weight Product Key Memory（FwPKM）架构，旨在解决现代语言模型中序列建模层在存储容量与计算效率之间的权衡问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的Softmax注意力虽然提供无限存储，但计算成本呈二次增长；线性注意力则计算高效，但存储受限且固定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将稀疏Product Key Memory（PKM）从静态模块转变为动态的“快权重”情节记忆，解决存储与效率的矛盾。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; FwPKM在训练和推理阶段通过局部块级梯度下降动态更新参数，使模型能够快速记忆并检索输入序列中的新键值对。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，FwPKM作为有效的情节记忆补充标准模块的语义记忆，在长上下文数据集上显著降低困惑度；在Needle in a Haystack评估中，即使仅在4K-token序列上训练，也能推广到128K-token上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FwPKM通过动态更新实现高效存储与计算的平衡，成为长上下文语言建模的有力工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 序列建模层在现代语言模型中通常面临存储容量与计算效率之间的权衡。Softmax注意力提供无限存储，但成本呈二次增长；线性变体提供效率，但受限于固定大小的存储。我们提出Fast-weight Product Key Memory（FwPKM），一种新架构，通过将稀疏Product Key Memory（PKM）从静态模块转变为动态的“快权重”情节记忆，解决了这一张力。与PKM不同，FwPKM在训练和推理时通过局部块级梯度下降动态更新其参数，使模型能够快速记忆并检索输入序列中的新键值对。实验表明，FwPKM作为有效的情节记忆，补充了标准模块的语义记忆，在长上下文数据集上显著降低了困惑度。值得注意的是，在Needle in a Haystack评估中，FwPKM能够推广到128K-token上下文，尽管它仅在4K-token序列上训练。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, &amp;quot;fast-weight&amp;quot; episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.&lt;/p&gt;</description></item><item><guid>2307.02397v2</guid><title>Route planning of mobile sensing fleets for repeatable visits</title><link>http://arxiv.org/abs/2307.02397v2</link><author>Wen Ji, Ke Han, Qian Ge</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究利用车辆移动进行城市数据采集的移动感知方法，提出一种开放团队路线规划问题，并给出一种适配的搜索算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市感知需要在低成本下对特定地点进行多次监测，传统方法缺乏针对性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出开放团队可重复访问的路线规划模型，并设计高效求解算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用自适应大邻域搜索算法，针对问题特征进行定制，并与现有求解器和启发式方法进行比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在小规模实例中，算法与 Gurobi 的最优解相同但计算更快；在大规模实例中，算法比贪心方法提升 9.7%~25.4%，比基于序列路线规划的启发式提升 6%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的模型和算法能够有效优化车辆感知任务，尤其适用于 VOCs 等特定监测场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Vehicle-based mobile sensing is an emerging data collection paradigm that leverages vehicle mobilities to scan a city at low costs. Certain urban sensing scenarios require dedicated vehicles for highly targeted monitoring, such as volatile organic compounds (VOCs, a type of air pollutant) sensing, road surface monitoring, and accident site investigation. A hallmark of these scenarios is that the points of interest (POIs) need to be repeatedly visited by a set of agents, whose routes should provide sufficient sensing coverage with coordinated overlap at certain important POIs. For these applications, this paper presents the open team orienteering problem with repeatable visits (OTOP-RV). The adaptive large neighborhood search (ALNS) algorithm is tailored to solve the OTOP-RV considering specific features of the problem. Test results on randomly generated datasets show that: (1) For small cases, the ALNS matches Gurobi in terms of optimality but with shorter computational times; (2) For large cases, the ALNS significantly outperforms the greedy algorithm (by 9.7% to 25.4%), and a heuristic based on sequential orienteering problems (by 6%). Finally, a real-world case study of VOCs sensing is presented, which highlights the unique applicability of the OTOP-RV to such specific sensing tasks, as well as the effectiveness of the proposed algorithms in optimizing the sensing utilities.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Vehicle-based mobile sensing is an emerging data collection paradigm that leverages vehicle mobilities to scan a city at low costs. Certain urban sensing scenarios require dedicated vehicles for highly targeted monitoring, such as volatile organic compounds (VOCs, a type of air pollutant) sensing, road surface monitoring, and accident site investigation. A hallmark of these scenarios is that the points of interest (POIs) need to be repeatedly visited by a set of agents, whose routes should provide sufficient sensing coverage with coordinated overlap at certain important POIs. For these applications, this paper presents the open team orienteering problem with repeatable visits (OTOP-RV). The adaptive large neighborhood search (ALNS) algorithm is tailored to solve the OTOP-RV considering specific features of the problem. Test results on randomly generated datasets show that: (1) For small cases, the ALNS matches Gurobi in terms of optimality but with shorter computational times; (2) For large cases, the ALNS significantly outperforms the greedy algorithm (by 9.7% to 25.4%), and a heuristic based on sequential orienteering problems (by 6%). Finally, a real-world case study of VOCs sensing is presented, which highlights the unique applicability of the OTOP-RV to such specific sensing tasks, as well as the effectiveness of the proposed algorithms in optimizing the sensing utilities.&lt;/p&gt;</description></item><item><guid>2308.13260v3</guid><title>Staying Fresh: Efficient Algorithms for Timely Social Information Distribution</title><link>http://arxiv.org/abs/2308.13260v3</link><author>Songhua Li, Lingjie Duan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在基于位置的社交网络中，如何通过选择热点用户来广播城市兴趣点信息，以提升信息传播效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在此类网络中，用户只能通过有限的社交关系分享信息，且新信息传播存在显著延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出并分析一个新的组合优化问题，目标是从所有用户中挑选若干热点用户，使其广播的兴趣点信息能覆盖整个社区。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先证明该问题为NP难，并指出现有近似方法不可行；随后通过分析网络间的相互作用，将信息共享过程转化为矩阵运算，得到具有子模性和单调性的闭式目标；基于此设计了多项式时间算法，并提出可适应用户移动的增量自适应算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 问题本身NP难；现有近似方案不适用；通过矩阵化转化得到可求解的子模目标；所设计的算法在理论上提供了近似保证，并在增量自适应版本中保持良好性能；实验结果与理论一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提供了有效的算法框架，既有理论近似保证，又在仿真中验证了其优越性，可为实际基于位置的社交网络改进信息传播提供参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在基于位置的社交网络中，用户会在附近感知城市兴趣点信息，并将这些信息分享给在线社交网络中的朋友。由于用户的社交连接有限且将新鲜兴趣点信息传播给所有人存在严重延迟，主要的基于位置的社交网络致力于通过选择一定数量的热点用户并将他们的新鲜兴趣点信息广播给整个用户社区，以提升用户的兴趣点共享效果。这促使我们研究了一个新的组合优化问题，该问题涉及城市感知网络与在线社交网络之间的相互作用。我们证明该问题是NP难的，并且现有的近似解法不可行。通过分析两网络之间的相互作用，我们成功地将跨两网络的兴趣点共享过程转化为矩阵运算，从而得到一个具有子模性和单调性的闭式目标函数。基于这一发现，我们设计了一个多项式时间算法，能够保证接近最优的近似比例。进一步地，我们允许每个被选中的用户在移动过程中感知更多兴趣点信息并进行分享，并提出了一种增量自适应算法，具有良好的性能保证。最后，我们的理论结果通过使用合成数据和真实数据的仿真验证得到了证实。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In location-based social networks (LBSNs), users sense urban point-of-interest (PoI) information in the vicinity and share such information with friends in online social networks. Given users&amp;#x27; limited social connections and severe lags in disseminating fresh PoI to all, major LBSNs aim to enhance users&amp;#x27; social PoI sharing by selecting $k$ out of $m$ users as hotspots and broadcasting their fresh PoI information to the entire user community. This motivates us to study a new combinatorial optimization problem that involves the interplay between an urban sensing network and an online social network. We prove that this problem is NP-hard and also renders existing approximation solutions not viable. Through analyzing the interplay effects between the two networks, we successfully transform the involved PoI-sharing process across two networks to matrix computations for deriving a closed-form objective to hold desirable properties (e.g., submodularity and monotonicity). This finding enables us to develop a polynomial-time algorithm that guarantees a ($1-\frac{m-2}{m}(\frac{k-1}{k})^k$) approximation of the optimum. Furthermore, we allow each selected user to move around and sense more PoI information to share and propose an augmentation-adaptive algorithm with decent performance guarantees. Finally, our theoretical results are corroborated by our simulation findings using both synthetic and real-world datasets.&lt;/p&gt;</description></item><item><guid>2309.03239v2</guid><title>Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference</title><link>http://arxiv.org/abs/2309.03239v2</link><author>Songyu Ke, Ting Li, Li Song, Yanping Sun, Qintian Sun, Junbo Zhang, Yu Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在兴趣点（POI）上准确获取人群流量的重要性，并提出了一种基于自监督图表示学习的对比自学习框架（CSST），通过构建空间邻接图并利用对比学习在大量无标签时空数据上预训练，随后用少量高质量流量数据微调，实验表明该方法在两个真实数据集上优于从零开始训练的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人群流量监测对交通管理、公共服务和城市规划至关重要，但由于城市感知技术的局限，现有数据质量不足，导致从低质量数据推断准确流量成为挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决低质量数据下准确推断POI人群流量的问题，克服标注稀缺、空间时间依赖复杂以及流量与GPS报告相关性多样等难点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将人群流量推断转化为自监督属性图表示学习任务，构建基于POI距离的空间邻接图，采用对比学习利用大量无标签时空数据进行预训练，使用交换预测方法预测目标子图的表示，随后用准确流量数据进行微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在两个真实数据集上，CSST在大量噪声数据预训练后，性能始终优于从零训练的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 对比自学习框架CSST能够有效利用无标签时空数据提升人群流量推断精度，为城市管理提供更可靠的数据支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确获取兴趣点（POI）的人群流量对于有效的交通管理、公共服务和城市规划至关重要。尽管如此，由于城市感知技术的局限，大多数来源的数据质量不足，无法监测每个POI的人群流量。这使得从低质量数据推断准确的人群流量成为一项关键且具有挑战性的任务。复杂性由三个关键因素加剧：1）标注数据的稀缺和罕见；2）POI之间复杂的时空依赖关系；3）精确人群流量与GPS报告之间的多种相关性。为解决这些挑战，我们将人群流量推断问题重新定义为自监督属性图表示学习任务，并提出了一种新颖的时空数据对比自学习框架（CSST）。我们的方法首先构建基于POI及其距离的空间邻接图。随后，我们采用对比学习技术利用大量无标签时空数据。我们采用交换预测方法，预测类似实例的目标子图表示。在预训练阶段之后，模型使用准确的人群流量数据进行微调。我们的实验在两个真实数据集上进行，结果表明，CSST在大量噪声数据上预训练后，始终优于从零开始训练的模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) The scarcity and rarity of labeled data, 2) The intricate spatio-temporal dependencies among POIs, and 3) The myriad correlations between precise crowd flow and GPS reports.   To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel Contrastive Self-learning framework for Spatio-Temporal data (CSST). Our approach initiates with the construction of a spatial adjacency graph founded on the POIs and their respective distances. We then employ a contrastive learning technique to exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped prediction approach to anticipate the representation of the target subgraph from similar instances. Following the pre-training phase, the model is fine-tuned with accurate crowd flow data. Our experiments, conducted on two real-world datasets, demonstrate that the CSST pre-trained on extensive noisy data consistently outperforms models trained from scratch.&lt;/p&gt;</description></item><item><guid>2311.10471v1</guid><title>Regions are Who Walk Them: a Large Pre-trained Spatiotemporal Model Based on Human Mobility for Ubiquitous Urban Sensing</title><link>http://arxiv.org/abs/2311.10471v1</link><author>Ruixing Zhang, Liangzhe Han, Leilei Sun, Yunqi Liu, Jibin Wang, Weifeng Lv</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种基于轨迹的大规模时空模型RAW，用于用户画像和区域分析，利用人类移动数据的语义信息，减少传统四步流程的时间成本，并在实验中验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 用户画像和区域分析在商业中具有重要价值，但传统特征建模需要繁琐的数据准备、处理、建模、评估和优化步骤，耗时且劳动强度大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过充分挖掘人口迁移数据中的丰富信息，构建一个可快速开发任务的时空模型，以提升用户画像和区域分析的效率和效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了一个GPT类结构的轨迹模型RAW，参数量可达10亿，加入时空微调模块，将轨迹视为用户集合以生成任意区域嵌入，实现快速任务开发。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仅使用人类移动数据而不依赖额外特征，RAW在用户画像和区域分析上表现出一定相关性，并在基于当前状态的轨迹生成任务中展现出良好的预测能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 大规模时空模型RAW能够有效利用人类移动数据进行用户画像和区域分析，并具备良好的轨迹生成预测潜力，为后续创新工作提供了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该研究提出了一种基于轨迹的大规模时空模型（RAW），旨在通过利用人类移动数据的语义信息，减少传统特征建模的时间和劳动成本。该模型采用类似GPT的结构，参数量可达10亿，并引入时空微调模块，将轨迹视为用户集合以生成任意区域嵌入。实验表明，RAW仅依赖人类移动数据即可在用户画像和区域分析中取得一定效果，并在基于当前状态的轨迹生成任务中展现出良好的预测能力，为进一步利用该模型开展创新工作提供了可能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;User profiling and region analysis are two tasks of significant commercial value. However, in practical applications, modeling different features typically involves four main steps: data preparation, data processing, model establishment, evaluation, and optimization. This process is time-consuming and labor-intensive. Repeating this workflow for each feature results in abundant development time for tasks and a reduced overall volume of task development. Indeed, human mobility data contains a wealth of information. Several successful cases suggest that conducting in-depth analysis of population movement data could potentially yield meaningful profiles about users and areas. Nonetheless, most related works have not thoroughly utilized the semantic information within human mobility data and trained on a fixed number of the regions. To tap into the rich information within population movement, based on the perspective that Regions Are Who walk them, we propose a large spatiotemporal model based on trajectories (RAW). It possesses the following characteristics: 1) Tailored for trajectory data, introducing a GPT-like structure with a parameter count of up to 1B; 2) Introducing a spatiotemporal fine-tuning module, interpreting trajectories as collection of users to derive arbitrary region embedding. This framework allows rapid task development based on the large spatiotemporal model. We conducted extensive experiments to validate the effectiveness of our proposed large spatiotemporal model. It&amp;#x27;s evident that our proposed method, relying solely on human mobility data without additional features, exhibits a certain level of relevance in user profiling and region analysis. Moreover, our model showcases promising predictive capabilities in trajectory generation tasks based on the current state, offering the potential for further innovative work utilizing this large spatiotemporal model.&lt;/p&gt;</description></item><item><guid>2312.09921v1</guid><title>Fog Architectures and Sensor Location Certification in Distributed Event-Based Systems</title><link>http://arxiv.org/abs/2312.09921v1</link><author>Fátima Castro-Jul, Rebeca Díaz Redondo, Ana Fernández-Vilas, Sophie Chabridon, Denis Conan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究在智慧城市中通过雾计算架构验证事件的位置信息，以提高事件过滤的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 智慧城市需要自监测、自响应，依赖大规模城市感知和资源监测，产生海量数据，需要事件过滤机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 满足城市事件系统对位置信息验证的需求，提升事件可信度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计三种结合近场与云通信的雾计算架构，并使用真实城市轨迹的网络仿真验证其性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 三种架构能正确识别73%至100%的虚假位置信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 雾计算与云通信相结合的架构有效验证位置信息，可提升智慧城市事件系统的可信度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自从智慧城市旨在成为自监测和自响应系统，其部署依赖于通过大规模城市感知进行密切资源监测。随后收集的大量数据使得开发事件过滤机制变得至关重要，以便选择相关且可信的事件。由于移动事件生产者的兴起，位置信息成为一种有价值的过滤标准，因为它不仅提供了关于所描述事件的额外信息，还增强了对生产者的信任。实现验证位置信息质量的机制变得势在必行。云架构缺乏此类策略迫使采用新的物联网（IoT）基础城市服务通信方案。为满足城市事件系统（DEBS）对位置信息验证的需求，我们设计了三种不同的雾架构，结合了近场和云通信。我们使用具有真实城市轨迹的网络仿真来证明这三种架构能够正确识别73%至100%的虚假位置信息。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Since smart cities aim at becoming self-monitoring and self-response systems, their deployment relies on close resource monitoring through large-scale urban sensing. The subsequent gathering of massive amounts of data makes essential the development of event-filtering mechanisms that enable the selection of what is relevant and trustworthy. Due to the rise of mobile event producers, location information has become a valuable filtering criterion, as it not only offers extra information on the described event, but also enhances trust in the producer. Implementing mechanisms that validate the quality of location information becomes then imperative. The lack of such strategies in cloud architectures compels the adoption of new communication schemes for Internet of Things (IoT)-based urban services. To serve the demand for location verification in urban event-based systems (DEBS), we have designed three different fog architectures that combine proximity and cloud communication. We have used network simulations with realistic urban traces to prove that the three of them can correctly identify between 73% and 100% of false location claims.&lt;/p&gt;</description></item><item><guid>2312.13126v1</guid><title>Generative agents in the streets: Exploring the use of Large Language Models (LLMs) in collecting urban perceptions</title><link>http://arxiv.org/abs/2312.13126v1</link><author>Deepank Verma, Olaf Mumm, Vanessa Miriam Carlow</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了利用大型语言模型驱动的生成式智能体，在城市环境中通过街景图像进行感知实验，以模拟人类对环境的评估与行为决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类天生会评估周围环境以形成视角并预测行为，但这些连续交互复杂多样，研究难度大。传统研究通过隔离环境特征来研究其对人类感知和行为的影响，使用虚拟媒介和访谈的复制尝试效果不一。最近的大型语言模型具备语境理解和语义推理能力，可模仿可信的人类行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估生成式智能体在城市环境中的感知与行为模拟能力，并探讨其在模拟人类行为方面的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建具备虚拟人格和记忆数据库的生成式智能体，配备运动与视觉模块以弥补LLM缺乏实体感知的不足。智能体使用街景图像规划路径，评估所遇环境的安全感与活跃度，并将思考过程存入记忆，随后查询以获取细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 评估周围环境以获得理解、构建视角并预测行为反应是人类固有的特质。然而，这些连续的遭遇多样且复杂，给其研究和实验带来挑战。研究人员能够隔离环境特征并研究其对人类感知和行为的影响。然而，尝试通过整合虚拟媒介和访谈等代理来复制和研究人类行为的研究结果不一致。大型语言模型（LLMs）最近被揭示为具备语境理解和语义推理的能力。这些模型已在大量文本上训练，并已发展为模仿可信的人类行为。本研究通过感知实验探讨了由LLMs驱动的生成式智能体的最新进展。实验使用生成式智能体通过街景图像与城市环境互动，规划其朝向特定目标的旅程。智能体被赋予虚拟人格，使其可区分。它们还提供了一个记忆数据库，用于存储其思考和必要的视觉信息，并在需要时检索以规划其移动。由于LLMs没有具身感知，也没有访问视觉领域，也缺乏运动或方向感，我们设计了运动和视觉模块，帮助智能体获得对周围环境的整体理解。智能体进一步用于根据其感知的安全感和活跃度对所遇环境进行评级。由于这些智能体将细节存储在其记忆中，我们查询发现以获取其思考过程的细节。总体而言，本研究实验了当前AI发展及其在城市环境中模拟人类行为的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Evaluating the surroundings to gain understanding, frame perspectives, and anticipate behavioral reactions is an inherent human trait. However, these continuous encounters are diverse and complex, posing challenges to their study and experimentation. Researchers have been able to isolate environmental features and study their effect on human perception and behavior. However, the research attempts to replicate and study human behaviors with proxies, such as by integrating virtual mediums and interviews, have been inconsistent. Large language models (LLMs) have recently been unveiled as capable of contextual understanding and semantic reasoning. These models have been trained on large amounts of text and have evolved to mimic believable human behavior. This study explores the current advancements in Generative agents powered by LLMs with the help of perceptual experiments. The experiment employs Generative agents to interact with the urban environments using street view images to plan their journey toward specific goals. The agents are given virtual personalities, which make them distinguishable. They are also provided a memory database to store their thoughts and essential visual information and retrieve it when needed to plan their movement. Since LLMs do not possess embodiment, nor have access to the visual realm, and lack a sense of motion or direction, we designed movement and visual modules that help agents gain an overall understanding of surroundings. The agents are further employed to rate the surroundings they encounter based on their perceived sense of safety and liveliness. As these agents store details in their memory, we query the findings to get details regarding their thought processes. Overall, this study experiments with current AI developments and their potential in simulated human behavior in urban environments.&lt;/p&gt;</description></item><item><guid>2403.00813v3</guid><title>UrbanGPT: Spatio-Temporal Large Language Models</title><link>http://arxiv.org/abs/2403.00813v3</link><author>Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 关注城市空间时间预测，旨在预测未来模式与趋势。 2. 传统方法依赖大量标注数据，现实中数据稀缺。 3. 提出利用大型语言模型的思路，构建可泛化的时空LLM。 4. 设计 UrbanGPT，结合时空依赖编码器与指令调优。 5. 在多公共数据集上实验，持续优于现有基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市环境的空间与时间动态变化需要预测与洞察，涵盖交通、人口流动、犯罪率等多方面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在多种下游城市任务中展现卓越泛化能力的时空大型语言模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; UrbanGPT 将时空依赖编码器与指令调优范式无缝集成，使模型能够理解时间与空间的复杂相互依赖，从而在数据稀缺情况下实现更全面、更准确的预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在不同的时空预测任务上进行的大量实验表明，UrbanGPT 在零样本场景下持续优于最先进的基线模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 利用大型语言模型进行时空学习具有显著潜力，UrbanGPT 的成功验证了在数据稀缺环境下实现高性能预测的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks. To achieve this objective, we present the UrbanGPT, which seamlessly integrates a spatio-temporal dependency encoder with the instruction-tuning paradigm. This integration enables LLMs to comprehend the complex inter-dependencies across time and space, facilitating more comprehensive and accurate predictions under data scarcity. To validate the effectiveness of our approach, we conduct extensive experiments on various public datasets, covering different spatio-temporal prediction tasks. The results consistently demonstrate that our UrbanGPT, with its carefully designed architecture, consistently outperforms state-of-the-art baselines. These findings highlight the potential of building large language models for spatio-temporal learning, particularly in zero-shot scenarios where labeled data is scarce.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks. To achieve this objective, we present the UrbanGPT, which seamlessly integrates a spatio-temporal dependency encoder with the instruction-tuning paradigm. This integration enables LLMs to comprehend the complex inter-dependencies across time and space, facilitating more comprehensive and accurate predictions under data scarcity. To validate the effectiveness of our approach, we conduct extensive experiments on various public datasets, covering different spatio-temporal prediction tasks. The results consistently demonstrate that our UrbanGPT, with its carefully designed architecture, consistently outperforms state-of-the-art baselines. These findings highlight the potential of building large language models for spatio-temporal learning, particularly in zero-shot scenarios where labeled data is scarce.&lt;/p&gt;</description></item><item><guid>2404.00392v1</guid><title>Designing a User-centric Framework for Information Quality Ranking of Large-scale Street View Images</title><link>http://arxiv.org/abs/2404.00392v1</link><author>Tahiya Chowdhury, Ilan Mandel, Jorge Ortiz, Wendy Ju</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了街景影像在城市感知与规划中的应用，探讨了其数据质量与使用障碍，并提出评估框架与案例验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 街景影像是通过车辆摄像头收集的大规模地理空间数据，已成为城市感知的重要来源，但其数据规模大、质量不一，限制了在城市规划中的广泛应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 识别街景影像用户面临的障碍与所需工具，提升数据利用价值，并为未来系统设计提供指导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对5名学术、城市规划等领域专家进行访谈，收集使用案例、挑战与机遇；基于访谈结果构建空间、时间、内容三属性的质量评估框架；通过案例研究验证框架并展示实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 用户普遍面临数据质量不均、获取方式不便等问题；评估框架能帮助用户量化数据价值并针对性改进；案例研究表明框架在实际项目中可提升数据使用效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提供的评估框架和案例为街景影像的有效利用提供了方法论支持，未来系统应结合该框架设计数据采集与使用流程，以更好服务城市感知与规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Street view imagery (SVI), largely captured via outfitted fleets or mounted dashcams in consumer vehicles is a rapidly growing source of geospatial data used in urban sensing and development. These datasets are often collected opportunistically, are massive in size, and vary in quality which limits the scope and extent of their use in urban planning. Thus far there has not been much work to identify the obstacles experienced and tools needed by the users of such datasets. This severely limits the opportunities of using emerging street view images in supporting novel research questions that can improve the quality of urban life. This work includes a formative interview study with 5 expert users of large-scale street view datasets from academia, urban planning, and related professions which identifies novel use cases, challenges, and opportunities to increase the utility of these datasets. Based on the user findings, we present a framework to evaluate the quality of information for street images across three attributes (spatial, temporal, and content) that stakeholders can utilize for estimating the value of a dataset, and to improve it over time for their respective use case. We then present a case study using novel street view images where we evaluate our framework and present practical use cases for users. We discuss the implications for designing future systems to support the collection and use of street view data to assist in sensing and planning the urban environment.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Street view imagery (SVI), largely captured via outfitted fleets or mounted dashcams in consumer vehicles is a rapidly growing source of geospatial data used in urban sensing and development. These datasets are often collected opportunistically, are massive in size, and vary in quality which limits the scope and extent of their use in urban planning. Thus far there has not been much work to identify the obstacles experienced and tools needed by the users of such datasets. This severely limits the opportunities of using emerging street view images in supporting novel research questions that can improve the quality of urban life. This work includes a formative interview study with 5 expert users of large-scale street view datasets from academia, urban planning, and related professions which identifies novel use cases, challenges, and opportunities to increase the utility of these datasets. Based on the user findings, we present a framework to evaluate the quality of information for street images across three attributes (spatial, temporal, and content) that stakeholders can utilize for estimating the value of a dataset, and to improve it over time for their respective use case. We then present a case study using novel street view images where we evaluate our framework and present practical use cases for users. We discuss the implications for designing future systems to support the collection and use of street view data to assist in sensing and planning the urban environment.&lt;/p&gt;</description></item><item><guid>2406.09998v1</guid><title>Understanding Pedestrian Movement Using Urban Sensing Technologies: The Promise of Audio-based Sensors</title><link>http://arxiv.org/abs/2406.09998v1</link><author>Chaeyeon Han, Pavan Seshadri, Yiwei Ding, Noah Posner, Bon Woo Koo, Animesh Agrawal, Alexander Lerch, Subhrajit Guhathakurta</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出利用音频技术扩大城市人群监测规模，评估麦克风传感器的优缺点，并通过ASPED数据集展示其在行人计数和轨迹预测中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统上已使用多种传感器监测车辆流量，但对行人运动的感知仍处于起步阶段。行走在欧洲、非洲和亚洲等城市中是重要的出行方式，了解行人数量和流动对设计安全、吸引人的步行基础设施及控制拥挤至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索音频传感器在城市行人监测中的可行性，评估其优势与局限，并为城市与交通规划提供数据支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过收集大规模ASPED数据集，包含高质量音频和用于标注行人计数的视频；对音频传感器进行基线分析，并利用数据进行行人轨迹预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 音频传感器在行人跟踪方面显示出良好前景，但仍需改进算法和技术以实现实用化；数据可用于预测行人轨迹。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 音频基础的行人感知技术具有支持更好城市与交通规划的潜力，但需要进一步技术提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然已经部署了多种传感器来监测车辆流量，但对行人运动的感知仍处于起步阶段。步行在许多城市，尤其是欧洲、非洲和亚洲的城市中，是重要的出行方式。了解行人数量和流动对于设计更安全、更具吸引力的步行基础设施以及控制周期性拥挤至关重要。本研究讨论了一种利用新型音频技术扩大城市人群感知规模的新方法。它评估了麦克风传感器相对于其他行人感知方式的优点和局限性。研究提供了一个名为ASPED的大规模数据集，其中包含高质量音频记录以及用于标注行人计数的视频记录。基线分析突出了使用音频传感器进行行人跟踪的前景，尽管仍需要在算法和技术上进行改进以使传感器实用。本研究还演示了如何利用这些数据预测行人轨迹。最后，它讨论了音频基础的行人感知可以支持更好城市和交通规划的使用案例和场景。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While various sensors have been deployed to monitor vehicular flows, sensing pedestrian movement is still nascent. Yet walking is a significant mode of travel in many cities, especially those in Europe, Africa, and Asia. Understanding pedestrian volumes and flows is essential for designing safer and more attractive pedestrian infrastructure and for controlling periodic overcrowding. This study discusses a new approach to scale up urban sensing of people with the help of novel audio-based technology. It assesses the benefits and limitations of microphone-based sensors as compared to other forms of pedestrian sensing. A large-scale dataset called ASPED is presented, which includes high-quality audio recordings along with video recordings used for labeling the pedestrian count data. The baseline analyses highlight the promise of using audio sensors for pedestrian tracking, although algorithmic and technological improvements to make the sensors practically usable continue. This study also demonstrates how the data can be leveraged to predict pedestrian trajectories. Finally, it discusses the use cases and scenarios where audio-based pedestrian sensing can support better urban and transportation planning.&lt;/p&gt;</description></item><item><guid>2409.05820v5</guid><title>Urban Sensing Using Existing Fiber-Optic Networks</title><link>http://arxiv.org/abs/2409.05820v5</link><author>Jingxiao Liu, Haipeng Li, Hae Young Noh, Paolo Santi, Biondo Biondi, Carlo Ratti</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究利用现有光纤网络作为分布式声学传感系统，准确定位城市地震源并绘制其随时间变化的空间-时间功率分布，克服了传统地震网络在高频人类活动源检测上的成本与空间限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市地震信号分析能为城市环境与社会提供重要见解，但传统地震网络因需要极高密度阵列而无法在城市范围内准确检测和定位高频人类活动源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用光纤网络实现城市范围内地震源的精确定位，并估计其强度随时间的变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将一条50公里的电信光纤改造为超高密度地震阵列，生成圣何塞市的地震源功率空间-时间映射。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 成功准确定位了由交通、施工和学校等城市活动产生的远程地震源；地震源功率与环境噪声水平、土地利用模式和人口统计特征存在强相关性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 分布式声学传感技术可克服城市地震感测的接近限制，提供精细的地震源映射，并揭示其与环境与人口因素的关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 对城市地震信号的分析为城市环境和社会提供了宝贵的见解。然而，使用传统地震网络在城市范围内准确检测和定位地震源是不可行的，因为需要极高密度的地震阵列来成像高频人类源，成本过高。我们利用现有的光纤网络作为分布式声学传感系统，准确定位城市地震源并估计其强度随时间的变化。通过将一条50公里的电信光纤改造为超高密度地震阵列，我们生成了圣何塞市的地震源功率空间-时间映射。我们的方法克服了城市地震感测的接近限制，能够准确定位由交通、施工和学校等城市活动产生的远程地震源。我们还展示了地震源功率值与环境噪声水平以及各种持续的城市特征（包括土地利用模式和人口统计）之间的强相关性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The analysis of urban seismic signals offers valuable insights into urban environments and society. Yet, accurate detection and localization of seismic sources on a city-wide scale with conventional seismographic network is unavailable due to the prohibitive costs of ultra-dense seismic arrays required for imaging high-frequency anthropogenic sources. Here, we leverage existing fiber-optic networks as a distributed acoustic sensing system to accurately locate urban seismic sources and estimate how their intensity varies over time. By repurposing a 50-kilometer telecommunication fiber into an ultra-dense seismic array, we generate spatiotemporal maps of seismic source power (SSP) across San Jose, California. Our approach overcomes the proximity limitations of urban seismic sensing, enabling accurate localization of remote seismic sources generated by urban activities, such as traffic, construction, and school operations. We also show strong correlations between SSP values and environmental noise levels, as well as various persistent urban features, including land use patterns and demographics.&lt;/p&gt;</description></item><item><guid>2409.06748v1</guid><title>EasyST: A Simple Framework for Spatio-Temporal Prediction</title><link>http://arxiv.org/abs/2409.06748v1</link><author>Jiabin Tang, Wei Wei, Lianghao Xia, Chao Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个名为 EasyST 的轻量化框架，用于空间-时间预测，通过从复杂的图神经网络中蒸馏知识训练多层感知机，并结合信息瓶颈和教师约束回归损失，提升模型鲁棒性和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 空间-时间预测在城市计算中具有重要意义，但现有基于图神经网络的方法在大规模数据上面临可扩展性和泛化性挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决大规模空间-时间数据的可扩展性和分布偏移导致的泛化问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用知识蒸馏将复杂 GNN 的知识迁移到轻量化 MLP；通过空间-时间信息瓶颈与教师约束回归损失过滤噪声；加入空间和时间提示增强下游任务上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个城市计算数据集上实验表明，EasyST 在效率和准确率上均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; EasyST 为城市空间-时间预测提供了一种高效、鲁棒且易部署的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 空间-时间预测是数据驱动城市计算中的关键研究领域，对交通、公共安全和环境监测具有重要意义。然而，可扩展性和泛化性挑战仍然是主要障碍。先进模型通常依赖图神经网络来编码空间和时间相关性，但在大规模数据集的复杂性增加时表现不佳。递归的 GNN 消息传递方案阻碍了它们在现实城市感知场景中的训练和部署。此外，跨越较长时间的大规模空间-时间数据引入分布偏移，需要改进泛化性能。为解决这些挑战，我们提出了一个简单的空间-时间预测框架——EasyST。它通过有效地从复杂的空间-时间 GNN 中蒸馏知识，学习轻量且鲁棒的多层感知机。我们通过将空间-时间信息瓶颈与教师约束回归损失相结合，确保稳健的知识蒸馏，过滤掉与任务无关的噪声并避免错误指导。我们进一步通过加入空间和时间提示来增强学生模型的泛化能力，为下游任务提供上下文。对三个城市计算任务的空间-时间数据集进行评估表明，EasyST 在效率和准确率方面超过了最先进的方法。实现代码可在 https://github.com/HKUDS/EasyST 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatio-temporal prediction is a crucial research area in data-driven urban computing, with implications for transportation, public safety, and environmental monitoring. However, scalability and generalization challenges remain significant obstacles. Advanced models often rely on Graph Neural Networks to encode spatial and temporal correlations, but struggle with the increased complexity of large-scale datasets. The recursive GNN-based message passing schemes used in these models hinder their training and deployment in real-life urban sensing scenarios. Moreover, long-spanning large-scale spatio-temporal data introduce distribution shifts, necessitating improved generalization performance. To address these challenges, we propose a simple framework for spatio-temporal prediction - EasyST paradigm. It learns lightweight and robust Multi-Layer Perceptrons (MLPs) by effectively distilling knowledge from complex spatio-temporal GNNs. We ensure robust knowledge distillation by integrating the spatio-temporal information bottleneck with teacher-bounded regression loss, filtering out task-irrelevant noise and avoiding erroneous guidance. We further enhance the generalization ability of the student model by incorporating spatial and temporal prompts to provide downstream task contexts. Evaluation on three spatio-temporal datasets for urban computing tasks demonstrates that EasyST surpasses state-of-the-art approaches in terms of efficiency and accuracy. The implementation code is available at: https://github.com/HKUDS/EasyST.&lt;/p&gt;</description></item><item><guid>2410.06996v1</guid><title>Enhancing the sensing power of bike-sharing system for urban environment</title><link>http://arxiv.org/abs/2410.06996v1</link><author>Wen Ji, Ke Han, Qi Hao, Qian Ge, Ying Long</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨利用共享单车平台进行城市环境监测的创新感知方案，提出通过优化传感器部署和用户骑行调度来提高数据采集效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着智慧城市建设的推进，低成本、高效的城市环境监测需求日益增长。共享单车因覆盖广、灵活移动且分布密集，被视为潜在的普适感知平台。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补现有研究仅关注被动感知数据收集、未考虑传感器部署与车辆调度优化的空白，提出一种集成概率模型与规划模型的优化方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 结合概率模型与混合整数线性规划模型，优化在单车停靠点的传感器分配；同时设计主动调度策略，引导用户选择配备传感器的单车以提升数据采集效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在曼哈顿案例中，仅在1%的单车上安装传感器即可在一天内覆盖约70%的路段，显示共享单车在城市感知中的巨大潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过合理的传感器部署与调度策略，共享单车系统可成为高效、低成本的城市环境监测平台，为智慧城市建设提供重要支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 智慧城市的发展需要创新的感知解决方案，以实现高效且低成本的城市环境监测。共享单车系统凭借其广泛覆盖、灵活移动和密集的城市分布，提供了一个有前景的普适感知平台。在相对早期阶段，基于单车的感知研究主要关注通过被动感知收集的数据，而未考虑通过传感器部署或车辆调度来优化数据收集。为解决这一空白，本文将二项概率模型与混合整数线性规划模型相结合，优化在单车停靠点的传感器分配。此外，主动调度策略引导用户选择单车，以提升数据收集的有效性。曼哈顿的案例研究验证了所提出的策略，结果显示仅在1%的单车上配备传感器即可在一天内覆盖约70%的路段，凸显了共享单车系统在城市感知中的显著潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The development of smart cities requires innovative sensing solutions for efficient and low-cost urban environment monitoring. Bike-sharing systems, with their wide coverage, flexible mobility, and dense urban distribution, present a promising platform for pervasive sensing. At a relative early stage, research on bike-based sensing focuses on the application of data collected via passive sensing, without consideration of the optimization of data collection through sensor deployment or vehicle scheduling. To address this gap, this study integrates a binomial probability model with a mixed-integer linear programming model to optimize sensor allocation across bike stands. Additionally, an active scheduling strategy guides user bike selection to enhance the efficacy of data collection. A case study in Manhattan validates the proposed strategy, showing that equipping sensors on just 1\% of the bikes covers approximately 70\% of road segments in a day, highlighting the significant potential of bike-sharing systems for urban sensing.&lt;/p&gt;</description></item><item><guid>2410.10915v2</guid><title>HGAurban: Heterogeneous Graph Autoencoding for Urban Spatial-Temporal Learning</title><link>http://arxiv.org/abs/2410.10915v2</link><author>Qianru Zhang, Xinyi Gao, Haixin Wang, Dong Huang, Siu-Ming Yiu, Hongzhi Yin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 HGAurban 的异构时空图掩码自编码器，利用生成式自监督学习来提升城市数据的表示能力，解决噪声和稀疏问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 时空图表示在城市感知应用（如交通分析、人类移动行为建模、城市犯罪预测）中起着关键作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服现有神经网络在噪声和稀疏时空数据下难以学习有意义区域表示的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建异构时空图编码器提取多源数据的区域依赖关系，并在自监督框架中使用掩码自编码器同时处理节点特征和图结构，自动学习异构时空模式并提升动态时间相关性的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多项时空挖掘任务中，HGAurban 超越了最先进方法，并能稳健处理空间和时间维度的噪声与稀疏。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架为城市数据提供了更鲁棒的表示，显著提升了时空挖掘性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时空图表示在城市感知应用中起着关键作用，包括交通分析、人类移动行为建模和全城犯罪预测。然而，时空数据的噪声和稀疏性是一个主要挑战，这限制了现有神经网络在时空图中学习有意义区域表示的能力。为克服这些限制，我们提出了 HGAurban，一种新型异构时空图掩码自编码器，利用生成式自监督学习实现鲁棒的城市数据表示。我们的框架引入了时空异构图编码器，从多源数据中提取区域依赖关系，全面建模多样的空间关系。在自监督学习范式中，我们实现了一个掩码自编码器，联合处理节点特征和图结构。该方法能够自动学习跨区域的异构时空模式，显著提升动态时间相关性的表示。通过在多项时空挖掘任务上的全面实验，结果表明我们的框架优于最先进方法，并能稳健处理空间和时间维度的噪声与稀疏性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatial-temporal graph representations play a crucial role in urban sensing applications, including traffic analysis, human mobility behavior modeling, and citywide crime prediction. However, a key challenge lies in the noisy and sparse nature of spatial-temporal data, which limits existing neural networks&amp;#x27; ability to learn meaningful region representations in the spatial-temporal graph. To overcome these limitations, we propose HGAurban, a novel heterogeneous spatial-temporal graph masked autoencoder that leverages generative self-supervised learning for robust urban data representation. Our framework introduces a spatial-temporal heterogeneous graph encoder that extracts region-wise dependencies from multi-source data, enabling comprehensive modeling of diverse spatial relationships. Within our self-supervised learning paradigm, we implement a masked autoencoder that jointly processes node features and graph structure. This approach automatically learns heterogeneous spatial-temporal patterns across regions, significantly improving the representation of dynamic temporal correlations. Comprehensive experiments across multiple spatiotemporal mining tasks demonstrate that our framework outperforms state-of-the-art methods and robustly handles real-world urban data challenges, including noise and sparsity in both spatial and temporal dimensions.&lt;/p&gt;</description></item><item><guid>2410.21286v1</guid><title>OpenCity: A Scalable Platform to Simulate Urban Activities with Massive LLM Agents</title><link>http://arxiv.org/abs/2410.21286v1</link><author>Yuwei Yan, Qingbin Zeng, Zhiheng Zheng, Jingzhe Yuan, Jie Feng, Jun Zhang, Fengli Xu, Yong Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一个名为 OpenCity 的可扩展仿真平台，旨在高效模拟大规模 LLM 代理在城市中的日常活动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的基于代理的模型能够解释微观行为与宏观社会现象的联系，但 LLM 代理的高计算成本限制了其大规模仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过优化系统和提示效率，降低 LLM 代理仿真所需的计算资源和时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 LLM 请求调度器以并行化 IO 多路复用，设计“分组与蒸馏”提示优化策略，将属性相似的代理聚类以减少冗余。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在六个全球城市的实验中，OpenCity 将每个代理的仿真时间提升 600 倍，LLM 请求减少 70%，令牌使用量减少 50%，并能在普通硬件上 1 小时内完成 10,000 代理的日常活动仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenCity 为利用 LLM 进行跨学科城市研究提供了关键基础设施，并首次建立了 LLM 代理的城市仿真基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 代理模型（ABMs）长期以来被用于探究个体行为如何在城市空间中聚合成复杂的社会现象。与黑盒预测模型不同，ABMs 在解释驱动此类涌现行为的微观-宏观联系方面表现出色。大型语言模型（LLMs）的崛起促成了能够以前所未有的逼真度模拟城市活动的 LLM 代理。然而，LLMs 的极高计算成本为 LLM 代理的规模化仿真带来了重大挑战。为解决此问题，我们提出了 OpenCity，一个针对系统和提示效率进行优化的可扩展仿真平台。具体而言，我们提出了一个 LLM 请求调度器，通过 IO 多路复用并行化请求来降低通信开销。此外，我们设计了“分组与蒸馏”提示优化策略，通过将属性相似的代理聚类来最小化冗余。通过在六个全球城市的实验，OpenCity 在每个代理的仿真时间上实现了 600 倍的加速，LLM 请求减少了 70%，令牌使用量减少了 50%。这些改进使得在普通硬件上 1 小时内即可模拟 10,000 个代理的日常活动。此外，OpenCity 的显著加速使我们首次建立了 LLM 代理的城市仿真基准，比较了六个主要城市的仿真城市活动与真实世界数据。我们认为 OpenCity 平台为利用 LLM 的力量进行跨学科城市研究提供了关键基础设施，促进了更广泛研究社区的集体努力。代码仓库可在 https://anonymous.4open.science/r/Anonymous-OpenCity-42BD 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Agent-based models (ABMs) have long been employed to explore how individual behaviors aggregate into complex societal phenomena in urban space. Unlike black-box predictive models, ABMs excel at explaining the micro-macro linkages that drive such emergent behaviors. The recent rise of Large Language Models (LLMs) has led to the development of LLM agents capable of simulating urban activities with unprecedented realism. However, the extreme high computational cost of LLMs presents significant challenges for scaling up the simulations of LLM agents. To address this problem, we propose OpenCity, a scalable simulation platform optimized for both system and prompt efficiencies. Specifically, we propose a LLM request scheduler to reduce communication overhead by parallelizing requests through IO multiplexing. Besides, we deisgn a &amp;quot;group-and-distill&amp;quot; prompt optimization strategy minimizes redundancy by clustering agents with similar static attributes. Through experiments on six global cities, OpenCity achieves a 600-fold acceleration in simulation time per agent, a 70% reduction in LLM requests, and a 50% reduction in token usage. These improvements enable the simulation of 10,000 agents&amp;#x27; daily activities in 1 hour on commodity hardware. Besides, the substantial speedup of OpenCity allows us to establish a urban simulation benchmark for LLM agents for the first time, comparing simulated urban activities with real-world data in 6 major cities around the globe. We believe our OpenCity platform provides a critical infrastructure to harness the power of LLMs for interdisciplinary studies in urban space, fostering the collective efforts of broader research communities. Code repo is available at https://anonymous.4open.science/r/Anonymous-OpenCity-42BD.&lt;/p&gt;</description></item><item><guid>2411.00773v2</guid><title>LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation</title><link>http://arxiv.org/abs/2411.00773v2</link><author>Bowen Li, Zhaoyu Li, Qiwei Du, Jinqi Luo, Wenshan Wang, Yaqi Xie, Simon Stepputtis, Chen Wang, Katia P. Sycara, Pradeep Kumar Ravikumar, Alexander G. Gray, Xujie Si, Sebastian Scherer</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了LogiCity，一个基于可定制一阶逻辑的城市模拟器，用于解决神经符号（NeSy）AI在长周期推理和多智能体交互方面的不足。LogiCity通过语义和空间概念（如IsAmbulance、IsClose）定义规则，支持用户自定义抽象层级，并提供两项任务：一项是长周期序列决策，另一项是一步视觉推理。实验表明NeSy框架在抽象推理上具有优势，但在处理更复杂抽象、长周期多智能体场景以及高维不平衡数据时仍面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来NeSy AI系统快速发展，但现有基准缺乏长周期推理任务和复杂多智能体交互，且受限于固定、简单的逻辑规则和有限实体，难以反映真实世界复杂性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 填补NeSy AI在长周期推理、多智能体交互以及可定制逻辑抽象方面的空白，提供一个可扩展、可配置的城市模拟平台。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建LogiCity模拟器，使用可定制的一阶逻辑模型城市元素和智能体行为；支持用户配置抽象层级；设计两项任务（长周期决策和一步视觉推理）以评估NeSy框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示NeSy框架在抽象推理任务中表现优异，但在更复杂抽象、长周期多智能体场景以及高维不平衡数据处理上仍存在显著挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LogiCity凭借其灵活设计、多功能特性和新提出的挑战，为推进下一代NeSy AI提供了重要一步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近年来，神经符号（NeSy）AI系统迅速发展，将符号推理与深度神经网络相结合。然而，现有的大多数NeSy AI基准缺乏具有复杂多智能体交互的长周期推理任务。它们通常受限于固定且简单的逻辑规则和有限实体，远未达到现实世界的复杂性。为解决这些关键缺口，我们推出了LogiCity，这是首个基于可定制一阶逻辑（FOL）的城市模拟器，适用于具有多动态智能体的城市环境。LogiCity使用语义和空间概念（如IsAmbulance(X)和IsClose(X, Y)）来建模多样化的城市元素，并利用这些概念定义治理各种智能体行为的FOL规则。由于这些概念和规则是抽象的，它们可以普遍应用于任何智能体组合的城市，方便实例化多种场景。此外，LogiCity的一个关键特性是支持用户可配置的抽象，使逻辑推理的模拟复杂度可定制。为探索NeSy AI的各个方面，LogiCity引入了两项任务，一项涉及长周期序列决策，另一项聚焦一步视觉推理，难度和智能体行为各异。我们的广泛评估揭示了NeSy框架在抽象推理方面的优势。同时，我们强调在长周期多智能体场景或高维不平衡数据下处理更复杂抽象所面临的重大挑战。凭借其灵活设计、多种功能和新提出的挑战，我们认为LogiCity是推动下一代NeSy AI向前迈进的重要一步。所有代码和数据已在我们的网站上开源：https://jaraxxus-me.github.io/LogiCity/&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks. However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions. Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities. To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents. LogiCity models diverse urban elements using semantic and spatial concepts, such as IsAmbulance(X) and IsClose(X, Y). These concepts are used to define FOL rules that govern the behavior of various agents. Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios. Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning. To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors. Our extensive evaluation reveals the advantage of NeSy frameworks in abstract reasoning. Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data. With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI. All the code and data are open-sourced at our website: https://jaraxxus-me.github.io/LogiCity/&lt;/p&gt;</description></item><item><guid>2411.04865v4</guid><title>ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset</title><link>http://arxiv.org/abs/2411.04865v4</link><author>Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了ZAHA数据集和LoFG分类体系，提供了最大规模的3D立面语义分割数据，并评估了基线方法，讨论了未解决的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 立面语义分割在摄影测量和计算机视觉中长期存在挑战，现有方法缺乏全面的立面类别和覆盖建筑多样性的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出基于国际城市建模标准的分层立面通用级别（LoFG）分类体系，并构建最大规模的3D立面分割数据集，以促进方法比较和发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计LoFG2和LoFG3两级分层分类，收集并标注6.01亿点的3D立面数据，使用基线语义分割方法进行性能分析，并讨论挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 构建了601百万点、5类和15类的LoFG2/LoFG3数据集，成为迄今最大规模的3D立面分割数据；基线方法在该数据集上的表现被评估，揭示了仍需解决的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ZAHA数据集和LoFG分类将推动3D立面语义分割技术进步，为城市数字孪生的稳健分割奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 立面语义分割是摄影测量和计算机视觉中的长期挑战。尽管过去几十年涌现了大量立面分割方法，但仍缺乏覆盖建筑多样性的全面立面类别和数据。在ZAHA中，我们引入了立面通用级别（LoFG），一种基于国际城市建模标准设计的分层立面类别，确保与现实世界的挑战性类别兼容，并实现方法的统一比较。实现LoFG后，我们发布了迄今为止最大的语义3D立面分割数据集，提供了6.01亿个标注点，分别对应LoFG2的5类和LoFG3的15类。此外，我们分析了基线语义分割方法在我们引入的LoFG类别和数据上的表现，并补充了对立面分割未解决挑战的讨论。我们坚信ZAHA将促进3D立面语义分割方法的进一步发展，为创建城市数字孪生提供不可或缺的稳健分割。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Facade semantic segmentation is a long-standing challenge in photogrammetry and computer vision. Although the last decades have witnessed the influx of facade segmentation methods, there is a lack of comprehensive facade classes and data covering the architectural variability. In ZAHA, we introduce Level of Facade Generalization (LoFG), novel hierarchical facade classes designed based on international urban modeling standards, ensuring compatibility with real-world challenging classes and uniform methods&amp;#x27; comparison. Realizing the LoFG, we present to date the largest semantic 3D facade segmentation dataset, providing 601 million annotated points at five and 15 classes of LoFG2 and LoFG3, respectively. Moreover, we analyze the performance of baseline semantic segmentation methods on our introduced LoFG classes and data, complementing it with a discussion on the unresolved challenges for facade segmentation. We firmly believe that ZAHA shall facilitate further development of 3D facade semantic segmentation methods, enabling robust segmentation indispensable in creating urban digital twins.&lt;/p&gt;</description></item><item><guid>2411.05636v1</guid><title>Video RWKV:Video Action Recognition Based RWKV</title><link>http://arxiv.org/abs/2411.05636v1</link><author>Zhuowen Yin, Chengru Li, Xingbo Dong</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的LSTM CrossRWKV框架，结合RWKV门控和LSTM递归机制，能够高效捕捉视频的空间和时间特征，并通过边缘信息和管道掩码策略降低计算成本和过拟合，已在视频理解任务中取得领先表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有视频理解方法如CNN和Transformer面临高计算成本和长距离依赖问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决高计算成本和长距离依赖问题，将RWKV引入视频领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出线性复杂度的LSTM CrossRWKV框架，使用Cross RWKV门实现当前帧边缘信息与过去特征的交互，LSTM递归执行机制存储长期记忆，边缘信息作为LSTM的遗忘门，管道掩码策略减少冗余信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LSTM CrossRWKV在视频理解任务中设定了新基准，表现出可扩展且高效的特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架提供了一种可扩展、高效的综合视频分析解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 为了解决现有视频理解方法（如CNN和Transformer）在高计算成本和长距离依赖方面的挑战，本文以新颖方式将RWKV引入视频领域。我们提出了LSTM CrossRWKV（LCR）框架，旨在进行时空表示学习以解决视频理解任务。具体而言，所提出的线性复杂度LCR结合了新型Cross RWKV门，促进当前帧边缘信息与过去特征之间的交互，通过边缘特征增强对主体的关注，并在时间上全局聚合帧间特征。LCR通过增强的LSTM递归执行机制为视频处理存储长期记忆。通过利用Cross RWKV门和递归执行，LCR有效捕捉空间和时间特征。此外，边缘信息充当LSTM的遗忘门，指导长期记忆管理。管道掩码策略减少冗余信息并降低过拟合。这些优势使LSTM CrossRWKV在视频理解中设定了新基准，提供了可扩展且高效的综合视频分析解决方案。所有代码和模型均公开可用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;To address the challenges of high computational costs and long-distance dependencies in exist ing video understanding methods, such as CNNs and Transformers, this work introduces RWKV to the video domain in a novel way. We propose a LSTM CrossRWKV (LCR) framework, designed for spatiotemporal representation learning to tackle the video understanding task. Specifically, the proposed linear complexity LCR incorporates a novel Cross RWKV gate to facilitate interaction be tween current frame edge information and past features, enhancing the focus on the subject through edge features and globally aggregating inter-frame features over time. LCR stores long-term mem ory for video processing through an enhanced LSTM recurrent execution mechanism. By leveraging the Cross RWKV gate and recurrent execution, LCR effectively captures both spatial and temporal features. Additionally, the edge information serves as a forgetting gate for LSTM, guiding long-term memory management.Tube masking strategy reduces redundant information in food and reduces overfitting.These advantages enable LSTM CrossRWKV to set a new benchmark in video under standing, offering a scalable and efficient solution for comprehensive video analysis. All code and models are publicly available.&lt;/p&gt;</description></item><item><guid>2412.06328v1</guid><title>Towards Civic Digital Twins: Co-Design the Citizen-Centric Future of Bologna</title><link>http://arxiv.org/abs/2412.06328v1</link><author>Massimiliano Luca, Bruno Lepri, Riccardo Gallotti, Stefania Paolazzi, Mauro Bigi, Marco Pistore</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出Civic Digital Twin（CDT），一种以市民为中心的城市数字孪生工具，旨在通过感知、建模和协同设计支持城市规划与治理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 博洛尼亚数字孪生计划的启动，城市需要创新数字工具支持决策和公民参与。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 支持市民中心的城市规划与治理转型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过空间、时间和社会数据感知城市，建模和模拟社会动态，吸引市民和利益相关者共同设计城市未来。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们介绍了Civic Digital Twin（CDT），它是城市数字孪生的演进版，旨在支持以市民为中心的城市规划和治理转型方法。CDT正在博洛尼亚数字孪生计划的范围内开发，该计划由博洛尼亚市在一年前启动，旨在实现该市采用创新数字工具以支持决策制定和公民参与的政治和战略目标。CDT除了通过空间、时间和社会数据感知城市的能力外，还必须能够建模和模拟城市中的社会动态：市民和集体的行为、态度和偏好，以及它们如何影响城市生活并推动转型过程。CDT的另一个显著特点是它必须能够吸引市民（个人、集体和有组织的公民社会）以及其他市民利益相关者（公用事业、经济参与者、第三部门）参与共同设计城市的未来。在本文中，我们讨论了导致定义CDT的动机，定义了其建模方面和关键研究挑战，并通过城市交通和城市发展两个案例说明了其预期用途。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce Civic Digital Twin (CDT), an evolution of Urban Digital Twins designed to support a citizen-centric transformative approach to urban planning and governance. CDT is being developed in the scope of the Bologna Digital Twin initiative, launched one year ago by the city of Bologna, to fulfill the city&amp;#x27;s political and strategic goal of adopting innovative digital tools to support decision-making and civic engagement. The CDT, in addition to its capability of sensing the city through spatial, temporal, and social data, must be able to model and simulate social dynamics in a city: the behavior, attitude, and preference of citizens and collectives and how they impact city life and transform transformation processes. Another distinctive feature of CDT is that it must be able to engage citizens (individuals, collectives, and organized civil society) and other civic stakeholders (utilities, economic actors, third sector) interested in co-designing the future of the city. In this paper, we discuss the motivations that led to the definition of the CDT, define its modeling aspects and key research challenges, and illustrate its intended use with two use cases in urban mobility and urban development.&lt;/p&gt;</description></item><item><guid>2412.17699v2</guid><title>Establishing Reality-Virtuality Interconnections in Urban Digital Twins for Superior Intelligent Road Inspection and Simulation</title><link>http://arxiv.org/abs/2412.17699v2</link><author>Yikang Zhang, Chuang-Wei Liu, Jiahang Li, Yingbing Chen, Jie Cheng, Rui Fan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种多模态传感平台与城市数字孪生系统相结合的智能道路检测方法，通过构建分层道路模型和数字孪生，生成高保真道路缺陷场景，显著提升感知与决策任务性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统人工评估道路缺陷耗时费力，数据驱动方法受限于缺陷数据稀缺与空间稀疏；现有仿真器缺乏道路缺陷模型，且缺陷区域的高级驾驶任务研究不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决道路缺陷数据不足、仿真缺陷模型缺失以及缺陷区域驾驶任务研究不足的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用车载多模态传感器采集真实驾驶数据，构建分层道路模型；生成数字道路孪生用于创建仿真环境；将场景导入仿真器进行数据采集与物理仿真；评估感知与决策算法性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 高保真道路缺陷场景显著提升了驾驶任务中的感知与决策性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多模态传感平台与城市数字孪生相结合的方案能够有效生成高质量道路缺陷数据，推动智能道路检测与高级驾驶任务研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 道路检查对于维护道路可用性和确保交通安全至关重要，因为道路缺陷会逐渐发展并削弱功能。传统的检查方法依赖人工评估，劳动强度大、成本高且耗时。虽然数据驱动方法正逐渐流行，但真实道路缺陷的稀缺和空间稀疏性给获取高质量数据集带来重大挑战。现有的用于生成详细合成驾驶场景的仿真器缺乏道路缺陷模型，且涉及道路表面交互的高级驾驶任务（如缺陷区域的规划与控制）仍未得到充分探索。为解决这些限制，我们提出了一个集成多模态传感平台与城市数字孪生（UDT）系统的智能道路检查方案。首先，利用车载传感器收集的真实驾驶数据构建分层道路模型，得到高度详细的道路缺陷结构和表面高程表示。随后生成数字道路孪生，用于创建仿真环境，以全面分析和评估算法性能。将这些场景导入仿真器，既可进行数据采集，也可进行物理仿真。实验结果表明，利用我们系统生成的高保真道路缺陷场景，驾驶任务（包括感知和决策）获得了显著收益。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决道路缺陷检测与仿真数据不足的问题。传统人工检查耗时且危险，现有数据驱动方法受限于缺乏高质量、稀疏的缺陷样本；而现有仿真器缺乏真实道路缺陷模型，导致驾驶算法难以在真实环境中验证。提供逼真的缺陷模型和仿真环境，可提升道路安全评估和自动驾驶系统的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别现有方法在数据获取、缺陷建模和仿真集成方面的不足，随后提出基于多模态传感器（LiDAR、立体摄像头、GNSS）的数据采集平台，并借鉴数字孪生、语义分割（Grounded‑SAM）和立体视差技术来重建道路表面与缺陷。随后利用Moller‑Trumbore算法和三角网格处理，将重建模型嵌入仿真器，形成完整的数字孪生系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次化建模将真实道路表面与缺陷分别重建为高精度三维网格，然后将这些模型无缝集成到城市数字孪生仿真环境中。实现流程包括：①使用多模态传感器采集道路数据；②粗粒度流利用语义掩码和LiDAR点云重建平整道路表面；③细粒度流利用立体视差提取缺陷区域并重建细节；④将两类模型存入库；⑤在仿真器中导出平面道路资产，随机投射缺陷模型并去除交叉三角面；⑥生成完整的仿真场景，用于数据合成和物理仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①可携带的多模态采集平台；②层次化道路模型生成器，既重建平整表面又捕获细节缺陷；③数字孪生生成器能够将缺陷模型准确投射到仿真道路并保持拓扑完整；④提供完整的感知与决策算法基准数据集；⑤采用轮胎级碰撞检测实现更灵活的缺陷避让策略。与以往仅使用平面道路或单独缺陷重建的工作不同，该方法实现了真实道路缺陷的三维再现与仿真集成，显著提升了数据真实性和算法迁移性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套基于多模态传感器的层次化道路缺陷建模与数字孪生集成流程，能够生成逼真的三维缺陷场景并用于感知与决策算法的训练与评估。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Road inspection is crucial for maintaining road serviceability and ensuring traffic safety, as road defects gradually develop and compromise functionality. Traditional inspection methods, which rely on manual evaluations, are labor-intensive, costly, and time-consuming. While data-driven approaches are gaining traction, the scarcity and spatial sparsity of real-world road defects present significant challenges in acquiring high-quality datasets. Existing simulators designed to generate detailed synthetic driving scenes, however, lack models for road defects. Moreover, advanced driving tasks that involve interactions with road surfaces, such as planning and control in defective areas, remain underexplored. To address these limitations, we propose a multi-modal sensor platform integrated with an urban digital twin (UDT) system for intelligent road inspection. First, hierarchical road models are constructed from real-world driving data collected using vehicle-mounted sensors, resulting in highly detailed representations of road defect structures and surface elevations. Next, digital road twins are generated to create simulation environments for comprehensive analysis and evaluation of algorithm performance. These scenarios are then imported into a simulator to facilitate both data acquisition and physical simulation. Experimental results demonstrate that driving tasks, including perception and decision-making, benefit significantly from the high-fidelity road defect scenes generated by our system.&lt;/p&gt;</description></item><item><guid>2501.05817v2</guid><title>RIS Optimization Algorithms for Urban Wireless Scenarios in Sionna RT</title><link>http://arxiv.org/abs/2501.05817v2</link><author>Ahmet Esad Güneşer, Berkay Şekeroğlu, Sefa Kayraklık, Erhan Karakoca, İbrahim Hökelek, Sultan Aldirmaz-Colak, Ali Görçin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文评估了基于信道估计的可重构智能表面（RIS）优化算法在城市数字孪生环境中的射线追踪仿真性能，并通过实现和基准测试额外算法，生成覆盖图，强调在近真实仿真环境中验证算法的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在城市数字孪生环境中使用射线追踪仿真评估RIS优化算法的性能，超越Sionna的原生功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估并基准测试基于信道估计的RIS优化算法，比较不同部署条件下的RIS策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 实现并基准测试额外的RIS优化算法，利用Sionna的射线追踪仿真生成RIS辅助通信系统的覆盖图，并进行真实世界实验验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在近真实仿真环境中验证算法至关重要，测量设置的细微变化会显著影响性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过集成Sionna射线追踪仿真生成覆盖图，并结合真实实验，本文提供了评估RIS策略的框架，强调了验证算法在近真实环境中的必要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文评估了基于信道估计的可重构智能表面（RIS）优化算法在城市数字孪生环境中的射线追踪仿真性能。超越Sionna的原生功能，我们实现并基准测试了额外的基于信道估计的RIS优化算法，使得能够在不同部署条件下评估RIS策略。通过集成Sionna的射线追踪仿真，生成了RIS辅助通信系统的覆盖图。此外，真实世界实验强调了在近真实仿真环境中验证算法的必要性，因为测量设置的细微变化可能显著影响性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper evaluates the performance of reconfigurable intelligent surface (RIS) optimization algorithms, which utilize channel estimation methods, in ray tracing (RT) simulations within urban digital twin environments. Beyond Sionna&amp;#x27;s native capabilities, we implement and benchmark additional RIS optimization algorithms based on channel estimation, enabling an evaluation of RIS strategies under various deployment conditions. Coverage maps for RIS-assisted communication systems are generated through the integration of Sionna&amp;#x27;s RT simulations. Moreover, real-world experimentation underscores the necessity of validating algorithms in near-realistic simulation environments, as minor variations in measurement setups can significantly affect performance.&lt;/p&gt;</description></item><item><guid>2501.08983v4</guid><title>Compositional Generative Model of Unbounded 4D Cities</title><link>http://arxiv.org/abs/2501.08983v4</link><author>Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 CityDreamer4D，一种专门用于生成无限扩展的 4D 城市的组合式生成模型。通过将动态对象与静态场景分离，并为建筑、车辆和背景等不同类型的对象使用不同的神经场，模型能够高效生成结构复杂、视觉多样的城市环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着 3D 场景生成技术的快速发展，生成 4D 城市面临更高的挑战，主要因为城市中存在结构复杂、视觉多样的建筑和车辆，以及人类对城市环境失真更敏感。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决 4D 城市生成中的挑战，提出一种能够生成无限扩展、真实感强的 4D 城市的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用组合式生成框架，将动态交通场景与静态城市布局分别由 Traffic Scenario Generator 和 Unbounded Layout Generator 生成，并使用紧凑的鸟瞰视图表示。城市中的对象通过结合面向背景的神经场和面向实例的神经场生成，神经场采用定制的哈希网格和周期性位置嵌入进行场景参数化。同时提供 OSM、Google Earth 和 CityTopia 三个数据集支持训练与评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CityDreamer4D 在生成真实感强的 4D 城市方面实现了最先进的性能，并支持实例编辑、城市风格化和城市仿真等下游应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 组合式设计使得 CityDreamer4D 能够高效、可扩展地生成结构复杂且视觉多样的 4D 城市，为城市生成与仿真研究提供了新的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 场景生成近年来受到越来越多关注，并取得了显著进展。与 3D 场景相比，生成 4D 城市更具挑战性，因为城市中存在结构复杂、视觉多样的建筑和车辆，并且人们对城市环境中的失真更为敏感。为了解决这些问题，我们提出了 CityDreamer4D，一种专门为生成无限扩展的 4D 城市而设计的组合式生成模型。我们的主要见解是：1）4D 城市生成应将动态对象（如车辆）与静态场景（如建筑和道路）分离；2）4D 场景中的所有对象都应由建筑、车辆和背景等不同类型的神经场组成。具体而言，我们提出了 Traffic Scenario Generator 和 Unbounded Layout Generator，利用高度紧凑的鸟瞰视图表示来生成动态交通场景和静态城市布局。4D 城市中的对象通过结合面向背景的神经场和面向实例的神经场生成，针对背景和实例的不同特性，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们提供了包含 OSM、Google Earth 和 CityTopia 的完整数据集套件。OSM 数据集提供了多种真实城市布局，而 Google Earth 和 CityTopia 数据集则提供了大规模、高质量的城市图像，并配有 3D 实例注释。凭借其组合式设计，CityDreamer4D 支持实例编辑、城市风格化和城市仿真等多种下游应用，并在生成逼真的 4D 城市方面实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在生成无限扩展的四维（空间+时间）城市场景，解决现有方法在处理复杂建筑、车辆等动态对象以及保持时间一致性方面的不足。此类生成技术对元宇宙、城市规划、环境模拟和游戏资产开发等领域具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到四维城市生成需要把静态建筑与道路与动态车辆分离，并借鉴 MaskGIT、VQVAE、NeRF 等已有技术来实现布局生成、BEV 表示和神经字段渲染。他们在此基础上设计了 Traffic Scenario Generator、Unbounded Layout Generator、以及针对背景、建筑和车辆的专用神经字段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将四维城市拆分为静态场景和动态交通两部分，分别用专门的生成器产生布局、背景、建筑实例和车辆实例，然后通过合成器将它们组合成完整的时间序列图像。流程包括：生成城市布局 → 生成背景图像 → 生成建筑实例 → 生成交通场景 → 生成车辆实例 → 合成最终四维图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）首次实现无限扩展的四维城市生成并分离动态与静态；2）使用专门的神经字段（背景用哈希网格，实例用周期性位置编码）来捕捉不同对象的多样性；3）引入高度紧凑的 BEV 表示和底部到顶部高度图；4）Traffic Scenario Generator 生成真实交通场景；5）Vehicle Instance Generator 基于规范化特征空间；6）提供新的 CityTopia 数据集。与以往仅生成 3D 场景或受限规模的 4D 场景不同，本文实现了大规模、可编辑的四维城市。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CityDreamer4D 提出了一个可拆分的生成框架，能够同时生成无限扩展的静态城市布局和动态交通场景，生成逼真的四维城市并支持实例级编辑。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.&lt;/p&gt;</description></item><item><guid>2501.15073v1</guid><title>SpatioTemporal Learning for Human Pose Estimation in Sparsely-Labeled Videos</title><link>http://arxiv.org/abs/2501.15073v1</link><author>Yingying Jiao, Zhigang Wang, Sifan Wu, Shaojing Fan, Zhenguang Liu, Zhuoyue Xu, Zheqi Wu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; STDPose是一种新框架，通过学习稀疏标注视频中的时空动态，提升视频中的人体姿态估计。它利用动态感知掩码捕捉长距离运动上下文，并通过编码与聚合时空表示来建模运动关系，从而在姿态传播和估计任务上取得了新的性能基准，并在仅使用26.7%标注数据时仍能保持竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频人体姿态估计面临挑战，主要因为需要大量人工标注，成本高且劳动强度大；现有方法难以捕捉长时序依赖，也忽视了时序姿态热图与视觉特征的互补关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述局限，提出STDPose框架，以稀疏标注视频学习时空动态，提升姿态估计的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; STDPose包含两项创新：一是动态感知掩码，用于捕捉长距离运动上下文；二是时空表示与运动动态的编码与聚合系统，能够有效建模时空关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; STDPose在三个大型评估数据集上为视频姿态传播和姿态估计任务设定了新的性能基准；利用姿态传播生成的伪标签，STDPose在仅使用26.7%标注数据时仍能取得竞争性表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; STDPose通过学习稀疏标注视频中的时空动态，显著提升了视频人体姿态估计的准确性和鲁棒性，并在标注稀缺的情况下保持高性能，成为该领域的新标杆。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类姿态估计在视频中仍然是一个挑战，主要是因为依赖大量手工标注的大型数据集，这既昂贵又劳动密集。 此外，现有方法往往难以捕捉长距离时间依赖，并忽视了时间姿态热图与视觉特征之间的互补关系。 为了解决这些局限，我们提出了STDPose，一种通过在稀疏标注视频中学习时空动态来增强人类姿态估计的新框架。 STDPose包含两项关键创新：1）一种新颖的动态感知掩码，用于捕捉长距离运动上下文，从而更细致地理解姿态变化。 2）一种用于编码和聚合时空表示与运动动态的系统，以有效建模时空关系，提升姿态估计的准确性和鲁棒性。 STDPose在三个大型评估数据集上为视频姿态传播（即将姿态注释从标注帧传播到未标注帧）和姿态估计任务设定了新的性能基准。 此外，利用姿态传播生成的伪标签，STDPose在仅使用26.7%标注数据时实现了竞争性表现。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Human pose estimation in videos remains a challenge, largely due to the reliance on extensive manual annotation of large datasets, which is expensive and labor-intensive. Furthermore, existing approaches often struggle to capture long-range temporal dependencies and overlook the complementary relationship between temporal pose heatmaps and visual features. To address these limitations, we introduce STDPose, a novel framework that enhances human pose estimation by learning spatiotemporal dynamics in sparsely-labeled videos. STDPose incorporates two key innovations: 1) A novel Dynamic-Aware Mask to capture long-range motion context, allowing for a nuanced understanding of pose changes. 2) A system for encoding and aggregating spatiotemporal representations and motion dynamics to effectively model spatiotemporal relationships, improving the accuracy and robustness of pose estimation. STDPose establishes a new performance benchmark for both video pose propagation (i.e., propagating pose annotations from labeled frames to unlabeled frames) and pose estimation tasks, across three large-scale evaluation datasets. Additionally, utilizing pseudo-labels generated by pose propagation, STDPose achieves competitive performance with only 26.7% labeled data.&lt;/p&gt;</description></item><item><guid>2502.05769v3</guid><title>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps Platform</title><link>http://arxiv.org/abs/2502.05769v3</link><author>Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种针对单栋建筑的数字孪生框架，利用云地图平台、先进的大语言模型和高斯散点网格提取技术，实现建筑三维模型和视觉描述的自动获取，并与云端地图和数据分析无缝集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市数字孪生通过多源数据和数据分析来优化城市规划、基础设施管理和决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种聚焦单栋建筑规模的数字孪生框架，以实现建筑级别的三维建模和信息集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 框架通过连接云地图平台（如谷歌地图API），使用多智能体大型语言模型（ChatGPT(4o)和Deepseek-V3/R1）进行数据分析，并采用基于高斯散点的网格提取管道来生成建筑三维模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架能够根据建筑地址、邮政编码或地理坐标检索建筑的三维模型和视觉描述，并实现云地图与大型语言模型数据分析的集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 单栋建筑级别的数字孪生框架可实现自动化三维建模和信息集成，为城市规划与管理提供精准的数据支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 城市数字孪生是利用多源数据和数据分析来优化城市规划、基础设施管理和决策的城市虚拟复制品。为此，我们提出了一个聚焦单栋建筑规模的框架。通过连接云地图平台（如谷歌地图平台API），利用最先进的多智能体大型语言模型（ChatGPT(4o)和Deepseek-V3/R1）进行数据分析，并使用基于高斯散点的网格提取管道，我们的数字孪生建筑框架能够根据建筑地址、邮政编码或地理坐标检索建筑的三维模型、视觉描述，并实现云端地图与大型语言模型数据分析的集成。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现一种自动化的数字建筑分析系统，能够从云地图服务获取建筑信息、生成三维网格模型，并利用多代理大型语言模型提供可视化描述。该问题在城市规划、建筑设计和数字孪生等领域具有重要意义，因为它能快速、准确地把现实建筑转化为可分析的数字资产。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了 Google Earth Studio、Segment Anything Model‑2、GroundingDINO、Gaussian Splatting 等现有技术，构建了一个端到端的管道。通过 Google Maps Platform 的 API 进行地理编码、地形和影像检索，并在此基础上使用多代理 LLM 进行图像分析和文本生成，充分借鉴了前人关于三维重建、云地图集成和 LLM 视觉分析的工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将云地图数据、三维重建和多代理 LLM 结合起来，实现从建筑地址到三维模型和视觉描述的全流程。实现流程包括：①使用 Google Maps API 获取地址、坐标、建筑多边形和地形；②用 Google Earth Studio 采集多视角图像；③通过 Gaussian Splatting 提取三维网格并生成合成图像；④启动多代理 LLM，分别提取关键词、聚合关键词并生成最终描述；⑤评估生成文本与图像的匹配度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：①整合云地图服务与三维重建的完整管道；②使用 Gaussian Splatting 进行高质量建筑网格提取；③设计多代理 LLM 模块实现多视角图像的关键词聚合与自动生成描述；④在无标注数据的情况下采用 CLIP/BLIP/PAC 评估指标。与以往工作相比，该方法首次将云地图、三维重建和多代理 LLM 统一到同一框架，并在多视角图像上实现自动化视觉描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套端到端的数字建筑分析框架，能够自动从 Google Maps 获取建筑信息，利用 Gaussian Splatting 重建三维网格，并通过多代理大型语言模型生成多视角视觉描述。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building&amp;#x27;s 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building&amp;#x27;s address, postal code, or geographic coordinates.&lt;/p&gt;</description></item><item><guid>2502.11822v1</guid><title>Assessing the impacts of tradable credit schemes through agent-based simulation</title><link>http://arxiv.org/abs/2502.11822v1</link><author>Renming Liu, Dimitrios Argyros, Yu Jiang, Moshe E. Ben-Akiva, Ravi Seshadri, Carlos Lima Azevedo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出并实现了一个基于代理与活动的综合仿真框架，用于评估可交易信用方案（TCS）的网络与市场表现，并通过大规模多模式网络实验验证其在减少拥堵方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 可交易信用方案因收入中立和公平性被视为拥堵定价的有吸引力替代方案，但现有研究多采用网络与市场均衡模型，对交通需求、供给、信用市场运作及行为的描述过于简化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个灵活、可扩展的仿真框架，以更真实地模拟多次行程、个体交易行为以及监管者、旅行者与市场之间的复杂互动，从而全面评估TCS。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在开源城市仿真平台SimMobility中实现该框架，包含：①考虑多次行程并显式建模个体交易行为的TCS设计；②捕捉监管者、旅行者与市场互动的仿真流程；③在大型mesoscopic多模式网络上进行实验，并结合贝叶斯优化寻找最优TCS设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示网络与市场性能在日常迭代中趋于稳定，符合已知理论；在所假设的市场行为下，TCS有效降低拥堵；仿真还能揭示不同用户、不同出行行为对TCS的影响，并通过设计调整避免负面交易行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于代理的仿真框架能够准确评估TCS的效能，为未来TCS设计与政策制定提供可靠工具，并证明其在减少拥堵方面的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可交易信用方案（TCS）因其收入中立和公平性的优势，已成为交通研究界关注的拥堵定价替代方案。然而，现有研究大多采用网络与市场均衡方法，对交通需求、供给、信用市场运作和市场行为的描述过于简化。基于代理与活动的仿真能够更真实地模拟需求、供给和个体市场互动，从而全面评估TCS。本文提出一个集成的仿真框架，用于建模TCS，并在最先进的开源城市仿真平台SimMobility中实现，包括：（a）一种灵活的TCS设计，考虑多次行程并显式考虑个体交易行为；（b）一个捕捉TCS监管者、旅行者与TCS市场自身复杂互动的仿真框架，具有测试未来TCS设计和相关出行模型的灵活性；以及（c）在大型mesoscopic多模式网络上进行的一系列仿真实验，并结合贝叶斯优化方法寻找TCS的最优设计。实验结果表明，网络和市场性能在日常迭代过程中趋于稳定，显示我们的基于代理的仿真与已知的TCS理论属性相一致。我们确认在所采用的市场行为假设下，TCS在降低拥堵方面的效率，并为模拟不同个体行为打开了道路。我们测量了TCS如何以不同方式影响本地网络、异质用户、不同出行行为，以及测试不同TCS设计如何避免负面市场交易行为。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Tradable credit schemes (TCS) have been attracting interest from the transportation research community as an appealing alternative to congestion pricing, due to the advantages of revenue neutrality and equity. Nonetheless, existing research has largely employed network and market equilibrium approaches with simplistic characterizations of transportation demand, supply, credit market operations, and market behavior. Agent- and activity-based simulation affords a natural means to comprehensively assess TCS by more realistically modeling demand, supply, and individual market interactions. We propose an integrated simulation framework for modeling a TCS, and implements it within the state-of-the-art open-source urban simulation platform SimMobility, including: (a) a flexible TCS design that considers multiple trips and explicitly accounts for individual trading behaviors; (b) a simulation framework that captures the complex interactions between a TCS regulator, the traveler, and the TCS market itself, with the flexibility to test future TCS designs and relevant mobility models; and (c) a set of simulation experiments on a large mesoscopic multimodal network combined with a Bayesian Optimization approach for TCS optimal design. The experiment results indicate network and market performance to stabilize over the day-to-day process, showing the alignment of our agent-based simulation with the known theoretical properties of TCS. We confirm the efficiency of TCS in reducing congestion under the adopted market behavioral assumptions and open the door for simulating different individual behaviors. We measure how TCS impacts differently the local network, heterogeneous users, the different travel behaviors, and how testing different TCS designs can avoid negative market trading behaviors.&lt;/p&gt;</description></item><item><guid>2502.12532v3</guid><title>CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space</title><link>http://arxiv.org/abs/2502.12532v3</link><author>Yong Zhao, Kai Xu, Zhengqiu Zhu, Yue Hu, Zhiheng Zheng, Yingfeng Chen, Yatai Ji, Chen Gao, Yong Li, Jincai Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了在城市环境中进行具身问答的新任务CityEQA，并提供了相应的基准数据集CityEQA-EC和一种新型的具身智能体PMA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往的具身问答研究主要聚焦于室内环境，城市环境的复杂性尚未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个在动态城市空间中通过主动探索回答开放词汇问题的任务，并提供支持该任务的数据集与智能体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 创建了包含1412个人工标注任务的CityEQA-EC数据集，并设计了Planner-Manager-Actor（PMA）架构的智能体，PMA通过规划、管理和执行三个层次实现长时程规划与分层任务执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明PMA在回答准确率上达到60.7%，显著优于现有基线，但与人类水平仍有差距，提示需要进一步提升视觉推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该工作为城市空间智能的未来发展奠定了基础，并展示了具身问答在城市环境中的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在真实城市环境中，具身代理如何通过主动探索来回答开放式自然语言问题的挑战。该问题在自动驾驶、城市治理和公共服务等实际应用中至关重要，因为它要求代理在复杂、动态的城市空间中进行感知、规划和推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先发现现有的具身问答研究几乎全部聚焦室内场景，缺乏城市尺度的开放式任务。为此，他们构建了 CityEQA‑EC 数据集，并借鉴了 LLM 的链式思考、VLM 的视觉语言理解、Ground‑SAM 的目标分割以及 A* 路径规划等技术，设计了 Planner‑Manager‑Actor 三层结构的层次化代理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将长时程的城市问答任务拆解为规划、映射与执行三大模块。流程为：1）Planner 用 LLM 解析问题并生成导航、探索、收集子任务；2）Manager 维护二维对象中心认知图和记忆，控制子任务执行；3）Actor 根据 Manager 指令调用 Navigator、Explorer 或 Collector 产生具体动作；4）循环更新地图与记忆，直至完成所有子任务后由 Manager 生成答案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①首个面向城市空间的开放式具身问答基准 CityEQA‑EC；②基于 LLM 的 Planner‑Manager‑Actor 层次化代理；③二维对象中心认知图与跨尺度动作控制；④将 VLM 作为收集子任务的视觉语言动作模块。与以往室内或固定视角的 EQA 任务不同，本文处理更大尺度、开放词汇、动态感知的城市环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出首个城市尺度的开放式具身问答基准，并设计了通过 LLM 规划、认知图映射和多模态动作执行的层次化代理，为实现城市环境中的自主感知与推理奠定了重要基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings-spanning environment, action, and perception-largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming competitive baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.&lt;/p&gt;</description></item><item><guid>2502.13078v3</guid><title>L4P: Towards Unified Low-Level 4D Vision Perception</title><link>http://arxiv.org/abs/2502.13078v3</link><author>Abhishek Badki, Hang Su, Bowen Wen, Orazio Gallo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; L4P 是一种前馈、通用的 4D 感知架构，能够在统一框架下解决多种低层次视频任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频像素的时空关系包含关键信息，现有方法多为任务专用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种通用模型，能够在单一框架中高效完成多种 4D 感知任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用预训练的 ViT 视频编码器，并为每个任务添加轻量级头部，保持前馈结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; L4P 在密集任务（深度、光流）和稀疏任务（2D/3D 跟踪）上与专用方法竞争，并且一次性完成所有任务的时间与单任务方法相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通用前馈架构可在多任务 4D 感知中实现高效且竞争性的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视频像素的时空关系携带着低层次 4D 感知任务的关键信息。一个能够推理它的单一模型应该能够很好地解决多种此类任务。然而，大多数最先进的方法依赖于专门为特定任务设计的架构。我们提出 L4P，一种前馈、通用架构，在统一框架下解决低层次 4D 感知任务。L4P 利用预训练的 ViT 视频编码器，并结合每个任务的轻量级头部，因此不需要大量训练。尽管其通用且前馈的形式，我们的方法在密集任务（如深度或光流估计）和稀疏任务（如 2D/3D 跟踪）上与现有专用方法具有竞争力，并且一次性解决所有任务的时间与单任务方法相当。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在统一低级4D视觉感知任务。其重要性未在文本中明确说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作，如VideoMAE、Omnimotion等。具体设计思路未在文本中详细说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是统一低级4D视觉感知。实现流程未在文本中说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点未在文本中说明。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The spatio-temporal relationship between the pixels of a video carries critical information for low-level 4D perception tasks. A single model that reasons about it should be able to solve several such tasks well. Yet, most state-of-the-art methods rely on architectures specialized for the task at hand. We present L4P, a feedforward, general-purpose architecture that solves low-level 4D perception tasks in a unified framework. L4P leverages a pre-trained ViT-based video encoder and combines it with per-task heads that are lightweight and therefore do not require extensive training. Despite its general and feedforward formulation, our method is competitive with existing specialized methods on both dense tasks, such as depth or optical flow estimation, and sparse tasks, such as 2D/3D tracking. Moreover, it solves all tasks at once in a time comparable to that of single-task methods.&lt;/p&gt;</description></item><item><guid>2502.15285v3</guid><title>Offload Rethinking by Cloud Assistance for Efficient Environmental Sound Recognition on LPWANs</title><link>http://arxiv.org/abs/2502.15285v3</link><author>Le Zhang, Quanling Zhao, Run Wang, Shirley Bian, Onat Gungor, Flavio Ponzina, Tajana Rosing</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 ORCA 系统，利用云协助在低功耗无线网络上实现高效环境声音识别，显著降低能耗和延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的设备端声音识别受限于资源，云端推理成本高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种资源高效、云协助的环境声音识别系统，适用于无电池、能量收集设备。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用自注意力的云端子频谱特征选择，减少设备推理所需数据，降低通信成本，并应对动态信道和不可靠的离线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实城市声学测试中，ORCA 在能耗上比现有方法高达 80 倍节省，在延迟上高达 220 倍缩短，同时保持相当的准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ORCA 在 LPWAN 上实现了高效、低能耗、低延迟的环境声音识别，证明了云协助策略的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 学习型环境声音识别已成为生物研究和城市规模传感系统中超低功耗环境监测的关键方法。这些系统通常在资源受限的环境下运行，并且常常由远程地区的能量收集供电。近期的设备端声音识别工作因资源限制而导致准确率低下，而云端卸载策略则受高通信成本的制约。在本研究中，我们提出了 ORCA，一种新型的资源高效云协助环境声音识别系统，适用于在低功耗广域网络（LPWAN）上运行的无电池设备，面向大范围音频感知应用。我们提出了一种云协助策略，既能弥补设备端推理的低准确率，又能最小化云端卸载的通信成本。通过利用基于自注意力的云端子频谱特征选择方法，ORCA 促进了高效的设备端推理，解决了在 LPWAN 上进行资源受限云卸载的三大关键挑战：1）高通信成本和低数据速率；2）动态无线信道条件；3）不可靠的卸载。我们在能量收集的无电池微控制器上实现了 ORCA，并在真实城市声学测试平台上进行了评估。结果表明，ORCA 在能耗上比最先进方法高达 80 倍节省，在延迟上高达 220 倍缩短，同时保持相当的准确率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Learning-based environmental sound recognition has emerged as a crucial method for ultra-low-power environmental monitoring in biological research and city-scale sensing systems. These systems usually operate under limited resources and are often powered by harvested energy in remote areas. Recent efforts in on-device sound recognition suffer from low accuracy due to resource constraints, whereas cloud offloading strategies are hindered by high communication costs. In this work, we introduce ORCA, a novel resource-efficient cloud-assisted environmental sound recognition system on batteryless devices operating over the Low-Power Wide-Area Networks (LPWANs), targeting wide-area audio sensing applications. We propose a cloud assistance strategy that remedies the low accuracy of on-device inference while minimizing the communication costs for cloud offloading. By leveraging a self-attention-based cloud sub-spectral feature selection method to facilitate efficient on-device inference, ORCA resolves three key challenges for resource-constrained cloud offloading over LPWANs: 1) high communication costs and low data rates, 2) dynamic wireless channel conditions, and 3) unreliable offloading. We implement ORCA on an energy-harvesting batteryless microcontroller and evaluate it in a real world urban sound testbed. Our results show that ORCA outperforms state-of-the-art methods by up to $80 \times$ in energy savings and $220 \times$ in latency reduction while maintaining comparable accuracy.&lt;/p&gt;</description></item><item><guid>2503.14558v2</guid><title>SuperPC: A Single Diffusion Model for Point Cloud Completion, Upsampling, Denoising, and Colorization</title><link>http://arxiv.org/abs/2503.14558v2</link><author>Yi Du, Zhipeng Zhao, Shaoshu Su, Sharath Golluri, Haoze Zheng, Runmao Yao, Chen Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SuperPC 是一种统一的扩散模型，能够同时完成点云的补全、上采样、去噪和上色，利用三层条件扩散框架和空间混合融合策略，显著优于单独任务模型及其组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云处理任务在自动驾驶和三维重建等应用中至关重要，但现有方法往往分别针对每个缺陷开发独立模型，忽视了缺陷共存和相互影响的现实。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种统一模型，解决点云多重缺陷的同时处理问题，减少误差累积和计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用三层条件扩散框架，并引入空间混合融合策略，利用四种缺陷之间的相关性实现并行处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 SuperPC 在补全、上采样、去噪和上色四项任务上均优于现有最先进的专用模型及其组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 统一扩散模型能够有效处理多重点云缺陷，提供更高效、更准确的点云处理方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; SuperPC is the first unified diffusion model that can simultaneously handle point cloud completion, upsampling, denoising, and colorization. It uses a three-level conditioned diffusion framework and a novel spatial-mix-fusion strategy to leverage correlations among these defects for efficient, concurrent processing, outperforming state-of-the-art specialized models and their combinations on all four tasks.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在一次性解决点云的四大处理任务——补全、上采样、去噪和上色。现实中这些缺陷往往同时出现，单独使用专门模型会导致误差累积和计算成本高。统一模型可提高效率并减少错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了扩散模型在点云生成中的成功，并结合早期和深度融合技术，提出三层条件（原始、局部、全局）与空间混合融合（SMF）策略。该设计在保持局部细节、全局语义和原始信息的同时，兼顾图像与点云两种模态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是用条件扩散模型生成点云，并通过三层条件引导去噪过程。流程包括：①将输入点云和图像通过SMF得到原始、局部、全局条件；②在扩散的前向噪声过程中逐步加入噪声；③在逆向过程中，网络预测噪声并利用三层条件逐步恢复干净、完整、上采样且上色的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①首次将四项点云任务统一到单一扩散模型；②提出三层条件框架，避免单一条件导致的局部最优；③设计空间混合融合策略，兼顾早期与深度融合；④在多场景基准上实现SOTA性能。与以往只处理单一任务或单模态的工作不同，SuperPC同时利用图像与点云信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SuperPC通过在原始、局部和全局层面融合图像与点云信息，构建单一扩散模型，能够同时完成点云补全、上采样、去噪和上色，并在多种基准上超越专用模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud (PC) processing tasks-such as completion, upsampling, denoising, and colorization-are crucial in applications like autonomous driving and 3D reconstruction. Despite substantial advancements, prior approaches often address each of these tasks independently, with separate models focused on individual issues. However, this isolated approach fails to account for the fact that defects like incompleteness, low resolution, noise, and lack of color frequently coexist, with each defect influencing and correlating with the others. Simply applying these models sequentially can lead to error accumulation from each model, along with increased computational costs. To address these challenges, we introduce SuperPC, the first unified diffusion model capable of concurrently handling all four tasks. Our approach employs a three-level-conditioned diffusion framework, enhanced by a novel spatial-mix-fusion strategy, to leverage the correlations among these four defects for simultaneous, efficient processing. We show that SuperPC outperforms the state-of-the-art specialized models as well as their combination on all four individual tasks.&lt;/p&gt;</description></item><item><guid>2503.15144v1</guid><title>PointSFDA: Source-free Domain Adaptation for Point Cloud Completion</title><link>http://arxiv.org/abs/2503.15144v1</link><author>Xing He, Zhe Zhu, Liangliang Nan, Honghua Chen, Jing Qin, Mingqiang Wei</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为PointSFDA的源无关域自适应框架，用于点云补全，克服了传统方法在真实扫描中的性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统点云补全方法多在合成数据集上训练，面对分布外的真实扫描时表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种仅使用预训练源模型和无标签目标数据的自适应方法，避免在实际场景中获取源数据的困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PointSFDA采用粗到细的蒸馏策略传递全局几何知识，并通过自监督的局部遮罩一致性训练学习目标域的局部几何信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，PointSFDA显著提升了跨域形状补全任务中最先进网络的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法是首个针对点云补全的源无关域自适应架构，提供了有效的解决方案并公开了代码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 传统的点云补全方法通常在合成数据集上训练，在应用于分布外的真实扫描时面临显著挑战。本文提出了一种有效且简单的源无关域自适应框架，称为PointSFDA。与通过直接利用标记源数据来减少域差距的无监督域自适应不同，PointSFDA仅使用预训练的源模型和无标签的目标数据进行自适应，避免了实际场景中无法获取源数据的需求。作为点云补全领域首个源无关域自适应架构，我们的方法提供了两个核心贡献。首先，我们引入了粗到细的蒸馏方案，显式传递源数据集学习到的全局几何知识。其次，由于域差距可能引入噪声，我们提出了一种自监督的局部遮罩一致性训练策略，以学习目标域中的局部几何信息。大量实验验证了我们的方法在跨域形状补全中显著提升了最先进网络的性能。我们的代码可在 https://github.com/Starak-x/PointSFDA 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Conventional methods for point cloud completion, typically trained on synthetic datasets, face significant challenges when applied to out-of-distribution real-world scans. In this paper, we propose an effective yet simple source-free domain adaptation framework for point cloud completion, termed \textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces the domain gap by directly leveraging labeled source data, PointSFDA uses only a pretrained source model and unlabeled target data for adaptation, avoiding the need for inaccessible source data in practical scenarios. Being the first source-free domain adaptation architecture for point cloud completion, our method offers two core contributions. First, we introduce a coarse-to-fine distillation solution to explicitly transfer the global geometry knowledge learned from the source dataset. Second, as noise may be introduced due to domain gaps, we propose a self-supervised partial-mask consistency training strategy to learn local geometry information in the target domain. Extensive experiments have validated that our method significantly improves the performance of state-of-the-art networks in cross-domain shape completion. Our code is available at \emph{\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}.&lt;/p&gt;</description></item><item><guid>2503.16326v1</guid><title>OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence</title><link>http://arxiv.org/abs/2503.16326v1</link><author>Long Yuan, Fengran Mo, Kaiyu Huang, Wenjie Wang, Wangyuxuan Zhai, Xiaoyu Zhu, You Li, Jinan Xu, Jian-Yun Nie</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究多模态大型语言模型在地理空间人工智能中的应用，并提出了OmniGeo模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态LLM快速发展，能够融合文本、图像和空间信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索多模态LLM在GeoAI领域的潜力，并提出适用于地理空间任务的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计OmniGeo模型，处理卫星影像、地理元数据和文本描述，结合自然语言理解与空间推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; OmniGeo在多种地理空间任务上优于专用模型和现有LLM，零样本任务表现竞争。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多模态LLM可显著提升GeoAI系统的指令遵循和准确性，OmniGeo为此提供有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大型语言模型（LLMs）的快速发展为人工智能开辟了新的前沿，使得能够整合文本、图像和空间信息等多种大规模数据类型。本文探讨了多模态LLMs（MLLM）在地理空间人工智能（GeoAI）中的潜力，GeoAI利用空间数据解决地理空间语义、健康地理、城市地理、城市感知和遥感等领域的挑战。我们提出了专为地理空间应用设计的MLLM（OmniGeo），能够处理和分析卫星影像、地理空间元数据和文本描述等异构数据源。通过结合自然语言理解和空间推理的优势，我们的模型提升了指令遵循能力和GeoAI系统的准确性。实验结果表明，我们的模型在多种地理空间任务上优于任务特定模型和现有LLMs，有效解决了多模态特性，并在零样本地理空间任务上取得了竞争性结果。代码将在发表后公开。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid advancement of multimodal large language models (LLMs) has opened new frontiers in artificial intelligence, enabling the integration of diverse large-scale data types such as text, images, and spatial information. In this paper, we explore the potential of multimodal LLMs (MLLM) for geospatial artificial intelligence (GeoAI), a field that leverages spatial data to address challenges in domains including Geospatial Semantics, Health Geography, Urban Geography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo) tailored to geospatial applications, capable of processing and analyzing heterogeneous data sources, including satellite imagery, geospatial metadata, and textual descriptions. By combining the strengths of natural language understanding and spatial reasoning, our model enhances the ability of instruction following and the accuracy of GeoAI systems. Results demonstrate that our model outperforms task-specific models and existing LLMs on diverse geospatial tasks, effectively addressing the multimodality nature while achieving competitive results on the zero-shot geospatial tasks. Our code will be released after publication.&lt;/p&gt;</description></item><item><guid>2503.17316v1</guid><title>Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors</title><link>http://arxiv.org/abs/2503.17316v1</link><author>Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, Jerome Revaud</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Pow3r 是一种新型的大规模 3D 视觉回归模型，能够灵活接受多种输入模态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往的前馈模型在测试时无法利用已知的相机或场景先验信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在单一网络中结合相机内参、相对姿态、稠密或稀疏深度等辅助信息的模型，以提升预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在 DUSt3R 变压器架构基础上加入轻量化的条件化机制，并在训练时随机提供不同子集的模态，使模型能在测试时适应不同程度的先验信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 3D 重建、深度补全、多视角深度预测、多视角立体匹配和多视角姿态估计等任务上，Pow3r 达到或超过了现有最优结果，验证了其充分利用所有可用信息的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Pow3r 通过灵活的模态融合和预训练的变压器架构，实现了在多种 3D 视觉任务中的卓越性能，并开启了原始分辨率推理和点云补全等新功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 Pow3r，一种新型的大规模 3D 视觉回归模型，能够高度灵活地接受多种输入模态。与以往缺乏在测试时利用已知相机或场景先验信息机制的前馈模型不同，Pow3r 在单一网络中结合了相机内参、相对姿态、稠密或稀疏深度等任何组合的辅助信息。基于最近的 DUSt3R 变压器范式，该模型利用强大的预训练，并通过轻量化且多功能的条件化作为额外引导，使网络在有辅助信息时能够预测更准确的结果。在训练过程中，我们在每一次迭代中向模型提供随机子集的模态，从而使模型能够在测试时在不同程度的已知先验下工作。这进一步开启了新的功能，例如在原生图像分辨率下进行推理，或完成点云。我们在 3D 重建、深度补全、多视角深度预测、多视角立体匹配和多视角姿态估计任务上的实验取得了最先进的结果，并证实了 Pow3r 在充分利用所有可用信息方面的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present Pow3r, a novel large 3D vision regression model that is highly versatile in the input modalities it accepts. Unlike previous feed-forward models that lack any mechanism to exploit known camera or scene priors at test time, Pow3r incorporates any combination of auxiliary information such as intrinsics, relative pose, dense or sparse depth, alongside input images, within a single network. Building upon the recent DUSt3R paradigm, a transformer-based architecture that leverages powerful pre-training, our lightweight and versatile conditioning acts as additional guidance for the network to predict more accurate estimates when auxiliary information is available. During training we feed the model with random subsets of modalities at each iteration, which enables the model to operate under different levels of known priors at test time. This in turn opens up new capabilities, such as performing inference in native image resolution, or point-cloud completion. Our experiments on 3D reconstruction, depth completion, multi-view depth prediction, multi-view stereo, and multi-view pose estimation tasks yield state-of-the-art results and confirm the effectiveness of Pow3r at exploiting all available information. The project webpage is https://europe.naverlabs.com/pow3r.&lt;/p&gt;</description></item><item><guid>2503.17475v1</guid><title>Spatiotemporal Learning with Context-aware Video Tubelets for Ultrasound Video Analysis</title><link>http://arxiv.org/abs/2503.17475v1</link><author>Gary Y. Li, Li Chen, Bryson Hicks, Nikolai Schnittke, David O. Kessler, Jeffrey Shupp, Maria Parker, Cristiana Baloescu, Christopher Moore, Cynthia Gregory, Kenton Gregory, Balasundar Raju, Jochen Kruecker, Alvin Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种轻量级的基于tubelet的视频检测与分类框架，能够同时保留全局空间信息和细粒度时空特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的基于tubelet的视频检测方法往往只关注检测ROI内的局部区域，导致丢失全局空间上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够在tubelet级别上保留全局空间上下文并提取细粒度时空特征的检测与分类框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在分类器中加入tubelet的位置、尺寸和置信度信息，并利用预训练检测模型的ROI对齐特征图来扩大感受野、降低计算复杂度；该时空tubelet分类器仅包含0.4M参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在14,804条超声视频（828名患者）的五折交叉验证中，该方法优于以往的tubelet基方法，且适用于实时工作流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该轻量级框架在肺实变和胸腔积液的检测与分类任务中表现优异，具有实时应用潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 计算机辅助病理检测算法需要准确解释多帧视频中的复杂时空信息。当前最先进的方法通过对视频子体积（tubelet）进行分类，但往往只关注检测ROI内的局部区域，导致丢失全局空间上下文。本文提出一种轻量级的tubelet基对象检测和视频分类框架，既保留全局空间上下文，又提取细粒度时空特征。为解决全局上下文缺失问题，我们将tubelet的位置、尺寸和置信度嵌入分类器输入；此外，利用预训练检测模型的ROI对齐特征图，借助学习到的特征表示扩大感受野并降低计算复杂度。该时空tubelet分类器仅包含0.4M参数，效率高。我们将该方法应用于超声视频中肺实变和胸腔积液的检测与分类。对14,804条视频（828名患者）进行五折交叉验证，结果显示该方法优于以往的tubelet基方法，且适用于实时工作流程。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Computer-aided pathology detection algorithms for video-based imaging modalities must accurately interpret complex spatiotemporal information by integrating findings across multiple frames. Current state-of-the-art methods operate by classifying on video sub-volumes (tubelets), but they often lose global spatial context by focusing only on local regions within detection ROIs. Here we propose a lightweight framework for tubelet-based object detection and video classification that preserves both global spatial context and fine spatiotemporal features. To address the loss of global context, we embed tubelet location, size, and confidence as inputs to the classifier. Additionally, we use ROI-aligned feature maps from a pre-trained detection model, leveraging learned feature representations to increase the receptive field and reduce computational complexity. Our method is efficient, with the spatiotemporal tubelet classifier comprising only 0.4M parameters. We apply our approach to detect and classify lung consolidation and pleural effusion in ultrasound videos. Five-fold cross-validation on 14,804 videos from 828 patients shows our method outperforms previous tubelet-based approaches and is suited for real-time workflows.&lt;/p&gt;</description></item><item><guid>2503.18007v1</guid><title>SymmCompletion: High-Fidelity and High-Consistency Point Cloud Completion with Symmetry Guidance</title><link>http://arxiv.org/abs/2503.18007v1</link><author>Hongyu Yan, Zijun Li, Kunming Luo, Li Lu, Ping Tan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于对称引导的点云补全方法 SymmCompletion，利用局部对称变换网络和对称引导变压器两大组件，能够在保持几何细节的同时提升补全结果的几何一致性，并在多个基准数据集上优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有点云补全方法虽然能得到全局完整的点云，但往往会丢失原始几何细节，并出现补全部分与已有点云之间的几何不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入对称引导机制，解决几何细节丢失和几何不一致的问题，提高补全点云的质量与一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 局部对称变换网络（LSTNet）估计点级局部对称变换，将部分输入的关键几何映射到缺失区域，生成几何对齐的部分-缺失对和初始点云；2) 对称引导变压器（SGFormer）利用这些对齐对的几何特征作为显式对称引导，约束初始点云的细化过程，从而得到高保真且几何一致的最终点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上的定性和定量评估表明，SymmCompletion 在补全质量和几何一致性方面均优于现有最先进的补全网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于对称引导的补全方法能够有效保留几何细节并提升几何一致性，显著提升点云补全性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分点云恢复完整点形。虽然现有方法能够在全局完整性上形成令人满意的点云，但它们往往失去原始几何细节，并面临现有点云与重建缺失部分之间几何不一致的问题。为解决此问题，我们提出了基于对称引导的高效补全方法 SymmCompletion。该方法由两个主要组件组成：局部对称变换网络（LSTNet）和对称引导变压器（SGFormer）。首先，LSTNet 高效估计点级局部对称变换，将部分输入的关键几何映射到缺失区域，从而生成几何对齐的部分-缺失对和初始点云。其次，SGFormer 利用部分-缺失对的几何特征作为显式对称引导，约束初始点云的细化过程。结果，SGFormer 能够利用提供的先验形成高保真且几何一致的最终点云。在多个基准数据集上的定性和定量评估表明，我们的方法优于现有最先进的补全网络。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to recover a complete point shape from a partial point cloud. Although existing methods can form satisfactory point clouds in global completeness, they often lose the original geometry details and face the problem of geometric inconsistency between existing point clouds and reconstructed missing parts. To tackle this problem, we introduce SymmCompletion, a highly effective completion method based on symmetry guidance. Our method comprises two primary components: a Local Symmetry Transformation Network (LSTNet) and a Symmetry-Guidance Transformer (SGFormer). First, LSTNet efficiently estimates point-wise local symmetry transformation to transform key geometries of partial inputs into missing regions, thereby generating geometry-align partial-missing pairs and initial point clouds. Second, SGFormer leverages the geometric features of partial-missing pairs as the explicit symmetric guidance that can constrain the refinement process for initial point clouds. As a result, SGFormer can exploit provided priors to form high-fidelity and geometry-consistency final point clouds. Qualitative and quantitative evaluations on several benchmark datasets demonstrate that our method outperforms state-of-the-art completion networks.&lt;/p&gt;</description></item><item><guid>2503.18082v1</guid><title>Vehicular Road Crack Detection with Deep Learning: A New Online Benchmark for Comprehensive Evaluation of Existing Algorithms</title><link>http://arxiv.org/abs/2503.18082v1</link><author>Nachuan Ma, Zhengfei Song, Qiang Hu, Chuang-Wei Liu, Yu Han, Yanting Zhang, Rui Fan, Lihua Xie</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文综述了城市数字孪生领域中基于深度学习的道路裂缝检测技术，提出了新的基准数据集UDTIRI-Crack，并对现有算法的检测效果、计算效率和泛化能力进行了系统评估，同时探讨了基础模型和大语言模型在该任务中的可行性，最后讨论了当前面临的挑战与未来发展趋势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着城市数字孪生的兴起，智能道路检测车辆需要具备自动裂缝识别功能。过去十年，深度学习方法已被用于提高裂缝检测的效率、准确性和客观性，但缺乏针对数据融合和标签高效算法的系统性综述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提供一份全面的综述，涵盖监督、无监督、半监督和弱监督四类深度学习算法；构建首个公开的UDTIRI-Crack数据集；对现有算法进行性能对比；探索基础模型和大语言模型在裂缝检测中的潜力；并讨论未来研究方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对现有深度学习算法进行分类与评述；收集七个公开标注源的高质量图像，构建2,500张样本的UDTIRI-Crack数据集；开展大规模实验，比较算法在检测精度、计算效率和泛化能力方面的表现；评估基础模型和大语言模型在该任务中的适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 现有算法在不同任务场景下表现各异，数据集的引入为公平评估提供了基础；基础模型和大语言模型在裂缝检测中展现出一定潜力，但仍需进一步研究以提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本综述为开发下一代智能道路检测车辆提供了实用指导，并指出了数据集、算法改进和模型迁移等未来研究重点；UDTIRI-Crack数据集已公开可用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在城市数字孪生的快速发展中，推进具备自动道路裂缝检测系统的智能道路检查车辆对于维护土木基础设施至关重要。过去十年，基于深度学习的道路裂缝检测方法已被开发，用以更高效、准确、客观地检测裂缝，目标是取代人工视觉检查。然而，关于该任务的最先进深度学习技术，尤其是数据融合和标签高效算法的系统综述仍然缺乏。本文全面回顾了最先进的基于深度学习的算法，包括监督、无监督、半监督和弱监督方法，并创建了名为UDTIRI-Crack的数据集，包含来自七个公开标注源的2,500张高质量图像，成为该领域首个大规模在线基准。我们进行了全面实验，比较了公共最先进深度学习算法在道路裂缝检测中的检测性能、计算效率和泛化能力。此外，还探讨了基础模型和大型语言模型在道路裂缝检测中的可行性。随后讨论了现有挑战和深度学习基于道路裂缝检测算法的未来发展趋势。我们认为这份综述可以为开发具备下一代道路状况评估系统的智能道路检测车辆提供实用指导。已发布的UDTIRI-Crack基准可在 https://udtiri.com/submission/ 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In the emerging field of urban digital twins (UDTs), advancing intelligent road inspection (IRI) vehicles with automatic road crack detection systems is essential for maintaining civil infrastructure. Over the past decade, deep learning-based road crack detection methods have been developed to detect cracks more efficiently, accurately, and objectively, with the goal of replacing manual visual inspection. Nonetheless, there is a lack of systematic reviews on state-of-the-art (SoTA) deep learning techniques, especially data-fusion and label-efficient algorithms for this task. This paper thoroughly reviews the SoTA deep learning-based algorithms, including (1) supervised, (2) unsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed for road crack detection. Also, we create a dataset called UDTIRI-Crack, comprising $2,500$ high-quality images from seven public annotated sources, as the first extensive online benchmark in this field. Comprehensive experiments are conducted to compare the detection performance, computational efficiency, and generalizability of public SoTA deep learning-based algorithms for road crack detection. In addition, the feasibility of foundation models and large language models (LLMs) for road crack detection is explored. Afterwards, the existing challenges and future development trends of deep learning-based road crack detection algorithms are discussed. We believe this review can serve as practical guidance for developing intelligent road detection vehicles with the next-generation road condition assessment systems. The released benchmark UDTIRI-Crack is available at https://udtiri.com/submission/.&lt;/p&gt;</description></item><item><guid>2503.18422v1</guid><title>Breaking the Encoder Barrier for Seamless Video-Language Understanding</title><link>http://arxiv.org/abs/2503.18422v1</link><author>Handong Li, Yiyuan Zhang, Longteng Guo, Xiangyu Yue, Jing Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种无编码器的视频大语言模型ELVA，直接建模视频与语言的细粒度交互，采用分层标记合并和视频引导监督，结合高低分辨率混合机制，使用仅700万对公开视频-文本数据，性能与传统编码器模型相当，同时显著降低计算量和推理延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的视频大语言模型使用编码器-解码器框架，视觉编码器提取帧特征，导致高计算成本、分辨率偏差和难以捕捉细粒度多模态交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服传统方法的高成本、分辨率偏差和细粒度交互捕捉困难，提出一种更高效、可扩展的无编码器视频大语言模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ELVA通过标记合并构建自下而上的分层表示，使用视频引导监督实现直接时空表示学习，并采用混合分辨率机制将高低分辨率帧结合，以平衡性能与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仅用700万公开视频-文本对，ELVA在性能上与基于编码器的视频大语言模型相当，同时将FLOPs降低95%和推理延迟降低92%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ELVA提供了一种可扩展且高效的实时视频理解方案，证明无编码器方法在保持性能的同时能显著提升计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大多数视频大语言模型采用编码器-解码器框架，其中视觉编码器提取逐帧特征供语言模型处理。然而，这种方法会产生高计算成本，引入分辨率偏差，并难以捕捉细粒度的多模态交互。为克服这些限制，我们提出了ELVA，一种无编码器的视频大语言模型，直接建模细致的视频-语言交互，而不依赖视觉编码器。ELVA采用标记合并构建自下而上的分层表示，并结合视频引导监督实现直接时空表示学习。此外，混合分辨率机制战略性地将高分辨率和低分辨率帧作为输入，以在性能和效率之间取得最佳平衡。仅使用700万公开可用的视频-文本对，ELVA在性能上与基于编码器的视频大语言模型相当，同时将FLOPs降低高达95%并将推理延迟降低92%，为实时视频理解提供了可扩展且高效的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder framework, where a vision encoder extracts frame-wise features for processing by a language model. However, this approach incurs high computational costs, introduces resolution biases, and struggles to capture fine-grained multimodal interactions. To overcome these limitations, we propose ELVA, an encoder-free Video-LLM that directly models nuanced video-language interactions without relying on a vision encoder. ELVA employs token merging to construct a bottom-up hierarchical representation and incorporates a video guidance supervisor for direct spatiotemporal representation learning. Additionally, a hybrid-resolution mechanism strategically integrates high- and low-resolution frames as inputs to achieve an optimal balance between performance and efficiency. With only 7M publicly available video-text pairs, ELVA achieves performance on par with encoder-based Video-LLMs while reducing FLOPs by up to 95\% and inference latency by 92\%, offering a scalable and efficient solution for real-time video understanding.&lt;/p&gt;</description></item><item><guid>2503.20748v1</guid><title>UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines</title><link>http://arxiv.org/abs/2503.20748v1</link><author>Chen Tang, Xinzhu Ma, Encheng Su, Xiufeng Song, Xiaohong Liu, Wei-Hong Li, Lei Bai, Wanli Ouyang, Xiangyu Yue</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; UniSTD是一种统一的Transformer框架，采用两阶段预训练+微调策略，先在二维视觉和视觉-文本数据上进行任务无关预训练，再在时空数据集上进行联合训练；通过分数插值的秩自适应混合专家适配和显式时序模块提升跨域学习能力；在覆盖10个任务、4个学科的大规模数据集上验证，证明单一模型即可同时支持10项任务，提升可扩展性并降低多域训练成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统时空模型往往依赖任务特定架构，受限于域特定设计，难以在多任务间泛化与扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种可统一处理多种时空任务的框架，提升模型的通用性与可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 任务无关的二维视觉与视觉-文本预训练；2) 通过分数插值实现秩自适应混合专家适配；3) 引入显式时序模块；4) 在时空数据集上进行联合微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 统一模型在10个任务、4个学科的数据集上实现了可扩展的跨任务学习，能够在单一模型中同时支持10项任务，并显著降低多域训练成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UniSTD证明了统一时空Transformer框架的可行性与优势，可在多任务场景下实现高效、可扩展的学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 传统的时空模型通常依赖于任务特定的架构，这限制了它们在不同任务之间的可泛化性和可扩展性。本文提出了UniSTD，一种基于Transformer的统一框架，受近期基础模型的两阶段预训练-适配范式启发。具体而言，我们的工作表明，在二维视觉和视觉-文本数据集上进行任务无关的预训练可以为时空学习构建一个可泛化的模型基础，随后在时空数据集上进行专门的联合训练以增强任务特定的适应性。为提升跨域学习能力，我们的框架采用秩自适应的混合专家适配，通过分数插值放松离散变量，使其能够在连续空间中优化。此外，我们引入了一个时序模块以显式地融合时序动态。我们在覆盖10个任务、4个学科的大规模数据集上评估了我们的方案，证明统一的时空模型能够实现可扩展的跨任务学习，并在单一模型中支持最多10个任务，同时降低多域应用中的训练成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Traditional spatiotemporal models generally rely on task-specific architectures, which limit their generalizability and scalability across diverse tasks due to domain-specific design requirements. In this paper, we introduce \textbf{UniSTD}, a unified Transformer-based framework for spatiotemporal modeling, which is inspired by advances in recent foundation models with the two-stage pretraining-then-adaption paradigm. Specifically, our work demonstrates that task-agnostic pretraining on 2D vision and vision-text datasets can build a generalizable model foundation for spatiotemporal learning, followed by specialized joint training on spatiotemporal datasets to enhance task-specific adaptability. To improve the learning capabilities across domains, our framework employs a rank-adaptive mixture-of-expert adaptation by using fractional interpolation to relax the discrete variables so that can be optimized in the continuous space. Additionally, we introduce a temporal module to incorporate temporal dynamics explicitly. We evaluate our approach on a large-scale dataset covering 10 tasks across 4 disciplines, demonstrating that a unified spatiotemporal model can achieve scalable, cross-task learning and support up to 10 tasks simultaneously within one model while reducing training costs in multi-domain applications. Code will be available at https://github.com/1hunters/UniSTD.&lt;/p&gt;</description></item><item><guid>2503.21104v1</guid><title>StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency</title><link>http://arxiv.org/abs/2503.21104v1</link><author>Yuyin Chen, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Yifei Zhan, Xianpeng Lang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 StyledStreets 多风格街景模拟器，实现指令驱动的场景编辑，保证空间和时间一致性，支持多种环境条件的真实感风格迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市场景重建需要同时建模静态基础设施和动态元素，并支持多样化环境条件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在不同季节、天气和摄像机设置下进行指令驱动的风格迁移，同时保持空间和时间一致性的街景模拟器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 基于 Gaussian Splatting 框架，加入姿态优化和多视角训练，提出三项创新：混合嵌入分离几何与风格、基于不确定性的渲染降低监督噪声、统一参数模型防止几何漂移，保证多视角一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示在雪、沙尘暴、夜晚等多种条件下实现了可信的过渡，且在风格迁移下保持了最先进的几何精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; StyledStreets 为城市模拟提供了新的能力，可用于自动驾驶测试和增强现实系统，代码将公开。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The paper presents StyledStreets, a multi-style street simulator that enables instruction-driven scene editing with guaranteed spatial and temporal consistency, built on a Gaussian Splatting framework enhanced by pose optimization and multi-view training. It introduces three key innovations: a hybrid embedding scheme that separates persistent geometry from transient style attributes, uncertainty-aware rendering to reduce supervision noise from diffusion priors, and a unified parametric model to prevent geometric drift and maintain multi-view consistency across seven vehicle-mounted cameras. The framework preserves motion patterns and geometric relationships, demonstrating plausible transitions across diverse conditions such as snow, sandstorms, and night, with state-of-the-art geometric accuracy under style transfers. This approach establishes new capabilities for urban simulation, applicable to autonomous vehicle testing and augmented reality systems requiring reliable environmental consistency. Code will be publicly available upon publication.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban scene reconstruction requires modeling both static infrastructure and dynamic elements while supporting diverse environmental conditions. We present \textbf{StyledStreets}, a multi-style street simulator that achieves instruction-driven scene editing with guaranteed spatial and temporal consistency. Building on a state-of-the-art Gaussian Splatting framework for street scenarios enhanced by our proposed pose optimization and multi-view training, our method enables photorealistic style transfers across seasons, weather conditions, and camera setups through three key innovations: First, a hybrid embedding scheme disentangles persistent scene geometry from transient style attributes, allowing realistic environmental edits while preserving structural integrity. Second, uncertainty-aware rendering mitigates supervision noise from diffusion priors, enabling robust training across extreme style variations. Third, a unified parametric model prevents geometric drift through regularized updates, maintaining multi-view consistency across seven vehicle-mounted cameras.   Our framework preserves the original scene&amp;#x27;s motion patterns and geometric relationships. Qualitative results demonstrate plausible transitions between diverse conditions (snow, sandstorm, night), while quantitative evaluations show state-of-the-art geometric accuracy under style transfers. The approach establishes new capabilities for urban simulation, with applications in autonomous vehicle testing and augmented reality systems requiring reliable environmental consistency. Codes will be publicly available upon publication.&lt;/p&gt;</description></item><item><guid>2503.21211v2</guid><title>Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years</title><link>http://arxiv.org/abs/2503.21211v2</link><author>Rixu Hao, Yuxin Zhao, Shaoqing Zhang, Guihua Wang, Xiong Deng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一种名为PTSTnet的可解释模型，结合动力学过程与跨尺度时空学习，并通过物理编码学习实现对ENSO长期预测的显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; ENSO对全球气候和社会有重要影响，但实现一年以上的实时预测仍具挑战性，传统动力学模型存在偏差，深度学习缺乏可解释性和多尺度处理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种可解释的神经网络框架，统一动力学过程与跨尺度学习，以提高ENSO长期预测精度并提供物理洞察。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PTSTnet采用物理编码学习，将动力学过程嵌入神经网络，学习稀疏数据下具有物理一致性的特征表示，处理海洋-大气相互作用的多尺度、多物理挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; PTSTnet在24个月以上的领先时间内显著优于现有基准，产生可解释预测，并揭示海洋-大气相互作用中误差传播的物理机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型通过物理一致性特征学习提升长期预测能力，标志着可解释神经海洋建模的重大进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 厄尔尼诺-南方涛动（ENSO）对全球气候和社会产生影响，但实现一年以上的实时预测仍具挑战性。动力学模型存在较大偏差和不确定性，深度学习在可解释性和多尺度动力学方面面临困难。本文提出了PTSTnet，一种可解释模型，在创新的神经网络框架中统一了动力学过程与跨尺度时空学习，并采用物理编码学习。PTSTnet在领先时间超过24个月时显著优于最先进的基准，产生可解释预测，并提供海洋-大气相互作用中误差传播的物理洞察。PTSTnet从稀疏数据中学习具有物理一致性的特征表示，以应对海洋-大气过程固有的多尺度和多物理挑战，从而本质上提升长期预测能力。我们的成功实现标志着在可解释神经海洋建模方面取得了重要进展。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;El Niño-Southern Oscillation (ENSO) exerts global climate and societal impacts, but real-time prediction with lead times beyond one year remains challenging. Dynamical models suffer from large biases and uncertainties, while deep learning struggles with interpretability and multi-scale dynamics. Here, we introduce PTSTnet, an interpretable model that unifies dynamical processes and cross-scale spatiotemporal learning in an innovative neural-network framework with physics-encoding learning. PTSTnet produces interpretable predictions significantly outperforming state-of-the-art benchmarks with lead times beyond 24 months, providing physical insights into error propagation in ocean-atmosphere interactions. PTSTnet learns feature representations with physical consistency from sparse data to tackle inherent multi-scale and multi-physics challenges underlying ocean-atmosphere processes, thereby inherently enhancing long-term prediction skill. Our successful realizations mark substantial steps forward in interpretable insights into innovative neural ocean modelling.&lt;/p&gt;</description></item><item><guid>2503.23394v2</guid><title>Spatiotemporal Learning of Brain Dynamics from fMRI Using Frequency-Specific Multi-Band Attention for Cognitive and Psychiatric Applications</title><link>http://arxiv.org/abs/2503.23394v2</link><author>Sangyoon Bae, Junbeom Kwon, Shinjae Yoo, Jiook Cha</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于变压器的多频段脑网络模型，能够捕捉 fMRI 数据中频率特异的时空动态，并在大规模人群中实现对精神疾病和认知功能的预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类大脑的功能动态表现出尺度无关和多分形特性，但传统神经影像分析方法假设线性和稳态，无法揭示频率特定的神经计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够显式建模频率特异脑动力学的框架，以提高精神疾病诊断和认知预测的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建 Multi-Band Brain Net (MBBN)，结合生物学基础的频率分解和多频段自注意力机制，对 49,673 名个体的 fMRI 数据进行训练，并在 UK Biobank、ABCD、ABIDE 三大队列中评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MBBN 在抑郁、注意缺陷多动障碍和自闭症谱系障碍的分类任务中，比现有方法提升高达52.5% 的 AUROC；在 ADHD 中，高频前额-感官运动连通性减弱，运算皮层节点成为动态枢纽；在 ASD 中，眶额-感官循环在高频出现局部破坏，且颞顶交界与前额叶在超低频上耦合增强。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过将尺度感知的神经动力学与深度学习相结合，MBBN 提供了更准确、更可解释的生物标志物，为精准精神病学和发展神经科学开辟了新路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种基于变压器的多频段脑网络框架，能够显式建模 fMRI 数据中的频率特异时空脑动力学。该框架结合了生物学基础的频率分解和多频段自注意力机制，能够发现以前难以检测的频率依赖网络交互。通过在 UK Biobank、ABCD 和 ABIDE 三大大型队列中训练 49,673 名个体，MBBN 在预测抑郁、注意缺陷多动障碍和自闭症谱系障碍等精神疾病以及认知智力得分方面实现了新的最佳水平，尤其在分类任务中提升了高达 52.5% 的 AUROC。频率分辨率分析揭示了疾病特异性特征：在 ADHD 中，高频前额-感官运动连通性减弱，运算皮层节点成为动态枢纽；在 ASD 中，眶额-感官循环在高频出现局部破坏，并且颞顶交界与前额叶在超低频上耦合增强。通过将尺度感知的神经动力学与深度学习相结合，MBBN 提供了更准确、更可解释的生物标志物，为精准精神病学和发展神经科学开辟了新途径。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding how the brain&amp;#x27;s complex nonlinear dynamics give rise to cognitive function remains a central challenge in neuroscience. While brain functional dynamics exhibits scale-free and multifractal properties across temporal scales, conventional neuroimaging analytics assume linearity and stationarity, failing to capture frequency-specific neural computations. Here, we introduce Multi-Band Brain Net (MBBN), the first transformer-based framework to explicitly model frequency-specific spatiotemporal brain dynamics from fMRI. MBBN integrates biologically-grounded frequency decomposition with multi-band self-attention mechanisms, enabling discovery of previously undetectable frequency-dependent network interactions. Trained on 49,673 individuals across three large-scale cohorts (UK Biobank, ABCD, ABIDE), MBBN sets a new state-of-the-art in predicting psychiatric and cognitive outcomes (depression, ADHD, ASD), showing particular strength in classification tasks with up to 52.5\% higher AUROC and provides a novel framework for predicting cognitive intelligence scores. Frequency-resolved analyses uncover disorder-specific signatures: in ADHD, high-frequency fronto-sensorimotor connectivity is attenuated and opercular somatosensory nodes emerge as dynamic hubs; in ASD, orbitofrontal-somatosensory circuits show focal high-frequency disruption together with enhanced ultra-low-frequency coupling between the temporo-parietal junction and prefrontal cortex. By integrating scale-aware neural dynamics with deep learning, MBBN delivers more accurate and interpretable biomarkers, opening avenues for precision psychiatry and developmental neuroscience.&lt;/p&gt;</description></item><item><guid>2504.00370v1</guid><title>Spatiotemporal Attention Learning Framework for Event-Driven Object Recognition</title><link>http://arxiv.org/abs/2504.00370v1</link><author>Tiantian Xie, Pengpai Wang, Rosa H. M. Chan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于VGG网络并加入CBAM注意力模块的时空学习框架，用于事件驱动的目标识别。该方法在保持与ResNet系列方法相当的性能的同时，参数量更少，且在未使用预训练权重时仍能保持较高准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 事件驱动视觉传感器模仿生物神经系统，能够异步捕捉像素级强度变化，具有高动态范围、低延迟和低功耗等优势，能够克服传统相机的运动模糊和冗余背景信息等局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在保持高识别性能的前提下，降低事件驱动模型的计算开销和参数复杂度，使其更适合实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用VGG网络并增强Convolutional Block Attention Module（CBAM），构建时空学习框架，对事件流进行特征提取和注意力加权，以实现目标识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架在CIFAR10-DVS和N-Caltech101数据集上，未使用预训练权重时分别取得71.3%和72.4%的Top-1准确率，超过MVF-Net等ResNet基方法；参数量比原VGG模型减少2.3%，并且对数据增强的依赖降低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在不依赖预训练权重和大量数据增强的情况下，仍能实现高效、鲁棒的事件驱动目标识别，适用于资源受限或无迁移学习场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 事件驱动视觉传感器受生物神经系统启发，能够异步捕捉像素级强度变化，形成包含位置、极性和时间戳信息的稀疏事件流。这些神经形态传感器在动态范围、延迟和功耗方面具有显著优势，能够本质上解决传统相机的运动模糊和冗余背景信息等局限，使其特别适用于动态视觉任务。近年来，已有研究提出越来越复杂的事件驱动架构，但这些方法的计算开销和参数复杂度限制了其实际部署。本文提出一种新颖的时空学习框架，用于事件驱动目标识别，采用增强了卷积块注意力模块（CBAM）的VGG网络。我们的方案在保持与ResNet系列方法相当的性能的同时，参数量比原始VGG模型减少2.3%。具体而言，它在CIFAR10-DVS数据集上预训练时取得最高的Top-1准确率76.4%，未预训练时为71.3%；在N-Caltech101数据集上未预训练时为72.4%。这些结果凸显了我们方法在未使用预训练权重时的鲁棒性，适用于无法使用迁移学习的场景。此外，我们的方法降低了对数据增强的依赖。对标准事件驱动数据集的实验结果表明，该框架在实际应用中具有高效性和有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Event-based vision sensors, inspired by biological neural systems, asynchronously capture local pixel-level intensity changes as a sparse event stream containing position, polarity, and timestamp information. These neuromorphic sensors offer significant advantages in dynamic range, latency, and power efficiency. Their working principle inherently addresses traditional camera limitations such as motion blur and redundant background information, making them particularly suitable for dynamic vision tasks. While recent works have proposed increasingly complex event-based architectures, the computational overhead and parameter complexity of these approaches limit their practical deployment. This paper presents a novel spatiotemporal learning framework for event-based object recognition, utilizing a VGG network enhanced with Convolutional Block Attention Module (CBAM). Our approach achieves comparable performance to state-of-the-art ResNet-based methods while reducing parameter count by 2.3% compared to the original VGG model. Specifically, it outperforms ResNet-based methods like MVF-Net, achieving the highest Top-1 accuracy of 76.4% (pretrained) and 71.3% (not pretrained) on CIFAR10-DVS, and 72.4% (not pretrained) on N-Caltech101. These results highlight the robustness of our method when pretrained weights are not used, making it suitable for scenarios where transfer learning is unavailable. Moreover, our approach reduces reliance on data augmentation. Experimental results on standard event-based datasets demonstrate the framework&amp;#x27;s efficiency and effectiveness for real-world applications.&lt;/p&gt;</description></item><item><guid>2504.12262v1</guid><title>SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields</title><link>http://arxiv.org/abs/2504.12262v1</link><author>David Keetae Park, Xihaier Luo, Guang Zhao, Seungjun Lee, Miruna Oprescu, Shinjae Yoo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SCENT是一种可扩展且考虑连续性的空间时间表示学习框架，统一了插值、重建和预测任务。它基于Transformer编码器-处理器-解码器结构，引入可学习查询和查询级交叉注意力机制，并采用稀疏注意力实现高效可扩展性，实验表明其在多项挑战任务中达到最先进性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 空间时间学习面临空间与时间依赖的复杂交互、高维数据以及可扩展性限制。在科学领域，数据往往不规则分布、存在缺失值且体量巨大，进一步增加了计算和建模难度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种可扩展、连续性感知的框架，能够统一处理空间时间数据的插值、重建和预测任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用Transformer编码器-处理器-解码器骨干，加入可学习查询以提升泛化能力，使用查询级交叉注意力捕捉多尺度依赖，并通过稀疏注意力机制实现对大规模数据和模型的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SCENT在大量仿真和真实世界实验中表现出最先进的性能，并在多项挑战任务中实现了卓越的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SCENT提供了一种统一且可扩展的空间时间表示学习解决方案，显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 空间时间学习因空间与时间依赖的复杂交互、高维数据以及可扩展性限制而具有挑战性。在科学领域，这些挑战进一步放大，因为数据往往不规则分布（例如传感器故障导致缺失值）且体量巨大（例如高保真模拟），这带来了额外的计算和建模困难。本文提出了 SCENT，一种可扩展且考虑连续性的空间时间表示学习框架。SCENT 将插值、重建和预测统一到一个架构中。基于 transformer 的编码器-处理器-解码器骨干，SCENT 引入可学习查询以增强泛化能力，并采用查询级交叉注意力机制有效捕捉多尺度依赖。为确保在数据规模和模型复杂度上的可扩展性，我们加入稀疏注意力机制，使输出表示灵活且在任意分辨率下高效评估。通过大量仿真和真实世界实验验证，SCENT 在多项挑战任务中实现了最先进的性能，并取得了卓越的可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints. These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties. In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning. SCENT unifies interpolation, reconstruction, and forecasting within a single architecture. Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions. We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability.&lt;/p&gt;</description></item><item><guid>2504.13580v4</guid><title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title><link>http://arxiv.org/abs/2504.13580v4</link><author>Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文展示了利用自动检索的合成CAD模型生成高质量3D标注，并证明在此基础上训练的深度学习模型在点云补全和单视角CAD检索与对齐任务上优于手工标注模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高层次3D场景理解在众多应用中至关重要，但准确的3D标注生成成本高、难度大，限制了深度学习模型的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证自动检索的合成CAD模型可作为高质量训练数据，并评估其在实际任务中的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用与ScanNet场景自动标注相似的管道，对缺乏标注的ScanNet++ v1数据集进行自动标注，生成CAD模型与9维姿态对应，并用这些标注训练监督式深度学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 自动获得的标注不仅可用于训练深度学习模型，而且训练出的模型在点云补全和单视角CAD检索与对齐任务上均优于使用手工标注的数据训练的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自动3D标注能够提升模型性能并显著降低标注成本，作者将发布标注集SCANnotate++及训练好的模型以支持后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 高层次3D场景理解在许多应用中至关重要。然而，生成准确的3D注释的挑战使得深度学习模型的开发变得困难。我们转向最近在自动检索合成CAD模型方面的进展，并展示了由此方法生成的数据可用作训练监督式深度学习模型的高质量真值。更确切地说，我们采用了与之前用于自动为ScanNet场景中的对象标注9维姿态和CAD模型相似的管道。这一次，我们将其应用于最近的ScanNet++ v1数据集，该数据集以前缺乏此类注释。我们的发现表明，不仅可以在这些自动获得的注释上训练深度学习模型，而且得到的模型在单视角CAD模型检索与对齐和点云补全等两个不同任务上优于使用手工注释的数据训练的模型。我们的结果强调了自动3D注释在提升模型性能的同时显著降低注释成本的潜力。为支持未来的3D场景理解研究，我们将发布我们的注释集，称为SCANnotate++，以及我们的训练模型。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决如何为室内 RGB‑D 场景自动生成高质量的 CAD 模型与姿态注释，从而降低人工标注成本并避免人工误差。准确的 3D 注释是训练深度学习模型进行点云补全、CAD 检索等任务的关键，而人工标注既耗时又不易保持一致；自动化方法可大幅提升数据规模与质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的 SCANnotate 自动标注框架基础上，引入更强大的 HOC‑Search 检索算法，并对阈值与姿态细化步骤做了改进。该方法借鉴了 ShapeNet、HOC‑Search、ShapeGF、ROCA 等现有工作，并结合 ScanNet++ 数据集进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 HOC‑Search 在大规模 CAD 数据库中联合搜索离散与连续参数，得到与目标物体最匹配的 CAD 模型与姿态，然后通过梯度优化细化姿态，并对同类物体进行聚类与克隆。实现流程为：① 读取 RGB‑D 场景与 3D 语义分割；② 计算每个物体的初始包围盒；③ 运行 HOC‑Search 进行 CAD 检索与姿态估计；④ 对候选模型做姿态细化；⑤ 对同类物体聚类、克隆并手工验证；⑥ 输出 SCANnotate++ 注释并用于后续训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 将 HOC‑Search 应用于 ScanNet++，生成规模达 5k 对象的自动注释；② 证明自动注释训练的模型在点云补全和单视图 CAD 检索任务上优于使用人工注释的模型；③ 公开 SCANnotate++ 数据集与预训练模型。与之前仅在 ScanNet 上使用手工或半自动注释的工作不同，本文实现了全自动、可扩展的注释流程，并展示了其在新数据集上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们证明了自动检索并对齐 CAD 模型的注释可以替代昂贵的人工标注，提升点云补全和单视图 CAD 检索的性能，并公开了 SCANnotate++ 数据集与训练模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.&lt;/p&gt;</description></item><item><guid>2504.13788v1</guid><title>RefComp: A Reference-guided Unified Framework for Unpaired Point Cloud Completion</title><link>http://arxiv.org/abs/2504.13788v1</link><author>Yixuan Yang, Jinyu Yang, Zixiang Zhao, Victor Sanchez, Feng Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种参考引导的点云补全框架RefComp，能够在有类别信息和无类别信息的训练环境下实现高质量的点云补全。该框架将无配对补全问题转化为形状翻译问题，并利用部分-完整点云对作为参考，引导补全过程。实验表明，RefComp在类别感知训练中达到最先进水平，在类别无关训练中也表现出竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的无配对点云补全方法多为类别感知，需要为每个物体类别训练单独模型，导致泛化能力有限，难以在真实场景中处理多样化的3D物体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种既能在类别感知又能在类别无关训练中表现优异的无配对点云补全框架，以提升在通用3D物体上的补全效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; RefComp框架通过在部分点云的潜在特征空间中进行形状翻译来完成补全。首先使用待补全的部分点云作为模板检索对应的完整点云，形成部分-完整点云对作为参考数据。框架包含共享参数的参考分支和目标分支，并通过潜在形状融合模块（LSFM）实现形状融合与翻译，强化补全过程中的结构特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在虚拟扫描和真实数据集上进行的大量实验表明，RefComp在类别感知训练设置下实现了最先进的性能，在类别无关训练设置下也取得了竞争性的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RefComp通过参考引导的形状翻译方法，显著提升了无配对点云补全的泛化能力，能够在多种训练场景下提供高质量的补全结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 无配对点云补全任务旨在使用未配对的模型完成部分点云。现有的无配对点云补全方法是类别感知的，即每个物体类别需要单独的模型。由于其泛化能力有限，这些方法在面对广泛的通用3D物体点云时在真实场景中表现不佳。本文提出了一种新颖的无配对点云补全框架——参考引导补全（RefComp）框架，在类别感知和类别无关的训练设置下均能取得强劲表现。RefComp框架将无配对补全问题转化为形状翻译问题，并在部分点云的潜在特征空间中解决。为此，我们引入了使用待补全的部分点云作为模板检索的部分-完整点云对，这些点云对被用作参考数据来引导补全过程。我们的RefComp框架使用共享参数的参考分支和目标分支，通过潜在形状融合模块（LSFM）实现形状融合和形状翻译，以增强补全管道中的结构特征。大量实验表明，RefComp框架在类别感知训练设置下不仅实现了最先进的性能，在类别无关训练设置下也取得了竞争性的结果，适用于虚拟扫描和真实世界数据集。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在没有配对完整点云的情况下完成部分点云的问题。传统方法需要为每个物体类别训练单独模型，难以在真实场景中泛化。能够在无监督、无类别限制的条件下完成点云，对自动驾驶、3D重建等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有无配对完成方法大多是类感知且依赖GAN或扩散模型。借鉴参考引导技术和形状翻译思路，他们提出用目标部分点云作为模板检索相似完整点云，形成参考对。随后在潜在空间中融合特征，完成点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是把无配对完成转化为形状翻译：为目标部分点云检索若干参考完整-部分对，使用共享编码器提取特征，Latent Shape Fusion Module 将目标特征与参考特征融合并映射到完整空间，最后解码得到完整点云。整个流程包括检索、编码、融合、解码和对齐损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首个统一的类无关无配对完成框架；2) 引入参考引导的形状翻译思路；3) 设计 Latent Shape Fusion Module 在潜在空间融合缺失结构；4) 通过检索得到的参考对实现无监督对齐。与以往依赖GAN或扩散、仅类感知的工作不同，RefComp 在类无关设置下也能取得竞争性甚至领先的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RefComp 通过参考引导的形状翻译和潜在特征融合，构建了一个同时支持类感知和类无关的无配对点云完成框架，并在多数据集上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The unpaired point cloud completion task aims to complete a partial point cloud by using models trained with no ground truth. Existing unpaired point cloud completion methods are class-aware, i.e., a separate model is needed for each object class. Since they have limited generalization capabilities, these methods perform poorly in real-world scenarios when confronted with a wide range of point clouds of generic 3D objects. In this paper, we propose a novel unpaired point cloud completion framework, namely the Reference-guided Completion (RefComp) framework, which attains strong performance in both the class-aware and class-agnostic training settings. The RefComp framework transforms the unpaired completion problem into a shape translation problem, which is solved in the latent feature space of the partial point clouds. To this end, we introduce the use of partial-complete point cloud pairs, which are retrieved by using the partial point cloud to be completed as a template. These point cloud pairs are used as reference data to guide the completion process. Our RefComp framework uses a reference branch and a target branch with shared parameters for shape fusion and shape translation via a Latent Shape Fusion Module (LSFM) to enhance the structural features along the completion pipeline. Extensive experiments demonstrate that the RefComp framework achieves not only state-of-the-art performance in the class-aware training setting but also competitive results in the class-agnostic training setting on both virtual scans and real-world datasets.&lt;/p&gt;</description></item><item><guid>2505.00690v1</guid><title>Towards Autonomous Micromobility through Scalable Urban Simulation</title><link>http://arxiv.org/abs/2505.00690v1</link><author>Wayne Wu, Honglin He, Chaoyuan Zhang, Jack He, Seth Z. Zhao, Ran Gong, Quanyi Li, Bolei Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个可扩展的城市仿真平台 URBAN-SIM，用于大规模训练嵌入式智能体，以实现自主微型移动设备的安全高效操作，并基于此平台开发了 URBAN-BENCH 评测套件，评估不同机器人在城市行走、导航和穿越任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 微型移动设备在城市公共空间中使用轻量化移动机器，如送货机器人和移动电动滑板车，已成为替代传统车辆的有前景方案。然而，目前的微型移动主要依赖人工操作，导致在拥挤的城市环境中存在安全和效率问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过人工智能代理辅助人类操控微型移动设备，提升其在复杂城市环境中的安全性和效率，并为此提供可扩展的仿真与评测工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建 URBAN-SIM 平台，包含分层城市生成、交互动力学生成和异步场景采样三大模块，以提高仿真多样性、真实性和学习效率；随后设计 URBAN-BENCH 评测套件，包含基于城市行走、导航和穿越三项核心技能的八个任务，用以评估不同机器人在多样地形和城市结构中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，四种不同形态的机器人（轮式和足式）在八项任务中各有优势和局限，仿真平台能够有效捕捉其在多样地形和城市结构下的表现差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; URBAN-SIM 与 URBAN-BENCH 为自主微型移动研究提供了高效、真实的训练与评测环境，能够帮助开发更安全、更高效的微型移动系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot&amp;#x27;s strengths and limitations.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot&amp;#x27;s strengths and limitations.&lt;/p&gt;</description></item><item><guid>2505.07396v2</guid><title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title><link>http://arxiv.org/abs/2505.07396v2</link><author>Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; TUM2TWIN 是首个面向城市数字孪生的多模态基准数据集，覆盖地面、移动、空中和卫星观测，包含 32 个子集、约 100,000 平方米面积和 767 GB 数据。该数据集提供地理参考、语义对齐的 3D 模型与网络，支持高精度室内外采集和多模态融合，帮助研究人员分析传感器性能并开发先进的重建方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市数字孪生在城市管理和多源异构数据集成中变得不可或缺，但现有数据集往往只覆盖处理链的一部分，限制了对完整数字孪生的验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个全面、可验证的多模态城市数字孪生基准数据集，以解决数据获取、模型重建、更新维护和互操作性等多阶段挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过地理参考、语义对齐的 3D 模型与网络，结合地面、移动、空中和卫星观测，创建 32 个子集，覆盖约 100,000 平方米面积，数据量约 767 GB。随后在该数据集上演示新视角合成、太阳能潜力分析、点云语义分割和 LoD3 建筑重建等下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; TUM2TWIN 能够支持传感器的稳健分析和先进重建方法的开发，并在新视角合成、太阳能分析、语义分割和建筑重建等任务中展示出显著潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该数据集为克服城市数字孪生创建中的现有限制奠定基础，推动新的研究方向和面向更智能、数据驱动城市环境的实用解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 城市数字孪生（UDTs）已成为管理城市和整合来自多样来源的复杂异构数据的关键。创建 UDTs 涉及多个过程阶段的挑战，包括获取准确的 3D 源数据、重建高保真 3D 模型、维护模型更新以及确保与下游任务的无缝互操作性。目前的数据集通常仅限于处理链的一部分，阻碍了对完整 UDTs 的验证。为了解决这些挑战，我们推出了首个全面的多模态城市数字孪生基准数据集：TUM2TWIN。该数据集包含地理参考、语义对齐的 3D 模型和网络，以及各种地面、移动、空中和卫星观测，拥有 32 个数据子集，覆盖约 100,000 平方米面积，当前数据量约 767 GB。通过确保地理参考的室内外采集、高精度和多模态数据集成，基准支持对传感器的稳健分析和先进重建方法的发展。此外，我们还探索了下游任务，展示了 TUM2TWIN 的潜力，包括 NeRF 和高斯散点的新视角合成、太阳能潜力分析、点云语义分割和 LoD3 建筑重建。我们相信这项贡献为克服 UDT 创建中的当前局限性奠定了基础，促进了新的研究方向和面向更智能、数据驱动城市环境的实用解决方案。项目可在 https://tum2t.win 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models&amp;#x27; updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win&lt;/p&gt;</description></item><item><guid>2505.08034v2</guid><title>Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey</title><link>http://arxiv.org/abs/2505.08034v2</link><author>Ankit Shetgaonkar, Dipen Pradhan, Lakshit Arora, Sanjay Surendranath Girija, Shashank Kapoor, Aman Raj</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这篇论文探讨了生成式人工智能在智慧城市中的应用，特别是通过对话式界面服务市民、运营者和规划者三类关键用户，并综述了相关模型与技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着城市物联网和数字孪生的普及，形成了丰富的数据基础，为智慧城市提升城市生活和运营提供了可能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在系统梳理并评估生成式人工智能在智慧城市中针对三类用户的对话式应用，及其在现有数据基础上的实现方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过文献回顾和案例分析，识别并评估已提出或部署的生成式人工智能模型与技术，聚焦于城市子系统与用户角色的匹配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 发现生成式人工智能能够利用多模态数据生成文本和仿真结果，天然语言理解与生成技术可降低用户与复杂系统交互的门槛，并且已有多种模型在不同子系统中得到应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该工作首次从关键用户视角全面总结了生成式人工智能在智慧城市中的技术与实践，为后续研究与应用提供了参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成式人工智能（GenAI）通过处理多模态内容并生成文本和仿真等新颖输出，显著提升了基于物联网和数字孪生的数据基础在智慧城市中的应用潜力。论文聚焦于面向市民、运营者和规划者的对话式界面，评估了相关模型与技术，并探讨了如何在官方记录、物联网数据流和城市数字孪生上构建 GenAI。该研究被认为是首个从关键用户视角对智慧城市中 GenAI 技术进行全面综述。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The proliferation of IoT in cities, combined with Digital Twins, creates a rich data foundation for Smart Cities aimed at improving urban life and operations. Generative AI (GenAI) significantly enhances this potential, moving beyond traditional AI analytics and predictions by processing multimodal content and generating novel outputs like text and simulations. Using specialized or foundational models, GenAI&amp;#x27;s natural language abilities such as Natural Language Understanding (NLU) and Natural Language Generation (NLG) can power tailored applications and unified interfaces, dramatically lowering barriers for users interacting with complex smart city systems. In this paper, we focus on GenAI applications based on conversational interfaces within the context of three critical user archetypes in a Smart City - Citizens, Operators and Planners. We identify and review GenAI models and techniques that have been proposed or deployed for various urban subsystems in the contexts of these user archetypes. We also consider how GenAI can be built on the existing data foundation of official city records, IoT data streams and Urban Digital Twins. We believe this work represents the first comprehensive summarization of GenAI techniques for Smart Cities from the lens of the critical users in a Smart City.&lt;/p&gt;</description></item><item><guid>2505.13803v3</guid><title>GenAI Models Capture Urban Science but Oversimplify Complexity</title><link>http://arxiv.org/abs/2505.13803v3</link><author>Yecheng Zhang, Rong Zhao, Zimu Huang, Xinyu Wang, Yue Ma, Ying Long</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 AI4US 框架，用于系统评估生成式人工智能模型在城市科学数据生成中的真实性。通过对符号和感知两类数据进行测试，发现模型能再现核心理论模式，但在多样性、参数偏差等方面存在局限，并通过后置校准提升数据质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成式人工智能模型在科学数据生成中应用日益广泛，但其与城市科学经验知识的一致性尚不清楚。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建 AI4US 框架，系统评估领先生成式人工智能模型在生成符号和感知城市数据时的真实性，并寻找提升方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在符号域将生成数据与城市尺度、空间和形态等基础理论进行对比；在感知域将模型的视觉判断与人类基准对照，并利用生成控制进行城市感知的因果实验；随后提出基于最优传输的后置校准程序以提高合成数据的真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 生成式人工智能模型能够再现核心理论模式，但生成数据存在多样性不足、系统参数偏差等问题；通过提示工程可部分改善；后置校准能显著提升合成符号数据的真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 虽然生成式人工智能在城市科学数据生成方面表现出一定的理论一致性，但仍需改进多样性和参数准确性；后置校准方法为提升数据质量提供了有效手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Generative artificial intelligence (GenAI) models are increasingly used for scientific data generation, yet their alignment with empirical knowledge in urban science remains unclear. Here, we introduce AI4US (Artificial Intelligence for Urban Science), a framework that systematically evaluates leading GenAI models by testing their fidelity in generating both symbolic and perceptual urban data. For the symbolic domain, we benchmark generated data against foundational urban theories concerning scale, space, and morphology. For the perceptual domain, we validate the models&amp;#x27; visual judgments against human benchmarks and, critically, leverage their generative control to conduct in causal experiments on urban perception. Our findings show that while GenAI models reproduce core theoretical patterns, the generated data exhibit crucial limitations: poor diversity, systematic parametric deviations, and improvement from prompt engineering. To address this, we introduce a post-hoc calibration procedure using optimal transport, which produces synthetic symbolic datasets with demonstrably higher fidelity.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generative artificial intelligence (GenAI) models are increasingly used for scientific data generation, yet their alignment with empirical knowledge in urban science remains unclear. Here, we introduce AI4US (Artificial Intelligence for Urban Science), a framework that systematically evaluates leading GenAI models by testing their fidelity in generating both symbolic and perceptual urban data. For the symbolic domain, we benchmark generated data against foundational urban theories concerning scale, space, and morphology. For the perceptual domain, we validate the models&amp;#x27; visual judgments against human benchmarks and, critically, leverage their generative control to conduct in causal experiments on urban perception. Our findings show that while GenAI models reproduce core theoretical patterns, the generated data exhibit crucial limitations: poor diversity, systematic parametric deviations, and improvement from prompt engineering. To address this, we introduce a post-hoc calibration procedure using optimal transport, which produces synthetic symbolic datasets with demonstrably higher fidelity.&lt;/p&gt;</description></item><item><guid>2505.14218v1</guid><title>Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion</title><link>http://arxiv.org/abs/2505.14218v1</link><author>Jie Li, Shengwei Tian, Long Yu, Xin Ning</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种灵活加权的Chamfer距离（FCD），用于点云生成任务，旨在解决传统Chamfer距离在平衡全局分布与局部性能时的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Chamfer距离由两个组成部分构成，可评估生成点云的全局分布和局部表现，因其计算效率高而被广泛用作点云完成任务中的相似度度量和目标函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决使用固定等权重的Chamfer距离作为目标函数时，虽然整体指标低，但全局分布差、EMD和DCD高、人工评估差的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出FCD，对Chamfer距离的全局分布部分赋予更高权重，并采用灵活加权策略动态调整两部分的平衡，以提升全局分布质量同时保持整体性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在两个先进网络上实验表明，FCD在Chamfer距离、EMD、DCD、F-Score以及人工评估等多项指标上均优于传统方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FCD能够在保持整体性能的同时显著改善点云生成的全局分布，证明了灵活加权策略的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Chamfer距离（CD）由两个组成部分构成，能够评估生成点云的全局分布和局部性能，使其在点云完成任务中广泛用作生成点云与目标点云之间的相似度度量。此外，CD的计算效率高，使其经常被用作指导点云生成的目标函数。然而，直接使用CD作为目标函数，并为其两个组成部分设定固定等权重，往往会导致整体性能看似很高（即CD分数低），但全局分布却不佳。这通常体现在Earth Mover&amp;#x27;s Distance（EMD）和Decomposed Chamfer Distance（DCD）分数较高，以及人工评估不佳。为了解决这一问题，我们提出了灵活加权Chamfer距离（FCD）来指导点云生成。FCD为CD的全局分布部分赋予更高权重，并采用灵活加权策略来调整两部分之间的平衡，旨在在保持整体性能稳健的同时改善全局分布。对两个最先进网络的实验结果表明，我们的方法在多个评估指标上表现更优，包括CD、EMD、DCD和F-Score，以及人工评估。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Chamfer Distance (CD) comprises two components that can evaluate the global distribution and local performance of generated point clouds, making it widely utilized as a similarity measure between generated and target point clouds in point cloud completion tasks. Additionally, CD&amp;#x27;s computational efficiency has led to its frequent application as an objective function for guiding point cloud generation. However, using CD directly as an objective function with fixed equal weights for its two components can often result in seemingly high overall performance (i.e., low CD score), while failing to achieve a good global distribution. This is typically reflected in high Earth Mover&amp;#x27;s Distance (EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human assessments. To address this issue, we propose a Flexible-Weighted Chamfer Distance (FCD) to guide point cloud generation. FCD assigns a higher weight to the global distribution component of CD and incorporates a flexible weighting strategy to adjust the balance between the two components, aiming to improve global distribution while maintaining robust overall performance. Experimental results on two state-of-the-art networks demonstrate that our method achieves superior results across multiple evaluation metrics, including CD, EMD, DCD, and F-Score, as well as in human evaluations.&lt;/p&gt;</description></item><item><guid>2505.14703v1</guid><title>A fast and automated approach for urban CFD simulations: integration with meteorological predictions and its application to drone flights</title><link>http://arxiv.org/abs/2505.14703v1</link><author>Marcos Suárez-Vázquez, Sylvana Varela Ballesta, Alberto Otero-Cacho, Alberto P. Muñuzuri, Jorge Mira</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种快速自动化方法，利用激光雷达、地籍数据与CFD模拟重建城市空气流动，并通过气象预测实现精确边界条件，验证结果与实测高度一致，进一步用于无人机与风场交互的风洞验证，显著提升计算效率，对城市CFD建模和城市发展具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 过去几年，研究者提出了多种城市风模拟的新方法和应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种快速、自动化的城市空气流动重建方法，并验证其准确性与计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 结合激光雷达与地籍数据生成地形，使用气象预测提供边界条件，直接在CFD环境中创建地形，进行流场模拟，并与实测数据对比验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模拟结果与实测数据高度一致，风向相关性极高，风速相关性也很高；使用风洞验证方法显著降低计算时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法有效提升城市CFD建模效率与准确性，对无人机与城市风场交互及城市规划具有重要价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 过去几年，几项研究提出了新的方法和应用用于城市风模拟。本文提出一种快速自动化的方法，利用激光雷达和地籍数据结合计算流体动力学模拟重建城市环境中的气流。我们的方法将气象预测与计算技术相结合，模拟风流、建筑、植被、水域和地形在城市环境中的复杂相互作用。基于气象预测的准确边界条件被引入到耦合方法中，直接在模拟环境中创建地形形状，简化了几何创建过程，这是CFD城市模拟中最普遍的问题之一。模拟结果与来自气象站的实测真实数据进行对比，显示与所提出的CFD模型生成的结果高度一致，风向的相关性几乎为1，风速的相关性也很高。随后将这些模拟结果用于验证一种风洞方法，该方法模拟移动无人机与提取的风流之间的相互作用，与将无人机嵌入完整城市景观的最直接方法相比，计算时间显著降低。该研究有助于推进城市CFD建模，并对多种应用具有重要意义，为城市发展提供有价值的见解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In past years, several studies have proposed new methods and applications for urban wind simulations. In this article, we present a fast and automatic methodology for reconstructing airflows within urban environments using LiDAR and cadastral data coupled with Computational Fluid Dynamics (CFD) simulations. Our approach integrates meteorological predictions with computational techniques to simulate the complex interactions between wind currents, buildings, vegetation, water zones and terrain morphology within urban environments. Accurate boundary conditions based on meteorological predictions are introduced into a coupled methodology that directly creates the terrain shape inside the simulation environment, simplifying the geometry creation process, which is one of the most prevalent problems in CFD urban simulations. The simulation results are confronted against ground-truth real data obtained from a meteorological station, showing strong agreement with the outcomes generated by the proposed CFD model, with a concordance correlation coefficient up to $ρ_c = 0.985$ for the wind direction and $ρ_c = 0.853$ for the wind speed. The results from these simulations are then used for validating a wind tunnel approach that mimics the interaction between a moving drone and the extracted wind currents, demonstrating a great improvement in computation times when compared to the most straightforward approach that consists in embedding the drone within the full urban landscape. This research contributes to the advancement of urban CFD modeling, and it has significant implications for various applications, providing valuable insights for urban development.&lt;/p&gt;</description></item><item><guid>2505.15519v1</guid><title>Exploiting Age of Information in Network Digital Twins for AI-driven Real-Time Link Blockage Detection</title><link>http://arxiv.org/abs/2505.15519v1</link><author>Michele Zhu, Francesco Linsalata, Silvia Mura, Lorenzo Cazzella, Damiano Badini, Umberto Spagnolini</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于网络数字孪生和信息新鲜度指标的实时视距识别方法，利用射线追踪自动收集通道数据，并通过将信息新鲜度融入损失函数实现模型微调，显著提升高频无线链路的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高频通信链路易受遮挡影响，视距识别对保证链路可靠性至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入信息新鲜度指标，提升网络数字孪生在动态环境中的实时遮挡检测能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 使用射线追踪技术自动收集并标注大规模通道数据；2) 将信息新鲜度融入损失函数，优先使用最新信息微调深度学习模型；3) 在城市仿真中评估分辨率、计算成本与模型性能的权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 分辨率降低4x8可使计算速度提升32倍；仅使用1%数据样本即可完成微调，成功缓解模型漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提方法在保持高性能的同时，显著降低计算成本并实现快速自动化的模型漂移补偿。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视距（LoS）识别对于确保高频通信链路的可靠性至关重要，尤其是易受遮挡影响的链路。网络数字孪生和人工智能是实现高频无线系统（如6 GHz以上）遮挡检测（LoS识别）的关键技术。在本研究中，我们通过引入信息新鲜度（AoI）指标来增强网络数字孪生，从而实现动态无线环境中可靠的实时遮挡检测。通过集成射线追踪技术，我们自动化地收集和标注大规模通道数据，专门针对环境的演变条件。所引入的AoI与损失函数结合，优先使用更近期的信息来微调深度学习模型，以应对性能退化（模型漂移）。在真实城市仿真中验证了所提方案的有效性，突出了输入分辨率、计算成本与模型性能之间的权衡。将原始通道样本尺寸（32, 1024）沿角度和子载波维度的分辨率降低4x8，可实现32倍的计算加速。所提微调方法成功缓解了性能退化，仅需1%的可用数据样本，即可实现模型漂移的自动化和快速补偿。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The Line-of-Sight (LoS) identification is crucial to ensure reliable high-frequency communication links, especially those vulnerable to blockages. Network Digital Twins and Artificial Intelligence are key technologies enabling blockage detection (LoS identification) for high-frequency wireless systems, e.g., 6&amp;gt;GHz. In this work, we enhance Network Digital Twins by incorporating Age of Information (AoI) metrics, a quantification of status update freshness, enabling reliable real-time blockage detection (LoS identification) in dynamic wireless environments. By integrating raytracing techniques, we automate large-scale collection and labeling of channel data, specifically tailored to the evolving conditions of the environment. The introduced AoI is integrated with the loss function to prioritize more recent information to fine-tune deep learning models in case of performance degradation (model drift). The effectiveness of the proposed solution is demonstrated in realistic urban simulations, highlighting the trade-off between input resolution, computational cost, and model performance. A resolution reduction of 4x8 from an original channel sample size of (32, 1024) along the angle and subcarrier dimension results in a computational speedup of 32 times. The proposed fine-tuning successfully mitigates performance degradation while requiring only 1% of the available data samples, enabling automated and fast mitigation of model drifts.&lt;/p&gt;</description></item><item><guid>2505.19026v3</guid><title>Staircase Recognition and Location Based on Polarization Vision</title><link>http://arxiv.org/abs/2505.19026v3</link><author>Weifeng Kong, Zhiying Tan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对楼梯场景感知技术的挑战，提出了融合偏振与光强的对比增强算法，并结合YOLOv11实现点云分割；通过融合偏振双目与TOF深度信息实现楼梯的三维重建；同时提出基于ICP配准和改进灰狼优化的单目相机与TOF相机联合标定方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 楼梯是人工场景中最常见的结构，机器人及下肢残疾或视力障碍者在无传感器与智能算法辅助下难以跨越；现有双目与TOF重建受光照与材质影响，偏振重建受纹理限制，且识别准确率低、噪声大、输出不稳定、计算量大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 实现楼梯检测与高质量三维重建，提升感知精度与鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 对比增强算法融合偏振与光强信息；2. 基于YOLOv11的点云分割；3. 融合偏振双目与TOF深度实现3D重建；4. 采用ICP配准结合改进灰狼优化算法进行单目相机与TOF相机联合标定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 楼梯是人工场景中最常见的结构。然而，缺乏传感器和智能算法的情况下，人形机器人以及下肢残疾或视力障碍者很难跨越楼梯。楼梯场景感知技术是识别与定位的前提，对机器人模式切换和脚印位置计算以适应不连续地形具有重要意义。然而，该技术仍存在许多限制，如识别准确率低、传感器初始噪声高、输出信号不稳定以及计算需求高。在场景重建方面，双目和飞行时间（TOF）重建容易受到环境光和目标表面材质的影响。相比之下，偏振器的特殊结构可以选择性地传输特定方向的偏振光，且该重建方法依赖于目标表面偏振信息，因而具有不易受环境光影响且不依赖纹理信息的优势。本文为实现楼梯检测，提出了一种融合偏振与光强信息的对比增强算法，并结合YOLOv11进行点云分割。为实现高质量重建，提出了融合偏振双目与TOF深度信息的三维重建方法。同时，还提出了基于ICP配准和改进灰狼优化算法的单目相机与TOF相机联合标定算法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Staircase is one of the most common structures in artificial scenes. However, it is difficult for humanoid robots and people with lower limb disabilities or visual impairment to cross the scene without the help of sensors and intelligent algorithms. Staircase scene perception technology is a prerequisite for recognition and localization. This technology is of great significance for the mode switching of the robot and the calculation of the footprint position to adapt to the discontinuous terrain. However, there are still many problems that constrain the application of this technology, such as low recognition accuracy, high initial noise from sensors, unstable output signals and high computational requirements. In terms of scene reconstruction, the binocular and time of flight (TOF) reconstruction of the scene can be easily affected by environmental light and the surface material of the target object. In contrast, due to the special structure of the polarizer, the polarization can selectively transmit polarized light in a specific direction and this reconstruction method relies on the polarization information of the object surface. So the advantages of polarization reconstruction are reflected, which are less affected by environmental light and not dependent on the texture information of the object surface. In this paper, in order to achieve the detection of staircase, this paper proposes a contrast enhancement algorithm that integrates polarization and light intensity information, and integrates point cloud segmentation based on YOLOv11. To realize the high-quality reconstruction, we proposed a method of fusing polarized binocular and TOF depth information to realize the three-dimensional (3D) reconstruction of the staircase. Besides, it also proposes a joint calibration algorithm of monocular camera and TOF camera based on ICP registration and improved gray wolf optimization algorithm.&lt;/p&gt;</description></item><item><guid>2505.19518v2</guid><title>Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions</title><link>http://arxiv.org/abs/2505.19518v2</link><author>Nakul Poudel, Zixin Yang, Kelly Merrell, Richard Simon, Cristian A. Linte</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出基于VN-OccNet的患者特异性点云补全方法，以解决术中点云部分可见导致的配准困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 术中图像引导手术中捕获的数据缺乏亚表面信息，关键区域如血管和肿瘤难以观测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过点云补全提升图像与物理配准的初始刚性配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用VN-OccNet在患者特异性训练下，从部分术中点云生成完整肝表面；随后将补全表面与Go-ICP配准算法结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; VN-OccNet的旋转等变特性有效恢复完整表面，补全后配准结果显著改善。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 患者特异性点云补全能缓解术中可见性不足问题，为鲁棒配准框架提供有力支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在图像引导手术中捕获的术中数据缺乏亚表面信息，关键感兴趣区域如血管和肿瘤位于表面之下。图像到物理的配准可以将术前信息与术中数据（通常表示为点云）融合。然而，由于术中点云的部分可见性，这一配准过程面临困难。在本研究中，我们提出了一种患者特异性的点云补全方法来辅助配准过程。具体而言，我们利用VN-OccNet从部分术中点云生成完整的肝表面。该网络以患者特异性方式训练，使用术前模型的模拟变形来训练模型。首先，我们深入分析了VN-OccNet的旋转等变属性及其在从部分术中表面恢复完整表面方面的有效性。接下来，我们将补全后的术中表面集成到Go-ICP配准算法中，以展示其在改善初始刚性配准结果方面的实用性。我们的结果突显了患者特异性补全方法在缓解部分术中可见性挑战方面的前景。VN-OccNet的旋转等变和表面生成能力为开发针对术中点云变化的鲁棒配准框架提供了强有力的前景。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在图像引导肝脏手术中，术中点云因视角受限而缺乏完整表面信息，导致图像与物理空间配准困难。完整的配准对于准确定位肿瘤和血管等亚表面目标至关重要，直接影响手术安全与效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了已有的点云补全网络和配准技术，提出使用向量神经元占据网络（VN‑OccNet）进行患者特定的点云补全，并通过模拟变形训练模型。该思路在前人工作（如Jia等和Foti等）的基础上，克服了需要手动对应或刚性初始化的缺点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用VN‑OccNet的旋转等变特性，从部分术中点云预测占据概率，构建完整表面网格，然后将网格顶点作为完整点云与预手术点云进行Go‑ICP刚性配准。实现流程包括：①将部分点云输入VN‑OccNet编码器得到潜在特征；②用解码器查询占据概率；③通过多分辨率等值面提取和Marching Cubes生成网格；④提取顶点得到完整点云；⑤使用Go‑ICP完成配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①患者特定的训练策略，利用预手术模型的模拟变形生成训练数据；②采用旋转等变的VN‑OccNet，能在任意姿态下完成补全；③生成完整网格实现均匀采样，提升配准质量；④直接与Go‑ICP配准，无需手动对应或刚性预估。与以往工作相比，它不需要手动对应或刚性初始化，且在旋转变化下表现更稳健。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种患者特定、旋转等变的点云补全管线，能够从部分术中肝脏表面重建完整网格，并显著提升图像与物理空间的配准精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Intra-operative data captured during image-guided surgery lacks sub-surface information, where key regions of interest, such as vessels and tumors, reside. Image-to-physical registration enables the fusion of pre-operative information and intra-operative data, typically represented as a point cloud. However, this registration process struggles due to partial visibility of the intra-operative point cloud. In this research, we propose a patient-specific point cloud completion approach to assist with the registration process. Specifically, we leverage VN-OccNet to generate a complete liver surface from a partial intra-operative point cloud. The network is trained in a patient-specific manner, where simulated deformations from the pre-operative model are used to train the model. First, we conduct an in-depth analysis of VN-OccNet&amp;#x27;s rotation-equivariant property and its effectiveness in recovering complete surfaces from partial intra-operative surfaces. Next, we integrate the completed intra-operative surface into the Go-ICP registration algorithm to demonstrate its utility in improving initial rigid registration outcomes. Our results highlight the promise of this patient-specific completion approach in mitigating the challenges posed by partial intra-operative visibility. The rotation equivariant and surface generation capabilities of VN-OccNet hold strong promise for developing robust registration frameworks for variations of the intra-operative point cloud.&lt;/p&gt;</description></item><item><guid>2505.19919v1</guid><title>Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time</title><link>http://arxiv.org/abs/2505.19919v1</link><author>Chen Sang, Yeqiang Qian, Jiale Zhang, Chunxiang Wang, Ming Yang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于高斯喷射的框架，用于快速重建真实场景并在合成的四维天气效果下进行渲染，解决了传统手工建模和渲染在复杂场景和天气效果上的高成本和低质量问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在城市数字孪生、VR/AR/游戏场景设计以及合成电影等任务中，传统工业方法依赖手工建模和多种渲染引擎，导致高劳动成本、硬件需求大且在复制复杂真实场景时质量不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有重建与渲染算法无法有效重建和渲染真实世界天气效果的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建基于高斯喷射的框架，利用捕获的真实场景数据进行重建，并通过高斯建模与渲染技术合成四维天气效果，实现连续动态天气变化并可细粒度控制效果细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架能够模拟多种常见天气效果，支持连续动态天气变化，细节可控，且硬件要求低，能够实现实时渲染。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于高斯喷射的框架在重建真实场景并渲染合成天气方面表现出色，具备低硬件需求和实时性能，适用于多种应用场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要：对于城市数字孪生、VR/AR/游戏场景设计或合成电影等任务，传统工业方法通常涉及手工建模场景并使用各种渲染引擎完成渲染过程。这种方法通常需要高劳动成本和硬件需求，并且在复制复杂真实场景时可能导致质量不佳。更高效的方法是使用捕获的真实世界场景数据，然后应用重建和渲染算法快速重现真实场景。然而，当前算法无法有效重建和渲染真实世界的天气效果。为了解决这个问题，我们提出了一个基于高斯喷射的框架，能够在合成的四维天气效果下重建真实场景并进行渲染。我们的工作可以通过应用高斯建模和渲染技术模拟各种常见天气效果。它支持连续动态天气变化，并且可以轻松控制效果的细节。此外，我们的工作具有低硬件要求并实现实时渲染性能。结果演示可在我们的项目主页访问：weathermagician.github.io&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现对真实场景的实时、高清重建与渲染，并在此基础上合成多种动态天气效果。此问题在数字孪生、VR/AR、游戏和影视制作等领域尤为重要，因为传统手工建模成本高、硬件需求大且难以真实再现复杂天气。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先构建无天气的清晰场景，再通过在3D高斯散射（3DGS）框架中添加天气高斯来实现天气合成。方法借鉴了3DGS、Rainy‑GS、ClimateNeRF等现有工作，但在此基础上引入深度与法线监督、显式天气高斯建模，并实现了实时渲染。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将天气元素视为可编辑的高斯分布，直接叠加到已重建的3DGS场景中。实现流程包括：①使用SfM生成点云；②在3DGS中训练并通过深度/法线监督优化几何；③根据天气类型生成相应的高斯（雨滴、雪花、雾等）并编辑其属性；④将天气高斯与原场景高斯一起渲染，得到实时天气合成图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①实时、低硬件需求的4D天气合成；②对动态天气（雨、雪）和静态天气（雾、雾霾）的可控显式建模；③利用深度与法线监督提升几何质量；④在3DGS框架中实现完整的天气编辑与渲染。与之前工作相比，它突破了仅能生成静态或非实时天气的局限，提供了更真实、更可控的动态天气效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Weather‑Magician 提供了一种基于 3D 高斯散射的实时 4D 天气合成框架，能够在消费级 GPU 上以高保真度渲染可控的动态与静态天气效果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;For tasks such as urban digital twins, VR/AR/game scene design, or creating synthetic films, the traditional industrial approach often involves manually modeling scenes and using various rendering engines to complete the rendering process. This approach typically requires high labor costs and hardware demands, and can result in poor quality when replicating complex real-world scenes. A more efficient approach is to use data from captured real-world scenes, then apply reconstruction and rendering algorithms to quickly recreate the authentic scene. However, current algorithms are unable to effectively reconstruct and render real-world weather effects. To address this, we propose a framework based on gaussian splatting, that can reconstruct real scenes and render them under synthesized 4D weather effects. Our work can simulate various common weather effects by applying Gaussians modeling and rendering techniques. It supports continuous dynamic weather changes and can easily control the details of the effects. Additionally, our work has low hardware requirements and achieves real-time rendering performance. The result demos can be accessed on our project homepage: weathermagician.github.io&lt;/p&gt;</description></item><item><guid>2505.24348v1</guid><title>A 3D Mobile Crowdsensing Framework for Sustainable Urban Digital Twins</title><link>http://arxiv.org/abs/2505.24348v1</link><author>Taku Yamazaki, Kaito Watanabe, Tatsuya Kase, Kenta Hasegawa, Koki Saida, Takumi Miyoshi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种面向可持续城市数字孪生的三维移动众包感知框架，包含四个关键机制：①三维移动众包机制（主动与被动模型）；②基于Geohash的空间信息管理机制；③动态点云集成机制；④基于网页的实时可视化工具。主动模型通过增强现实领土上色游戏收集点云数据；被动模型让参与者佩戴手机进行不干扰日常的感知。空间管理通过Geohash划分区域，点云集成通过全局与局部配准实现。通过真实世界实验验证了模型的有效性，并分析了点云集成性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市数字孪生需要大量高质量三维数据，传统采集方式成本高、效率低，移动众包提供了低成本、可扩展的数据获取途径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种可持续、低成本的三维移动众包框架，以支持城市数字孪生的持续更新与可视化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计主动与被动两种感知模型，使用Geohash进行空间划分，开发动态点云集成与网页可视化工具，并在真实环境中进行实验评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 主动游戏化模型和被动佩戴模型均能有效收集点云数据；主观评价与数据分析表明两模型均具备可行性；动态点云集成在实验数据集上表现良好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的三维移动众包框架能够有效支持可持续城市数字孪生的构建与更新，主动与被动模型相结合可满足不同场景需求，集成与可视化机制实现了数据的高效利用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed at sustainable urban digital twins (UDTs). The framework comprises four key mechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models; (2) the Geohash-based spatial information management mechanism; (3) the dynamic point cloud integration mechanism for UDTs; and (4) the web-based real-time visualizer for 3D-MCS and UDTs. The active sensing model features a gamified 3D-MCS approach, where participants collect point cloud data through an augmented reality territory coloring game. In contrast, the passive sensing model employs a wearable 3D-MCS approach, where participants wear smartphones around their necks without disrupting daily activities. The spatial information management mechanism efficiently partitions the space into regions using Geohash. The dynamic point cloud integration mechanism incorporates point clouds collected by 3D-MCS into UDTs through global and local point cloud registration. Finally, we evaluated the proposed framework through real-world experiments. We verified the effectiveness of the proposed 3D-MCS models from the perspectives of subjective evaluation and data collection and analysis. Furthermore, we analyzed the performance of the dynamic point cloud integration using a dataset.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决如何持续、准确地收集大规模三维空间信息，以支持可持续的城市数字孪生（UDT）的构建与维护。该问题重要，因为城市数字孪生需要实时、精确的三维数据来实现智能城市服务，而传统的测绘方法成本高、更新慢。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于移动众测（MCS）与LiDAR技术的进步，提出了主动（AR游戏）和被动（可穿戴）两种感知模型，并结合Geohash空间管理和动态点云注册。设计借鉴了先前的AR游戏、MCS、Geohash编码以及点云配准等工作（如文献[7]–[10]）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过主动游戏和被动可穿戴两种方式收集LiDAR点云，使用Geohash对数据进行分区管理，并在服务器端通过全局+局部配准将新点云动态合并到城市级点云（UDT）中，最后通过Web可视化展示。实现流程包括：用户加入游戏/佩戴设备 → 采集点云并上传 → 服务器预处理、特征匹配、RANSAC+ICP配准 → 合并到UDT → 实时可视化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 结合主动AR游戏与被动可穿戴的双模式MCS框架；② 基于Geohash的可扩展空间信息管理；③ 采用全局+局部配准实现动态点云集成；④ Web实时可视化与机器学习辅助信息。与以往工作相比，该框架将感知、管理、集成和可视化四个环节统一在一个可扩展系统中，并通过游戏化激励提升参与度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 提出了一套完整的3D移动众测框架，融合AR游戏与可穿戴LiDAR采集、Geohash空间管理和动态点云集成，能够实时构建和维护可持续的城市数字孪生。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed at sustainable urban digital twins (UDTs). The framework comprises four key mechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models; (2) the Geohash-based spatial information management mechanism; (3) the dynamic point cloud integration mechanism for UDTs; and (4) the web-based real-time visualizer for 3D-MCS and UDTs. The active sensing model features a gamified 3D-MCS approach, where participants collect point cloud data through an augmented reality territory coloring game. In contrast, the passive sensing model employs a wearable 3D-MCS approach, where participants wear smartphones around their necks without disrupting daily activities. The spatial information management mechanism efficiently partitions the space into regions using Geohash. The dynamic point cloud integration mechanism incorporates point clouds collected by 3D-MCS into UDTs through global and local point cloud registration. Finally, we evaluated the proposed framework through real-world experiments. We verified the effectiveness of the proposed 3D-MCS models from the perspectives of subjective evaluation and data collection and analysis. Furthermore, we analyzed the performance of the dynamic point cloud integration using a dataset.&lt;/p&gt;</description></item><item><guid>2506.00600v2</guid><title>SatDreamer360: Multiview-Consistent Generation of Ground-Level Scenes from Satellite Imagery</title><link>http://arxiv.org/abs/2506.00600v2</link><author>Xianghui Ze, Beiyi Zhu, Zhenbo Song, Jianfeng Lu, Yujiao Shi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了 SatDreamer360 框架，能够从单张卫星图像和预定义的姿态轨迹生成几何一致的多视角 360 度地面全景。通过三平面表示、基于射线的像素注意机制以及全景极线约束注意模块，解决了视角差异和多帧一致性问题，并在新构建的 VIGOR++ 数据集上验证了其优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成多视角一致的 360 度地面场景是模拟、自动驾驶和数字孪生城市等领域的重要任务，但现有方法多聚焦单一全景，依赖高度图或手工投影，难以实现多视角一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够从单张卫星图像生成几何一致、多视角地面全景的方法，并提供相应的数据集用于评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用三平面表示编码场景特征，使用射线基像素注意机制提取视角特定特征；引入全景极线约束注意模块根据已知相对姿态对齐多帧特征；构建 VIGOR++ 数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SatDreamer360 在卫星到地面对齐和多视角一致性方面优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架有效解决了卫星图像到多视角地面全景的生成问题，并通过新数据集验证了其性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成多视角一致的 360 度地面场景从卫星图像是一个具有广泛应用的挑战性任务，涉及模拟、自动驾驶和数字孪生城市等领域。现有方法主要关注合成单个地面视图全景，往往依赖高度图或手工投影，并难以生成多视角一致的序列。本文提出 SatDreamer360 框架，能够从单张卫星图像和预定义的姿态轨迹生成几何一致的多视角地面全景。为解决地面与卫星图像之间的巨大视角差异，我们采用三平面表示编码场景特征，并设计基于射线的像素注意机制，从三平面中检索视角特定特征。为保持多帧一致性，我们引入基于全景极线约束的注意模块，根据已知相对姿态对齐不同帧的特征。为支持评估，我们构建了 VIGOR++ 数据集，在原始 VIGOR 数据集的基础上增加了更多地面视图图像及其姿态标注。实验表明，SatDreamer360 在卫星到地面对齐和多视角一致性方面优于现有方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在从单张卫星图像和预定义轨迹生成多视角一致的 360° 地面全景图。此问题重要，因为卫星图像获取成本低、覆盖广泛，能够为仿真、自动驾驶和数字孪生城市等应用提供可视化支持，但现有方法多聚焦单帧生成，缺乏连续性和几何一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出单视角生成方法在多帧一致性和几何对齐上的不足，随后借鉴了潜在扩散模型、三平面（triplane）表示、光线引导注意力以及视差约束等技术。通过将这些技术整合到一个统一框架中，作者设计了 SatDreamer360。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 triplane 表示捕捉场景几何，并通过光线引导像素注意力提取视角特定特征，再用潜在扩散模型生成地面全景。实现流程为：① 用卫星图像构建 triplane；② 对每个轨迹姿态，计算光线并在 triplane 上采样特征；③ 将聚合特征作为条件输入到扩散模型；④ 通过视差约束注意力在连续帧间对齐特征，保证多视角一致性；⑤ 输出连续的 360° 全景序列。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 统一框架实现从单张卫星图像到连续地面全景的生成；② 光线引导的跨视角特征条件化，利用 triplane 进行几何感知；③ 适用于全景图的视差约束注意力模块，提升多帧一致性；④ 新的 VIGOR++ 数据集提供大规模评估。与以往单帧或依赖高度图、手工投影的方法不同，SatDreamer360 在保持几何一致性的同时实现了多视角连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SatDreamer360 通过 triplane 几何表示、光线引导注意力和视差约束扩散模型，实现了从单张卫星图像生成连续、几何一致的 360° 地面全景。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generating multiview-consistent $360^\circ$ ground-level scenes from satellite imagery is a challenging task with broad applications in simulation, autonomous navigation, and digital twin cities. Existing approaches primarily focus on synthesizing individual ground-view panoramas, often relying on auxiliary inputs like height maps or handcrafted projections, and struggle to produce multiview consistent sequences. In this paper, we propose SatDreamer360, a framework that generates geometrically consistent multi-view ground-level panoramas from a single satellite image, given a predefined pose trajectory. To address the large viewpoint discrepancy between ground and satellite images, we adopt a triplane representation to encode scene features and design a ray-based pixel attention mechanism that retrieves view-specific features from the triplane. To maintain multi-frame consistency, we introduce a panoramic epipolar-constrained attention module that aligns features across frames based on known relative poses. To support the evaluation, we introduce {VIGOR++}, a large-scale dataset for generating multi-view ground panoramas from a satellite image, by augmenting the original VIGOR dataset with more ground-view images and their pose annotations. Experiments show that SatDreamer360 outperforms existing methods in both satellite-to-ground alignment and multiview consistency.&lt;/p&gt;</description></item><item><guid>2506.02692v1</guid><title>Large-scale Self-supervised Video Foundation Model for Intelligent Surgery</title><link>http://arxiv.org/abs/2506.02692v1</link><author>Shu Yang, Fengtao Zhou, Leon Mayer, Fuxiang Huang, Yiliang Chen, Yihui Wang, Sunan He, Yuxiang Nie, Xi Wang, Ömer Sümer, Yueming Jin, Huihui Sun, Shuchang Xu, Alex Qinyang Liu, Zheng Li, Jing Qin, Jeremy YuenChun Teoh, Lena Maier-Hein, Hao Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了首个基于视频级别的外科预训练框架SurgVISTA，利用大规模外科视频数据实现空间与时间特征的联合学习，并通过专家引导的知识蒸馏提升细粒度解剖与语义特征的学习效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 计算机辅助干预（CAI）有望彻底改变现代手术，而外科场景理解是支持决策、提升手术效率和保证术中安全的关键组成部分。现有的AI驱动方法通过自监督空间表示学习减轻标注负担，但在预训练阶段缺乏显式的时间建模，导致无法完整捕捉动态手术上下文，进而限制了时空理解的完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个视频级别的外科预训练框架，实现对大规模外科视频数据的联合时空表示学习，以提升外科场景理解的完整性和实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①构建包含3650段视频、约355万帧、20种手术流程和10种解剖结构的大规模外科视频数据集；②提出SurgVISTA预训练方法，采用重建式学习实现空间结构与时间动态的联合建模；③加入基于外科专家指导的图像级知识蒸馏，强化细粒度解剖与语义特征的学习；④在13个视频级别数据集（涵盖6种手术流程、4项任务）上建立基准进行验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SurgVISTA在所有基准任务上均显著优于自然领域和外科领域预训练模型，证明其在时空特征学习上的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SurgVISTA展示了在临床意义场景中推动智能外科系统发展的强大潜力，为实现更安全、更高效的手术提供了重要技术支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 计算机辅助干预（CAI）有潜力彻底改变现代手术，外科场景理解是支持决策、提升程序效率和确保术中安全的关键组成部分。现有的AI驱动方法通过自监督空间表示学习减轻标注负担，但在预训练阶段缺乏显式的时间建模，导致无法完整捕捉动态手术上下文，进而限制了时空理解的完整性。在本文中，我们提出了首个视频级别的外科预训练框架，能够从大规模外科视频数据中实现空间与时间特征的联合学习。为此，我们构建了一个包含3650段视频、约355万帧、涵盖20种手术流程和10种解剖结构的大规模外科视频数据集。在此基础上，我们提出了SurgVISTA（外科视频级空间-时间架构），一种基于重建的预训练方法，通过联合时空建模捕捉复杂的空间结构和时间动态。此外，SurgVISTA结合了由外科专家指导的图像级知识蒸馏，以增强对细粒度解剖和语义特征的学习。为验证其有效性，我们建立了一个涵盖六种手术流程、四项任务的13个视频级别数据集的综合基准。大量实验表明，SurgVISTA在所有基准任务上均优于自然领域和外科领域预训练模型，显示出在临床意义场景中推动智能外科系统发展的强大潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Computer-Assisted Intervention (CAI) has the potential to revolutionize modern surgery, with surgical scene understanding serving as a critical component in supporting decision-making, improving procedural efficacy, and ensuring intraoperative safety. While existing AI-driven approaches alleviate annotation burdens via self-supervised spatial representation learning, their lack of explicit temporal modeling during pre-training fundamentally restricts the capture of dynamic surgical contexts, resulting in incomplete spatiotemporal understanding. In this work, we introduce the first video-level surgical pre-training framework that enables joint spatiotemporal representation learning from large-scale surgical video data. To achieve this, we constructed a large-scale surgical video dataset comprising 3,650 videos and approximately 3.55 million frames, spanning more than 20 surgical procedures and over 10 anatomical structures. Building upon this dataset, we propose SurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a reconstruction-based pre-training method that captures intricate spatial structures and temporal dynamics through joint spatiotemporal modeling. Additionally, SurgVISTA incorporates image-level knowledge distillation guided by a surgery-specific expert to enhance the learning of fine-grained anatomical and semantic features. To validate its effectiveness, we established a comprehensive benchmark comprising 13 video-level datasets spanning six surgical procedures across four tasks. Extensive experiments demonstrate that SurgVISTA consistently outperforms both natural- and surgical-domain pre-trained models, demonstrating strong potential to advance intelligent surgical systems in clinically meaningful scenarios.&lt;/p&gt;</description></item><item><guid>2506.03388v1</guid><title>Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery</title><link>http://arxiv.org/abs/2506.03388v1</link><author>Pengyu Chen, Xiao Huang, Teng Fei, Sicheng Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究探讨城市环境声音与视觉场景的对应关系，比较不同视觉表示方法在捕捉声学语义方面的效果。通过将地理参考的声音记录与街景图像和遥感图像结合，利用多模态模型提取嵌入和类别特征，评估跨模态相似度。结果显示，街景嵌入与环境声音的匹配度高于分割输出；遥感分割在生物声、地声与人声（BGA）框架下更能解释生态类别。嵌入模型在语义对齐上表现更佳，而分割方法则提供了视觉结构与声学生态之间可解释的关联。该研究为多模态城市感知提供了新的视角，推动声音在地理空间分析中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 环境声音景观能够传递丰富的生态与社会信息，但在大规模地理分析中的潜力尚未得到充分挖掘。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究城市声音与视觉场景的对应程度，并比较不同视觉表示策略在捕捉声学语义方面的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用多模态方法，将地理参考的声音记录与街景图像和遥感图像结合，覆盖伦敦、纽约和东京三大城市。使用AST模型提取音频嵌入，CLIP和RemoteCLIP提取图像嵌入，CLIPSeg和Seg-Earth OV进行语义分割，随后提取嵌入和类别特征，评估跨模态相似度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 街景嵌入与环境声音的匹配度高于分割输出；遥感分割在BGA框架下更能解释生态类别；嵌入模型在语义对齐上更优；分割方法提供了视觉结构与声学生态之间可解释的关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本研究推动了多模态城市感知的发展，为将声音纳入地理空间分析提供了新视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 环境声音景观传递了关于城市环境的丰富生态和社会信息；然而，它们在大规模地理分析中的潜力尚未得到充分利用。本研究通过比较各种视觉表示策略来探讨城市声音与视觉场景的对应程度。我们采用多模态方法，将地理参考的声音记录与街景图像和遥感图像结合，覆盖伦敦、纽约和东京三大城市。使用AST模型提取音频嵌入，CLIP和RemoteCLIP提取图像嵌入，CLIPSeg和Seg-Earth OV进行语义分割，随后提取嵌入和类别特征，评估跨模态相似度。结果表明，街景嵌入与环境声音的匹配度高于分割输出；遥感分割在BGA框架下更能解释生态类别。嵌入模型在语义对齐上更优，而分割方法提供了视觉结构与声学生态之间可解释的关联。本研究为多模态城市感知的发展提供了新的视角，并为将声音纳入地理空间分析提供了新思路。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Environmental soundscapes convey substantial ecological and social information regarding urban environments; however, their potential remains largely untapped in large-scale geographic analysis. In this study, we investigate the extent to which urban sounds correspond with visual scenes by comparing various visual representation strategies in capturing acoustic semantics. We employ a multimodal approach that integrates geo-referenced sound recordings with both street-level and remote sensing imagery across three major global cities: London, New York, and Tokyo. Utilizing the AST model for audio, along with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV for semantic segmentation, we extract embeddings and class-level features to evaluate cross-modal similarity. The results indicate that street view embeddings demonstrate stronger alignment with environmental sounds compared to segmentation outputs, whereas remote sensing segmentation is more effective in interpreting ecological categories through a Biophony--Geophony--Anthrophony (BGA) framework. These findings imply that embedding-based models offer superior semantic alignment, while segmentation-based methods provide interpretable links between visual structure and acoustic ecology. This work advances the burgeoning field of multimodal urban sensing by offering novel perspectives for incorporating sound into geospatial analysis.&lt;/p&gt;</description></item><item><guid>2506.04837v1</guid><title>OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model</title><link>http://arxiv.org/abs/2506.04837v1</link><author>Kunshen Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 OpenMaskDINO3D，一种基于大语言模型的 3D 理解与分割框架，能够通过点云数据和自然语言提示直接生成高精度实例分割掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然 2D 视觉分割技术已取得显著进展，但现有系统仍需明确的人类指令或预定义类别来识别目标物体，且缺乏对应的 3D 分割框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在 3D 场景中实现与 2D 系统相当的隐式意图理解与分割的完整框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用大语言模型处理点云与文本提示，加入 SEG 令牌和对象标识符，实现从自然语言指令到点云分割掩码的直接映射。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，OpenMaskDINO3D 在 ScanNet 大规模数据集上实现了高精度 3D 分割，并在多种任务中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenMaskDINO3D 成功填补了 3D 语义分割的空白，为基于语言的 3D 视觉理解提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管感知系统近年来取得了显著进展，尤其是在二维推理分割方面，但这些系统仍然依赖明确的人类指令或预定义类别来识别目标对象，然后再执行视觉识别任务。这些系统已显著成熟，能够在二维环境中推理并理解隐式用户意图，基于复杂且隐式的查询文本生成准确的分割掩码。然而，缺乏与之相当的三维推理分割框架和结构。本文提出了 OpenMaskDINO3D，一种为全面三维理解与分割而设计的大语言模型。OpenMaskDINO3D 处理点云数据和文本提示，生成实例分割掩码，在多种三维任务中表现出色。通过引入 SEG 令牌和对象标识符，我们实现了高精度的三维分割掩码生成，使模型能够直接根据自然语言指令生成准确的点云分割结果。对大规模 ScanNet 数据集的实验结果验证了我们 OpenMaskDINO3D 在各类任务中的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从3D点云和自然语言指令生成3D分割的任务，即3D推理分割。该问题重要，因为它使系统能够在没有预定义类别或人工指令的情况下理解并定位三维场景中的对象，广泛应用于机器人导航、增强现实和智能环境等领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了2D推理分割中的SEG token和位置标记技术，并结合大型语言模型（LLM）来引导分割。通过将3D、2D特征与文本融合到LLM中，利用&amp;lt;SEG&amp;gt; token生成分割查询，最终实现3D分割。该设计参考了OpenMask3D、DINO、CLIP等相关工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让LLM在融合3D、2D和文本信息后，通过&amp;lt;SEG&amp;gt; token产生的嵌入作为查询，驱动Transformer解码器生成3D分割掩码。实现流程包括：①点云通过backbone提取超体素特征；②使用Uni3D和DINOv2分别提取3D和2D特征；③将这些特征与文本编码后输入LLM；④LLM输出&amp;lt;SEG&amp;gt; token嵌入，经过MLP得到查询；⑤查询与backbone的键值对输入解码器，得到最终分割结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①引入SEG token和对象标识符以实现语言驱动的分割；②采用embedding-as-mask范式，将LLM输出直接映射为分割查询；③在3D场景中实现零样本推理并通过少量微调提升性能；④构建了包含1000+样本的3D推理分割基准。与以往工作相比，OpenMaskDINO3D首次将LLM与3D分割深度融合，支持复杂语义指令和空间关系推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenMaskDINO3D通过将大型语言模型与3D/2D特征融合，并利用专门的&amp;lt;SEG&amp;gt; token，实现了从自然语言指令直接生成高精度3D分割掩码的开源框架，并在ScanNet上展示了强大的零样本和微调性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although perception systems have made remarkable advancements in recent years, particularly in 2D reasoning segmentation, these systems still rely on explicit human instruction or pre-defined categories to identify target objects before executing visual recognition tasks. Such systems have matured significantly, demonstrating the ability to reason and comprehend implicit user intentions in two-dimensional contexts, producing accurate segmentation masks based on complex and implicit query text. However, a comparable framework and structure for 3D reasoning segmentation remain absent. This paper introduces OpenMaskDINO3D, a LLM designed for comprehensive 3D understanding and segmentation. OpenMaskDINO3D processes point cloud data and text prompts to produce instance segmentation masks, excelling in many 3D tasks. By introducing a SEG token and object identifier, we achieve high-precision 3D segmentation mask generation, enabling the model to directly produce accurate point cloud segmentation results from natural language instructions. Experimental results on large-scale ScanNet datasets validate the effectiveness of our OpenMaskDINO3D across various tasks.&lt;/p&gt;</description></item><item><guid>2506.05009v1</guid><title>Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2506.05009v1</link><author>Alfred T. Christiansen, Andreas H. Højrup, Morten K. Stephansen, Md Ibtihaj A. Sakib, Taman S. Poojary, Filip Slezak, Morten S. Laursen, Thomas B. Moeslund, Joakim B. Haurum</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用3D高斯喷射和高斯不透明度场生成多种农业车辆3D资产的合成数据管线，并通过模拟激光雷达生成点云。实验表明，仅使用合成数据训练的语义分割模型可达到高精度，并在某些情况下优于使用真实数据训练的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 训练3D点云语义分割模型需要大量标注数据，但获取和标注真实点云成本高且劳动强度大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够生成逼真合成点云数据的新管线，以降低数据获取成本并提升模型训练效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先使用3D高斯喷射和高斯不透明度场生成多种农业车辆的3D模型，然后将这些模型放置在模拟环境中，通过模拟激光雷达生成点云。该方法支持随意更改激光雷达参数，无需额外成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在仅使用合成数据训练的情况下，Point Transformer V3模型取得91.35%的mIoU；合成数据训练的模型在某些场景下表现优于使用真实数据训练的模型；模型能够跨语义类别泛化，在从未训练过的网格模型上也能准确预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 合成数据管线能够生成高质量点云，训练出的模型不仅精度高，还能在不同语义类别和未见模型上保持良好泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 训练用于3D点云语义分割等任务的神经网络需要大量数据集，但获取和标注真实世界点云成本高且劳动强度大。本研究旨在通过利用3D高斯喷射（3DGS）和高斯不透明度场（GOF）生成多种不同农业车辆的3D资产，替代使用通用模型，从而引入一种新的生成逼真合成数据的管线。这些资产被放置在模拟环境中，使用模拟激光雷达生成点云。这是一种灵活的方法，允许在不产生额外成本的情况下更改激光雷达规格。我们通过仅使用合成数据训练和验证模型，评估了合成数据对PointNet++、Point Transformer V3和OACNN等分割模型的影响。值得注意的是，PTv3模型在未使用任何真实数据训练或验证的情况下，取得了91.35%的mIoU。进一步研究甚至表明，在某些场景下，仅使用合成生成的数据训练的模型表现优于使用真实世界数据训练的模型。最后，实验表明模型能够跨语义类别泛化，在从未训练过的网格模型上实现准确预测。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决农业车辆点云语义分割训练所需的大规模标注数据缺乏问题。由于真实点云采集和标注成本高且耗时，缺少足够的数据会限制深度学习模型的性能和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了 3D Gaussian Splatting（3DGS）和 Gaussian Opacity Fields（GOF）技术，从无人机拍摄的图像中快速生成高质量车辆网格，并在 Gazebo 中模拟 Ouster LiDAR 采样。该思路借鉴了 3DGS、GOF、以及已有的点云合成与语义分割模型（如 PTv3、PointNet++）等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过 3DGS+GOF 从实景图像生成真实感车辆网格，然后在仿真环境中以真实 LiDAR 采样方式生成带标签的点云。实现流程包括：无人机拍摄 → SfM 生成稀疏点云 → GOF 提取网格 → 裁剪单个车辆 → 导入 Gazebo 并模拟 LiDAR → 生成合成点云并训练分割模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 用 3DGS+GOF 自动生成高质量车辆网格；② 在仿真中使用真实 LiDAR 采样模式生成点云；③ 证明仅用合成数据训练的模型可在真实数据上取得竞争性能，并能泛化到未见车辆。与以往使用通用模型或均匀采样的合成数据不同，该方法提供更逼真、更可调的训练资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一套完整的从无人机图像到仿真 LiDAR 点云的流水线，利用 3D Gaussian splatting 与 Gaussian opacity fields 生成逼真车辆网格，使得仅用合成数据即可训练出高性能的农业车辆点云语义分割模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Training neural networks for tasks such as 3D point cloud semantic segmentation demands extensive datasets, yet obtaining and annotating real-world point clouds is costly and labor-intensive. This work aims to introduce a novel pipeline for generating realistic synthetic data, by leveraging 3D Gaussian Splatting (3DGS) and Gaussian Opacity Fields (GOF) to generate 3D assets of multiple different agricultural vehicles instead of using generic models. These assets are placed in a simulated environment, where the point clouds are generated using a simulated LiDAR. This is a flexible approach that allows changing the LiDAR specifications without incurring additional costs. We evaluated the impact of synthetic data on segmentation models such as PointNet++, Point Transformer V3, and OACNN, by training and validating the models only on synthetic data. Remarkably, the PTv3 model had an mIoU of 91.35\%, a noteworthy result given that the model had neither been trained nor validated on any real data. Further studies even suggested that in certain scenarios the models trained only on synthetically generated data performed better than models trained on real-world data. Finally, experiments demonstrated that the models can generalize across semantic classes, enabling accurate predictions on mesh models they were never trained on.&lt;/p&gt;</description></item><item><guid>2506.05087v1</guid><title>Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics</title><link>http://arxiv.org/abs/2506.05087v1</link><author>HaoTian Lan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了一种多模态街道评估框架（MSEF），通过将视觉变换器与大型语言模型结合，实现对街景的可解释双重输出评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的基于影像或GIS的客观街道指标已成为城市分析的标准，但不足以捕捉包容性城市设计所需的主观感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够同时评估客观特征和居民主观感知的框架，以弥补现有方法的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用哈尔滨超过一万五千张标注街景图像，对框架进行LoRA和P‑Tuning v2的参数高效微调，并在不同社会经济层级中验证模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在客观特征上的准确率为0.84，与居民感知的一致率约为89%；能够捕捉情境相关的矛盾（如非正式商业提升活力但降低行人舒适度），识别非线性和语义依赖的模式，并通过自然语言解释阐明注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在城市感知建模方面提供了方法创新，并为规划系统提供了将基础设施精度与居住体验相结合的实用工具，符合可持续发展目标11。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然基于影像或GIS的客观街道指标已成为城市分析的标准，但它们仍不足以捕捉包容性城市设计所必需的主观感知。本研究提出了一种新型多模态街道评估框架（MSEF），将视觉变换器（VisualGLM‑6B）与大型语言模型（GPT‑4）融合，实现对街景的可解释双重输出评估。我们利用来自中国哈尔滨的超过15,000张标注街景图像，对框架进行LoRA和P‑Tuning v2的参数高效微调。模型在客观特征上的准确率为0.84，与居民感知的一致率约为89%，并在不同社会经济层级中得到验证。除了分类准确性，MSEF还能捕捉情境相关的矛盾，例如非正式商业提升感知活力的同时降低行人舒适度。它还识别出非线性和语义依赖的模式——如建筑透明度在住宅区和商业区的感知效果不同——揭示了通用空间启发式的局限性。通过生成基于注意力机制的自然语言解释，框架将感官数据与社会情感推理相结合，实现与可持续发展目标11一致的透明诊断。本工作在城市感知建模方面提供了方法创新，并为寻求将基础设施精度与居住体验相调和的规划系统提供了实用价值。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While objective street metrics derived from imagery or GIS have become standard in urban analytics, they remain insufficient to capture subjective perceptions essential to inclusive urban design. This study introduces a novel Multimodal Street Evaluation Framework (MSEF) that fuses a vision transformer (VisualGLM-6B) with a large language model (GPT-4), enabling interpretable dual-output assessment of streetscapes. Leveraging over 15,000 annotated street-view images from Harbin, China, we fine-tune the framework using LoRA and P-Tuning v2 for parameter-efficient adaptation. The model achieves an F1 score of 0.84 on objective features and 89.3 percent agreement with aggregated resident perceptions, validated across stratified socioeconomic geographies. Beyond classification accuracy, MSEF captures context-dependent contradictions: for instance, informal commerce boosts perceived vibrancy while simultaneously reducing pedestrian comfort. It also identifies nonlinear and semantically contingent patterns -- such as the divergent perceptual effects of architectural transparency across residential and commercial zones -- revealing the limits of universal spatial heuristics. By generating natural-language rationales grounded in attention mechanisms, the framework bridges sensory data with socio-affective inference, enabling transparent diagnostics aligned with SDG 11. This work offers both methodological innovation in urban perception modeling and practical utility for planning systems seeking to reconcile infrastructural precision with lived experience.&lt;/p&gt;</description></item><item><guid>2506.09552v1</guid><title>Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments</title><link>http://arxiv.org/abs/2506.09552v1</link><author>Fatemeh Mohammadi Amin, Darwin G. Caldwell, Hans Wernher van de Venn</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种双流网络架构（FUSION），用于将3D点云语义分割从模拟环境迁移到真实工业环境，从而提升人机协作的安全性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在工业人机协作中，3D环境的鲁棒解释至关重要，语义分割能够提供精确的环境理解，但需要大量真实标注数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够从模拟环境稳健迁移到真实世界的网络，以增强工业人机协作的实用性和安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用双流网络FUSION，融合动态图卷积神经网络（DGCNN）和带残差层的卷积神经网络（CNN），实现Sim2Real领域适配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实人机协作设置和工业模拟点云上评估，模型实现了97.76%的语义分割准确率，并表现出优于现有方法的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型通过高精度语义分割和强鲁棒性，显著提升了工业人机协作的安全性和操作效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 对3D环境的鲁棒解释对于人机协作至关重要，安全和操作效率是首要考虑。语义分割在此背景下发挥关键作用，使环境得到精确细致的理解。由于现实工业环境需要大量标注数据来实现有效的语义分割，本文提出了一种在Sim2Real领域适配3D点云语义分割的创新方法，专为人机协作设计。我们致力于开发一种能够从模拟环境稳健迁移到真实世界的网络，从而提升其实用性和对安全人机协作的影响。本文提出了一个双流网络架构（FUSION），结合动态图卷积神经网络（DGCNN）和带残差层的卷积神经网络（CNN），作为工业环境的Sim2Real领域适配算法。该模型在真实人机协作设置和工业模拟点云上进行评估，表现出更高的最先进性能，语义分割准确率达到97.76%，并且相较于现有方法具有更好的鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决工业环境中人机协作（HRC）场景下3D点云语义分割的准确性问题，尤其是模拟与真实数据之间的差距。准确的感知对于保障人机交互的安全和效率至关重要，但真实标注数据稀缺，导致模型在真实环境中的表现受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将现有的点云网络DGCNN与CNN（含残差层）结合，构建了双流网络FUSION，并利用NVIDIA IsaacSim生成的合成数据进行预训练。随后采用域适应技术将模型迁移到真实数据，借鉴了Sim2Real适应、COVERED数据集以及相关点云分割研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双流网络融合局部几何特征和全局语义特征，并通过域适应将模型从合成环境迁移到真实环境。实现流程包括：①使用IsaacSim生成合成点云数据；②在合成数据上训练FUSION；③使用真实数据进行域适应或微调；④在在线和离线场景下评估分割精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①双流FUSION架构用于点云分割；②针对工业HRC的Sim2Real域适应流程；③公开了包含真实与合成数据的完整数据集。与以往工作不同的是，该方法实现了实时高精度（97.6%）的分割，并在真实工业环境中表现出更强的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 论文提出一种基于双流网络的Sim2Real域适应框架，利用合成3D点云训练并在工业人机协作环境中实现高精度实时语义分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The robust interpretation of 3D environments is crucial for human-robot collaboration (HRC) applications, where safety and operational efficiency are paramount. Semantic segmentation plays a key role in this context by enabling a precise and detailed understanding of the environment. Considering the intense data hunger for real-world industrial annotated data essential for effective semantic segmentation, this paper introduces a pioneering approach in the Sim2Real domain adaptation for semantic segmentation of 3D point cloud data, specifically tailored for HRC. Our focus is on developing a network that robustly transitions from simulated environments to real-world applications, thereby enhancing its practical utility and impact on a safe HRC.   In this work, we propose a dual-stream network architecture (FUSION) combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional Neural Networks (CNN) augmented with residual layers as a Sim2Real domain adaptation algorithm for an industrial environment. The proposed model was evaluated on real-world HRC setups and simulation industrial point clouds, it showed increased state-of-the-art performance, achieving a segmentation accuracy of 97.76%, and superior robustness compared to existing methods.&lt;/p&gt;</description></item><item><guid>2506.10964v3</guid><title>The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins</title><link>http://arxiv.org/abs/2506.10964v3</link><author>Rico H Herzog, Till Degkwitz, Trivik Verma</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 城市数字孪生被视为整合城市数字资源、实现可持续与综合规划的手段；模型与仿真是核心，能进行假设情景分析并揭示数据关系；但整合与使用模型复杂，涉及城市复杂性、假设不确定性与权力关系；现有方法多为单一集中式，限制多元开放模型；通过与汉堡市的参与式设计，发现开放城市模型平台既可作为技术骨干，也可作为协作多元的社会技术框架；该平台基于开放标准，支持去中心化模型集成、模型间通信和多模型表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市数字孪生正被越来越多地视为整合城市数字资源、实现更可持续、更综合的城市规划的方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨开放城市模型平台在城市数字孪生中的作用，评估其作为技术骨干和社会技术框架的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用参与式设计与参与式系统方法，与德国汉堡市合作开展研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 开放城市模型平台能够同时充当公共技术骨干和协作多元的社会技术框架；它基于开放标准，支持去中心化模型集成、模型间通信，并促进多模型方法来表示城市系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 开放标准与去中心化集成为城市数字孪生提供了更灵活、包容的模型环境，能够更好地处理城市复杂性、不确定性和权力关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 城市数字孪生正被越来越多地视为整合城市日益增长的数字资源，以实现更可持续、更综合的城市规划的一种方式。模型和仿真是这一工作的重要组成部分：它们能够实现“如果……会怎样？”的情景分析，提供洞察，并描述正在收集的大量数据之间的关系。然而，在城市数字孪生中整合并随后使用模型的过程本质上是一个复杂的任务。它提出了如何表示城市复杂性、如何处理不确定假设和建模范式，以及如何捕捉潜在权力关系的问题。该领域现有的方法主要集中在单一且集中式的解决方案上，遵循新自由主义城市建设的传统，往往禁止多元化和开放互操作的模型。通过与德国汉堡市合作，采用参与式设计和参与式系统方法，我们发现开放城市模型平台既可以作为城市数字孪生中建模和仿真的公共技术骨干，也可以作为协作和多元化城市过程表示的社会技术框架。该平台基于开放标准，允许模型的去中心化集成，支持模型间通信，并支持多模型方法来表示城市系统。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban digital twins are increasingly perceived as a way to pool the growing digital resources of cities for the purpose of a more sustainable and integrated urban planning. Models and simulations are central to this undertaking: They enable &amp;quot;what if?&amp;quot; scenarios, create insights and describe relationships between the vast data that is being collected. However, the process of integrating and subsequently using models in urban digital twins is an inherently complex undertaking. It raises questions about how to represent urban complexity, how to deal with uncertain assumptions and modeling paradigms, and how to capture underlying power relations. Existent approaches in the domain largely focus on monolithic and centralized solutions in the tradition of neoliberal city-making, oftentimes prohibiting pluralistic and open interoperable models. Using a participatory design for participatory systems approach together with the City of Hamburg, Germany, we find that an open Urban Model Platform can function both as a public technological backbone for modeling and simulation in urban digital twins and as a socio-technical framework for a collaborative and pluralistic representation of urban processes. Such a platform builds on open standards, allows for a decentralized integration of models, enables communication between models and supports a multi-model approach to representing urban systems.&lt;/p&gt;</description></item><item><guid>2506.14066v1</guid><title>A Point Cloud Completion Approach for the Grasping of Partially Occluded Objects and Its Applications in Robotic Strawberry Harvesting</title><link>http://arxiv.org/abs/2506.14066v1</link><author>Ali Abouzeid, Malak Mansour, Chengsong Hu, Dezhen Song</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种端到端的框架，用于解决机器人果实采摘中因部分遮挡导致的物体检测、分割和抓取规划问题，以草莓采摘为例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在无结构环境下，物体遮挡是设计抓取算法的主要挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过改进点云处理和抓取规划，提高机器人在草莓采摘中的抓取成功率和安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先对点云进行去噪和分割，定位果实；使用点云补全模型生成完整的三维重建；挑选成熟草莓为目标，其他视为障碍物；将处理后的点云转换为占据图，用于碰撞感知的运动规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，形状重建误差最低，抓取成功率提升至79.17%，整体成功率为89.58%；障碍物碰撞率从43.33%降至13.95%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该管线显著提升了自主草莓采摘的效率和可靠性，为更高效的机器人果实采摘系统奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在机器人果实采摘应用中，管理无结构环境下的物体遮挡对抓取算法的设计提出了重大挑战。以草莓采摘为案例，我们提出了一套端到端的框架，用于有效的物体检测、分割和抓取规划，以解决部分遮挡导致的问题。我们的策略首先对点云进行去噪和分割，以准确定位果实。为补偿因遮挡导致的不完整扫描，我们应用点云补全模型生成草莓的密集三维重建。目标选择聚焦成熟草莓，将其他果实归类为障碍物，然后将精炼后的点云转换为占据图，用于碰撞感知的运动规划。实验结果表明，形状重建误差最低，抓取成功率显著提升至79.17%，在真实世界草莓采摘中的整体成功率为89.58%。此外，我们的方法将障碍物碰撞率从43.33%降低至13.95%，凸显了其在提升抓取质量和安全性方面相较于先前方法的有效性。这一管线大幅提升了自主草莓采摘，推动了更高效、更可靠的机器人果实采摘系统的发展。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决机器人在不规则、遮挡严重的环境中对部分可见水果（如草莓）的识别、重建和抓取问题。该问题重要，因为传统的抓取方法依赖完整视角，而在真实农田中叶片、茎和其他果实会遮挡目标，导致抓取失败，影响自动化采摘的效率和质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先对深度图进行去噪和体素下采样，再利用实例分割提取单个水果的点云，随后采用基于 PointAttn 的深度学习完成网络重建完整形状。该思路借鉴了现有的点云完成方法、Sim‑to‑Real 适配技术（如 R2SGrasp）以及前期的机器人采摘研究，结合了完整形状重建与碰撞感知规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点云去噪、分割、深度学习完成和目标选择，构建完整的三维水果模型并在此基础上进行碰撞感知的抓取规划。实现流程为：RGB‑D 输入 → 深度去噪与体素下采样 → 实例分割与点云提取 → 余量去噪 → 通过完成网络生成完整点云 → 识别熟果并选取最近目标 → 将其余果体视为障碍物构建占据图 → 进行碰撞感知的逆运动学与路径规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 通过轻量级去噪技术有效弥合 Sim‑to‑Real 差距；2) 对部分可见草莓进行完整 3D 重建，显著降低 Chamfer 距离；3) 将非目标草莓视为障碍物实现安全抓取；4) 在真实采摘实验中实现更高的抓取成功率和更低的碰撞率。与以往仅关注单视角或仅完成形状的研究不同，该工作同时解决了遮挡、完整重建和安全规划三大难点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套端到端的点云去噪、分割、完整重建与碰撞感知抓取管线，在真实草莓采摘中显著提升了抓取成功率并降低了碰撞风险。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In robotic fruit picking applications, managing object occlusion in unstructured settings poses a substantial challenge for designing grasping algorithms. Using strawberry harvesting as a case study, we present an end-to-end framework for effective object detection, segmentation, and grasp planning to tackle this issue caused by partially occluded objects. Our strategy begins with point cloud denoising and segmentation to accurately locate fruits. To compensate for incomplete scans due to occlusion, we apply a point cloud completion model to create a dense 3D reconstruction of the strawberries. The target selection focuses on ripe strawberries while categorizing others as obstacles, followed by converting the refined point cloud into an occupancy map for collision-aware motion planning. Our experimental results demonstrate high shape reconstruction accuracy, with the lowest Chamfer Distance compared to state-of-the-art methods with 1.10 mm, and significantly improved grasp success rates of 79.17%, yielding an overall success-to-attempt ratio of 89.58\% in real-world strawberry harvesting. Additionally, our method reduces the obstacle hit rate from 43.33% to 13.95%, highlighting its effectiveness in improving both grasp quality and safety compared to prior approaches. This pipeline substantially improves autonomous strawberry harvesting, advancing more efficient and reliable robotic fruit picking systems.&lt;/p&gt;</description></item><item><guid>2506.14817v1</guid><title>Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning</title><link>http://arxiv.org/abs/2506.14817v1</link><author>Simon P. von der Maase</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新型神经网络架构，用于在子国家级别预测三类暴力事件（国家主导、非国家主导和单边）至36个月前景。该模型同时完成分类和回归，输出事件概率和预期强度，并实现了所有任务的最先进性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高空间和时间分辨率的暴力冲突预测仍是研究者和决策者面临的核心挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种无需手工特征工程、仅基于历史冲突数据的模型，能够自动学习复杂的时空模式，并实现高精度预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用蒙特卡洛Dropout LSTM U-Net架构，结合卷积层捕捉空间依赖、循环层建模时间动态，使用历史冲突数据训练，无需手工特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在分类和回归任务上均达到最先进水平，能够生成预测后验分布量化不确定性；能够无缝集成额外数据源并联合预测辅助变量，显示出高度可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型为早期预警、人工救援响应规划和基于证据的和平建设提供了有前景的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在高空间和时间分辨率下预测暴力冲突仍是研究者和政策制定者面临的核心挑战。本研究提出了一种新型神经网络架构，用于在子国家（priogrid-month）层面预测三种不同类型的暴力——国家主导、非国家主导和单边——并可提前36个月进行预测。该模型同时执行分类和回归任务，输出未来事件的概率估计和预期强度。它在所有任务上实现了最先进的性能，并生成近似的预测后验分布以量化预测不确定性。该架构基于蒙特卡洛Dropout长短期记忆（LSTM）U-Net，融合卷积层以捕捉空间依赖，并使用递归结构建模时间动态。与许多现有方法不同，它不需要手工特征工程，仅依赖历史冲突数据。此设计使模型能够自主学习暴力冲突背后的复杂时空模式。除了实现最先进的预测性能外，该模型还具有高度可扩展性：它可以轻松集成额外数据源并联合预测辅助变量。这些能力使其成为早期预警系统、人道主义响应规划和基于证据的和平建设倡议的有前景工具。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Forecasting violent conflict at high spatial and temporal resolution remains a central challenge for both researchers and policymakers. This study presents a novel neural network architecture for forecasting three distinct types of violence -- state-based, non-state, and one-sided -- at the subnational (priogrid-month) level, up to 36 months in advance. The model jointly performs classification and regression tasks, producing both probabilistic estimates and expected magnitudes of future events. It achieves state-of-the-art performance across all tasks and generates approximate predictive posterior distributions to quantify forecast uncertainty.   The architecture is built on a Monte Carlo Dropout Long Short-Term Memory (LSTM) U-Net, integrating convolutional layers to capture spatial dependencies with recurrent structures to model temporal dynamics. Unlike many existing approaches, it requires no manual feature engineering and relies solely on historical conflict data. This design enables the model to autonomously learn complex spatiotemporal patterns underlying violent conflict.   Beyond achieving state-of-the-art predictive performance, the model is also highly extensible: it can readily integrate additional data sources and jointly forecast auxiliary variables. These capabilities make it a promising tool for early warning systems, humanitarian response planning, and evidence-based peacebuilding initiatives.&lt;/p&gt;</description></item><item><guid>2506.15747v2</guid><title>A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion</title><link>http://arxiv.org/abs/2506.15747v2</link><author>Fangzhou Lin, Zilin Dai, Rigved Sanku, Songlin Hou, Kazunori D Yamada, Haichong K. Zhang, Ziming Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种仅使用部分点云而不依赖单视角图像的点云补全方法，并通过实验验证其优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单视角图像引导点云补全（SVIPC）已被证明有效，但图像引导的必要性尚未得到充分探讨。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究图像引导在SVIPC中的核心作用，并提出一种无视角的基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建基于注意力的多分支编码-解码网络，采用层次自融合机制，利用交叉注意力和自注意力层融合多条信息流，从而增强特征表达并提升几何结构捕捉能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ShapeNet-ViPC数据集上，所提出的无视角框架在实验和消融研究中均优于现有最先进的SVIPC方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 研究结果为SVIPC多模态学习的发展提供了新的见解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 单视角图像引导点云补全（SVIPC）任务旨在利用单视角图像帮助从部分输入重建完整点云。虽然以前的工作已证明这种多模态方法的有效性，但图像引导的基本必要性仍未得到充分检验。为此，我们提出了一种基于注意力的多分支编码-解码网络的强基线方法，该方法仅以部分点云为输入，完全不依赖视角。我们的层次自融合机制由交叉注意力和自注意力层驱动，有效整合多条流的信息，丰富特征表示并增强网络捕捉几何结构的能力。对ShapeNet-ViPC数据集进行的大量实验和消融研究表明，我们的无视角框架在性能上优于现有最先进的SVIPC方法。我们希望我们的发现为SVIPC多模态学习的发展提供新的洞见。我们的演示代码将发布在 https://github.com/Zhang-VISLab。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; The paper investigates whether single‑view images are necessary for completing partial point clouds and proposes a strong baseline that relies only on the point cloud. Completing 3D shapes from incomplete scans is vital for robotics, autonomous driving, and AR, where sensor data is often occluded or limited.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; The authors noted the drawbacks of image‑guided pipelines—image quality, calibration, and extra computation—and therefore designed a view‑free architecture. They built on encoder‑decoder frameworks such as PointNet++ and SnowflakeNet, and incorporated attention mechanisms inspired by XMFNet and transformer‑based point methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; The core idea is to extract multi‑scale features from the partial point cloud using several independent encoder branches, fuse these features with cross‑attention and self‑attention (self‑fusion), and then feed the enriched representation into a decoder that predicts the missing points. The pipeline is: partial point cloud → multi‑branch encoder → hierarchical feature extraction → self‑fusion network → concatenated fused features → decoder → complete point cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; Key innovations include a view‑free baseline that outperforms image‑guided methods, a multi‑branch encoder that captures diverse geometric cues, an attention‑based self‑fusion module that adaptively merges features across branches and levels, and a flexible architecture that scales to more branches. Unlike prior work that fuses image and point‑cloud modalities with simple concatenation, this approach relies solely on point‑cloud data and uses cross‑ and self‑attention for richer fusion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; We demonstrate that a multi‑branch attention‑enhanced encoder‑decoder using only partial point clouds can surpass state‑of‑the‑art image‑guided completion methods, proving that image guidance is not essential for high‑quality point‑cloud reconstruction.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The single-view image guided point cloud completion (SVIPC) task aims to reconstruct a complete point cloud from a partial input with the help of a single-view image. While previous works have demonstrated the effectiveness of this multimodal approach, the fundamental necessity of image guidance remains largely unexamined. To explore this, we propose a strong baseline approach for SVIPC based on an attention-based multi-branch encoder-decoder network that only takes partial point clouds as input, view-free. Our hierarchical self-fusion mechanism, driven by cross-attention and self-attention layers, effectively integrates information across multiple streams, enriching feature representations and strengthening the networks ability to capture geometric structures. Extensive experiments and ablation studies on the ShapeNet-ViPC dataset demonstrate that our view-free framework performs superiorly to state-of-the-art SVIPC methods. We hope our findings provide new insights into the development of multimodal learning in SVIPC. Our demo code will be available at https://github.com/Zhang-VISLab.&lt;/p&gt;</description></item><item><guid>2506.15849v2</guid><title>PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps</title><link>http://arxiv.org/abs/2506.15849v2</link><author>Kirill Muravyev, Artem Kobozev, Vasily Yuryev, Alexander Melekhin, Oleg Bulichev, Dmitry Yudin, Konstantin Yakovlev</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; PRISM-Loc 是一种轻量且鲁棒的定位方法，适用于大型户外环境，结合紧凑拓扑表示与基于原始 LiDAR 扫描的扫描匹配与路缘检测模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统定位方法在资源受限平台上难以实时运行，且易受城市环境传感挑战影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种资源友好、实时且对城市感知挑战具有弹性的定位方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用紧凑拓扑地图进行全局位置识别，并使用原创扫描匹配技术直接处理原始 LiDAR 点云，实现路缘检测与匹配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ITLP-Campus 大规模数据集上实现 99% 成功率，定位时间 150 毫秒，地图大小仅 20 MB。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PRISM-Loc 在资源受限平台上实现高效、准确的城市规模定位，证明其轻量化与鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; PRISM-Loc 是一种轻量且鲁棒的定位方法，适用于大型户外环境，结合紧凑拓扑表示与基于原始 LiDAR 扫描的扫描匹配与路缘检测模块。该方法针对资源受限平台设计，强调实时性能和对常见城市感知挑战的弹性。它通过全局位置识别和原创扫描匹配技术，在紧凑拓扑地图上实现准确定位。实验在标准基准和嵌入式平台上验证了其有效性，在大型 ITLP-Campus 数据集上实现 99% 的成功率，定位时间为 150 毫秒，使用 20 MB 的地图。主要贡献包括：① 城市规模定位的紧凑表示；② 直接在原始 LiDAR 点云上运行的路缘检测与扫描匹配管道；③ 对方法进行全面评估与性能分析。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现一种轻量级、实时的长距离 LiDAR 定位方法，适用于城市环境中的资源受限平台。该问题重要，因为传统的高精度定位需要密集的全局 LiDAR 地图，既占用大量内存又计算量大，难以在嵌入式设备上实时运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将已有的 PRISM‑TopoMap 图结构、MinkLoc3D 轻量级点云识别以及传统的扫描匹配技术结合起来，并在此基础上加入了基于原始 LiDAR 点云的路缘检测。设计思路是先用全局识别快速定位到候选节点，再用精细的扫描匹配校正姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用图结构的拓扑地图和原始 LiDAR 的路缘信息实现快速、精确的定位。流程包括：1）使用里程计更新当前节点和相对姿态；2）检查扫描与当前节点的重叠；3）若重叠不足，先尝试沿图边移动；4）若仍失败，使用 MinkLoc3D 进行全局识别得到候选节点；5）对候选节点执行基于路缘的扫描匹配，得到精确变换；6）更新当前节点和全局姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 采用极小的拓扑地图，每个节点仅占几 KB，支持城市级规模；2) 开发了基于原始 LiDAR 的路缘检测与扫描匹配算法，提升了在低重叠场景下的匹配精度；3) 在嵌入式平台上实现 150 ms 的实时定位，地图体积仅 20 MB。与以往依赖密集度量地图或昂贵学习模型的定位方法不同，PRISM‑Loc 在保持高精度的同时大幅降低了计算和存储成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PRISM‑Loc 通过将轻量级拓扑地图、MinkLoc3D 识别和路缘感知扫描匹配相结合，提供了一种在城市大范围内实时、资源友好的 LiDAR 定位方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose PRISM-Loc - a lightweight and robust approach for localization in large outdoor environments that combines a compact topological representation with a novel scan-matching and curb-detection module operating on raw LiDAR scans. The method is designed for resource-constrained platforms and emphasizes real-time performance and resilience to common urban sensing challenges. It provides accurate localization in compact topological maps using global place recognition and an original scan matching technique. Experiments on standard benchmarks and on an embedded platform demonstrate the effectiveness of our approach. Our method achieves a 99\% success rate on the large-scale ITLP-Campus dataset while running at 150 ms per localization and using a 20 MB map for localization. We highlight three main contributions: (1) a compact representation for city-scale localization; (2) a novel curb detection and scan matching pipeline operating directly on raw LiDAR points; (3) a thorough evaluation of our method with performance analysis.&lt;/p&gt;</description></item><item><guid>2506.17290v1</guid><title>SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation</title><link>http://arxiv.org/abs/2506.17290v1</link><author>Yuqi Li, Junhao Dong, Zeyu Dong, Chuanguang Yang, Zhulin An, Yongjun Xu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种结构与关系感知的知识蒸馏框架SRKD，能够将大型冻结教师模型的几何与语义知识迁移到轻量级学生模型，从而在保持高精度的同时显著降低模型复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云分割常用的大规模变压器模型计算复杂度高，部署受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过知识蒸馏将大模型的丰富信息转移到小模型，实现高效分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SRKD包括基于亲和矩阵的关系对齐模块、跨样本小批量构造策略、KL散度对齐语义分布以及真实标签监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明SRKD在大幅减小模型规模的同时，取得了最先进的分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在实际部署场景中既高效又有效，证明了知识蒸馏在点云分割中的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云分割面临实际挑战，因为大型基于变压器的模型计算复杂度高且部署受限。为了解决这一问题，我们提出了一种新颖的结构与关系感知知识蒸馏框架，称为SRKD，它将来自大型冻结教师模型（&amp;gt;100M）的丰富几何和语义知识迁移到轻量级学生模型（&amp;lt;15M）。具体而言，我们提出了基于亲和矩阵的关系对齐模块，通过点对点相似度匹配将教师的结构依赖关系蒸馏给学生，从而增强学生学习上下文交互的能力。同时，我们引入了跨样本小批量构造策略，使学生能够感知稳定且通用的几何结构，这种对齐发生在教师的不同点云实例之间，而不是单个样本内部。此外，使用KL散度对齐语义分布，并通过真实标签监督进一步强化准确分割。我们的方法在显著降低模型复杂度的同时实现了最先进的性能，证明了其在真实部署场景中的有效性和效率。代码可在 https://github.com/itsnotacie/SRKD 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云语义分割中模型体量大、计算复杂度高的问题，使得在自动驾驶、机器人和AR/VR等需要实时、边缘部署的场景中能够使用轻量化模型，同时保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于知识蒸馏的思路，结合Transformer在点云分割中的优势，借鉴了2D视觉中的KD技术、关系蒸馏和体素采样方法，提出了跨样本几何对齐和基于亲和矩阵的关系对齐两大模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多种蒸馏目标让轻量化学生模型学习教师模型的语义分布、通道特征、点间与体素间的关系以及跨样本的几何相似性。实现流程包括：1）用冻结的教师和学生网络分别提取特征和预测；2）计算亲和矩阵并对齐点/体素关系；3）构造跨样本相似矩阵并对齐；4）联合KL、交叉熵和亲和损失进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①跨样本mini-batch几何蒸馏，捕获全局几何一致性；②基于亲和矩阵的关系蒸馏，传递点间和体素间的结构信息；③多目标蒸馏框架，将语义、通道、关系和几何信息统一融合。与以往仅在单样本内进行语义或特征蒸馏的工作不同，SRKD同时利用跨样本几何和关系信息显著提升学生模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SRKD通过联合跨样本几何对齐和关系亲和矩阵蒸馏，将大型Transformer点云分割模型压缩为轻量化网络，同时保持甚至超越原模型的精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D point cloud segmentation faces practical challenges due to the computational complexity and deployment limitations of large-scale transformer-based models. To address this, we propose a novel Structure- and Relation-aware Knowledge Distillation framework, named SRKD, that transfers rich geometric and semantic knowledge from a large frozen teacher model (&amp;gt;100M) to a lightweight student model (&amp;lt;15M). Specifically, we propose an affinity matrix-based relation alignment module, which distills structural dependencies from the teacher to the student through point-wise similarity matching, enhancing the student&amp;#x27;s capability to learn contextual interactions. Meanwhile, we introduce a cross-sample mini-batch construction strategy that enables the student to perceive stable and generalized geometric structure. This aligns across diverse point cloud instances of the teacher, rather than within a single sample. Additionally, KL divergence is applied to align semantic distributions, and ground-truth supervision further reinforces accurate segmentation. Our method achieves state of the art performance with significantly reduced model complexity, demonstrating its effectiveness and efficiency in real-world deployment scenarios. Our Code is available at https://github.com/itsnotacie/SRKD.&lt;/p&gt;</description></item><item><guid>2506.17516v1</guid><title>EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization</title><link>http://arxiv.org/abs/2506.17516v1</link><author>Zhou Chen, Sanjoy Kundu, Harsimran S. Baweja, Sathyanarayanan N. Aakur</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为EASE的自监督框架，用于在没有预定义动作空间、标注数据或外部奖励的情况下，实现实时事件感知、跟踪和总结。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在人工智能与人类协作、辅助机器人和自主导航等任务中，动态检测、跟踪和总结事件的能力至关重要，但现有方法往往依赖预定义动作空间、标注数据和外部奖励，限制了其在动态真实场景中的适应性和可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够自我驱动、可扩展且适应动态环境的事件感知系统，减少对人工标注和外部奖励的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; EASE通过自由能最小化，将预测误差和熵作为内在信号，用于事件分割、观察总结和主动跟踪重要主体。它将生成式感知模型与动作驱动的控制策略耦合，使预测与观测动态对齐，从而实现隐式记忆、目标连续性和对新环境的适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在仿真和真实环境中的广泛评估表明，EASE能够实现隐私保护且可扩展的事件感知，为无脚本、动态任务中的具身系统提供了稳健的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; EASE展示了在无标注、无外部奖励条件下，通过自监督学习实现高效事件感知的可行性，为未来具身智能系统的研究和应用奠定了坚实基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 主动事件感知，即在实时动态检测、跟踪和总结事件的能力，对于人机协作、辅助机器人和自主导航等任务中的具身智能至关重要。然而，现有方法往往依赖预定义的动作空间、标注数据和外部奖励，限制了它们在动态真实场景中的适应性和可扩展性。受认知理论的事件感知和预测编码的启发，我们提出了EASE，一种自监督框架，通过自由能最小化统一时空表示学习和具身控制。EASE利用预测误差和熵作为内在信号来分割事件、总结观测并主动跟踪显著主体，且不需要显式标注或外部奖励。通过将生成式感知模型与动作驱动的控制策略耦合，EASE动态地将预测与观测对齐，促成隐式记忆、目标连续性以及对新环境的适应等自发行为。在仿真和真实环境中的广泛评估表明，EASE能够实现隐私保护且可扩展的事件感知，为无脚本、动态任务中的具身系统提供了稳健的基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Active event perception, the ability to dynamically detect, track, and summarize events in real time, is essential for embodied intelligence in tasks such as human-AI collaboration, assistive robotics, and autonomous navigation. However, existing approaches often depend on predefined action spaces, annotated datasets, and extrinsic rewards, limiting their adaptability and scalability in dynamic, real-world scenarios. Inspired by cognitive theories of event perception and predictive coding, we propose EASE, a self-supervised framework that unifies spatiotemporal representation learning and embodied control through free energy minimization. EASE leverages prediction errors and entropy as intrinsic signals to segment events, summarize observations, and actively track salient actors, operating without explicit annotations or external rewards. By coupling a generative perception model with an action-driven control policy, EASE dynamically aligns predictions with observations, enabling emergent behaviors such as implicit memory, target continuity, and adaptability to novel environments. Extensive evaluations in simulation and real-world settings demonstrate EASE&amp;#x27;s ability to achieve privacy-preserving and scalable event perception, providing a robust foundation for embodied systems in unscripted, dynamic tasks.&lt;/p&gt;</description></item><item><guid>2506.18292v2</guid><title>Three-dimentional reconstruction of complex, dynamic population canopy architecture for crops with a novel point cloud completion model: A case study in Brassica napus rapeseed</title><link>http://arxiv.org/abs/2506.18292v2</link><author>Ziyue Guo, Xin Yang, Yutao Shen, Yang Zhu, Lixi Jiang, Haiyan Cen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对油菜种群的复杂动态冠层结构的三维重建方法，利用点云补全模型实现完整冠层的重建，并通过自动化标注框架生成训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确描述完整冠层结构对于评估作物光合作用和产量、指导理想型设计至关重要，但现有三维重建技术因冠层遮挡问题难以获得精确结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种有效的点云补全方法，克服遮挡限制，实现油菜种群冠层的完整三维重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建完整点云生成框架进行自动标注；设计CP-PCN网络，包含多分辨率动态图卷积编码器(MRDG)、点金字塔解码器(PPD)和动态图卷积特征提取模块(DGCFE)以预测被遮挡点并捕捉生长期间结构变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CP-PCN在四个生长阶段的Chamfer距离为3.35–4.51厘米，优于基于Transformer的PoinTr；消融实验验证了MRDG和DGCFE模块的有效性；基于CP-PCN的果荚效率指数将油菜产量预测精度提升了11.2%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CP-PCN管线可推广至其他作物，显著提升田间种群冠层结构的定量分析能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 定量描述完整冠层结构对于准确评估作物光合作用和产量表现至关重要，可指导理想型设计。虽然已经开发了多种用于单株植物和冠层三维重建的传感技术，但由于复杂冠层结构中的严重遮挡，它们未能获得准确的冠层结构描述。我们提出了一种针对油菜作物的复杂动态种群冠层结构三维重建的有效方法，采用新颖的点云补全模型。通过区分冠层内表面点与被遮挡点，开发了完整点云生成框架以实现训练数据的自动标注。随后设计了作物种群点云补全网络（CP-PCN），该网络包含多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD），用于预测被遮挡点。为进一步增强特征提取，提出了动态图卷积特征提取模块（DGCFE），以捕捉整个油菜生长期间的结构变化。结果表明，CP-PCN在四个生长阶段的Chamfer距离（CD）值为3.35厘米–4.51厘米，优于基于Transformer的方法（PoinTr）。消融研究确认了MRDG和DGCFE模块的有效性。此外，验证实验表明，基于CP-PCN的果荚效率指数将油菜产量预测的整体准确性提高了11.2%，相较于使用不完整点云。CP-PCN管线有望扩展到其他作物，显著推进田间种群冠层结构的定量分析。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文试图解决在密集、动态的油菜田中，如何准确重建完整的冠层三维结构，克服遮挡导致的点云缺失问题。完整的冠层描述对评估光合效率、产量预测和理想品种设计至关重要，能够帮助农学家和育种者做出更科学的决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先收集单株油菜的高质量三维数据，然后通过虚实融合（VRI）方法模拟整个田间种植模式，生成包含遮挡信息的完整点云。为训练数据自动标注，他们设计了遮挡点检测算法。网络设计借鉴了图卷积、Transformer和GAN等现有技术，提出了多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD），并加入动态图卷积特征提取器（DGCFE）以捕捉生长阶段的结构变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用模拟数据构建完整的冠层点云，再训练一个点云补全网络，使其能够在真实田间采集的缺失点云中恢复被遮挡的部分。实现流程包括：① UAV和NeRF获取单株三维模型；② VRI生成种植模式下的完整种群点云；③ 遮挡点检测实现自动标注；④ 用这些数据训练CP‑PCN网络；⑤ 在田间采集的缺失点云上应用CP‑PCN完成冠层；⑥ 通过完成的点云提取结构特征并计算种子效率指数以提升产量预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① VRI方法生成与真实田间极为相似的种群点云；② 自动遮挡点检测实现大规模训练集标注；③ CP‑PCN网络结合多分辨率动态图卷积和点金字塔解码器，显著提升补全精度；④ 引入动态图卷积特征提取器捕捉生长阶段的结构变化；⑤ 通过完整点云改进产量预测，提升准确率。与以往仅针对单株或简单结构的补全方法不同，该工作针对密集、动态的田间冠层实现了高精度补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一套从模拟生成完整冠层点云到训练专用补全网络，再到在真实田间数据上实现高精度冠层重建和产量预测的完整流程，显著提升了油菜田间冠层三维建模的准确性和实用价值。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Quantitative descriptions of the complete canopy architecture are essential for accurately evaluating crop photosynthesis and yield performance to guide ideotype design. Although various sensing technologies have been developed for three-dimensional (3D) reconstruction of individual plants and canopies, they failed to obtain an accurate description of canopy architectures due to severe occlusion among complex canopy architectures. We proposed an effective method for 3D reconstruction of complex, dynamic population canopy architecture for rapeseed crops with a novel point cloud completion model. A complete point cloud generation framework was developed for automated annotation of the training dataset by distinguishing surface points from occluded points within canopies. The crop population point cloud completion network (CP-PCN) was then designed with a multi-resolution dynamic graph convolutional encoder (MRDG) and a point pyramid decoder (PPD) to predict occluded points. To further enhance feature extraction, a dynamic graph convolutional feature extractor (DGCFE) module was proposed to capture structural variations over the whole rapeseed growth period. The results demonstrated that CP-PCN achieved chamfer distance (CD) values of 3.35 cm -4.51 cm over four growth stages, outperforming the state-of-the-art transformer-based method (PoinTr). Ablation studies confirmed the effectiveness of the MRDG and DGCFE modules. Moreover, the validation experiment demonstrated that the silique efficiency index developed from CP-PCN improved the overall accuracy of rapeseed yield prediction by 11.2% compared to that of using incomplete point clouds. The CP-PCN pipeline has the potential to be extended to other crops, significantly advancing the quantitatively analysis of in-field population canopy architectures.&lt;/p&gt;</description></item><item><guid>2506.21805v1</guid><title>CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation</title><link>http://arxiv.org/abs/2506.21805v1</link><author>Nicolas Bougie, Narimasa Watanabe</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出CitySim城市模拟器，利用大型语言模型实现更逼真的人类行为模拟，支持长期、灵活的仿真，并通过大规模实验验证其在预测人群密度、地点受欢迎程度和福祉评估等方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类行为建模在社会科学、行为研究和城市规划中至关重要，但传统方法依赖僵化规则，难以捕捉细腻意图和适应性行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统模型的局限，开发基于大语言模型的城市模拟器，以更真实地模拟人类日常活动和长期目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CitySim使用递归价值驱动方法生成日程，平衡必需活动、个人习惯和情境因素；为代理赋予信念、长期目标和空间记忆，以实现长期仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CitySim在微观和宏观层面上更贴近真实人类行为；大规模实验显示其能准确估计人群密度、预测地点受欢迎程度并评估福祉。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CitySim是一个可扩展、灵活的测试平台，可用于理解和预测城市现象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.&lt;/p&gt;</description></item><item><guid>2506.23227v1</guid><title>High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation</title><link>http://arxiv.org/abs/2506.23227v1</link><author>Lunhao Duan, Shanshan Zhao, Xingxing Weng, Jing Zhang, Gui-Song Xia</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究在仅有场景级注释的室内点云语义分割问题，提出一种高质量伪标签生成框架，通过多模态信息和区域-点语义一致性提升分割精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统方法依赖稀疏点级标签，场景级注释的点云分割研究相对较少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在缺乏精确点级标签的情况下，生成准确的伪标签以训练分割模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用跨模态特征引导模块将2D图像像素与3D点云特征对齐；引入区域-点语义一致性模块，通过区域投票产生区域语义来指导点级预测，从而纠正训练过程中的错误伪标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ScanNet v2和S3DIS数据集上，方法相较于之前工作取得显著提升；消融实验验证了各模块的贡献。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架能够有效生成高质量伪标签，显著提升场景级注释下的点云语义分割性能，代码已公开。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究在仅有场景级注释的室内点云语义分割问题。缺乏精确点级标签时，现有方法先生成点级伪标签，再训练分割模型。然而，仅凭场景级注释生成准确伪标签具有挑战性，严重影响分割效果。为提高精度，本文提出高质量伪标签生成框架，利用多模态信息和区域-点语义一致性。具体而言，跨模态特征引导模块利用2D-3D对应关系，将点云特征与对应的2D图像像素对齐，帮助点云特征学习。为进一步缓解场景级注释带来的挑战，引入区域-点语义一致性模块。该模块通过基于点级语义的区域投票策略产生区域语义，用于指导点级语义预测。利用上述模块，方法可在训练期间纠正不准确的点级语义预测，获得高质量伪标签。在ScanNet v2和S3DIS数据集上，本文方法在场景级注释下显著优于以往工作，验证了其有效性。进一步的消融研究验证了各组件的贡献。代码已公开。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决仅使用场景级标签进行室内点云语义分割的问题。由于点级标注成本高，场景级标注更易获取，提升其下的分割性能对实际应用和大规模数据集的研究都具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为伪标签质量是关键，借鉴了跨模态知识蒸馏、区域投票和无监督分割等现有技术（如PCAM、WyPR、MIT、PPKT）。他们结合2D-3D对应关系和区域-点一致性，设计了跨模态特征引导和区域-点语义一致性模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点对像素的对比蒸馏将2D图像特征引导3D点云特征，并利用教师网络生成的区域语义来监督学生网络的点级预测，从而迭代提升伪标签质量。实现流程包括：① 2D/3D网络提取特征和预测；② 跨模态对比蒸馏对齐特征；③ 教师网络产生区域语义；④ 区域-点一致性模块监督学生网络；⑤ 生成高质量伪标签并训练分割模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 采用点对像素的对比蒸馏在场景级标注下实现跨模态特征对齐；② 引入动态阈值的区域-点语义一致性模块，缓解无监督分割噪声；③ 通过教师-学生迭代提升伪标签质量。与之前的工作相比，本文不再依赖粗糙的CAM或单一区域标签，而是结合2D信息和一致性约束显著提升伪标签精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种结合跨模态对比蒸馏和区域-点一致性的框架，在仅有场景级标签的条件下生成高质量伪标签，显著提升室内点云语义分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper investigates indoor point cloud semantic segmentation under scene-level annotation, which is less explored compared to methods relying on sparse point-level labels. In the absence of precise point-level labels, current methods first generate point-level pseudo-labels, which are then used to train segmentation models. However, generating accurate pseudo-labels for each point solely based on scene-level annotations poses a considerable challenge, substantially affecting segmentation performance. Consequently, to enhance accuracy, this paper proposes a high-quality pseudo-label generation framework by exploring contemporary multi-modal information and region-point semantic consistency. Specifically, with a cross-modal feature guidance module, our method utilizes 2D-3D correspondences to align point cloud features with corresponding 2D image pixels, thereby assisting point cloud feature learning. To further alleviate the challenge presented by the scene-level annotation, we introduce a region-point semantic consistency module. It produces regional semantics through a region-voting strategy derived from point-level semantics, which are subsequently employed to guide the point-level semantic predictions. Leveraging the aforementioned modules, our method can rectify inaccurate point-level semantic predictions during training and obtain high-quality pseudo-labels. Significant improvements over previous works on ScanNet v2 and S3DIS datasets under scene-level annotation can demonstrate the effectiveness. Additionally, comprehensive ablation studies validate the contributions of our approach&amp;#x27;s individual components. The code is available at https://github.com/LHDuan/WSegPC .&lt;/p&gt;</description></item><item><guid>2507.00914v1</guid><title>Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications</title><link>http://arxiv.org/abs/2507.00914v1</link><author>Jindong Han, Yansong Ning, Zirui Yuan, Hang Ni, Fan Liu, Tengfei Lyu, Hao Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文综述了利用大型语言模型（LLM）构建城市智能代理的研究进展，阐述了其概念、工作流程、应用领域以及面临的信任与评估挑战，并为未来研究指明方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 智能城市的目标是通过大数据和人工智能实现高效、宜居、可持续的城市环境。LLM的出现为实现这一愿景提供了新的技术手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过对城市LLM代理的概念、研究现状、应用场景和关键问题进行系统梳理，建立该领域的基础并提供发展路线图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先介绍城市LLM代理的定义和特点；随后从代理工作流（感知、记忆、推理、执行、学习）角度综述现有研究；接着将应用领域分为城市规划、交通、环境、公共安全和城市社会，并列举代表性工作；最后讨论可信度与评估问题，并提出开放性研究课题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LLM可作为半具身的城市代理，具备强大的语义理解和推理能力；研究已覆盖感知、记忆管理、推理、执行和学习等工作流；应用已扩展至五大领域；在实际部署中，可信度和评估是关键挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本综述为城市LLM代理的研究奠定基础，指出了技术路线和未来工作，并提供了持续更新的论文与开源资源列表。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 长期以来，智能城市的愿景是利用大数据和人工智能技术创建高效、宜居、可持续的城市环境。最近，大型语言模型（LLM）的出现为实现这一愿景开辟了新途径。凭借强大的语义理解和推理能力，LLM可以被部署为能够在各个领域自主解决复杂问题的智能代理。在本文中，我们关注城市LLM代理，即在城市的混合网络-物理-社会空间中半具身化的、由LLM驱动的代理，用于系统级的城市决策。首先，我们介绍城市LLM代理的概念，讨论其独特的能力和特征。其次，我们从代理工作流的角度综述当前的研究现状，包括城市感知、记忆管理、推理、执行和学习。第三，我们将城市LLM代理的应用领域分为五类：城市规划、交通、环境、公共安全和城市社会，并在每个类别中展示代表性工作。最后，我们讨论了在实际部署中至关重要的可信度和评估问题，并指出了未来研究的若干开放问题。本综述旨在为新兴的城市LLM代理领域奠定基础，并为推进LLM与城市智能的交叉提供路线图。相关论文和开源资源的精选列表正在 https://github.com/usail-hkust/Awesome-Urban-LLM-Agents 上维护并持续更新。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.&lt;/p&gt;</description></item><item><guid>2507.05211v2</guid><title>All in One: Visual-Description-Guided Unified Point Cloud Segmentation</title><link>http://arxiv.org/abs/2507.05211v2</link><author>Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为VDG-Uni3DSeg的新框架，通过融合预训练的视觉-语言模型和大型语言模型，利用多模态信息提升3D点云的统一分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云的统一分割对场景理解至关重要，但由于点云稀疏、标注有限以及在复杂环境中区分细粒度物体类别的难度，现有方法难以充分捕获语义与上下文信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述挑战，提升3D点云在语义、实例和全景分割任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用LLM生成文本描述和网络参考图像作为多模态查询，设计语义-视觉对比损失将点特征与查询对齐，并引入空间增强模块高效建模全局关系；整个过程在离线生成的多模态知识下进行闭集推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在语义、实例和全景分割任务上，VDG-Uni3DSeg实现了最先进的性能，验证了多模态知识融合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架提供了一种可扩展、实用的3D理解方案，为未来点云分割研究提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云的统一分割对于场景理解至关重要，但由于其稀疏结构、标注有限以及在复杂环境中区分细粒度物体类别的挑战，仍面临困难。现有方法往往难以捕获丰富的语义和上下文信息，主要原因是监督不足和缺乏多样化的多模态线索，导致类别和实例的区分效果不佳。为解决这些问题，我们提出了VDG-Uni3DSeg，一种新颖的框架，将预训练的视觉-语言模型（如CLIP）和大型语言模型（LLM）相结合，以提升3D分割性能。通过利用LLM生成的文本描述和来自互联网的参考图像，我们的方法引入了丰富的多模态线索，促进了细粒度类别和实例的分离。我们进一步设计了语义-视觉对比损失，将点特征与多模态查询对齐，并引入空间增强模块，以高效建模全局场景关系。在离线生成的多模态知识支持下，VDG-Uni3DSeg在语义、实例和全景分割任务中实现了最先进的结果，提供了一种可扩展且实用的3D理解方案。代码已公开在 https://github.com/Hanzy1996/VDG-Uni3DSeg。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现统一的3D点云分割，包括语义、实例和全景分割。由于点云稀疏、注释稀缺且难以区分细粒度类别，现有方法在复杂场景中表现不佳。统一分割对自动驾驶、机器人和AR/VR等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了OneFormer3D等统一分割框架，发现缺乏多模态信息。为此，他们利用预训练的CLIP和LLM生成文本描述与参考图像，构建离线多模态查询。通过语义-视觉对比损失和空间增强模块，提升点云特征与多模态查询的对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将LLM生成的类描述和互联网图像作为多模态查询，使用CLIP编码后与点云特征融合，生成统一的分割掩码。实现流程包括：① 3D骨干提取点特征；② 空间增强模块捕获全局关系；③ 生成描述和图像查询并编码；④ 通过多模态融合层和掩码解码器得到语义、实例和全景掩码；⑤ 对语义结果进行投票融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 离线生成的LLM描述和互联网图像作为多模态查询；② 语义-视觉对比损失对齐点云与多模态特征；③ 空间增强模块高效建模全局关系；④ 统一掩码解码器与投票融合实现三种分割任务。与以往需要实时图像-文本配对或额外模块的开放词汇方法不同，本文在闭集范式下实现高效、稳健的分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VDG-Uni3DSeg通过离线的LLM文本描述和互联网图像查询，结合CLIP编码和空间增强，统一实现语义、实例和全景3D点云分割，并在保持高效推理的同时取得了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.&lt;/p&gt;</description></item><item><guid>2507.06564v1</guid><title>SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments</title><link>http://arxiv.org/abs/2507.06564v1</link><author>Tianshun Li, Tianyi Huai, Zhen Li, Yichun Gao, Haoang Li, Xinhu Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SkyVLN 是一种将视觉与语言导航与非线性模型预测控制相结合的框架，旨在提升无人机在复杂城市环境中的自主导航能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 无人机因其机动性和适应性在各行业得到广泛应用，但传统导航方法在动态三维城市空间中面临精度和鲁棒性挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过利用大型语言模型解释自然语言指令和视觉观测，改进无人机在动态环境中的导航准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建多模态导航代理，配备细粒度空间口头化器和历史路径记忆机制，并集成非线性模型预测控制模块实现动态障碍物避让和轨迹跟踪。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在使用 AirSim 的高保真三维城市仿真环境中，SkyVLN 在新颖和未见环境中显著提升了导航成功率和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SkyVLN 通过结合语言理解与预测控制，能够在复杂城市环境中实现更准确、更鲁棒的无人机导航。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 无人机因其机动性和适应性在各行业得到广泛应用。本文提出了 SkyVLN，一种将视觉与语言导航（VLN）与非线性模型预测控制（NMPC）相结合的新框架，以提升无人机在复杂城市环境中的自主性。与传统导航方法不同，SkyVLN 利用大型语言模型（LLM）解释自然语言指令和视觉观测，使无人机能够在动态三维空间中以更高的准确性和鲁棒性导航。我们提出了一个多模态导航代理，配备细粒度空间口头化器和历史路径记忆机制。这些组件使无人机能够消除空间语境歧义、处理模糊指令，并在必要时回溯。该框架还集成了 NMPC 模块，用于动态障碍物避让，确保精确的轨迹跟踪和碰撞预防。为验证我们的方案，我们使用 AirSim 开发了一个高保真三维城市仿真环境，具有逼真的图像和动态城市元素。大量实验表明，SkyVLN 在新颖和未见环境中显著提高了导航成功率和效率。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在让无人机在复杂城市环境中通过视觉和自然语言指令实现自主导航，解决传统导航方法在动态障碍、GNSS失效和多维动作空间下的准确性与鲁棒性不足的问题。此类能力对于无人机在监视、物流、救援等实际应用中的安全、高效执行至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了大型语言模型（LLM）对指令的语义理解、视觉语言模型（VLM）对景物的检测以及非线性模型预测控制（NMPC）的轨迹规划，形成端到端的多模态决策框架。设计过程中借鉴了GroundingDINO、LMAR、WPO等现有视觉语言导航与控制技术，并在此基础上加入了高分辨率空间描述器和历史路径记忆。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视觉感知、语言推理与控制优化三者无缝集成。流程为：①前视摄像头捕获图像并用VLM检测地标；②LLM提取子目标并生成空间描述；③Wayfinding Prompt Optimization 将视觉信息转化为高分辨率文本并利用历史路径记忆补充上下文；④LLM运动生成器根据提示输出动作；⑤NMPC根据动作规划并执行轨迹，同时避障。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① SkyVLN框架将VLN与NMPC结合，实现自然语言驱动的三维导航；②细粒度空间描述器提供精确的地标定位；③历史路径记忆支持模糊指令下的回溯与重规划；④在AirSim中构建高保真城市模拟并验证性能。与以往仅关注视觉或语言导航、或仅使用传统控制的工作不同，SkyVLN在动态障碍、三维动作空间和指令歧义处理上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SkyVLN通过将大型语言模型的语义推理与视觉感知和非线性模型预测控制相结合，使无人机能够以自然语言指令在复杂城市环境中实现安全、高效的自主三维导航。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments.&lt;/p&gt;</description></item><item><guid>2507.06592v1</guid><title>Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning</title><link>http://arxiv.org/abs/2507.06592v1</link><author>Yang Chen, Yueqi Duan, Haowen Sun, Jiwen Lu, Yap-Peng Tan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种适用于点云3D语义分割的自适应边缘对比学习方法，能够根据每个点的模糊程度动态调整学习目标，从而在保持低模糊点准确性的同时容忍高模糊点的错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法采用统一惩罚目标，忽略了点级模糊性和过渡区域的特征差异，导致对高度模糊点的硬约束产生次优模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种将对比学习与模糊估计框架结合的自适应方法，以更合理地处理不同模糊程度的点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先提出 AMContrast3D，将对比学习嵌入模糊估计框架，根据点的模糊度设定自适应目标；随后改进为 AMContrast3D++，采用并行双分支训练，并加入模糊预测模块和掩码细化机制，使模糊嵌入更可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 S3DIS 和 ScanNet 两个室内场景数据集上实验表明，该方法显著提升了分割精度，并增强了模型鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应边缘对比学习结合模糊估计能够有效解决点云语义分割中的模糊问题，提供更稳健的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种用于点云3D语义分割的自适应边缘对比学习方法。大多数现有方法使用等惩罚目标，忽略了每个点的模糊性以及来自过渡区域的特征差异。然而，极度模糊的点即使对人类也难以区分，其人工标注标签也不够可靠，对这些点施加硬约束会导致次优模型。为了解决这一问题，我们首先设计了 AMContrast3D，它将对比学习嵌入模糊估计框架，并根据模糊程度为每个点制定自适应目标。结果，该方法促进了模型训练，确保低模糊点的正确性，同时允许高模糊点出现错误。模糊性是基于标签间位置差异来定义的，推理过程中的优化受限于假设所有未标注点均为无模糊，缺乏模糊意识。受到联合训练洞察的启发，我们进一步提出了 AMContrast3D++，将其与两个并行训练的分支集成，其中一个新颖的模糊预测模块同时学习来自生成嵌入的点模糊性。为此，我们设计了一种掩码细化机制，利用预测的模糊性使模糊嵌入更可靠，从而提升分割性能并增强鲁棒性。在 3D 室内场景数据集 S3DIS 和 ScanNet 上的实验结果证明了所提出方法的有效性。代码可在 https://github.com/YangChenApril/AMContrast3D 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注点云分割中点的模糊性，尤其是过渡区域的点难以区分。传统方法对所有点使用相同的训练难度，导致高模糊点被过度关注，低模糊点被忽视，从而影响整体分割精度。解决这一问题能提升室内场景等复杂环境下的分割性能和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有对比学习使用固定边界，忽略点级模糊性。借鉴 2D 任务中通过决策边距调节训练难度的做法，提出根据点的邻域标签分布估计模糊度，并将其映射为可调边距。随后将该边距嵌入对比损失中，并在分割网络中加入模糊预测分支，形成联合训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让每个点的对比学习边距随其模糊度自适应变化，从而在训练时对高模糊点放宽约束、对低模糊点加大约束。实现流程包括：①在编码阶段提取位置嵌入；②通过邻域标签分布计算每点的模糊度；③将模糊度映射为边距；④在解码阶段使用自适应边距对比损失正负样本；⑤在 AMContrast3D++ 中加入模糊预测分支和掩码细化机制，联合训练分割与模糊预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出基于邻域标签分布的模糊度估计与自适应边距生成；②将自适应边距嵌入对比学习，形成 AMContrast3D；③在 AMContrast3D++ 中加入轻量化模糊预测分支和掩码细化机制，实现推理时的模糊感知；④通过联合训练提升分割鲁棒性。与以往使用固定边距的对比学习或仅关注全局特征的分割方法不同，本文在点级别引入模糊感知并动态调节训练难度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种自适应边距对比学习框架，结合点级模糊度估计和联合模糊预测分支，显著提升 3D 点云分割的准确性与鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper proposes an adaptive margin contrastive learning method for 3D semantic segmentation on point clouds. Most existing methods use equally penalized objectives, which ignore the per-point ambiguities and less discriminated features stemming from transition regions. However, as highly ambiguous points may be indistinguishable even for humans, their manually annotated labels are less reliable, and hard constraints over these points would lead to sub-optimal models. To address this, we first design AMContrast3D, a method comprising contrastive learning into an ambiguity estimation framework, tailored to adaptive objectives for individual points based on ambiguity levels. As a result, our method promotes model training, which ensures the correctness of low-ambiguity points while allowing mistakes for high-ambiguity points. As ambiguities are formulated based on position discrepancies across labels, optimization during inference is constrained by the assumption that all unlabeled points are uniformly unambiguous, lacking ambiguity awareness. Inspired by the insight of joint training, we further propose AMContrast3D++ integrating with two branches trained in parallel, where a novel ambiguity prediction module concurrently learns point ambiguities from generated embeddings. To this end, we design a masked refinement mechanism that leverages predicted ambiguities to enable the ambiguous embeddings to be more reliable, thereby boosting segmentation performance and enhancing robustness. Experimental results on 3D indoor scene datasets, S3DIS and ScanNet, demonstrate the effectiveness of the proposed method. Code is available at https://github.com/YangChenApril/AMContrast3D.&lt;/p&gt;</description></item><item><guid>2507.06618v2</guid><title>PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation</title><link>http://arxiv.org/abs/2507.06618v2</link><author>Yang Chen, Yueqi Duan, Haowen Sun, Ziwei Wang, Jiwen Lu, Yap-Peng Tan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种视角依赖投影方法（VDP），用于点云分割，通过动态适应视角变化的三维到二维映射生成高信息量的单图像输入，并通过颜色正则化提升投影效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统投影方法采用视角无关投影，受限于人工预设参数，导致投影多样性不足且计算冗余。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决视角无关投影的局限性，降低计算开销，提高投影多样性与信息量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计VDP框架，从3D点分布生成数据驱动投影，预测类似烟花自适应的射线；并加入颜色正则化，突出语义像素，抑制非语义黑色像素，最大化二维空间利用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明PointVDP在S3DIS和ScanNet数据集上取得竞争性结果，且计算成本低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PointVDP提供了一种轻量化、资源高效的点云语义理解方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出视角依赖投影（VDP）以促进点云分割，设计了高效的三维到二维映射，能够根据视角变化动态适应空间几何。现有基于投影的方法在复杂场景中使用视角无关投影，依赖直线生成直接射线或向上曲线以减少遮挡。然而，它们的视角无关性导致投影射线受人工预设参数限制，限制了点的感知并无法捕捉不同视角平面上的足够投影多样性。虽然常用多视角投影来增强空间多样性，但投影冗余导致计算开销过大且图像处理效率低。为解决这些限制，我们设计了VDP框架，从3D点分布生成数据驱动投影，通过预测类似烟花自适应行为的射线产生高度信息化的单图像输入。此外，我们构建了颜色正则化来优化框架，强调语义像素中的关键特征并抑制黑色像素中的非语义特征，从而最大化投影图像中的二维空间利用。结果，我们的方法PointVDP在S3DIS和ScanNet基准上实现了竞争性结果，提供了一种资源高效的语义理解方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we propose view-dependent projection (VDP) to facilitate point cloud segmentation, designing efficient 3D-to-2D mapping that dynamically adapts to the spatial geometry from view variations. Existing projection-based methods leverage view-independent projection in complex scenes, relying on straight lines to generate direct rays or upward curves to reduce occlusions. However, their view independence provides projection rays that are limited to pre-defined parameters by human settings, restricting point awareness and failing to capture sufficient projection diversity across different view planes. Although multiple projections per view plane are commonly used to enhance spatial variety, the projected redundancy leads to excessive computational overhead and inefficiency in image processing. To address these limitations, we design a framework of VDP to generate data-driven projections from 3D point distributions, producing highly informative single-image inputs by predicting rays inspired by the adaptive behavior of fireworks. In addition, we construct color regularization to optimize the framework, which emphasizes essential features within semantic pixels and suppresses the non-semantic features within black pixels, thereby maximizing 2D space utilization in a projected image. As a result, our approach, PointVDP, develops lightweight projections in marginal computation costs. Experiments on S3DIS and ScanNet benchmarks show that our approach achieves competitive results, offering a resource-efficient solution for semantic understanding.&lt;/p&gt;</description></item><item><guid>2507.07393v3</guid><title>KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos</title><link>http://arxiv.org/abs/2507.07393v3</link><author>Jinseong Kim, Jeonghoon Song, Gyeongseon Baek, Byeongjoon Noh</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了KeyRe-ID框架，利用人体关键点指导的全局和局部分支，提升视频人重识别的时空表示学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种基于关键点的视频人重识别方法，以增强时空特征学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 全局分支通过Transformer进行时序聚合，捕捉整体身份语义；局部分支根据关键点动态划分身体区域，生成细粒度、部位感知特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在MARS和iLIDS-VID基准上，KeyRe-ID实现了91.73% mAP、97.32% Rank-1（MARS）以及96.00% Rank-1、100% Rank-5（iLIDS-VID）的最先进性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出KeyRe-ID，一种基于关键点的视频人重识别框架，包含全局和局部分支，利用人体关键点提升时空表示学习。全局分支通过基于Transformer的时序聚合捕捉整体身份语义；局部分支根据关键点动态划分身体区域，生成细粒度、部位感知特征。我们在MARS和iLIDS-VID基准上进行了广泛实验，展示了最先进的性能，在MARS上取得91.73% mAP和97.32% Rank-1准确率，在iLIDS-VID上取得96.00% Rank-1和100.0% Rank-5准确率。该工作代码将在发布后在GitHub公开。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose \textbf{KeyRe-ID}, a keypoint-guided video-based person re-identification framework consisting of global and local branches that leverage human keypoints for enhanced spatiotemporal representation learning. The global branch captures holistic identity semantics through Transformer-based temporal aggregation, while the local branch dynamically segments body regions based on keypoints to generate fine-grained, part-aware features. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate state-of-the-art performance, achieving 91.73\% mAP and 97.32\% Rank-1 accuracy on MARS, and 96.00\% Rank-1 and 100.0\% Rank-5 accuracy on iLIDS-VID. The code for this work will be publicly available on GitHub upon publication.&lt;/p&gt;</description></item><item><guid>2507.11037v1</guid><title>A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion</title><link>http://arxiv.org/abs/2507.11037v1</link><author>Jie-Wen Li, Zi-Han Ye, Qingyuan Zhou, Jiayi Song, Ying He, Ben Fei, Wen-Ming Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了FootGait3D数据集，专注于步态过程中足踝部位的三维点云捕捉，为足踝运动学分析和临床评估提供高分辨率数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 足踝运动学分析对生物力学研究和临床评估至关重要，但在步态动态条件下获取准确的足踝表面几何数据因足部遮挡和视角限制而具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个多视角、高分辨率的足踝点云数据集，以支持足踝形状补全方法的评估和比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用自定义的五摄像头深度传感系统采集46名受试者的8403帧点云，每帧包含完整的5视角重建（真值）以及仅使用4、3、2视角得到的部分点云，形成不同遮挡级别的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该数据集为单模态和多模态形状补全网络提供了基准，验证了多视角信息对恢复完整足踝几何的重要性，并为多段足模型研究提供了实验平台。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FootGait3D为足踝运动学、生物力学、假肢设计和机器人等领域提供了高质量的三维模型数据，能够推动相关技术的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; FootGait3D数据集是一个多视角、高分辨率的足踝表面点云数据集，专门用于步态分析。与现有的全身或下肢运动数据集不同，FootGait3D聚焦于足踝区域的细致建模，提供更高粒度的运动数据。该数据集包含8403帧点云，来自46名受试者，使用自定义的五摄像头深度传感系统采集。每帧包括完整的5视角重建（作为真值）以及仅使用四、三、两视角得到的部分点云。此结构化的变化使得在不同遮挡水平和视角下对3D点云补全方法进行严格评估成为可能。该数据集旨在用于形状补全任务，方便对单模态（如PointTr、SnowflakeNet、Anchorformer）和多模态（如SVDFormer、PointSea、CSDN）补全网络进行基准测试，以从遮挡输入中恢复完整足部几何。FootGait3D具有显著潜力，可推动生物力学和多段足模型研究的发展，为临床步态分析、假肢设计和需要足部三维模型的机器人应用提供有价值的测试平台。数据集已在 https://huggingface.co/datasets/ljw285/FootGait3D 上公开。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在步态过程中获取完整、准确的足踝三维点云数据的难题。由于步态时足部被另一只脚遮挡以及摄像角度受限，传统方法难以得到完整表面信息，而足踝运动学分析对临床评估、假肢设计和运动科学至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出现有数据集缺乏真实的部分-完整点云配对，随后借鉴了先前的Markerless PFA系统和多摄像头深度传感技术，设计了一个五摄像头的多视角捕捉装置，并结合现有的深度相机与点云配准方法实现数据采集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多视角深度相机获取足踝完整表面，并通过选择不同摄像头子集生成对应的部分点云，从而得到完整-部分配对数据。实现流程包括：① 采集五摄像头深度图；② 校准并将各视角点云转换到全局坐标系；③ 合并并裁剪得到完整点云；④ 通过检测足部接触事件提取步态站立阶段；⑤ 通过去除另一只脚和背景噪声得到干净的完整点云；⑥ 选取不同摄像头组合生成部分点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 提供了大规模、真实动态的足踝点云数据集；② 生成了完整-部分配对样本，满足监督式点云补全训练需求；③ 采用多视角高分辨率深度相机，覆盖足踝全表面；④ 为足踝运动学和假肢设计提供了细粒度数据。与以往仅有合成数据或全身/下肢数据集不同，FootGait3D专注于足踝细节并提供真实的缺失模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FootGait3D提供了首个大规模、真实动态的多视角足踝点云数据集，包含完整-部分配对样本，为足踝运动学分析和三维补全算法的评估与发展奠定了基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The kinematics analysis of foot-ankle complex during gait is essential for advancing biomechanical research and clinical assessment. Collecting accurate surface geometry data from the foot and ankle during dynamic gait conditions is inherently challenging due to swing foot occlusions and viewing limitations. Thus, this paper introduces FootGait3D, a novel multi-view dataset of high-resolution ankle-foot surface point clouds captured during natural gait. Different from existing gait datasets that typically target whole-body or lower-limb motion, FootGait3D focuses specifically on the detailed modeling of the ankle-foot region, offering a finer granularity of motion data. To address this, FootGait3D consists of 8,403 point cloud frames collected from 46 subjects using a custom five-camera depth sensing system. Each frame includes a complete 5-view reconstruction of the foot and ankle (serving as ground truth) along with partial point clouds obtained from only four, three, or two views. This structured variation enables rigorous evaluation of 3D point cloud completion methods under varying occlusion levels and viewpoints. Our dataset is designed for shape completion tasks, facilitating the benchmarking of state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the challenge of recovering the full foot geometry from occluded inputs. FootGait3D has significant potential to advance research in biomechanics and multi-segment foot modeling, offering a valuable testbed for clinical gait analysis, prosthetic design, and robotics applications requiring detailed 3D models of the foot during motion. The dataset is now available at https://huggingface.co/datasets/ljw285/FootGait3D.&lt;/p&gt;</description></item><item><guid>2507.14485v1</guid><title>Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion</title><link>http://arxiv.org/abs/2507.14485v1</link><author>Hongye Hou, Liu Zhan, Yang Yang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种检索增强的点云补全框架，通过跨模态检索学习结构先验信息，利用结构共享特征编码器和层次特征融合生成器，实现了对稀疏点云和未见类别的高质量补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全是挑战性任务，尤其在残留点云缺乏典型结构特征时。现有跨模态学习方法引入实例图像帮助结构特征学习，但仍局限于特定输入类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将跨模态检索融入补全任务，学习相似参考样本的结构先验信息，以提升补全效果并增强生成能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计结构共享特征编码器（SSFE）提取跨模态特征并重建参考特征作为先验；使用双通道控制门增强相关结构特征并抑制无关信息；提出渐进检索增强生成器（PRAG）采用层次特征融合机制，从全局到局部整合参考先验与输入特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多数据集和真实场景评估中，方法在生成细粒度点云方面表现出色，并在处理稀疏数据和未见类别时具有良好的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 检索增强的点云补全框架通过跨模态检索和层次特征融合，有效提升了点云补全质量，并展现出对稀疏数据和新类别的强大适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于不完整点云完成整个三维结构是一项具有挑战性的任务，尤其当残余点云缺乏典型结构特征时。最近基于跨模态学习的方法尝试引入实例图像来辅助结构特征学习。然而，它们仍然聚焦于每个特定输入类别，限制了其生成能力。在本研究中，我们提出了一种新颖的检索增强点云补全框架。核心思路是将跨模态检索融入补全任务，以从相似参考样本中学习结构先验信息。具体而言，我们设计了一个结构共享特征编码器（SSFE），用于联合提取跨模态特征并重建参考特征作为先验。借助编码器中的双通道控制门，增强参考样本中的相关结构特征并抑制无关信息干扰。此外，我们提出了一个渐进检索增强生成器（PRAG），采用层次特征融合机制，从全局到局部整合参考先验信息与输入特征。通过在多个数据集和真实场景上的广泛评估，我们的方法在生成细粒度点云方面表现出其有效性，并在处理稀疏数据和未见类别时展现出良好的泛化能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决基于不完整点云恢复完整三维结构的问题，尤其是在残留点云缺乏典型结构特征时。该问题在自动驾驶、机器人感知和三维场景理解等实际应用中至关重要，因为完整的三维模型能显著提升后续任务的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人类大脑在修复未知结构时先参考已知相似结构的启发，提出将跨模态检索与点云完成结合。方法借鉴了跨模态学习、检索增强生成（RAG）以及现有点云完成框架（如Encoder‑Decoder、Transformer等），并结合CLIP进行检索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过跨模态检索得到相似的完整点云作为参考，并在编码阶段使用SSFE与SACG门控增强相关结构特征、抑制噪声；随后在解码阶段使用PRAG从全局到局部逐层融合参考先验与输入特征，最终生成细粒度完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）检索增强的点云完成框架；2）SSFE编码器配合SACG双门控机制，能够自适应提取参考中的相关结构；3）PRAG解码器采用层次化全局‑局部融合；4）去掉检索点云的绝对位置信息，提升对姿态变化的鲁棒性。相比以往仅利用单模态或简单融合的完成方法，该方案在细节恢复和对稀疏/未知类别的泛化上表现更优。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种检索增强的跨模态点云完成框架，利用双门控编码器和层次化解码器从相似参考中提取结构先验，实现了更高质量、更具泛化能力的三维重建。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Completing the whole 3D structure based on an incomplete point cloud is a challenging task, particularly when the residual point cloud lacks typical structural characteristics. Recent methods based on cross-modal learning attempt to introduce instance images to aid the structure feature learning. However, they still focus on each particular input class, limiting their generation abilities. In this work, we propose a novel retrieval-augmented point cloud completion framework. The core idea is to incorporate cross-modal retrieval into completion task to learn structural prior information from similar reference samples. Specifically, we design a Structural Shared Feature Encoder (SSFE) to jointly extract cross-modal features and reconstruct reference features as priors. Benefiting from a dual-channel control gate in the encoder, relevant structural features in the reference sample are enhanced and irrelevant information interference is suppressed. In addition, we propose a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical feature fusion mechanism to integrate reference prior information with input features from global to local. Through extensive evaluations on multiple datasets and real-world scenes, our method shows its effectiveness in generating fine-grained point clouds, as well as its generalization capability in handling sparse data and unseen categories.&lt;/p&gt;</description></item><item><guid>2507.16743v1</guid><title>Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption</title><link>http://arxiv.org/abs/2507.16743v1</link><author>Keneni W. Tesema, Lyndon Hill, Mark W. Jones, Gary K. L. Tam</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究点云补全与去噪，提出一种新框架DWCNet，能够在噪声和遮挡严重的部分点云上实现高质量补全，并在多种数据集上取得最优表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全在自动驾驶、增强现实和机器人等领域至关重要，但真实环境中的噪声和遮挡使得获取干净完整的点云变得困难。现有网络多在合成数据上训练，难以处理真实世界的退化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在多重退化影响下的高度损坏部分点云的补全与去噪问题，并评估现有方法在此场景下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先构建了Corrupted Point Cloud Completion Dataset (CPCCD) 用于评估鲁棒性；随后提出DWCNet框架，包含噪声管理模块NMM，利用对比学习和自注意力机制抑制噪声并建模结构关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DWCNet在干净与受损、合成与真实数据集上均实现了最先进的性能，证明了噪声管理模块的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入专门的噪声管理模块，点云补全网络可以在复杂退化环境中保持高精度，DWCNet为鲁棒点云补全提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全对于自动驾驶、增强现实和机器人等3D计算机视觉任务至关重要。然而，在真实环境中获取干净完整的点云因噪声和遮挡而具有挑战性。因此，大多数现有的补全网络在合成数据上训练后，难以处理真实世界的退化。在本研究中，我们解决了在多重退化影响下高度损坏的部分点云的补全和去噪问题。为评估鲁棒性，我们引入了Corrupted Point Cloud Completion Dataset (CPCCD)，该数据集突出了当前方法在多样化退化下的局限性。在此基础上，我们提出了DWCNet（Denoising-While-Completing Network），该补全框架通过噪声管理模块（NMM）利用对比学习和自注意力来抑制噪声并建模结构关系。DWCNet在干净和受损、合成和真实数据集上均实现了最先进的性能。数据集和代码将公开发布。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在完成并去噪受多种噪声和遮挡严重破坏的部分点云。现实中，激光雷达和深度相机捕获的点云往往包含噪声、外部干扰和遮挡，导致现有仅在干净合成数据上训练的完成网络性能大幅下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先发现缺乏针对点云完成的鲁棒性基准，随后构建了 CPCCD 数据集，模拟真实环境中的多种噪声。方法借鉴了现有的点云完成网络、对比学习、以及多头自注意力和多尺度卷积等技术，整合进一个噪声管理模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将去噪与完成统一到同一网络中。流程为：输入受损点云 → 提取特征 → 噪声管理模块将特征分为干净与噪声两类 → 对干净特征进行对比学习过滤噪声 → 多头自注意力捕捉结构关系并通过多尺度卷积处理不同尺度噪声 → 输出去噪且完成的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) CPCCD 作为首个针对点云完成的鲁棒性基准；2) DWCNet 通过噪声管理模块实现去噪与完成的协同；3) 采用对比学习和自注意力来抑制多种噪声。与以往仅处理高斯噪声或在完成后单独去噪的工作不同，DWCNet 能同时应对多种真实噪声并在合成与真实数据上取得最优表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一个同时去噪和完成受多种噪声破坏的点云的鲁棒框架，并通过新构建的 CPCCD 基准验证其优越性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is crucial for 3D computer vision tasks in autonomous driving, augmented reality, and robotics. However, obtaining clean and complete point clouds from real-world environments is challenging due to noise and occlusions. Consequently, most existing completion networks -- trained on synthetic data -- struggle with real-world degradations. In this work, we tackle the problem of completing and denoising highly corrupted partial point clouds affected by multiple simultaneous degradations. To benchmark robustness, we introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which highlights the limitations of current methods under diverse corruptions. Building on these insights, we propose DWCNet (Denoising-While-Completing Network), a completion framework enhanced with a Noise Management Module (NMM) that leverages contrastive learning and self-attention to suppress noise and model structural relationships. DWCNet achieves state-of-the-art performance on both clean and corrupted, synthetic and real-world datasets. The dataset and code will be publicly available at https://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions&lt;/p&gt;</description></item><item><guid>2507.22020v1</guid><title>XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation</title><link>http://arxiv.org/abs/2507.22020v1</link><author>Raju Ningappa Mulawade, Christoph Garth, Alexander Wiebel</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于分割的可解释人工智能方法，用于点云分类网络。核心是新颖的点位移机制，用于在点云中引入扰动，从而生成易于人类理解的显著性图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着人工智能在关键领域的广泛应用，理解其决策过程变得尤为重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 生成可被人类直观解释的显著性图，以帮助分析点云分类算法的工作原理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用点云分割模型生成分段，利用点位移机制对输入点云进行扰动，进而产生显著性图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 所提出的分段更具可解释性，生成的显著性图比传统聚类方法更有意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法能够提供更易解释的显著性图，提升对点云分类模型的理解与决策支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种新颖的基于分割的可解释人工智能（XAI）方法，用于处理点云分类的神经网络。作为该方法的一个构建块，我们提出了一种新颖的点位移机制，以在点云数据中引入扰动。近年来，人工智能呈指数级增长。因此，在将人工智能算法应用于关键领域时，了解其决策过程变得重要。我们的工作聚焦于解释分类点云数据的人工智能算法。解释人工智能算法的方法的一个重要方面是它们能够生成易于人类理解的解释。这使得人们能够更好地分析人工智能算法，并根据该分析做出适当的决策。因此，在本工作中，我们打算生成易于人类解释的有意义的解释。我们考虑的点云数据代表了汽车、吉他和笔记本电脑等三维物体。我们利用点云分割模型为分类模型的工作生成解释。分段用于在输入点云数据中引入扰动并生成显著性图。扰动是使用本文提出的新颖点位移机制引入的，该机制确保位移后的点不再影响分类算法的输出。与以前的方法相比，我们的方法使用的分段是有意义的，即人类可以轻易解释分段的含义。因此，我们的方法相对于其他方法的优点在于能够生成更有意义的显著性图。我们将我们的方法与使用经典聚类算法生成解释进行比较。我们还分析了使用我们的方法为示例输入生成的显著性图，以展示该方法在生成有意义解释方面的有用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在为点云分类模型提供可解释性，解决其黑盒特性导致的决策不透明问题。点云数据在自动驾驶、机器人和工业检测等高风险领域被广泛使用，缺乏可解释性会阻碍其安全可靠部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的基于扰动的XAI方法（如SHAP、LIME、PointMask等）的基础上，发现这些方法往往使用无意义的聚类或单点扰动，难以产生易于人类理解的解释。于是他们引入了语义分割模型和新的点位移机制，构建了一个模型无关、可解释性更强的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用语义分割将点云划分为人类可识别的部件，再通过点位移扰动这些部件，观察分类器输出的变化来计算重要性。实现流程包括：① 用分类器预测原始点云类别；② 选取对应的分割模型对点云进行语义分割；③ 对分割得到的每个片段使用点位移机制产生扰动；④ 通过比较扰动前后分类器输出，生成每个片段的显著性映射。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 采用语义分割得到的有意义片段作为扰动单元；② 提出了新的点位移机制，使扰动后的点不再影响分类结果；③ 设计了两种扰动方式（保留/移除）以提供不同视角的解释；④ 对比聚类方法，证明语义分割能生成更易解释的显著性图。与以往使用无意义聚类或单点扰动的方法不同，本文的方案更贴合人类认知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种基于语义分割和点位移的扰动式XAI框架，能够为点云分类模型生成易于人类理解的显著性图，显著提升了解释的可读性和实用性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose a novel segmentation-based explainable artificial intelligence (XAI) method for neural networks working on point cloud classification. As one building block of this method, we propose a novel point-shifting mechanism to introduce perturbations in point cloud data. Recently, AI has seen an exponential growth. Hence, it is important to understand the decision-making process of AI algorithms when they are applied in critical areas. Our work focuses on explaining AI algorithms that classify point cloud data. An important aspect of the methods used for explaining AI algorithms is their ability to produce explanations that are easy for humans to understand. This allows them to analyze the AI algorithms better and make appropriate decisions based on that analysis. Therefore, in this work, we intend to generate meaningful explanations that can be easily interpreted by humans. The point cloud data we consider represents 3D objects such as cars, guitars, and laptops. We make use of point cloud segmentation models to generate explanations for the working of classification models. The segments are used to introduce perturbations into the input point cloud data and generate saliency maps. The perturbations are introduced using the novel point-shifting mechanism proposed in this work which ensures that the shifted points no longer influence the output of the classification algorithm. In contrast to previous methods, the segments used by our method are meaningful, i.e. humans can easily interpret the meaning of the segments. Thus, the benefit of our method over other methods is its ability to produce more meaningful saliency maps. We compare our method with the use of classical clustering algorithms to generate explanations. We also analyze the saliency maps generated for example inputs using our method to demonstrate the usefulness of the method in generating meaningful explanations.&lt;/p&gt;</description></item><item><guid>2507.22668v1</guid><title>Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation</title><link>http://arxiv.org/abs/2507.22668v1</link><author>Hongbin Lin, Yifan Jiang, Juangui Xu, Jesse Jiaxi Xu, Yi Lu, Zhengyu Hu, Ying-Cong Chen, Hao Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于图引导的双层约束数据增强框架，用于生成逼真的3D场景，从而提升点云分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法主要采用局部变换或语义重组的数据增强，缺乏对场景全局结构依赖的考虑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决缺乏全局结构约束的问题，提升增强数据的真实性和多样性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 从真实数据学习对象关系统计，构建引导图；局部约束保证几何可行性和语义一致性；全局约束通过与引导图对齐保持场景拓扑结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在室内外数据集上实验表明，该框架生成多样且高质量的增强场景，显著提升多种模型的点云分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 双层约束图引导的数据增强能有效提升3D点云分割效果，具有广泛应用前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云分割旨在为场景中的每个点分配语义标签，以实现细粒度空间理解。现有方法通常采用数据增强来减轻大规模标注的负担。然而，大多数增强策略仅关注局部变换或语义重组，缺乏对场景内全局结构依赖的考虑。为解决这一限制，我们提出了一个基于图引导的双层约束数据增强框架，用于生成逼真的3D场景。我们的方法从真实数据中学习对象关系统计，构建用于场景生成的引导图。局部层约束强制几何可行性和对象间语义一致性，而全局层约束通过将生成的布局与引导图对齐来保持场景的拓扑结构。在室内和室外数据集上进行的大量实验表明，我们的框架生成多样且高质量的增强场景，在各种模型上均实现了持续的点云分割性能提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 3D 点云分割中缺乏真实、结构化数据增强的问题。由于标注成本高，现有方法往往只能通过局部几何变换或简单语义插入来扩充数据，导致生成的场景缺乏全局结构一致性，影响模型在真实环境中的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统增强仅关注局部变换，忽略了场景中的全局拓扑关系，随后借鉴场景图、图神经网络以及基于统计的关系建模等已有研究，提出通过构造对象关系图（ORG）并使用 Jensen‑Shannon 散度匹配真实分布，来引导场景生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将场景拆分为背景与前景对象，构建描述对象共现与空间关系的图，然后在图的引导下生成新场景，同时通过局部几何约束（碰撞、功能关系）和全局图一致性损失（GGCL）保证生成结果既几何合理又结构一致。实现流程包括：场景分解 → 构造 ORG → 采样节点并生成条件图 → 位置与方向调整（局部约束） → 通过 GNN 对齐生成图与指导图（全局约束） → 输出增强点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 采用双层约束（局部几何 + 全局拓扑）实现更真实的场景合成；2) 使用对象关系图和 Jensen‑Shannon 散度保证采样分布与真实数据一致；3) 引入 Graph Global Constraint Loss 通过 GNN 对齐生成图与指导图，确保全局结构。与以往仅做局部变换或简单语义插入的增强方法不同，该框架同时考虑了全局拓扑和局部语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种基于场景图的双层约束增强框架，通过学习对象共现与空间关系，生成既几何合理又结构一致的 3D 点云场景，从而显著提升点云分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D point cloud segmentation aims to assign semantic labels to individual points in a scene for fine-grained spatial understanding. Existing methods typically adopt data augmentation to alleviate the burden of large-scale annotation. However, most augmentation strategies only focus on local transformations or semantic recomposition, lacking the consideration of global structural dependencies within scenes. To address this limitation, we propose a graph-guided data augmentation framework with dual-level constraints for realistic 3D scene synthesis. Our method learns object relationship statistics from real-world data to construct guiding graphs for scene generation. Local-level constraints enforce geometric plausibility and semantic consistency between objects, while global-level constraints maintain the topological structure of the scene by aligning the generated layout with the guiding graph. Extensive experiments on indoor and outdoor datasets demonstrate that our framework generates diverse and high-quality augmented scenes, leading to consistent improvements in point cloud segmentation performance across various models.&lt;/p&gt;</description></item><item><guid>2508.00259v1</guid><title>PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting</title><link>http://arxiv.org/abs/2508.00259v1</link><author>Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 提出 PointGauss 框架，实现实时多目标分割；2. 通过点云驱动的高效解码器在一分钟内生成 3D 实例掩码；3. GPU 加速的 2D 掩码渲染保证多视角一致性；4. 在多视角 mIoU 上比现有方法提升 1.89% 至 31.78%；5. 同时保持更高的计算效率；6. 引入 DesktopObjects-360 数据集，解决单目标、评估不一致、规模小等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法在 Gaussian Splatting 表示中进行多目标分割时，往往需要较长的初始化时间，且多视角一致性不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 实现一种高效、实时的多目标分割框架，兼顾速度与准确性，并提供更完善的评估数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 基于点云的 Gaussian 原语解码器，可在一分钟内生成 3D 实例掩码；2) GPU 加速的 2D 掩码渲染系统，确保多视角一致性；3) 直接通过点云分割驱动的管线解析 Gaussian 原语，实现高效 3D 分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，PointGauss 在多视角 mIoU 上比之前的最先进方法提升 1.89% 至 31.78%，同时保持更优的计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PointGauss 成功实现了实时、多目标的 Gaussian Splatting 分割，并通过 DesktopObjects-360 数据集为未来研究提供了更全面的评估平台。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 PointGauss，一种新颖的点云引导框架，用于在高斯喷射表示中实现实时多目标分割。与现有方法相比，后者在初始化时间长且多视角一致性有限的情况下，PointGauss 通过点云分割驱动的管线直接解析高斯原语，实现了高效的 3D 分割。其核心创新体现在两个方面：一是基于点云的高斯原语解码器，能够在一分钟内生成 3D 实例掩码；二是 GPU 加速的 2D 掩码渲染系统，确保多视角一致性。大量实验表明，与之前的最先进方法相比，PointGauss 在多视角 mIoU 上提升了 1.89% 至 31.78%，同时保持了卓越的计算效率。为了解决当前基准测试的局限性（单目标聚焦、3D 评估不一致、小规模、部分覆盖），我们提出了 DesktopObjects-360，一个用于高斯辐射场 3D 分割的新型综合数据集，包含：复杂的多目标场景、全局一致的 2D 注释、超过 27000 张 2D 掩码的大规模训练数据、完整的 360° 覆盖以及 3D 评估掩码。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现对 Gaussian Splatting 场景的实时多物体分割，解决现有方法在初始化慢、跨视角一致性差以及对 3D 结构利用不足等问题。该问题在增强现实、机器人导航等需要快速、准确 3D 语义理解的应用中至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有基于 2D Mask 或 SAM 的 3DGS 分割方法的局限，随后借鉴点云分割技术，提出直接在 Gaussian 原语上进行点云分割的思路。方法中融合了 Prompt Encoder、Gaussian Decoder 和 GPU 加速的 Splatting Projection，构成了端到端的分割流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 Gaussian 原语视为点云，利用点云分割网络生成实例标签，再通过可微分渲染将标签投影到任意视角。实现流程包括：①构建 Gaussian 模型；②将用户点击转化为空间特征并与 Gaussian 属性融合；③使用 PointTransformerV3 对融合后的点云进行语义/实例分割；④通过 GPU 加速的 splatting 渲染得到一致的 2D 掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①点云引导的 Gaussian 原语解码器，可在一分钟内生成 3D 实例掩码；②GPU 加速的 2D 掩码渲染实现多视角一致性；③提出 DesktopObjects‑360 大规模、360° 覆盖的评测基准。与以往依赖 2D 迁移或多阶段对齐的工作不同，PointGauss 直接利用 3D 结构，简化了网络架构并显著提升了速度和一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointGauss 通过在 Gaussian 原语上直接进行点云分割，实现了实时、多视角一致的 3D 多物体分割，并在新基准 DesktopObjects‑360 上显著优于现有方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce PointGauss, a novel point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations. Unlike existing methods that suffer from prolonged initialization and limited multi-view consistency, our approach achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline. The key innovation lies in two aspects: (1) a point cloud-based Gaussian primitive decoder that generates 3D instance masks within 1 minute, and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view consistency. Extensive experiments demonstrate significant improvements over previous state-of-the-art methods, achieving performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency. To address the limitations of current benchmarks (single-object focus, inconsistent 3D evaluation, small scale, and partial coverage), we present DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in radiance fields, featuring: (1) complex multi-object scenes, (2) globally consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D masks), (4) full 360° coverage, and (5) 3D evaluation masks.&lt;/p&gt;</description></item><item><guid>2508.03614v1</guid><title>Minimal Convolutional RNNs Accelerate Spatiotemporal Learning</title><link>http://arxiv.org/abs/2508.03614v1</link><author>Coşku Can Horuz, Sebastian Otte, Martin V. Butz, Matthias Karlbauer</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 介绍了两种新的时空模型 MinConvLSTM 和 MinConvGRU，结合卷积递归网络的空间先验和最小化可并行 RNN 的训练效率，提供了更快、更小参数的时空序列建模方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统 ConvRNN 在教师强迫期间需要顺序隐藏状态更新，导致训练瓶颈；最小化 RNN 通过 log 域前缀和实现并行训练，但尚未应用于卷积架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将最小化 RNN 的 log 域前缀和扩展到卷积网络，并在 MinConvLSTM 中加入指数门控机制，以进一步简化计算，构建高效的时空模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用 MinLSTM/MinGRU 的 log 域前缀和公式，改写为卷积形式，实现全并行训练；在 MinConvLSTM 中加入受 xLSTM 启发的指数门控；保持结构最小化，减少参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 Navier-Stokes 动力学和地球势能数据的两项时空预测任务中，MinConvLSTM/GRU 在训练速度上显著优于标准 ConvLSTM/GRU，并在闭环自回归模式下也取得更低的预测误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 最小化递归结构与卷积输入聚合相结合，为时空序列建模提供了既简单又高效的替代方案，弥合了递归简洁性与空间复杂性之间的差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 MinConvLSTM 和 MinConvGRU 两种新型时空模型，它们将卷积递归网络的空间先验与最小化、可并行 RNN 的训练效率相结合。我们的方法将 MinLSTM 和 MinGRU 的 log 域前缀和公式推广到卷积架构，实现了完全并行训练，同时保留了局部空间建模能力。这消除了传统 ConvRNN 在教师强迫期间对顺序隐藏状态更新的需求——这是一个主要瓶颈。此外，我们在 MinConvLSTM 中加入了受 xLSTM 架构启发的指数门控机制，进一步简化了 log 域计算。我们的模型在结构上保持最小化，计算上高效，参数量减少，扩展性更好。我们在两个时空预测任务上评估了模型：Navier-Stokes 动力学和真实世界的地球势能数据。在训练速度方面，我们的架构显著优于标准 ConvLSTM 和 ConvGRU。更重要的是，即使在闭环自回归模式下，我们的模型在两个领域也取得了更低的预测误差。这些发现表明，最小化递归结构与卷积输入聚合相结合，为时空序列建模提供了一个既有吸引力又高效的替代方案，弥合了递归简洁性与空间复杂性之间的差距。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce MinConvLSTM and MinConvGRU, two novel spatiotemporal models that combine the spatial inductive biases of convolutional recurrent networks with the training efficiency of minimal, parallelizable RNNs. Our approach extends the log-domain prefix-sum formulation of MinLSTM and MinGRU to convolutional architectures, enabling fully parallel training while retaining localized spatial modeling. This eliminates the need for sequential hidden state updates during teacher forcing - a major bottleneck in conventional ConvRNN models. In addition, we incorporate an exponential gating mechanism inspired by the xLSTM architecture into the MinConvLSTM, which further simplifies the log-domain computation. Our models are structurally minimal and computationally efficient, with reduced parameter count and improved scalability. We evaluate our models on two spatiotemporal forecasting tasks: Navier-Stokes dynamics and real-world geopotential data. In terms of training speed, our architectures significantly outperform standard ConvLSTMs and ConvGRUs. Moreover, our models also achieve lower prediction errors in both domains, even in closed-loop autoregressive mode. These findings demonstrate that minimal recurrent structures, when combined with convolutional input aggregation, offer a compelling and efficient alternative for spatiotemporal sequence modeling, bridging the gap between recurrent simplicity and spatial complexity.&lt;/p&gt;</description></item><item><guid>2508.04705v1</guid><title>Occupancy Learning with Spatiotemporal Memory</title><link>http://arxiv.org/abs/2508.04705v1</link><author>Ziyang Leng, Jiawei Yang, Wenlong Yi, Bolei Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为ST-Occ的场景级占位符表示学习框架，旨在高效聚合多帧输入的3D占位符信息，提升时间一致性和预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D占位符在自动驾驶中被视为细粒度环境建模的有前景的感知表示，但在多帧时间上聚合其信息面临高处理成本、体素不确定性和动态变化等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决多帧3D占位符聚合效率低、时间不一致性高的问题，构建能够捕捉时空特征并保持时间一致性的表示学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ST-Occ包含两大核心设计：1）时空记忆模块，用场景级表示高效存储历史信息；2）记忆注意力模块，利用不确定性和动态感知模型将当前占位符表示与时空记忆关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明ST-Occ在3D占位符预测任务中显著提升时空表示，mIoU提升3分，时间不一致性降低29%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过利用多帧输入的时间依赖性，ST-Occ在占位符预测上优于现有方法，证明了时空记忆与注意力机制的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在高效地将多帧输入的 3D 占据信息进行时空融合，解决占据体素在时间维度上的高计算成本、噪声不确定性和动态位移问题。3D 占据是自动驾驶感知的细粒度环境表示，能够更好地捕捉周围物体的几何与语义信息，对安全与决策至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有基于队列、递归或堆叠的时空融合方法在 3D 占据中的内存与计算瓶颈，并指出它们未充分考虑不确定性与动态位移。随后借鉴 BEV 时空建模（如 BEVFormer、BEVDet）和占据预测的相关工作（OccNet、FB-OCC、FlashOcc 等），提出在场景中心坐标系下构建统一的时空记忆，并通过记忆注意力结合不确定性与流信息实现融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用场景级的时空记忆来存储历史占据特征及其不确定性和流信息，并通过记忆注意力将当前帧的占据表示与记忆中的历史信息进行条件融合。实现流程为：多视角图像 → 视角编码器得到 ego‑centered 占据 V_t → 记忆注意力利用记忆 M_t 采样得到 H_t 并与 V_t 融合得到 V~_t → 用 V~_t 及其属性更新记忆 M_{t+1} → 输出占据预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 统一时空建模范式：在场景中心坐标系下仅使用一个记忆而非 k 个帧的队列，显著降低内存与计算开销；2) 时空记忆包含历史类别激活、方差与占据流，提供不确定性与动态信息；3) 记忆注意力将当前占据与记忆条件化，提升时空一致性。与以往仅在 ego‑centered 维度堆叠或递归融合的做法不同，ST‑Occ 在效率、鲁棒性和精度上均取得显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ST‑Occ 通过场景级时空记忆与记忆注意力，实现高效、鲁棒的多帧 3D 占据融合，显著提升精度与时空一致性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.&lt;/p&gt;</description></item><item><guid>2508.05531v1</guid><title>Point cloud segmentation for 3D Clothed Human Layering</title><link>http://arxiv.org/abs/2508.05531v1</link><author>Davide Garavaso, Federico Masi, Pietro Musoni, Umberto Castellani</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的3D点云分割范式——穿着人类分层，允许同一点同时属于多层，从而实现对底层身体和被遮挡服装区域的估计，并通过合成数据集和多种神经网络设置验证了该方法在合成与真实扫描数据上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D服装建模与仿真在时尚、娱乐和动画等领域至关重要，但实现高质量结果因穿着体形的巨大变异和逼真皱纹的生成而具有挑战性；3D扫描采集能提供更高精度，却缺乏可推断的语义信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建可靠的语义重建管线，解决穿着体形建模中形状分割的不足，提出能够处理多层重叠的分割方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出每个3D点可同时关联多层的分割范式；创建合成数据集提供服装层的真值；设计并评估多种神经网络设置，涵盖粗粒度和细粒度的每层服装识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，在合成和真实扫描数据集上引入适当的服装域分割策略能够显著提升分割与重建效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 穿着人类分层范式及其配套数据集为3D服装建模提供了有效的语义分割与重建方案，神经网络设置在该领域具有显著优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 服装建模与仿真对于角色创建在时尚、娱乐和动画等领域至关重要。实现高质量结果具有挑战性，主要因为穿着体形的巨大变异，尤其是逼真皱纹的生成。3D 扫描采集能更准确地表示现实世界对象，但缺乏可通过可靠语义重建管线推断的语义信息。为此，形状分割在识别语义形状部件方面起着关键作用。然而，目前的 3D 形状分割方法主要用于场景理解和解释，只有少数工作关注建模。在穿着体形建模的背景下，分割是完全语义形状部件重建的前置步骤，即底层身体和涉及的服装。这些部件代表多层并具有强重叠，与标准分割方法提供的互斥集合形成对比。本研究提出一种新的 3D 点云分割范式，每个 3D 点可同时关联到不同层。通过这种方式，我们可以估计底层身体部件和未被上层服装遮挡的服装区域。我们将此分割范式命名为“穿着人类分层”。我们创建了一个新的合成数据集，模拟非常逼真的 3D 扫描，并提供涉及服装层的真值。我们提出并评估了不同的神经网络设置，以处理 3D 服装分层。我们考虑了粗粒度和细粒度的每层服装识别。实验表明，在合成和真实扫描数据集上引入适当的服装域分割策略具有优势。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从3D扫描得到的穿着人体点云中同时识别可见和被遮挡的多层服装以及底层身体的分割问题。该问题对服装建模、虚拟试衣和动画等领域至关重要，因为它直接影响到服装的真实感和后续的几何重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的点云分割网络（如PointNet++、DGCNN、Point Transformer）基础上，提出将每个点的标签从单一类别扩展为多维向量，以表示多层服装。为实现这一思路，他们构建了一个包含真实扫描噪声的合成数据集，并参考了之前的服装分割与点云分割研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是“clothed human layering”，即为每个点预测一个包含身体和所有穿着服装层的多维标签。实现流程包括：①使用结构光扫描仿真器生成带噪声的点云；②利用原始服装网格投影得到多层标注；③训练改进的点云分割网络，使其输出多维标签；④在合成和真实扫描数据上评估粗细分割效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 提出了多层分割范式，允许单点同时属于多个服装层；2) 公开了一个包含多层标注的高质量合成扫描数据集；3) 将现有点云分割网络改造为向量化输出。与以往只产生互斥分割的工作不同，该方法能够恢复被遮挡的服装部分和底层身体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种多层点云分割框架和对应的合成数据集，使得从3D扫描中同时识别可见与被遮挡的服装层及底层身体成为可能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Cloth modeling and simulation is essential for avatars creation in several fields, such as fashion, entertainment, and animation. Achieving high-quality results is challenging due to the large variability of clothed body especially in the generation of realistic wrinkles. 3D scan acquisitions provide more accuracy in the representation of real-world objects but lack semantic information that can be inferred with a reliable semantic reconstruction pipeline. To this aim, shape segmentation plays a crucial role in identifying the semantic shape parts. However, current 3D shape segmentation methods are designed for scene understanding and interpretation and only few work is devoted to modeling. In the context of clothed body modeling the segmentation is a preliminary step for fully semantic shape parts reconstruction namely the underlying body and the involved garments. These parts represent several layers with strong overlap in contrast with standard segmentation methods that provide disjoint sets. In this work we propose a new 3D point cloud segmentation paradigm where each 3D point can be simultaneously associated to different layers. In this fashion we can estimate the underlying body parts and the unseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer above. We name this segmentation paradigm clothed human layering. We create a new synthetic dataset that simulates very realistic 3D scans with the ground truth of the involved clothing layers. We propose and evaluate different neural network settings to deal with 3D clothing layering. We considered both coarse and fine grained per-layer garment identification. Our experiments demonstrates the benefit in introducing proper strategies for the segmentation on the garment domain on both the synthetic and real-world scan datasets.&lt;/p&gt;</description></item><item><guid>2508.13073v2</guid><title>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</title><link>http://arxiv.org/abs/2508.13073v2</link><author>Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本综述系统梳理了基于大型视觉语言模型（VLM）的视觉-语言-动作（VLA）模型在机器人操控中的应用，提出了两大架构范式（单体与分层），并探讨了其与强化学习、无训练优化、人类视频学习、世界模型等高级领域的融合，归纳了模型特征、优势与数据集，展望了记忆机制、4D感知、低成本适配、多智能体协作等未来方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 机器人操控需要精准的运动控制和多模态理解，但传统基于规则的方法在非结构化、未知环境中难以扩展和泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 首次以分类学为导向，对大型VLM驱动的VLA模型在机器人操控领域进行系统综述，厘清研究现状与空白。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①定义大型VLM驱动的VLA模型；②划分单体（单系统/双系统）与分层（规划与执行分离）两大架构；③深入分析其与强化学习、无训练优化、人类视频学习、世界模型等高级领域的融合；④总结模型特征、优势、数据集与基准；⑤提出记忆机制、4D感知、低成本适配、多智能体协作等前沿方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通过系统整合，消除了现有分类学的不一致性，减少了研究碎片化，填补了大型VLM与机器人操控交叉领域的空白，并为后续研究提供了清晰的技术路线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 综述整合了最新进展，提供了持续更新的项目页面（https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation），为该交叉领域的研究者提供了参考与资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 机器人操控是机器人学和具身人工智能的关键前沿领域，需要精准的运动控制和多模态理解，但传统基于规则的方法在非结构化、未知环境中难以扩展和泛化。近年来，基于大型视觉语言模型（VLM）预训练的大规模图像-文本数据集的视觉-语言-动作（VLA）模型出现，成为一种变革性范式。本综述首次以分类学为导向，对基于大型VLM的VLA模型在机器人操控中的应用进行系统综述。我们首先明确了大型VLM驱动的VLA模型的定义，并划分了两大主要架构范式：①单体模型，包括单系统和双系统设计，具有不同程度的集成；②分层模型，显式地通过可解释的中间表示将规划与执行分离。基于此基础，我们深入考察了大型VLM驱动的VLA模型：①与强化学习、无训练优化、人类视频学习、世界模型等高级领域的融合；②对其独特特征的综合，整合了架构特征、操作优势以及支持其发展的数据集和基准；③识别了有前景的方向，包括记忆机制、4D感知、低成本适配、多智能体协作以及其他新兴能力。本综述整合了最新进展，解决了现有分类学的不一致性，减少了研究碎片化，并通过系统整合大型VLM与机器人操控交叉研究填补了关键空白。我们提供了一个持续更新的项目页面，以记录持续进展：https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Robotic manipulation, a key frontier in robotics and embodied AI, requires precise motor control and multimodal understanding, yet traditional rule-based methods fail to scale or generalize in unstructured, novel environments. In recent years, Vision-Language-Action (VLA) models, built upon Large Vision-Language Models (VLMs) pretrained on vast image-text datasets, have emerged as a transformative paradigm. This survey provides the first systematic, taxonomy-oriented review of large VLM-based VLA models for robotic manipulation. We begin by clearly defining large VLM-based VLA models and delineating two principal architectural paradigms: (1) monolithic models, encompassing single-system and dual-system designs with differing levels of integration; and (2) hierarchical models, which explicitly decouple planning from execution via interpretable intermediate representations. Building on this foundation, we present an in-depth examination of large VLM-based VLA models: (1) integration with advanced domains, including reinforcement learning, training-free optimization, learning from human videos, and world model integration; (2) synthesis of distinctive characteristics, consolidating architectural traits, operational strengths, and the datasets and benchmarks that support their development; (3) identification of promising directions, including memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation, and other emerging capabilities. This survey consolidates recent advances to resolve inconsistencies in existing taxonomies, mitigate research fragmentation, and fill a critical gap through the systematic integration of studies at the intersection of large VLMs and robotic manipulation. We provide a regularly updated project page to document ongoing progress: https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;/p&gt;</description></item><item><guid>2508.19806v1</guid><title>Context-aware Sparse Spatiotemporal Learning for Event-based Vision</title><link>http://arxiv.org/abs/2508.19806v1</link><author>Shenqi Wang, Guangzhi Tang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于上下文感知的稀疏时空学习框架，能够在事件摄像机视觉任务中实现高稀疏度和优异性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 事件摄像机因其高时间分辨率、高动态范围和抗运动模糊的特性，已成为机器人感知的有前景的技术。然而，现有深度学习方法未能充分利用事件数据的稀疏性，难以在资源受限的边缘设备上部署；而脉冲神经网络在复杂事件视觉任务中仍难以与最先进模型匹敌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够在不显式稀疏约束的情况下，自适应调节神经元激活的框架，以提升事件视觉任务的效率和性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 Context-aware Sparse Spatiotemporal Learning (CSSL)，通过上下文感知阈值动态调节激活，基于输入分布自然降低激活密度，从而实现高稀疏度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在事件对象检测和光流估计任务中，CSSL 的性能与最先进方法相当或更优，同时保持极高的神经元稀疏度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CSSL 为神经形态处理的事件视觉提供了高效的解决方案，证明了上下文感知阈值在实现稀疏高效网络中的关键作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Event-based camera has emerged as a promising paradigm for robot perception, offering advantages with high temporal resolution, high dynamic range, and robustness to motion blur. However, existing deep learning-based event processing methods often fail to fully leverage the sparse nature of event data, complicating their integration into resource-constrained edge applications. While neuromorphic computing provides an energy-efficient alternative, spiking neural networks struggle to match the performance of state-of-the-art models in complex event-based vision tasks, like object detection and optical flow. Moreover, achieving high activation sparsity in neural networks is still difficult and often demands careful manual tuning of sparsity-inducing loss terms. Here, we propose Context-aware Sparse Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware thresholding to dynamically regulate neuron activations based on the input distribution, naturally reducing activation density without explicit sparsity constraints. Applied to event-based object detection and optical flow estimation, CSSL achieves comparable or superior performance to state-of-the-art methods while maintaining extremely high neuronal sparsity. Our experimental results highlight CSSL&amp;#x27;s crucial role in enabling efficient event-based vision for neuromorphic processing.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Event-based camera has emerged as a promising paradigm for robot perception, offering advantages with high temporal resolution, high dynamic range, and robustness to motion blur. However, existing deep learning-based event processing methods often fail to fully leverage the sparse nature of event data, complicating their integration into resource-constrained edge applications. While neuromorphic computing provides an energy-efficient alternative, spiking neural networks struggle to match of performance of state-of-the-art models in complex event-based vision tasks, like object detection and optical flow. Moreover, achieving high activation sparsity in neural networks is still difficult and often demands careful manual tuning of sparsity-inducing loss terms. Here, we propose Context-aware Sparse Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware thresholding to dynamically regulate neuron activations based on the input distribution, naturally reducing activation density without explicit sparsity constraints. Applied to event-based object detection and optical flow estimation, CSSL achieves comparable or superior performance to state-of-the-art methods while maintaining extremely high neuronal sparsity. Our experimental results highlight CSSL&amp;#x27;s crucial role in enabling efficient event-based vision for neuromorphic processing.&lt;/p&gt;</description></item><item><guid>2508.19909v1</guid><title>Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</title><link>http://arxiv.org/abs/2508.19909v1</link><author>Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用二维基础模型生成的分割掩码来最大化稀疏三维标注的利用率，并通过几何对应将二维掩码传播到三维空间，从而显著扩充可用标签并提升弱监督三维语义分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 三维点云数据难以大规模标注，现有方法多仅关注三维域，且对稀疏标注或伪标签的利用不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在有限的三维标注条件下，充分利用二维基础模型的强大分割能力，提升三维弱监督分割效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①使用二维基础模型生成分割掩码；②通过建立三维场景与二维视图的几何对应，将二维掩码投射到三维空间；③将稀疏三维标注扩展到掩码覆盖区域；④对三维点云做增强后应用置信度与不确定性一致性正则化，筛选可靠伪标签并进一步扩散到三维掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该策略显著增加了可用标签数量，弥补了三维标注不足，并通过一致性正则化提升了模型的鲁棒性，最终提升了三维弱监督分割的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将二维基础模型与几何对应相结合，可有效解决三维标注稀缺问题，显著提升弱监督三维语义分割效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 当前针对三维语义分割的方法提出使用有限标注训练模型，以解决标注大型、形状不规则且无序的三维点云数据的难题。它们通常仅关注三维域，未能利用二维与三维数据的互补性。此外，一些方法扩展原始标签或生成伪标签来指导训练，但往往未能充分利用这些标签或解决其中的噪声。与此同时，全面且可适应的基础模型的出现为二维数据分割提供了有效解决方案。借助这一进展，我们提出了一种新方法，通过结合二维基础模型生成的分割掩码，最大化稀疏可用三维标注的利用率。我们进一步通过建立三维场景与二维视图之间的几何对应，将二维分割掩码传播到三维空间。我们将高度稀疏的标注扩展到三维掩码所划定的区域，从而大幅增加可用标签池。此外，我们在三维点云的增强上应用基于置信度和不确定性的“一致性正则化”，并选择可靠的伪标签，再将其进一步传播到三维掩码以生成更多标签。这一创新策略弥合了有限三维标注与二维基础模型强大能力之间的差距，最终提升了三维弱监督分割的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在3D点云分割中，如何在仅有稀疏标注的情况下获得高质量分割结果。由于3D点云数据难以大规模标注，弱监督方法可以显著降低人工成本。通过有效利用有限标注并结合2D图像信息，提升分割性能具有重要的实际和科研意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了2D基础模型（如Semantic‑SAM）在图像分割上的强大能力，并结合已有的弱监督3D方法（如PointMatch、RAC‑Net、OTOC、ActiveST）和噪声鲁棒学习技术。思路是先用2D模型生成分割掩码，再将其投影到3D空间，扩展稀疏标注，并通过一致性正则化筛选可靠伪标签，最终使用噪声鲁棒损失进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型生成的分割掩码来丰富3D点云的标签信息。实现流程包括：1）用Semantic‑SAM生成每个视角的2D掩码；2）将掩码投影到3D空间并融合多视角掩码；3）将稀疏3D标注扩展到掩码区域；4）通过一致性正则化得到可靠伪标签并传播到3D掩码；5）使用噪声鲁棒的归一化损失训练模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①将2D基础模型掩码投影到3D并完整融合多视角信息；②利用一致性正则化筛选可靠伪标签并在3D掩码上扩展；③采用噪声鲁棒归一化损失处理投影掩码中的噪声。与之前工作相比，它充分利用了2D掩码在3D空间的潜力，而非仅在2D平面使用，并通过伪标签扩展显著提升了可用标签量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种弱监督3D点云分割框架，利用2D基础模型掩码投影到3D、扩展稀疏标注并通过一致性正则化筛选伪标签，最终使用噪声鲁棒损失实现了状态‑最优的分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Current methods for 3D semantic segmentation propose training models with limited annotations to address the difficulty of annotating large, irregular, and unordered 3D point cloud data. They usually focus on the 3D domain only, without leveraging the complementary nature of 2D and 3D data. Besides, some methods extend original labels or generate pseudo labels to guide the training, but they often fail to fully use these labels or address the noise within them. Meanwhile, the emergence of comprehensive and adaptable foundation models has offered effective solutions for segmenting 2D data. Leveraging this advancement, we present a novel approach that maximizes the utility of sparsely available 3D annotations by incorporating segmentation masks generated by 2D foundation models. We further propagate the 2D segmentation masks into the 3D space by establishing geometric correspondences between 3D scenes and 2D views. We extend the highly sparse annotations to encompass the areas delineated by 3D masks, thereby substantially augmenting the pool of available labels. Furthermore, we apply confidence- and uncertainty-based consistency regularization on augmentations of the 3D point cloud and select the reliable pseudo labels, which are further spread on the 3D masks to generate more labels. This innovative strategy bridges the gap between limited 3D annotations and the powerful capabilities of 2D foundation models, ultimately improving the performance of 3D weakly supervised segmentation.&lt;/p&gt;</description></item><item><guid>2508.20135v1</guid><title>Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads</title><link>http://arxiv.org/abs/2508.20135v1</link><author>Andrew Yarovoi, Christopher R. Valenta</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了一种数据高效的点云分割管线和训练框架，能够在仅有少量标注数据的情况下，对未改造道路及七类其他目标实现鲁棒分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在低数据场景下，3D语义分割的泛化能力差，传统方法需要大量标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过多数据集预训练和轻量化微调，提升在目标域的分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用两阶段训练：先用投影卷积网络在公共城市数据集与少量域内数据上预训练；随后仅在域内数据上微调轻量预测头；同时引入点提示训练、Manifold Mixup 正则化以及直方图归一化环境增强。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仅使用50个标注点云，所提方法将平均交并比从33.5%提升至51.8%，整体准确率从85.5%提升至90.8%；表明跨数据集预训练是提升泛化的关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在低数据、挑战性环境下提供了实用且鲁棒的3D语义分割方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在本案例研究中，我们提出了一种数据高效的点云分割管线和训练框架，用于对未改造道路和另外七个类别进行鲁棒分割。我们的方法采用两阶段训练框架：首先，使用基于投影的卷积神经网络在公共城市数据集和少量精心策划的域内数据上进行预训练；然后，使用轻量级预测头仅在域内数据上进行微调。在此过程中，我们探讨了将点提示训练应用于批归一化层以及在管线中使用流形混合作为正则化的效果。我们还探讨了引入直方图归一化环境以进一步提升性能的影响。仅使用目标域的50个标注点云，我们展示了所提出的训练方法将平均交并比从33.5%提升至51.8%，整体准确率从85.5%提升至90.8%，与在域内数据上进行简单训练相比。关键的是，我们的结果表明，在多个数据集上进行预训练是提升泛化并在有限域内监督下实现鲁棒分割的关键。本研究总体上展示了在挑战性、低数据场景下实现鲁棒3D语义分割的实用框架。我们的代码可在 https://github.com/andrewyarovoi/MD-FRNet 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在稀缺标注数据下，对乡村、碎石、森林小径等非改良道路环境进行高质量点云语义分割的问题。由于手工标注耗时且昂贵，传统方法需要成千上万的扫描，而实际应用往往只能获得几十个扫描，缺乏数据的情况下仍需实现可靠的道路识别，对自动驾驶、基础设施检测等领域具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将任务视为少样本学习（Few‑Shot Learning）问题，借鉴了图像领域的迁移学习和度量学习思想，并结合点云专属技术如Point Prompt Training（PPT）和Manifold Mixup（MM）。他们先在大规模公共数据集上预训练特征提取器，再用少量目标域数据微调轻量化分类头，并通过数据增强、环境归一化等手段提升泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是两阶段训练：①预训练阶段，用FRNet等投影卷积网络在Semantic KITTI、Waymo等多域数据上学习通用特征；②微调阶段，冻结特征提取器，仅训练一个小型多层感知机（MLP）作为预测头，仅使用目标域的50个标注扫描。整个流程包括数据统一格式、增强（强度、环境、直方图归一化、旋转）、PPT应用于批归一化、MM正则化以及最终评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出针对低数据点云分割的实用两阶段管线；②首次将PPT迁移到卷积网络而非仅限Transformer；③在分割头中使用MM正则化；④利用直方图归一化的环境信息提升性能；⑤在仅50扫描的条件下实现显著IoU提升。与以往仅关注图像或单物体分类的少样本方法不同，该工作针对大规模场景分割，并在投影CNN框架中实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 论文提出一种两阶段、数据高效的点云语义分割管线，结合多域预训练、PPT、MM和环境归一化，在仅50个标注扫描的情况下显著提升乡村道路环境的分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this case study, we present a data-efficient point cloud segmentation pipeline and training framework for robust segmentation of unimproved roads and seven other classes. Our method employs a two-stage training framework: first, a projection-based convolutional neural network is pre-trained on a mixture of public urban datasets and a small, curated in-domain dataset; then, a lightweight prediction head is fine-tuned exclusively on in-domain data. Along the way, we explore the application of Point Prompt Training to batch normalization layers and the effects of Manifold Mixup as a regularizer within our pipeline. We also explore the effects of incorporating histogram-normalized ambients to further boost performance. Using only 50 labeled point clouds from our target domain, we show that our proposed training approach improves mean Intersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5% to 90.8%, when compared to naive training on the in-domain data. Crucially, our results demonstrate that pre-training across multiple datasets is key to improving generalization and enabling robust segmentation under limited in-domain supervision. Overall, this study demonstrates a practical framework for robust 3D semantic segmentation in challenging, low-data scenarios. Our code is available at: https://github.com/andrewyarovoi/MD-FRNet.&lt;/p&gt;</description></item><item><guid>2509.01997v1</guid><title>ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting</title><link>http://arxiv.org/abs/2509.01997v1</link><author>Jiacheng Shi, Haibin Wei, Jiang Wang, Xiaowei Xu, Longzhi Du, Taixu Jiang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用两张图（正在图和全局图）进行时空学习的模型，用于预测按需外卖平台的物流供需压力，显著优于传统长序列方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 按需外卖平台的物流供需预测对效率和质量至关重要，需评估预期供给与需求的匹配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 改进未来订单分布信息的学习，以提升物流供需预测的准确性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建包含正在图和全局图的图学习网络框架，采用自适应未来图学习和交叉注意机制ACA-Net，提取未来订单分布信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 引入两张图显著提升预测性能；自适应图学习和交叉注意机制有效提取信息；在真实生产环境中验证了方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型在实际环境中表现优异，可作为按需外卖平台物流供需预测的有效工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 物流供需预测评估预期供给与预期需求的匹配，对按需外卖平台的效率和质量至关重要，也是调度决策的关键指标。未来订单分布信息反映了按需外卖平台订单的分布，对物流供需预测的表现至关重要。现有研究利用时空分析方法从严谨的时间切片中建模未来订单分布信息。然而，在在线配送平台学习未来订单分布是一个对时间序列不敏感且随机性强的问题。这些方法往往难以在保持效率的同时有效捕捉这些信息。本文提出一种创新的时空学习模型，仅使用两张图（正在图和全局图）来学习未来订单分布信息，取得了比传统时空长序列方法更优的性能。主要贡献包括：1）在物流供需压力预测中引入正在图和全局图，相比传统长时间序列显著提升预测性能；2）提出一种创新的图学习网络框架，使用自适应未来图学习和创新的交叉注意机制（ACA-Net）提取未来订单分布信息，有效学习稳健的未来图，显著提升物流供需压力预测结果；3）在真实生产环境中验证了所提方法的有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Logistical demand-supply forecasting that evaluates the alignment between projected supply and anticipated demand, is essential for the efficiency and quality of on-demand food delivery platforms and serves as a key indicator for scheduling decisions. Future order distribution information, which reflects the distribution of orders in on-demand food delivery, is crucial for the performance of logistical demand-supply forecasting. Current studies utilize spatial-temporal analysis methods to model future order distribution information from serious time slices. However, learning future order distribution in online delivery platform is a time-series-insensitive problem with strong randomness. These approaches often struggle to effectively capture this information while remaining efficient. This paper proposes an innovative spatiotemporal learning model that utilizes only two graphs (ongoing and global) to learn future order distribution information, achieving superior performance compared to traditional spatial-temporal long-series methods. The main contributions are as follows: (1) The introduction of ongoing and global graphs in logistical demand-supply pressure forecasting compared to traditional long time series significantly enhances forecasting performance. (2) An innovative graph learning network framework using adaptive future graph learning and innovative cross attention mechanism (ACA-Net) is proposed to extract future order distribution information, effectively learning a robust future graph that substantially improves logistical demand-supply pressure forecasting outcomes. (3) The effectiveness of the proposed method is validated in real-world production environments.&lt;/p&gt;</description></item><item><guid>2509.08280v1</guid><title>Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration</title><link>http://arxiv.org/abs/2509.08280v1</link><author>Hyeonseok Kim, Byeongkeun Kang, Yeejin Lee</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种新的方法E3DPC-GZSL，用于3D点云的泛化零样本语义分割，解决了模型对已见类别过度自信的偏差问题，并在ScanNet v2和S3DIS数据集上取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在3D点云语义分割中，模型往往倾向于已训练过的类别，导致对未见类别的预测不准确，尤其在训练数据规模较小的3D任务中更为突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过改进不确定性估计和训练策略，降低对已见类别的过度自信，提高对未见类别的识别准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 将基于证据的不确定性估计器嵌入分类器；2) 使用动态校准堆叠因子根据点级不确定性调整预测概率；3) 通过将可学习参数与文本特征融合来细化语义空间，从而提升不确定性估计和模型优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明E3DPC-GZSL在ScanNet v2和S3DIS等数据集上实现了最优的泛化零样本语义分割效果，显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过整合证据不确定性估计和语义空间细化，E3DPC-GZSL有效缓解了3D点云分割中已见类别的过度自信问题，为泛化零样本语义分割提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该摘要的中文翻译：泛化零样本3D点云语义分割旨在将每个点分类为已见和未见类别。现有模型往往偏向训练时出现的类别，尤其在3D任务中更为明显。我们提出了E3DPC-GZSL方法，通过将基于证据的不确定性估计器嵌入分类器，并使用动态校准堆叠因子调整预测概率，来降低对已见类别的过度自信。同时，采用新的训练策略，将可学习参数与文本特征融合，细化语义空间，从而提升对未见数据的优化。实验表明，该方法在ScanNet v2和S3DIS数据集上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云的通用零样本语义分割中出现的过度自信偏向已见类别的问题。该问题在自动驾驶、医疗成像等需要对未知物体进行精确分割的实际场景中尤为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已知的二分类与校准堆叠方法，借鉴了证据理论的置信度估计和先前的零样本生成技术，提出在分类器中嵌入基于证据的不确定性估计器，并通过动态校准因子自适应调整概率。该设计在保持无超参数的同时，兼顾了对已见与未见类别的统一处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用证据‑基础的不确定性估计来动态校准每个点的预测概率，从而抑制对已见类别的过度自信。实现流程分为三阶段：①训练编码器提取特征；②训练解码器在文本嵌入和场景语义的条件下合成特征；③训练分类器与不确定性估计器，使用动态校准因子调整最终分割结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①无超参数的动态校准因子；②将证据理论与分类器耦合的置信度估计；③通过可学习的场景语义向量细化文本嵌入，提升合成特征质量；④在训练阶段使用解码器生成未见类别特征，弥补数据稀缺。与以往需要固定校准因子或分离已见/未见分类器的方法不同，E3DPC‑GZSL实现了统一、可自适应的推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; E3DPC‑GZSL通过证据‑基础的动态校准和语义细化的特征合成，消除了通用零样本3D点云分割中的过度自信偏差，实现了无超参数、统一的高性能分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generalized zero-shot semantic segmentation of 3D point clouds aims to classify each point into both seen and unseen classes. A significant challenge with these models is their tendency to make biased predictions, often favoring the classes encountered during training. This problem is more pronounced in 3D applications, where the scale of the training data is typically smaller than in image-based tasks. To address this problem, we propose a novel method called E3DPC-GZSL, which reduces overconfident predictions towards seen classes without relying on separate classifiers for seen and unseen data. E3DPC-GZSL tackles the overconfidence problem by integrating an evidence-based uncertainty estimator into a classifier. This estimator is then used to adjust prediction probabilities using a dynamic calibrated stacking factor that accounts for pointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel training strategy that improves uncertainty estimation by refining the semantic space. This is achieved by merging learnable parameters with text-derived features, thereby improving model optimization for unseen data. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on generalized zero-shot semantic segmentation datasets, including ScanNet v2 and S3DIS.&lt;/p&gt;</description></item><item><guid>2509.08799v1</guid><title>Unidimensional semi-discrete partial optimal transport</title><link>http://arxiv.org/abs/2509.08799v1</link><author>Adrien Cances, Hugo Leclerc</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究一维部分最优传输的半离散形式，并提出一种基于辅助维度加厚的正则化方法，证明其收敛率为二次，并给出数值方案验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 部分最优传输在风险管理、人群运动建模以及点云配准等领域有重要应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决一维部分最优传输中双重函数正则性不足的问题，并提升数值稳定性与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过在辅助维度加厚密度实现正则化，分析正则化双重问题的极大值收敛，并设计利用该正则化函数的数值算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 正则化极大值以二次速率收敛到原问题极大值；数值实验验证了这一收敛率；与全离散方法相比，半离散方法更稳定、更高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该正则化策略在一维部分传输问题中既保证了理论收敛，又提升了计算性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; We study the semi-discrete formulation of one-dimensional partial optimal transport with quadratic cost, where a probability density is partially transported to a finite sum of Dirac masses of smaller total mass. This problem arises naturally in applications such as risk management, the modeling of crowd motion, and sliced partial transport algorithms for point cloud registration. Unlike higher-dimensional settings, the dual functional in the unidimensional case exhibits reduced regularity. To overcome this difficulty, we introduce a regularization procedure based on thickening the density along an auxiliary dimension. We prove that the maximizers of the regularized dual problem converge to those of the original dual problem, with quadratic rate in the introduced thickness. We further provide a numerical scheme that leverages the regularized functional, and we validate our analysis with simulations that confirm the quadratic convergence rate. Finally, we compare the semi-discrete and fully discrete settings, demonstrating that our approach offers both improved stability and computational efficiency for unidimensional partial transport problems.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We study the semi-discrete formulation of one-dimensional partial optimal transport with quadratic cost, where a probability density is partially transported to a finite sum of Dirac masses of smaller total mass. This problem arises naturally in applications such as risk management, the modeling of crowd motion, and sliced partial transport algorithms for point cloud registration. Unlike higher-dimensional settings, the dual functional in the unidimensional case exhibits reduced regularity. To overcome this difficulty, we introduce a regularization procedure based on thickening the density along an auxiliary dimension. We prove that the maximizers of the regularized dual problem converge to those of the original dual problem, with quadratic rate in the introduced thickness. We further provide a numerical scheme that leverages the regularized functional, and we validate our analysis with simulations that confirm the quadratic convergence rate. Finally, we compare the semi-discrete and fully discrete settings, demonstrating that our approach offers both improved stability and computational efficiency for unidimensional partial transport problems.&lt;/p&gt;</description></item><item><guid>2509.08982v1</guid><title>iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning</title><link>http://arxiv.org/abs/2509.08982v1</link><author>Karim Slimani, Catherine Achard, Brahim Tamadazte</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; iMatcher 是一种完全可微分的点云特征匹配框架，利用学习到的特征预测几何一致的置信矩阵，结合局部和全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准需要高精度的特征匹配，传统方法受限于局部匹配误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种同时考虑局部图嵌入和全局几何一致性的匹配方法，以提升刚性配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先使用局部图嵌入模块初始化分数矩阵；随后通过双向最近邻搜索重新定位并细化矩阵；最后将配对特征堆叠并通过全局几何一致性学习得到点级匹配概率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 KITTI、KITTI-360、3DMatch、TUD-L、MVP-RG 等数据集上，iMatcher 在入模率和配准精度上均超过现有方法，最高入模率达 97%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; iMatcher 在多种室外、室内和姿态估计任务中表现出色，证明了局部与全局一致性结合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper presents iMatcher, a fully differentiable framework for feature matching in point cloud registration. The proposed method leverages learned features to predict a geometrically consistent confidence matrix, incorporating both local and global consistency. First, a local graph embedding module leads to an initialization of the score matrix. A subsequent repositioning step refines this matrix by considering bilateral source-to-target and target-to-source matching via nearest neighbor search in 3D space. The paired point features are then stacked together to be refined through global geometric consistency learning to predict a point-wise matching probability. Extensive experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch) datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial matching (MVP-RG), demonstrate that iMatcher significantly improves rigid registration performance. The method achieves state-of-the-art inlier ratios, scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch, highlighting its robustness across diverse settings.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决了在部分重叠点云中准确匹配点对以提升配准精度的问题，这对自动驾驶、机器人导航和三维重建等领域至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了图卷积网络、加权SVD预对齐、双向最近邻匹配以及受 LightGlue 启发的全局一致性学习，借鉴了 GeoTransformer、Diffusion 模型和 Sinkhorn 等现有技术，构建了一个全微分的匹配框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用图卷积提取局部几何特征生成初始匹配分数，再通过加权SVD预对齐源点云并进行双向最近邻匹配，随后堆叠匹配对特征并学习全局一致性以得到匹配概率，最后将局部分数与全局匹配概率融合得到最终软分配矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 全微分的局部到全局一致性学习框架；2) 通过加权SVD和双向最近邻实现的重定位步骤；3) 受 LightGlue 启发的全局一致性模块；4) 通过融合局部分数与全局匹配概率生成置信矩阵，避免了迭代 Sinkhorn。与以往方法相比，它不依赖昂贵的 transformer 或迭代优化，且在多数据集上实现了更高的内点比例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; iMatcher 提出了一种全微分的局部到全局几何一致性学习框架，显著提升点云配准的匹配准确性，并在多种真实数据集上实现了领先的内点比例。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents iMatcher, a fully differentiable framework for feature matching in point cloud registration. The proposed method leverages learned features to predict a geometrically consistent confidence matrix, incorporating both local and global consistency. First, a local graph embedding module leads to an initialization of the score matrix. A subsequent repositioning step refines this matrix by considering bilateral source-to-target and target-to-source matching via nearest neighbor search in 3D space. The paired point features are then stacked together to be refined through global geometric consistency learning to predict a point-wise matching probability. Extensive experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch) datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial matching (MVP-RG), demonstrate that iMatcher significantly improves rigid registration performance. The method achieves state-of-the-art inlier ratios, scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch, highlighting its robustness across diverse settings.&lt;/p&gt;</description></item><item><guid>2509.10156v3</guid><title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title><link>http://arxiv.org/abs/2509.10156v3</link><author>Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew A. Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi S. M. Sajjadi, Joao Carreira</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LayerLock 是一种自监督视觉表征学习方法，通过逐步冻结层实现从像素到潜在预测的渐进过渡。它利用 ViT 层在训练中按深度收敛的观察，按明确时间表冻结模型，从而加速标准 MAE 并避免表征坍塌。该方法在多达 4B 参数的大型模型上应用，结果优于非潜在掩码预测，尤其在 4DS 感知套件上表现突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自监督视觉表征学习，特别是视频掩码自编码（MAE）模型和 Vision Transformer（ViT）层的训练行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 加速标准 MAE 训练，并提供一种可扩展的潜在预测方法，避免表征坍塌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 按层深度收敛顺序逐步冻结 ViT 层，使用明确的时间表在整个训练过程中冻结模型，并将同一时间表用于潜在预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该冻结时间表能加速 MAE 并实现无表征坍塌的潜在预测；LayerLock 在 4DS 感知套件上对 4B 参数模型的表现优于非潜在掩码预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LayerLock 是一种简单、有效且可扩展的自监督视觉表征学习方法，能够提升大模型的性能并避免表征坍塌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 LayerLock，一种简单而有效的自监督视觉表征学习方法，通过逐步冻结层实现从像素到潜在预测的渐进过渡。首先，我们观察到在视频掩码自编码（MAE）模型训练过程中，ViT 层的收敛顺序与其深度相关：浅层先收敛，深层后收敛。随后，我们展示了如何利用这一观察结果，通过在整个训练过程中按明确的时间表逐步冻结模型来加速标准 MAE。更进一步，这一相同的时间表可用于一种简单且可扩展的潜在预测方法，且不会出现“表征坍塌”。我们将 LayerLock 应用于多达 4B 参数的大型模型，并在 4DS 感知套件上取得了超过非潜在掩码预测的结果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce LayerLock, a simple yet effective approach for self-supervised visual representation learning, that gradually transitions from pixel to latent prediction through progressive layer freezing. First, we make the observation that during training of video masked-autoencoding (MAE) models, ViT layers converge in the order of their depth: shallower layers converge early, deeper layers converge late. We then show that this observation can be exploited to accelerate standard MAE by progressively freezing the model according to an explicit schedule, throughout training. Furthermore, this same schedule can be used in a simple and scalable approach to latent prediction that does not suffer from &amp;quot;representation collapse&amp;quot;. We apply our proposed approach, LayerLock, to large models of up to 4B parameters with results surpassing those of non-latent masked prediction on the 4DS perception suite.&lt;/p&gt;</description></item><item><guid>2509.10842v1</guid><title>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</title><link>http://arxiv.org/abs/2509.10842v1</link><author>Chongyu Wang, Kunlei Jing, Jihua Zhu, Di Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了OpenUrban3D，一种针对大规模城市点云的开词汇语义分割框架，能够在没有多视角图像、预训练网络或人工标注的情况下，实现对任意文本查询的零样本分割，并在城市场景中表现出更高的分割精度和更好的跨场景泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 开词汇语义分割使模型能够识别并分割来自任意自然语言描述的对象，适用于数字孪生、智慧城市管理和城市分析等应用。然而，在大规模城市点云中缺乏高质量、多视角图像且现有3D分割方法在不同城市环境中的泛化能力差，导致该技术在此领域尚未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决缺乏多视角图像和预训练网络的限制，构建一种能够在大规模城市点云中实现零样本、开词汇语义分割的框架，并提升分割精度与跨场景泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过多视角、多粒度渲染生成鲁棒语义特征，使用基于掩码的视觉-语言特征提取和样本平衡融合，再将结果蒸馏到3D骨干网络，实现对原始点云的直接特征学习，并支持任意文本查询的零样本分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在SensatUrban和SUM等大规模城市基准上，OpenUrban3D在分割准确率和跨场景泛化方面显著优于现有方法，验证了其在3D城市场景理解中的可行性与优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenUrban3D为大规模城市点云提供了一种灵活、可扩展的开词汇语义分割解决方案，能够在缺乏多视角图像和人工标注的情况下实现高精度、零样本分割，为城市数字化和智慧城市建设提供了重要技术支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Open-vocabulary semantic segmentation enables models to recognize and segment objects from arbitrary natural language descriptions, offering the flexibility to handle novel, fine-grained, or functionally defined categories beyond fixed label sets. While this capability is crucial for large-scale urban point clouds that support applications such as digital twins, smart city management, and urban analytics, it remains largely unexplored in this domain. The main obstacles are the frequent absence of high-quality, well-aligned multi-view imagery in large-scale urban point cloud datasets and the poor generalization of existing three-dimensional (3D) segmentation pipelines across diverse urban environments with substantial variation in geometry, scale, and appearance. To address these challenges, we present OpenUrban3D, the first 3D open-vocabulary semantic segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud segmentation networks, or manual annotations. Our approach generates robust semantic features directly from raw point clouds through multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model. This design enables zero-shot segmentation for arbitrary text queries while capturing both semantic richness and geometric priors. Extensive experiments on large-scale urban benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves significant improvements in both segmentation accuracy and cross-scene generalization over existing methods, demonstrating its potential as a flexible and scalable solution for 3D urban scene understanding.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现对大规模城市点云的开放词汇语义分割，而不需要对齐的多视角图像、预训练的点云分割网络或人工标注。该问题重要，因为城市数字孪生、智慧城市管理和城市分析等应用需要能够识别和分割任意自然语言描述的对象，尤其是新颖或细粒度的类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了2D开放词汇分割、Mask-Clip、SAM等技术，提出了多视角多粒度投影来生成信息丰富的渲染图像，并使用预训练的视觉-语言模型提取掩码级特征。随后通过将这些2D特征投影回点云并进行样本平衡融合，利用知识蒸馏训练3D骨干网络，使其与2D特征对齐。该思路在保持开放词汇能力的同时，克服了缺乏高质量图像和大规模标注的限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先从原始点云生成多视角、多粒度的渲染图像，利用视觉-语言模型提取掩码特征，再将这些特征映射回点云并融合，最后通过知识蒸馏让3D骨干学习与2D特征一致的表示。实现流程包括：1）多视角多粒度投影生成图像；2）使用VLM提取掩码特征；3）投影回点云并进行样本平衡融合得到2D特征库；4）用该特征库蒸馏训练3D骨干；5）推理时融合2D和3D特征，并与文本嵌入计算相似度得到开放词汇分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①完全无标注、无对齐图像的开放词汇点云分割框架；②多视角多粒度投影生成高质量渲染图像；③掩码级视觉-语言特征提取与投影回点云；④样本平衡融合与知识蒸馏实现3D特征对齐；⑤实现零样本分割并在大规模城市数据上表现出色。与之前的工作相比，OpenUrban3D不依赖预训练的点云分割网络或高质量图像，能够处理尺度差异大、遮挡严重的城市场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenUrban3D提供了一种完全无标注、无对齐图像的开放词汇点云分割方法，通过多视角投影、掩码特征提取和知识蒸馏，实现了在大规模城市点云上的零样本语义分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Open-vocabulary semantic segmentation enables models to recognize and segment objects from arbitrary natural language descriptions, offering the flexibility to handle novel, fine-grained, or functionally defined categories beyond fixed label sets. While this capability is crucial for large-scale urban point clouds that support applications such as digital twins, smart city management, and urban analytics, it remains largely unexplored in this domain. The main obstacles are the frequent absence of high-quality, well-aligned multi-view imagery in large-scale urban point cloud datasets and the poor generalization of existing three-dimensional (3D) segmentation pipelines across diverse urban environments with substantial variation in geometry, scale, and appearance. To address these challenges, we present OpenUrban3D, the first 3D open-vocabulary semantic segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud segmentation networks, or manual annotations. Our approach generates robust semantic features directly from raw point clouds through multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model. This design enables zero-shot segmentation for arbitrary text queries while capturing both semantic richness and geometric priors. Extensive experiments on large-scale urban benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves significant improvements in both segmentation accuracy and cross-scene generalization over existing methods, demonstrating its potential as a flexible and scalable solution for 3D urban scene understanding.&lt;/p&gt;</description></item><item><guid>2509.12595v1</guid><title>DisorientLiDAR: Physical Attacks on LiDAR-based Localization</title><link>http://arxiv.org/abs/2509.12595v1</link><author>Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, Wanpeng Shao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对基于 LiDAR 的定位系统的对抗攻击框架 DisorientLiDAR，利用逆向工程定位模型识别关键点并有针对性地移除，从而破坏点云配准和车辆定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度学习模型易受视觉上不可察觉的扰动攻击，尤其对自动驾驶车辆的定位安全构成威胁，但针对 LiDAR 的攻击研究较少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索并验证对 LiDAR 基础定位的有效对抗攻击方法，并评估其在真实系统中的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过逆向工程特征提取网络，定位关键点，删除包含 Top-K 关键点的区域；在 KITTI 数据集上对 HRegNet、D3Feat、GeoTransformer 进行实验；在 Autoware 平台上验证定位漂移；在物理世界中使用近红外吸收材料遮挡关键区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 删除包含 Top-K 关键点的区域显著降低点云配准精度；在 Autoware 中仅遮挡少数关键区域即可导致明显定位漂移；物理遮挡实验成功复制了数据集中的攻击效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DisorientLiDAR 能有效破坏 LiDAR 定位，攻击在仿真和物理环境均可实现，表明该方法具有真实性和普适性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Deep learning models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Even this poses a serious security challenge for the localization of self-driving cars, there has been very little exploration of attack on it, as most of adversarial attacks have been applied to 3D perception. In this work, we propose a novel adversarial attack framework called DisorientLiDAR targeting LiDAR-based localization. By reverse-engineering localization models (e.g., feature extraction networks), adversaries can identify critical keypoints and strategically remove them, thereby disrupting LiDAR-based localization. Our proposal is first evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and GeoTransformer) using the KITTI dataset. Experimental results demonstrate that removing regions containing Top-K keypoints significantly degrades their registration accuracy. We further validate the attack&amp;#x27;s impact on the Autoware autonomous driving platform, where hiding merely a few critical regions induces noticeable localization drift. Finally, we extended our attacks to the physical world by hiding critical regions with near-infrared absorptive materials, thereby successfully replicate the attack effects observed in KITTI data. This step has been closer toward the realistic physical-world attack that demonstrate the veracity and generality of our proposal.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在揭示并评估一种可在物理世界中实施的攻击方式，该攻击通过隐藏关键的 LiDAR 可见区域来破坏基于深度学习的定位系统。定位是自动驾驶车辆安全的核心功能，若被攻击会导致车辆误定位、路径规划错误，进而产生严重安全风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到现有的点云配准网络高度依赖于局部几何特征（关键点）的匹配。基于此，他们逆向工程目标模型，识别出对配准贡献最大的关键点，并提出用近红外吸收材料遮挡这些区域来实现攻击。该思路借鉴了对感知模块的对抗攻击研究，但在定位领域提出了新的物理实现方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过物理遮挡关键点区域，使 LiDAR 无法获取这些重要几何信息，从而迫使配准网络只能使用次优匹配导致定位误差。实现流程包括：① 用与目标车辆相同的网络复制模型提取两帧点云中的高置信度关键点；② 对关键点按置信度排序，挑选前 K 个；③ 在物理环境中用近红外吸收材料遮挡对应区域；④ 评估遮挡后对三种主流配准模型和 Autoware 平台的定位误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①首次提出无需物理接触 LiDAR 的关键点隐藏攻击；②在三种先进的点云配准网络和真实 AV 平台上验证攻击效果；③实现了基于近红外吸收材料的物理遮挡方案；④提出了对抗训练和异常检测两种防御思路。与以往需要硬件注入或直接操纵激光的攻击不同，DisorientLiDAR 通过简单的遮挡即可实现高效破坏。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DisorientLiDAR 证明了通过物理遮挡少量关键 LiDAR 区域即可可靠破坏深度学习定位系统，揭示了自动驾驶车辆安全的新漏洞。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Deep learning models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Even this poses a serious security challenge for the localization of self-driving cars, there has been very little exploration of attack on it, as most of adversarial attacks have been applied to 3D perception. In this work, we propose a novel adversarial attack framework called DisorientLiDAR targeting LiDAR-based localization. By reverse-engineering localization models (e.g., feature extraction networks), adversaries can identify critical keypoints and strategically remove them, thereby disrupting LiDAR-based localization. Our proposal is first evaluated on three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and GeoTransformer) using the KITTI dataset. Experimental results demonstrate that removing regions containing Top-K keypoints significantly degrades their registration accuracy. We further validate the attack&amp;#x27;s impact on the Autoware autonomous driving platform, where hiding merely a few critical regions induces noticeable localization drift. Finally, we extended our attacks to the physical world by hiding critical regions with near-infrared absorptive materials, thereby successfully replicate the attack effects observed in KITTI data. This step has been closer toward the realistic physical-world attack that demonstrate the veracity and generality of our proposal.&lt;/p&gt;</description></item><item><guid>2509.12924v2</guid><title>MATTER: Multiscale Attention for Registration Error Regression</title><link>http://arxiv.org/abs/2509.12924v2</link><author>Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forssén</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于回归的点云配准质量验证方法，利用多尺度特征提取和注意力聚合，能够更细粒度地量化配准误差，并在多样化数据集上实现准确、鲁棒的误差估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是许多下游任务（如SLAM和目标跟踪）的关键步骤，检测和量化配准误差（即配准质量验证）因此成为重要任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将配准质量验证从传统的分类任务转为回归任务，以实现更细粒度的误差量化，并通过改进特征提取提升估计精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用回归模型对配准误差进行预测，扩展了以往的误差相关特征，使用多尺度提取和基于注意力的聚合方式来构建更丰富的特征表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在多种数据集上，尤其是空间密度不均匀的点云中，能够实现准确且稳健的误差估计；并且在指导后续映射任务时，显著提升了在相同重配准帧数下的映射质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于回归的配准质量验证方法在精度和鲁棒性上优于现有的分类方法，并能有效提升下游映射任务的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准（PCR）对于许多下游任务至关重要，例如同步定位与地图构建（SLAM）和目标跟踪。这使得检测和量化配准失配，即PCR质量验证，成为一项重要任务。现有的所有方法都将验证视为分类任务，旨在将PCR质量分配到少数几个类别。本文我们改用回归进行PCR验证，允许对配准质量进行更细粒度的量化。我们还通过多尺度提取和基于注意力的聚合扩展了先前使用的失配相关特征。这在多样化数据集上实现了准确且稳健的配准误差估计，尤其适用于空间密度不均匀的点云。此外，当用于指导下游映射任务时，我们的方法在给定数量的重配准帧下显著提升了映射质量，优于最先进的基于分类的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在精确估计点云配准后的对齐误差，即检测配准误差。该问题重要，因为配准误差会在 SLAM、地图构建和机器人导航等后续任务中累积，导致地图失真和定位失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有方法将误差检测视为分类任务，粗略划分误差等级，缺乏细粒度评估。于是他们将任务转化为回归问题，并借鉴了 FACT、CorAl 等工作中的特征提取和网络结构，进一步引入多尺度特征和注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多尺度几何特征并通过注意力自适应加权，随后利用点变换器和 MLP 回归对齐误差。实现流程包括：对齐后采样锚点，计算不同半径下的熵、Sinkhorn 散度和覆盖率等特征；用注意力网络为每个尺度分配权重；将加权特征送入点变换器和回归头，输出误差估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 将误差检测从分类改为回归，提供连续误差估计；2) 引入多尺度注意力机制，自动选择合适的邻域尺度；3) 在多样化数据集上实现更高的精度和鲁棒性；4) 在地图构建任务中显著提升重定位质量。与之前的工作相比，MATTER 通过注意力融合多尺度特征，避免了单尺度或粗分类的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATTER 提出一种多尺度注意力回归框架，能够精确估计点云配准误差，并在多种场景下优于传统分类方法，显著提升后续地图构建质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e., PCR quality validation, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.&lt;/p&gt;</description></item><item><guid>2509.13692v1</guid><title>HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion</title><link>http://arxiv.org/abs/2509.13692v1</link><author>Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; HGACNet是一种通过层次化图注意力编码和单视RGB图像引导的先验融合，实现完整点云重建的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云不完整会影响机器人感知、物体重建以及抓取、避障等下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出HGACNet以解决因自遮挡和传感器限制导致的几何缺失问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用层次化图注意力编码器自适应选择关键局部点并逐层细化几何特征；通过多尺度跨模态融合模块对几何特征与视觉表示进行注意力对齐；采用对比损失对齐跨模态特征分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ShapeNet-ViPC和YCB-Complete数据集上实验表明，HGACNet达到或超过现有最优性能，并在真实机器人操作中表现出良好适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; HGACNet通过层次化编码和跨模态融合显著提升点云完成质量，具有实际应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云完成对于机器人感知、物体重建以及抓取规划、避障和操作等下游任务至关重要。然而，由于自遮挡和传感器限制导致的几何不完整会显著降低下游推理和交互的效果。为解决这些挑战，我们提出了HGACNet，一种通过层次化编码3D几何特征并与单视RGB图像的图像引导先验融合来重建单个物体完整点云的新框架。我们的核心方法是层次化图注意力（HGA）编码器，它通过基于图注意力的下采样自适应选择关键局部点，并逐步细化层次化几何特征，以更好地捕捉结构连续性和空间关系。为加强跨模态交互，我们进一步设计了多尺度跨模态融合（MSCF）模块，该模块通过注意力对齐层次化几何特征与结构化视觉表示，实现细粒度语义指导完成。除此之外，我们提出了对比损失（C-Loss）来显式对齐跨模态特征分布，在模态差异下提升完成精度。最后，在ShapeNet-ViPC基准和YCB-Complete数据集上进行的大量实验验证了HGACNet的有效性，展示了其在实时机器人操作任务中的先进性能和强大适用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从不完整的点云中恢复完整三维形状的问题。由于传感器遮挡、噪声和视角限制，实际点云往往缺失重要几何信息，导致机器人抓取、路径规划等任务的性能下降。完整的点云能显著提升这些应用的可靠性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到单模态方法难以捕捉细节，随后结合图注意力和视觉先验，设计了分层的图注意力编码器和多尺度跨模态融合模块。该思路借鉴了现有的图网络、Swin Transformer、跨模态注意力以及对比学习等技术，并在此基础上提出了新的层次化融合与对比损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次化的图注意力编码器提取点云的全局与局部特征，并利用多尺度跨模态注意力将这些几何特征与来自单视RGB图像的视觉特征对齐，最后用对比损失进一步缩小模态差距。实现流程为：输入部分点云和RGB图像 → HGA编码器得到全局/局部点云特征；Swin Transformer提取图像特征 → MSCF模块进行自注意力、跨注意力和跨模态注意力融合 → 对齐特征后通过解码器生成完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) HGA编码器采用图注意力下采样，能够自适应选择关键点并保留结构连续性；2) MSCF模块实现多尺度的跨模态注意力融合，既保留全局语义又细化局部细节；3) 引入对比损失显式对齐模态特征，减少模态差异。与以往早期融合或全局注意力密集的方案不同，HGACNet在层次化层面高效对齐模态，显著提升重建精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HGACNet通过层次化图注意力编码和多尺度跨模态注意力融合，并结合对比损失，实现了基于RGB引导的高精度点云完成，取得了跨模态点云完成的最新性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is essential for robotic perception, object reconstruction and supporting downstream tasks like grasp planning, obstacle avoidance, and manipulation. However, incomplete geometry caused by self-occlusion and sensor limitations can significantly degrade downstream reasoning and interaction. To address these challenges, we propose HGACNet, a novel framework that reconstructs complete point clouds of individual objects by hierarchically encoding 3D geometric features and fusing them with image-guided priors from a single-view RGB image. At the core of our approach, the Hierarchical Graph Attention (HGA) encoder adaptively selects critical local points through graph attention-based downsampling and progressively refines hierarchical geometric features to better capture structural continuity and spatial relationships. To strengthen cross-modal interaction, we further design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs attention-based feature alignment between hierarchical geometric features and structured visual representations, enabling fine-grained semantic guidance for completion. In addition, we proposed the contrastive loss (C-Loss) to explicitly align the feature distributions across modalities, improving completion fidelity under modality discrepancy. Finally, extensive experiments conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset confirm the effectiveness of HGACNet, demonstrating state-of-the-art performance as well as strong applicability in real-world robotic manipulation tasks.&lt;/p&gt;</description></item><item><guid>2509.14574v2</guid><title>Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark</title><link>http://arxiv.org/abs/2509.14574v2</link><author>Rashid Mushkani</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文通过构建一个包含100张蒙特利尔街景图像的基准数据集，评估视觉语言模型在城市感知任务中的表现，并探讨模型与人类评估的一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市设计与规划需要了解人们如何感知城市景观。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证视觉语言模型在城市感知任务中的能力，并提供可复现的评估工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 收集12名社区成员填写的230份注释表，涵盖30个维度；使用七个视觉语言模型在零样本设置下进行评估；采用准确率和Jaccard重叠度衡量模型表现；用Krippendorff α和配对Jaccard评估人类一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在可见的客观属性上表现更好，主观评价上表现较弱；最佳模型在多标签项上宏观准确率为0.31，平均Jaccard为0.48；人类一致性越高，模型得分越好；合成图像的得分略低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 视觉语言模型在城市感知任务中具有一定的可解释性，但仍需改进对主观评价的捕捉；提供的基准和工具可支持参与式城市分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 了解人们如何阅读城市场景可以为设计和规划提供信息。我们提出了一个小型基准，用于测试视觉语言模型（VLM）在城市感知方面的表现，使用100张蒙特利尔街景图像，照片与逼真合成场景各占一半。来自七个社区团体的12名参与者提供了230份注释表，涵盖30个维度，混合了物理属性和主观印象。法语回答被标准化为英语。我们在零样本设置下使用结构化提示和确定性解析器评估了七个VLM。对于单选项目使用准确率，对于多标签项目使用Jaccard重叠度；人类一致性使用Krippendorff α和配对Jaccard。结果表明，模型在可见的客观属性上与人类更一致，而在主观评价上则较弱。最佳系统（claude-sonnet）在多标签项目上宏观准确率为0.31，平均Jaccard为0.48。人类一致性越高，模型得分越好。合成图像的得分略低。我们发布了基准、提示和工具，以实现可复现、考虑不确定性的参与式城市分析评估。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding how people read city scenes can inform design and planning. We introduce a small benchmark for testing vision-language models (VLMs) on urban perception using 100 Montreal street images, evenly split between photographs and photorealistic synthetic scenes. Twelve participants from seven community groups supplied 230 annotation forms across 30 dimensions mixing physical attributes and subjective impressions. French responses were normalized to English. We evaluated seven VLMs in a zero-shot setup with a structured prompt and deterministic parser. We use accuracy for single-choice items and Jaccard overlap for multi-label items; human agreement uses Krippendorff&amp;#x27;s alpha and pairwise Jaccard. Results suggest stronger model alignment on visible, objective properties than subjective appraisals. The top system (claude-sonnet) reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human agreement coincides with better model scores. Synthetic images slightly lower scores. We release the benchmark, prompts, and harness for reproducible, uncertainty-aware evaluation in participatory urban analysis.&lt;/p&gt;</description></item><item><guid>2509.15882v1</guid><title>Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration</title><link>http://arxiv.org/abs/2509.15882v1</link><author>Xingmei Wang, Xiaoyu Hu, Chengkai Huang, Ziyan Zeng, Guohao Nie, Quan Z. Sheng, Lina Yao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个名为CrossI2P的自监督框架，用于将二维图像与三维点云进行配准。该框架通过双路径对比学习构建几何-语义融合嵌入空间，并采用粗到细的两阶段配准策略，最终在KITTI和nuScenes数据集上显著提升了配准精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶等自主系统中，融合二维图像与三维点云是实现稳健感知的关键，但由于图像纹理丰富但深度不确定，点云稀疏但精确，传统的图像到点云配准面临语义几何差距和局部最优问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服现有方法在语义几何差距和局部最优收敛方面的局限，提出一种统一的跨模态学习与两阶段配准的端到端自监督框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 通过双路径对比学习学习几何-语义融合嵌入空间，实现无标注的双向对齐；2) 采用粗到细配准：全局阶段通过联合模态内上下文与跨模态交互建立超点-超像素对应；细化阶段在几何约束下进行点级精细配准；3) 使用动态训练机制和梯度归一化平衡特征对齐、对应细化和位姿估计的损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CrossI2P在KITTI Odometry基准上比最先进方法提升23.7%，在nuScenes提升37.9%，在准确性和鲁棒性方面均有显著改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CrossI2P通过跨模态学习与两阶段配准的结合，显著解决了图像-点云配准中的语义几何差距和局部最优问题，为自动驾驶等领域提供了更可靠的感知方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 跨模态的二维图像与三维点云的桥接对于自动驾驶系统的稳健感知至关重要。然而，由于纹理丰富但深度不确定的图像与稀疏但精确的点云之间存在语义几何差距，以及现有方法易陷入局部最优的倾向，图像到点云（I2P）配准仍然具有挑战性。为克服这些限制，我们提出了CrossI2P，一个自监督框架，将跨模态学习和两阶段配准统一到一个端到端管道中。首先，我们通过双路径对比学习学习几何-语义融合嵌入空间，实现无标注的双向对齐二维纹理与三维结构。其次，我们采用粗到细的配准范式：全局阶段通过联合模态内上下文和跨模态交互建模，建立超点-超像素对应；随后进行几何约束的点级细化，以实现精确配准。第三，我们采用动态训练机制和梯度归一化，平衡特征对齐、对应细化和位姿估计的损失。大量实验表明，CrossI2P在KITTI Odometry基准上比最先进方法提升23.7%，在nuScenes提升37.9%，显著提高了准确性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决图像与点云之间的配准问题，即在没有预先标定的情况下，将二维图像与三维点云对齐。该问题在自动驾驶、机器人感知等场景中至关重要，因为多模态传感器的精确配准是实现语义一致性、环境建模和决策的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别了语义-几何鸿沟、局部最优陷阱和非可微 PnP 的三大挑战，随后借鉴了对比学习（如 CLIP、Contrastive Learning）和 Transformer 的全局上下文建模技术，设计了自监督跨模态对比学习、两阶段粗细配准以及可微 PnP 的端到端框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督对比学习将图像和点云映射到共享特征空间，然后使用 Transformer 进行粗略的超点-超像素匹配，接着在细粒度上进行点级精细化，最后利用可微 PnP 估计相机位姿，并通过动态协同训练平衡各损失。整体流程包括特征提取、跨模态对齐、粗配准、细配准、位姿回归和动态损失调节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 双路径自监督对比学习实现语义与几何的融合嵌入；2) 两阶段粗细配准结合超点-超像素与点级匹配；3) 可微 PnP 使整个系统可端到端训练；4) 动态协同训练平衡多任务损失。与以往方法相比，CrossI2P 解决了语义鸿沟、局部最优和非可微问题，显著提升了配准精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CrossI2P 提出了一种自监督、端到端的图像-点云配准框架，通过跨模态对比学习、两阶段粗细匹配和可微 PnP，实现了无标定、精确且鲁棒的 2D‑3D 对齐。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Bridging 2D and 3D sensor modalities is critical for robust perception in autonomous systems. However, image-to-point cloud (I2P) registration remains challenging due to the semantic-geometric gap between texture-rich but depth-ambiguous images and sparse yet metrically precise point clouds, as well as the tendency of existing methods to converge to local optima. To overcome these limitations, we introduce CrossI2P, a self-supervised framework that unifies cross-modal learning and two-stage registration in a single end-to-end pipeline. First, we learn a geometric-semantic fused embedding space via dual-path contrastive learning, enabling annotation-free, bidirectional alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine registration paradigm: a global stage establishes superpoint-superpixel correspondences through joint intra-modal context and cross-modal interaction modeling, followed by a geometry-constrained point-level refinement for precise registration. Third, we employ a dynamic training mechanism with gradient normalization to balance losses for feature alignment, correspondence refinement, and pose estimation. Extensive experiments demonstrate that CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry benchmark and by 37.9% on nuScenes, significantly improving both accuracy and robustness.&lt;/p&gt;</description></item><item><guid>2509.15886v3</guid><title>RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation</title><link>http://arxiv.org/abs/2509.15886v3</link><author>Paul Julius Kühn, Duc Anh Nguyen, Arjan Kuijper, Holger Graf, Saptarshi Neil Sinha</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了将最新视觉基础模型 SAM2 适配为激光雷达点云在视角投影下的分割框架，利用 2D 语义分割技术实现高效、可扩展的 3D 分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云分割是自动驾驶和 3D 场景理解的核心，传统的体素和点基方法虽然能捕捉细粒度几何，但计算成本高、内存访问不规则，实时性受限；视角投影方法尚未被充分探索，可利用成熟的 2D 分割技术实现快速准确的预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估 SAM2 作为 3D 分割的强大骨干网络，探究其在视角投影下对激光雷达点云分割的适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建首个将 SAM2 适配为 3D 分割的视角投影框架，结合 2D 特征提取与投影/反投影操作；对编码器做三项改进：①强调水平空间依赖的模块；②针对球面投影的几何属性定制配置；③专门捕捉视角伪图像中独特空间模式和不连续性的机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 SemanticKITTI 数据集上实现了与现有方法相当的性能，同时获得了 2D 方案的速度、可扩展性和部署简易性；验证了视觉基础模型可作为 3D 感知的通用骨干网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 使用视觉基础模型的视角投影分割方法表现出良好效果，为统一、基于基础模型的激光雷达分割奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云分割是自动驾驶和 3D 场景理解的核心。体素和点基方法因与深度架构兼容且能捕捉细粒度几何而占据主导地位，但往往导致高计算成本、不规则内存访问和有限的实时效率。相比之下，视角投影方法虽然相对未被充分探索，但可利用成熟的 2D 语义分割技术实现快速准确的预测。受视觉基础模型在图像字幕、零样本识别和多模态任务方面快速进展的激励，我们研究了 SAM2——当前最先进的视觉基础模型——是否能作为激光雷达点云在视角投影下的强大骨干网络。我们提出了首个将 SAM2 适配为 3D 分割的视角投影框架，将高效的 2D 特征提取与标准投影/反投影相结合，以在点云上操作。为优化 SAM2 在视角投影表示上的表现，我们对编码器做了多项架构改进：①一个新模块强调激光雷达视角图像中固有的水平空间依赖；②针对球面投影几何属性的定制配置；③在编码器骨干中专门设计的机制，用以捕捉视角伪图像中独特的空间模式和不连续性。我们的方法在 SemanticKITTI 上实现了竞争性性能，同时受益于 2D 中心化管线的速度、可扩展性和部署简易性。此工作凸显了视觉基础模型作为 3D 感知通用骨干网络的可行性，并为统一、基于基础模型的激光雷达分割开辟了道路。结果表明，使用视觉基础模型的视角投影分割方法具有有前景的效果。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在实现高效、准确的 LiDAR 点云语义分割，满足自动驾驶和三维场景理解的需求。传统的体素或点云方法计算量大、内存访问不规则，导致实时性差；而基于范围视图的方法可以利用成熟的二维分割技术，但尚未得到充分探索。解决这一问题可显著提升车辆感知系统的性能与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从 SAM2 这一先进的视觉基础模型出发，设计了一个能够处理范围视图图像的编码器。通过引入 Stem 模块将 LiDAR 数据转换为 96 通道张量，并在 Hiera 块中使用水平窗口注意力来适配球面投影。解码器采用 Receptive Field Blocks 进行多尺度特征融合。该设计借鉴了 SAM2‑UNet、RangeFormer 等先前工作，并结合了范围视图投影与反投影技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 SAM2 的二维分割能力迁移到 LiDAR 点云，通过范围视图投影实现。流程包括：① 将点云投影为密集的二维范围图；② 通过 Stem 模块和改进的 SAM2 编码器提取特征；③ 使用 Receptive Field Blocks 解码得到二维分割图；④ 将二维分割结果反投影回原始点云得到三维标签；⑤ 可选地使用 k‑NN 传播进行后处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; RangeSAM 首次将视觉基础模型 SAM2 应用于范围视图 LiDAR 分割；提出了强调水平空间依赖的 Stem 模块；定制了适用于球面投影的 Hiera 块和窗口注意力；解码器采用 Receptive Field Blocks 提升分割质量。与以往仅使用 CNN 或 Transformer 的范围视图方法不同，RangeSAM 利用预训练的基础模型，实现了竞争性的准确率，同时保持更快的推理速度和更简洁的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RangeSAM 展示了视觉基础模型可以被有效改造用于范围视图 LiDAR 分割，提供了一种快速、准确且易于部署的替代方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.&lt;/p&gt;</description></item><item><guid>2509.16832v2</guid><title>L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models</title><link>http://arxiv.org/abs/2509.16832v2</link><author>Ziyang Xu, Benedikt Schwab, Yihui Yang, Thomas H. Kolbe, Christoph Holst</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出L2M-Reg方法，利用平面对应、伪平面约束的Gauss-Helmert模型和自适应垂直平移估计，实现了在LoD2模型不确定性下的建筑级LiDAR与3D城市模型精确配准，实验表明其精度和效率均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在城市数字孪生中，LiDAR点云与语义3D城市模型的精确配准是基础，也是数字建造、变化检测和模型精细化等下游任务的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在LoD2级别存在模型不确定性时，单栋建筑级LiDAR与模型配准的准确性挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; L2M-Reg方法包括三步：1) 建立可靠的平面对应；2) 构建伪平面约束的Gauss-Helmert模型；3) 自适应估计垂直平移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明L2M-Reg在三组真实数据集上比现有ICP和其他平面基方法更精确且计算更高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; L2M-Reg为存在模型不确定性的建筑级LiDAR-模型配准提供了新方案，提升了配准精度和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Accurate registration between LiDAR point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection and model refinement. However, achieving accurate LiDAR-to-Model registration at individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model, and adaptively estimating vertical translation. Experiments on three real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现单栋建筑级别的 LiDAR 点云与 LoD2 城市模型之间的精确配准，并且显式考虑模型的不确定性。该问题在数字孪生、建筑施工、变更检测等高精度应用中至关重要，因为 LoD2 模型往往基于地籍边界生成，导致地基与立面之间存在水平偏移，若忽略会导致配准误差累积。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出现有配准方法普遍假设模型无误差的缺陷，随后结合平面特征匹配、ICP 变体和 Gauss–Helmert 模型的优势，设计了 L2M‑Reg。方法借鉴了已有的平面基配准（如 Scantra、PLADE）和 ICP 变体，但在此基础上加入了对模型不确定性的显式建模和垂直平移自适应估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可靠的平面对应、伪平面约束的 Gauss–Helmert 模型以及自适应垂直平移估计，来补偿 LoD2 模型的几何偏差。实现流程包括：①预处理 LiDAR 点云和 LoD2 模型；②利用语义信息提取并匹配平面；③构建伪平面约束的 Gauss–Helmert 模型求解 6-DoF 变换；④将垂直和平面分离，进行自适应垂直平移估计，最终得到精细配准结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①显式考虑 LoD2 模型的不确定性并通过伪平面约束进行补偿；②采用 2D‑3D 分离的变换估计策略，降低地面模型误差对水平配准的影响；③利用语义信息实现轻量级平面对应，无需将模型转为点云；④在保持高精度的同时显著提升计算效率。与以往仅假设模型无误差或依赖点云转换的 ICP/平面基方法不同，L2M‑Reg 在不确定性建模和效率上实现了突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L2M‑Reg 提供了一种快速、精确的建筑级 LiDAR‑to‑LoD2 配准方案，显式建模并补偿城市模型的几何不确定性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate registration between LiDAR (Light Detection and Ranging) point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection and model refinement. However, achieving accurate LiDAR-to-Model registration at individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model, and adaptively estimating vertical translation. Experiments on three real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present.&lt;/p&gt;</description></item><item><guid>2509.18786v2</guid><title>Human-Interpretable Uncertainty Explanations for Point Cloud Registration</title><link>http://arxiv.org/abs/2509.18786v2</link><author>Johannes A. Gaus, Loris Schneider, Yitian Shi, Jongseok Lee, Rania Rayyes, Rudolph Triebel</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的点云配准方法GP-CA，能够量化并解释配准不确定性，并通过主动学习发现新的不确定性来源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的ICP等方法在传感器噪声、姿态估计误差和遮挡导致的部分重叠等不确定性下表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够同时量化配准不确定性并解释其来源的算法，并通过主动学习提高样本效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GP-CA利用高斯过程概念归因技术，结合主动学习查询信息量大的实例，识别并归因配准误差来源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个公开数据集和真实机器人实验中，GP-CA在运行时长、样本效率和准确率上均优于现有最先进方法，并能实现有效的失败恢复。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GP-CA在点云配准任务中表现出更快的速度、更高的样本效率和更好的准确性，且在实际机器人感知中具有可行性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究点云配准问题，传统方法如ICP在传感器噪声、姿态估计误差和遮挡导致的部分重叠等不确定性下表现不佳。我们提出一种新方法Gaussian Process Concept Attribution (GP-CA)，该方法不仅量化配准不确定性，还通过归因将不确定性解释为已知的配准误差来源。GP-CA利用主动学习，通过查询信息量大的实例来发现野外的新不确定性来源。我们在三个公开数据集和真实机器人实验中验证了GP-CA。大量消融实验支持我们的设计选择。GP-CA在运行时间、样本效率和准确率方面均优于其他最先进方法。我们的真实实验清晰展示了其适用性。视频还展示了GP-CA实现有效的失败恢复行为，从而提升了机器人感知的鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose-estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception.&lt;/p&gt;</description></item><item><guid>2509.20705v1</guid><title>Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework</title><link>http://arxiv.org/abs/2509.20705v1</link><author>Reza Akhavian, Mani Amani, Johannes Mootz, Robert Ashe, Behrad Beheshti</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了BIM2RDT框架，将静态建筑信息模型转化为可供机器人使用的动态数字孪生，并通过实时感知与机器人操作提升施工现场数字管理与安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着网络物理系统和工地智能化的发展，连接设计模型、实时现场感知与自主操作的技术正在显著提升建筑行业的数字化管理水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够将已有BIM数据与现场实时信息融合，并生成安全优先的机器人可用数字孪生的智能框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 框架通过整合BIM几何语义、IoT传感器活动数据和机器人采集的视觉空间数据，采用基于大型语言模型推理的SG-ICP点云配准算法、YOLOE目标检测、Shi‑Tomasi角点检测以及实时手臂振动监测，实现数字孪生的持续更新与路径优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SG-ICP在遮挡特征场景下的配准误差比传统ICP低64.3%–88.3%，并能提供合理的物体朝向；手臂振动监测在超过ISO 5349‑1阈值时能及时发出警报，提升安全合规性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BIM2RDT框架通过高精度配准、实时感知与安全监测，显著提升了施工现场的数字孪生质量与机器人作业安全，为建筑行业的数字化管理提供了可行的技术路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The adoption of cyber‑physical systems and jobsite intelligence that connects design models, real‑time site sensing, and autonomous field operations can dramatically enhance digital management in the construction industry. This paper introduces BIM2RDT (Building Information Models to Robot‑Ready Site Digital Twins), an agentic artificial intelligence (AI) framework designed to transform static Building Information Modeling (BIM) into dynamic, robot‑ready digital twins (DTs) that prioritize safety during execution. The framework bridges the gap between pre‑existing BIM data and real‑time site conditions by integrating three key data streams: geometric and semantic information from BIM models, activity data from IoT sensor networks, and visual‑spatial data collected by robots during site traversal. The methodology introduces Semantic‑Gravity ICP (SG‑ICP), a point cloud registration algorithm that leverages large language model (LLM) reasoning. Unlike traditional methods, SG‑ICP utilizes an LLM to infer object‑specific, plausible orientation priors based on BIM semantics, improving alignment accuracy by avoiding convergence on local minima. This creates a feedback loop where robot‑collected data updates the DT, which in turn optimizes paths for missions. The framework employs YOLOE object detection and Shi‑Tomasi corner detection to identify and track construction elements while using BIM geometry as a priori maps. The framework also integrates real‑time Hand‑Arm Vibration (HAV) monitoring, mapping sensor‑detected safety events to the digital twin using IFC standards for intervention. Experiments demonstrate SG‑ICP&amp;#x27;s superiority over standard ICP, achieving RMSE reductions of 64.3%–88.3% in alignment across scenarios with occluded features, ensuring plausible orientations. HAV integration triggers warnings upon exceeding exposure limits, enhancing compliance with ISO 5349‑1.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在把静态的建筑信息模型（BIM）转化为能够实时更新、支持机器人导航并优先考虑安全的数字孪生（DT），从而解决施工现场信息滞后、工人安全风险高以及机器人在动态环境中定位困难等问题。该问题在现实中重要，因为施工现场复杂且安全事故频发，缺乏实时、可操作的数字化工具会导致效率低下和安全隐患。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已有的SLAM、ICP配准技术、YOLOE目标检测、Shi‑Tomasi角点检测以及工业物联网传感器，并在此基础上引入了大型语言模型（LLM）推理来提供语义重力先验，形成SG‑ICP算法。框架还借鉴了数字孪生与机器人协同、工人振动监测等前沿研究，形成了一个以安全为核心的代理式AI系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用BIM作为先验地图，让机器人通过视觉与深度传感器采集现场数据，并用语义驱动的SG‑ICP进行点云配准，实时更新数字孪生；随后代理AI根据更新后的DT规划路径、识别危险并触发HAV警报。整体流程包括：BIM加载 → 机器人巡检 → 目标检测与角点提取 → SG‑ICP配准 → DT更新 → 安全监测与路径规划 → 下一轮巡检。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) SG‑ICP利用LLM推理提供物体特定的重力先验，显著提升配准精度；2) 将工人手臂振动监测与IFC标准集成到DT中，实现实时安全干预；3) 采用YOLOE开放词汇检测与BIM语义提示相结合，增强目标识别；4) 构建完整的代理式AI安全优先框架，实现机器人与工人协同。与以往仅关注SLAM或单一传感器融合的工作不同，BIM2RDT实现了多模态数据融合、语义驱动配准和安全监测的统一系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BIM2RDT通过将BIM语义与机器人感知、LLM驱动的配准和实时安全监测融合，构建了一个安全优先的代理式AI框架，使施工现场能够实时生成机器人可用的数字孪生并主动预警风险。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The adoption of cyber-physical systems and jobsite intelligence that connects design models, real-time site sensing, and autonomous field operations can dramatically enhance digital management in the construction industry. This paper introduces BIM2RDT (Building Information Models to Robot-Ready Site Digital Twins), an agentic artificial intelligence (AI) framework designed to transform static Building Information Modeling (BIM) into dynamic, robot-ready digital twins (DTs) that prioritize safety during execution. The framework bridges the gap between pre-existing BIM data and real-time site conditions by integrating three key data streams: geometric and semantic information from BIM models, activity data from IoT sensor networks, and visual-spatial data collected by robots during site traversal. The methodology introduces Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that leverages large language model (LLM) reasoning. Unlike traditional methods, SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors based on BIM semantics, improving alignment accuracy by avoiding convergence on local minima. This creates a feedback loop where robot-collected data updates the DT, which in turn optimizes paths for missions. The framework employs YOLOE object detection and Shi-Tomasi corner detection to identify and track construction elements while using BIM geometry as a priori maps. The framework also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping sensor-detected safety events to the digital twin using IFC standards for intervention. Experiments demonstrate SG-ICP&amp;#x27;s superiority over standard ICP, achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with occluded features, ensuring plausible orientations. HAV integration triggers warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.&lt;/p&gt;</description></item><item><guid>2509.21038v1</guid><title>OmniPlantSeg: Species Agnostic 3D Point Cloud Organ Segmentation for High-Resolution Plant Phenotyping Across Modalities</title><link>http://arxiv.org/abs/2509.21038v1</link><author>Andreas Gilson, Lukas Meyer, Oliver Scholz, Ute Schmid</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为KD-SS的轻量级子采样算法，用于植物点云的全分辨率分割，避免了传统方法的预处理和下采样步骤。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在三维植物表型学中，准确的植物器官点云分割至关重要，但现有方法往往针对特定植物种类或传感器模式，并需要大量预处理和下采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种与传感器和植物种类无关的子采样方法，使得能够在不降低分辨率的情况下完成点云分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; KD-SS算法对生物点云进行子采样，保持原始分辨率；随后将其与当前最先进的分割模型结合，评估在摄影测量、激光三角测量和激光雷达等多种模式下的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，KD-SS与先进分割模型结合后，在不同传感器模式和植物种类上均取得令人满意的分割结果，证明了其在保持分辨率方面的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; KD-SS提供了一种轻量、可保持分辨率的替代方案，适用于各种植物种类和传感器模式的器官分割，减少了繁重的预处理和下采样工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 植物器官的准确点云分割对于三维植物表型学至关重要。现有解决方案往往针对特定问题，聚焦于某些植物种类或指定的传感器模式进行数据采集。此外，常见做法是对植物点云进行大量预处理并下采样，以满足硬件或神经网络输入尺寸的要求。我们提出了一种简单而有效的算法KD-SS，用于生物点云的子采样，该算法对传感器数据和植物种类不敏感。该方法的主要优势在于不需要对输入数据进行下采样，从而能够对全分辨率点云进行分割。将KD-SS与当前最先进的分割模型相结合，在不同模式（如摄影测量、激光三角测量和激光雷达）以及多种植物种类上评估，结果令人满意。我们将KD-SS作为一种轻量级、保持分辨率的替代方案，适用于无论使用何种种类和传感器模式的植物器官分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate point cloud segmentation for plant organs is crucial for 3D plant phenotyping. Existing solutions are designed problem-specific with a focus on certain plant species or specified sensor-modalities for data acquisition. Furthermore, it is common to use extensive pre-processing and down-sample the plant point clouds to meet hardware or neural network input size requirements. We propose a simple, yet effective algorithm KDSS for sub-sampling of biological point clouds that is agnostic to sensor data and plant species. The main benefit of this approach is that we do not need to down-sample our input data and thus, enable segmentation of the full-resolution point cloud. Combining KD-SS with current state-of-the-art segmentation models shows satisfying results evaluated on different modalities such as photogrammetry, laser triangulation and LiDAR for various plant species. We propose KD-SS as lightweight resolution-retaining alternative to intensive pre-processing and down-sampling methods for plant organ segmentation regardless of used species and sensor modality.&lt;/p&gt;</description></item><item><guid>2509.22058v1</guid><title>An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose</title><link>http://arxiv.org/abs/2509.22058v1</link><author>Qifeng Wang, Weigang Li, Lei Nie, Xin Xu, Wenping Liu, Zhe Xu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文提出一种自适应ICP的LiDAR里程计方法，利用可靠初始位姿和动态阈值实现高精度配准，实验表明优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统ICP方法在移动机器人里程计中广泛使用，但缺乏对初始位姿可靠性和动态环境适应性的考虑，易陷入局部最优。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决ICP方法对初始位姿不可靠和缺乏自适应机制导致的配准精度下降问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用基于密度过滤的分布式粗配准得到初始位姿，再与运动预测位姿比较挑选可靠位姿；随后结合当前和历史误差动态调整阈值；最后在可靠位姿和自适应阈值下执行点到平面ICP配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在KITTI数据集上实验显示该方法在复杂动态环境下的配准精度显著提升，优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应ICP配准结合可靠初始位姿和动态阈值能显著提高LiDAR里程计的精度，具有较好的实用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该论文提出一种自适应ICP的LiDAR里程计方法，依赖可靠的初始位姿。首先使用基于密度过滤的分布式粗配准获取初始位姿估计。通过与运动预测位姿比较，选择可靠的初始位姿，减少源点云与目标点云之间的初始误差。随后，结合当前和历史误差，动态调整自适应阈值，以适应实时变化的动态环境。最后，在可靠初始位姿和自适应阈值的基础上，从当前帧到局部地图执行点到平面自适应ICP配准，实现源点云与目标点云的高精度对齐。对公共KITTI数据集进行的大量实验表明，所提出的方法优于现有方法，并显著提升了LiDAR里程计的精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As a key technology for autonomous navigation and positioning in mobile robots, light detection and ranging (LiDAR) odometry is widely used in autonomous driving applications. The Iterative Closest Point (ICP)-based methods have become the core technique in LiDAR odometry due to their efficient and accurate point cloud registration capability. However, some existing ICP-based methods do not consider the reliability of the initial pose, which may cause the method to converge to a local optimum. Furthermore, the absence of an adaptive mechanism hinders the effective handling of complex dynamic environments, resulting in a significant degradation of registration accuracy. To address these issues, this paper proposes an adaptive ICP-based LiDAR odometry method that relies on a reliable initial pose. First, distributed coarse registration based on density filtering is employed to obtain the initial pose estimation. The reliable initial pose is then selected by comparing it with the motion prediction pose, reducing the initial error between the source and target point clouds. Subsequently, by combining the current and historical errors, the adaptive threshold is dynamically adjusted to accommodate the real-time changes in the dynamic environment. Finally, based on the reliable initial pose and the adaptive threshold, point-to-plane adaptive ICP registration is performed from the current frame to the local map, achieving high-precision alignment of the source and target point clouds. Extensive experiments on the public KITTI dataset demonstrate that the proposed method outperforms existing approaches and significantly enhances the accuracy of LiDAR odometry.&lt;/p&gt;</description></item><item><guid>2509.22132v1</guid><title>Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud</title><link>http://arxiv.org/abs/2509.22132v1</link><author>Jingjing Lu, Huilong Pi, Yunchuan Qin, Zhuo Tang, Ruihui Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督点云补全方法，利用多视角增强生成自监督信号，并引入 Mamba 模型提升生成质量，实验表明该方法在合成与真实数据集上均达到最先进水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有监督方法依赖真实标签，难以泛化到真实数据；无监督方法需要完整点云；弱监督方法需要多视角观测；现有自监督方法因信号有限导致预测不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服上述局限，提出一种能够在仅有单一部分点云的情况下进行自监督学习的补全方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计基于单个部分点云的多视角增强的自监督信号，并将 Mamba 模型引入自监督补全任务，以提升学习效果和生成点云质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成和真实数据集上的实验表明，该方法在点云补全任务中取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该自监督方法通过多视角增强和 Mamba 模型实现了高质量点云补全，并显著提升了对真实世界数据的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分观测中重建完整形状。虽然当前方法已取得显著性能，但仍存在一些局限：监督方法高度依赖真实标签，因合成到真实的域差导致在真实数据集上的泛化受限；无监督方法需要完整点云来构成无配对训练数据；弱监督方法需要对象的多视角观测。现有自监督方法由于自监督信号能力有限，往往产生不令人满意的预测。为克服这些挑战，我们提出了一种新颖的自监督点云补全方法。我们基于单个部分点云的多视角增强设计了一组新颖的自监督信号。此外，为了增强模型的学习能力，我们首先将 Mamba 引入自监督点云补全任务，鼓励模型生成质量更好的点云。对合成和真实数据集的实验表明，我们的方法实现了最先进的结果。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在完成从单一不完整点云中恢复完整三维形状的问题。现实中传感器往往产生缺失或遮挡的点云，缺失信息会影响机器人导航、三维重建等应用。研究中，如何在没有完整标注的情况下实现高质量补全是一个关键挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了监督、无监督和弱监督方法的局限，发现自监督方法缺乏多样化的自监督信号。为此他们借鉴了多视角数据增强、Hilbert曲线序列化、DGCNN补丁嵌入以及Mamba的选择性状态空间模型等已有技术，构建了基于多视角增强的自监督框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用单一部分点云生成多视角的合成部分点云，并让模型在所有视角下产生相同的完整点云，从而形成强大的自监督信号。实现流程包括：FPS采样、Hilbert曲线序列化、KNN补丁构造、DGCNN嵌入、八层Mamba编码器提取全局特征、特征拼接与池化得到512维全局向量，最后通过三层全连接生成8192点的完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①基于单一部分点云的多视角合成增强，提供多样化的自监督信号；②首次将Mamba编码器应用于点云补全，提升全局与局部特征提取能力；③结合加权Chamfer距离与一致性损失，强化模型对不同视角的鲁棒性。与以往自监督方法仅使用原始部分点云或中间表示不同，本方法通过视角多样化显著提升了补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种利用单一部分点云的多视角增强并结合Mamba编码器的自监督点云补全框架，在无需完整标注的情况下实现了更高质量、更具鲁棒性的三维形状恢复。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model&amp;#x27;s learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.&lt;/p&gt;</description></item><item><guid>2509.23375v1</guid><title>CasPoinTr: Point Cloud Completion with Cascaded Networks and Knowledge Distillation</title><link>http://arxiv.org/abs/2509.23375v1</link><author>Yifan Yang, Yuxiang Yan, Boda Liu, Jian Pu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 CasPoinTr 框架，通过级联网络和知识蒸馏实现点云补全，显著提升形状恢复和细节保留。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 真实环境中采集的点云常因传感器分辨率、单视角、遮挡和噪声等因素不完整，点云补全成为关键技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决从高度不完整的点云中预测整体形状并重建缺失区域的难题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CasPoinTr 将补全任务拆分为形状重建和融合补全两阶段，利用形状重建生成辅助信息，融合补全结合知识蒸馏将稠密点云的完整-不完整关联知识迁移给学生模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ShapeNet-55 的不同难度设置下，CasPoinTr 在形状恢复和细节保留方面优于现有方法，验证了级联结构和蒸馏策略的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 级联网络与知识蒸馏的结合能更好捕捉全局形状上下文并细化局部细节，显著提升点云补全性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从真实环境中采集的点云往往因传感器分辨率有限、单一视角、遮挡和噪声等因素而不完整。这些挑战使得点云补全成为各类应用的必要技术。该任务的关键难点在于从高度不完整的点云中预测整体形状并重建缺失区域。为此，我们提出了 CasPoinTr，一种利用级联网络和知识蒸馏的全新点云补全框架。CasPoinTr 将补全任务拆分为两个协同阶段：形状重建阶段生成辅助信息，融合补全阶段则利用这些信息以及知识蒸馏来生成最终结果。通过知识蒸馏，训练于稠密点云的教师模型将不完整-完整关联知识迁移给学生模型，提升其估计整体形状和预测缺失区域的能力。级联网络与知识蒸馏共同增强模型捕捉全局形状上下文并细化局部细节的能力，有效弥合不完整输入与完整目标之间的差距。在 ShapeNet-55 的不同难度设置下的实验表明，CasPoinTr 在形状恢复和细节保留方面优于现有方法，凸显了我们级联结构和蒸馏策略的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成问题，即从不完整的点云中恢复完整的三维形状。现实中，传感器受限、单视角、遮挡和噪声导致点云缺失，影响机器人导航、AR/VR 等应用。研究中，准确完成点云能提升后续任务的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴人类先推断整体形状再细化细节的思路，结合现有的 PoinTr、AdaPoinTr、级联网络和知识蒸馏技术。通过分析现有方法在错误累积和教师-学生差距上的不足，提出了辅助完成和适当特权输入的策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云完成拆分为形状重建和融合完成两阶段，并用知识蒸馏让学生模型学习教师模型的完整-不完整关联。流程为：①形状重建阶段对不完整点云进行 4 倍上采样，生成稠密点云并提取辅助特征；②融合完成阶段以原始不完整点云为输入，结合辅助特征进行最终重建；③教师模型使用 2N 解析度的特权输入训练，学生通过 KL 散度蒸馏学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出辅助完成的级联结构，避免传统两步上采样导致的误差累积；②设计适当分辨率的特权输入（2N）来平衡教师-学生差距并防止捷径学习；③在特征层使用 KL 散度进行蒸馏，提升整体形状感知。与以往直接使用粗糙上采样或全分辨率特权输入的方法不同，CasPoinTr 在保持细节的同时显著提升了重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CasPoinTr 通过辅助完成的级联网络和基于适当特权输入的知识蒸馏，显著提升点云完成的整体形状恢复和细节保留效果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point clouds collected from real-world environments are often incomplete due to factors such as limited sensor resolution, single viewpoints, occlusions, and noise. These challenges make point cloud completion essential for various applications. A key difficulty in this task is predicting the overall shape and reconstructing missing regions from highly incomplete point clouds. To address this, we introduce CasPoinTr, a novel point cloud completion framework using cascaded networks and knowledge distillation. CasPoinTr decomposes the completion task into two synergistic stages: Shape Reconstruction, which generates auxiliary information, and Fused Completion, which leverages this information alongside knowledge distillation to generate the final output. Through knowledge distillation, a teacher model trained on denser point clouds transfers incomplete-complete associative knowledge to the student model, enhancing its ability to estimate the overall shape and predict missing regions. Together, the cascaded networks and knowledge distillation enhance the model&amp;#x27;s ability to capture global shape context while refining local details, effectively bridging the gap between incomplete inputs and complete targets. Experiments on ShapeNet-55 under different difficulty settings demonstrate that CasPoinTr outperforms existing methods in shape recovery and detail preservation, highlighting the effectiveness of our cascaded structure and distillation strategy.&lt;/p&gt;</description></item><item><guid>2509.23703v1</guid><title>DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph</title><link>http://arxiv.org/abs/2509.23703v1</link><author>Zhenyu Shu, Jian Yao, Shiqing Xin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于可变节点度的点云完成网络，利用细节感知度量和几何感知图集成模块，显著提升点云重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云完成旨在补全因遮挡和传感器分辨率限制导致的不完整点云。传统方法使用固定局部划分，无法处理形状不同区域几何复杂度不均匀的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 改进点云完成方法，使其在细节丰富或结构不连续的区域实现更高效的表示和更优的重建效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出DFG-PCN框架，采用细节感知度量自适应分配节点度，并引入基于曼哈顿距离的几何感知图集成模块，将局部与全局特征进行细节引导融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上实验表明，该方法在重建精度上持续优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应节点度和几何感知图集成显著提升点云完成性能，证明了细节感知与结构重要性考虑的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云完成是一项重要任务，旨在重建完整的点云，并解决因遮挡和传感器分辨率有限导致的不完整性。传统方法依赖固定的局部区域划分，例如k近邻，无法考虑形状不同区域几何复杂度高度不均匀的情况。这一限制导致表示效率低下，重建效果不佳，尤其在细粒度细节或结构不连续的区域。本文提出一种名为Degree-Flexible Point Graph Completion Network（DFG-PCN）的点云完成框架。它使用结合特征变化和曲率的细节感知度量自适应地分配节点度，聚焦于结构重要区域。我们进一步引入几何感知图集成模块，使用曼哈顿距离进行边聚合，并通过细节引导融合局部和全局特征以增强表示。对多个基准数据集进行的大量实验表明，我们的方法始终优于最先进的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进点云补全技术，解决因遮挡和传感器分辨率有限导致的点云不完整问题。传统方法使用固定的k近邻划分，无法适应几何复杂度不均匀的区域，导致细节缺失和重建质量下降。点云补全在三维感知、机器人导航和虚拟现实等领域具有重要应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有补全方法在图结构上的“等度”缺陷，提出需要自适应节点度的图网络。设计中借鉴了PointNet、Point Transformer、Upsample Transformer等成熟模块，并在此基础上引入了度可调图构造、细节感知度分配和几何感知融合等新组件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个度可调的点图网络（DFG），根据点的细节丰富度（特征变化+曲率）动态分配邻接度。实现流程包括：①特征提取器提取全局与局部特征；②种子生成器利用Upsample Transformer生成粗略完整点云；③点生成模块由三层DFG块组成，每层使用PointNet、图构造、图聚合、图融合、MLP和去卷积逐步上采样，最终得到高分辨率完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①度可调点图网络，动态分配节点度；②细节感知度分配策略，结合特征变化和曲率；③几何感知图集成模块，使用曼哈顿距离聚合并细节引导的局部-全局特征融合。与以往固定k近邻或统一度的图网络不同，DFG-PCN能够在细节丰富区域提供更高的连接度，从而提升重建精度和细节保留。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DFG-PCN通过自适应度分配的点图网络，结合细节感知和几何融合，实现了比传统固定度方法更精细、更准确的点云补全。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is a vital task focused on reconstructing complete point clouds and addressing the incompleteness caused by occlusion and limited sensor resolution. Traditional methods relying on fixed local region partitioning, such as k-nearest neighbors, which fail to account for the highly uneven distribution of geometric complexity across different regions of a shape. This limitation leads to inefficient representation and suboptimal reconstruction, especially in areas with fine-grained details or structural discontinuities. This paper proposes a point cloud completion framework called Degree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns node degrees using a detail-aware metric that combines feature variation and curvature, focusing on structurally important regions. We further introduce a geometry-aware graph integration module that uses Manhattan distance for edge aggregation and detail-guided fusion of local and global features to enhance representation. Extensive experiments on multiple benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches.&lt;/p&gt;</description></item><item><guid>2509.23723v1</guid><title>DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for Point Cloud Completion</title><link>http://arxiv.org/abs/2509.23723v1</link><author>Zijun Li, Hongyu Yan, Shijie Li, Kunming Luo, Li Lu, Xulei Yang, Weisi Lin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 DiffPCN，一种基于扩散模型的粗细两阶段点云补全框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 扩散模型在低级视觉任务中表现出色，但由于点云的无序和不规则特性，点云补全尚未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用扩散模型的生成与理解能力，构建高质量、高完整度的点云补全方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先将不完整点云投影为结构化深度图，使用 DepthLDM 生成多视角完整深度图并构成粗点云；随后通过点去噪网络去除噪声并预测距离分数；最后使用关联感知点上采样器利用局部关联特征实现稠密高保真补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 DiffPCN 在几何精度和形状完整度上达到最先进水平，显著提升了点云补全的鲁棒性和一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DiffPCN 通过粗细两阶段的扩散与后处理，成功克服点云无序性，提供了高质量的点云补全方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 潜在扩散模型（LDMs）在各种低级视觉任务中展现了卓越的生成能力。然而，由于点云的无序和不规则特性，其在点云补全方面的潜力尚未得到充分探索。在本研究中，我们提出了 DiffPCN，一种新颖的基于扩散的粗细两阶段点云补全框架。我们的方法包括两个阶段：初始阶段用于生成粗略点云，精细阶段通过点去噪和上采样来提升其质量。具体而言，我们首先将无序且不规则的部分点云投影为结构化深度图，作为精心设计的 DepthLDM 的条件，用以合成完整的多视角深度图，随后将其用于构建粗点云。通过这种方式，DiffPCN 能够利用 LDM 强大的生成和理解能力，生成高质量且完整度高的粗点云。随后，由于 LDM 在生成深度图时不可避免地会引入离群点，我们设计了点去噪网络，通过预测每个点的距离分数来去除粗点云中的伪影。最后，我们提出了关联感知点上采样器，利用输入点云与对应粗点之间的局部关联特征来指导上采样过程，进一步得到稠密且高保真度的输出。实验结果表明，DiffPCN 在几何精度和形状完整度方面达到了最先进的性能，显著提升了点云补全的鲁棒性和一致性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成问题，即从部分、稀疏的点云中恢复完整、密集且结构连贯的三维点云。该问题在现实中十分重要，因为实际传感器（如 LiDAR、深度相机）往往受到遮挡、噪声和分辨率限制，导致采集到的点云不完整，影响后续的三维感知、导航和重建任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者意识到传统点云完成方法多依赖全局特征，导致细节缺失且效率低下，借鉴了图像领域的潜在扩散模型（Latent Diffusion Model）以及多视角深度图生成技术（如 MVDD、Wonder3D）。他们将点云投影为多视角深度图，使用 VAE 编码后通过 DepthLDM（结合跨视角注意力和点对齐注意力）生成完整深度图，再反投影得到粗点云，并在此基础上设计点去噪网络和关联感知上采样器进行细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云完成拆分为粗略生成和细化两个阶段，并利用潜在扩散模型在结构化的深度图空间中生成高质量粗点云。实现流程包括：① 将部分点云投影为六个视角深度图；② 用 VAE 编码并训练 DepthLDM，利用跨视角和点对齐注意力生成完整深度图；③ 将生成的深度图反投影得到粗点云；④ 用点去噪网络预测并剔除异常点；⑤ 用关联感知上采样器通过关联变换和自注意力进一步细化并上采样，得到最终完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 在点云完成中首次引入潜在扩散模型 DepthLDM，利用多视角深度图实现高效一致的粗点云生成；② 结合跨视角注意力和点对齐注意力提升三维结构感知；③ 设计点去噪网络通过距离分数剔除噪声点；④ 设计关联感知上采样器利用部分点云与粗点云的关联关系实现细粒度上采样。与以往直接在三维空间或仅使用全局特征的扩散/点云方法不同，DiffPCN 在结构化二维空间中生成并细化点云，显著提升了几何精度和完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DiffPCN 提出一种两阶段潜在扩散框架，将部分点云投影为多视角深度图，通过 DepthLDM 生成高质量粗点云，并通过点去噪与关联感知上采样实现细化，达成点云完成领域的最新性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Latent diffusion models (LDMs) have demonstrated remarkable generative capabilities across various low-level vision tasks. However, their potential for point cloud completion remains underexplored due to the unstructured and irregular nature of point clouds. In this work, we propose DiffPCN, a novel diffusion-based coarse-to-fine framework for point cloud completion. Our approach comprises two stages: an initial stage for generating coarse point clouds, and a refinement stage that improves their quality through point denoising and upsampling. Specifically, we first project the unordered and irregular partial point cloud into structured depth images, which serve as conditions for a well-designed DepthLDM to synthesize completed multi-view depth images that are used to form coarse point clouds. In this way, our DiffPCN can yield high-quality and high-completeness coarse point clouds by leveraging LDM&amp;#x27; s powerful generation and comprehension capabilities. Then, since LDMs inevitably introduce outliers into the generated depth maps, we design a Point Denoising Network to remove artifacts from the coarse point cloud by predicting a per-point distance score. Finally, we devise an Association-Aware Point Upsampler, which guides the upsampling process by leveraging local association features between the input point cloud and the corresponding coarse points, further yielding a dense and high-fidelity output. Experimental results demonstrate that our DiffPCN achieves state-of-the-art performance in geometric accuracy and shape completeness, significantly improving the robustness and consistency of point cloud completion.&lt;/p&gt;</description></item><item><guid>2509.23772v1</guid><title>A Modality-Tailored Graph Modeling Framework for Urban Region Representation via Contrastive Learning</title><link>http://arxiv.org/abs/2509.23772v1</link><author>Yaya Zhao, Kaiqi Zhao, Zixuan Tang, Zhiyuan Liu, Xiaoling Lu, Yalei Du</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出MTGRR框架，针对多模态城市数据的图模型进行模态定制，解决现有方法在模态统一架构和空间异质性融合上的不足，并通过联合对比学习提升区域表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统图神经网络在多模态城市数据中往往使用相同架构，无法捕捉模态特征；融合阶段也忽视空间异质性，导致表示欠佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种模态定制的图模型，既能捕捉不同模态的结构特征，又能在空间上动态调整融合权重，从而获得更优的区域表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将模态分为聚合级和点级两类；聚合级采用混合专家MoE图架构，每个专家GNN专门处理一种模态；点级采用双层GNN提取细粒度视觉语义；随后设计空间感知多模态融合机制，动态推断区域特定的融合权重；最后使用联合对比学习，将聚合级、点级和融合级目标结合优化表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在包含六种模态的两个真实数据集和三项任务上，MTGRR在所有基准模型上均表现更好，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 模态定制与空间感知融合相结合的MTGRR框架显著提升了多模态城市区域表示的质量，为下游任务提供更可靠特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于图的模型已成为建模多模态城市数据并学习区域表示以用于各种下游任务的强大范式。然而，现有方法面临两个主要局限。 (1) 它们通常在所有模态上使用相同的图神经网络架构，未能捕捉模态特定的结构和特征。 (2) 在融合阶段，它们往往忽略空间异质性，假设不同模态的聚合权重在各区域保持不变，导致表示次优。为解决这些问题，我们提出MTGRR，一种针对城市区域表示的模态定制图建模框架，基于包含兴趣点（POI）、出租车移动、土地利用、道路要素、遥感和街景图像的多模态数据集。 (1) MTGRR根据空间密度和数据特征将模态分为聚合级和点级两组。对于聚合级模态，MTGRR采用混合专家（MoE）图架构，每个模态由专门的专家GNN处理，以捕捉不同模态特定特征。对于点级模态，构建双层GNN以提取细粒度视觉语义特征。 (2) 为在空间异质性下获得有效的区域表示，设计了空间感知多模态融合机制，动态推断区域特定的模态融合权重。在此图建模框架基础上，MTGRR进一步采用联合对比学习策略，整合区域聚合级、点级和融合级目标来优化区域表示。对两个真实数据集、六种模态和三项任务的实验表明，MTGRR始终优于最先进的基线，验证了其有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph-based models have emerged as a powerful paradigm for modeling multimodal urban data and learning region representations for various downstream tasks. However, existing approaches face two major limitations. (1) They typically employ identical graph neural network architectures across all modalities, failing to capture modality-specific structures and characteristics. (2) During the fusion stage, they often neglect spatial heterogeneity by assuming that the aggregation weights of different modalities remain invariant across regions, resulting in suboptimal representations. To address these issues, we propose MTGRR, a modality-tailored graph modeling framework for urban region representation, built upon a multimodal dataset comprising point of interest (POI), taxi mobility, land use, road element, remote sensing, and street view images. (1) MTGRR categorizes modalities into two groups based on spatial density and data characteristics: aggregated-level and point-level modalities. For aggregated-level modalities, MTGRR employs a mixture-of-experts (MoE) graph architecture, where each modality is processed by a dedicated expert GNN to capture distinct modality-specific characteristics. For the point-level modality, a dual-level GNN is constructed to extract fine-grained visual semantic features. (2) To obtain effective region representations under spatial heterogeneity, a spatially-aware multimodal fusion mechanism is designed to dynamically infer region-specific modality fusion weights. Building on this graph modeling framework, MTGRR further employs a joint contrastive learning strategy that integrates region aggregated-level, point-level, and fusion-level objectives to optimize region representations. Experiments on two real-world datasets across six modalities and three tasks demonstrate that MTGRR consistently outperforms state-of-the-art baselines, validating its effectiveness.&lt;/p&gt;</description></item><item><guid>2509.24273v1</guid><title>Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds</title><link>http://arxiv.org/abs/2509.24273v1</link><author>Yongqiang Wang, Weigang Li, Wenping Liu, Zhiqiang Tian, Jinling Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于骨架的鲁棒点云配准框架，利用抗干扰的骨架表示提升配准的稳健性和精度，并通过分布距离损失函数加强源目标骨架的一致性。实验表明该方法在多种噪声、密度失真和几何畸变场景下均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准在三维视觉应用中至关重要，但真实点云常受传感器限制、环境噪声和预处理误差影响，导致密度失真、噪声污染和几何畸变，传统直接匹配或表面特征提取方法易受干扰，精度下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决受损点云配准的挑战，提升配准的鲁棒性和准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入抗干扰骨架表示，将骨架结构融入配准流程；同时结合受损点云与骨架配准得到的变换以获得最优配准；设计分布距离损失函数以强制源目标骨架一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多种受损数据集上，所提框架SRRF在密度失真、噪声污染和几何畸变等场景中均持续优于最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SRRF在处理受损点云时表现出高度鲁棒性，可作为真实场景三维感知任务的潜在方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准是3D视觉应用中的基础，包括自动驾驶、机器人和医学成像等领域，精确对齐多点云对于准确的环境重建至关重要。然而，真实点云往往受到传感器限制、环境噪声和预处理错误的影响，导致密度失真、噪声污染和几何畸变，使配准变得具有挑战性。现有配准方法依赖直接点匹配或表面特征提取，易受这些破坏的影响，导致对齐精度下降。为解决这些挑战，本文提出了一个基于骨架的鲁棒配准框架，引入抗干扰骨架表示以提升配准的稳健性和准确性。该框架将骨架结构整合到配准过程中，并结合受损点云配准和其骨架配准得到的变换以实现最佳配准。此外，设计了分布距离损失函数以强制源目标骨架之间的一致性，显著提升配准性能。该框架确保对齐同时考虑原始局部几何特征和骨架结构的全局稳定性，得到鲁棒且准确的配准结果。对多种受损数据集的实验评估表明，SRRF在密度失真、噪声污染和几何畸变等多种破坏场景下始终优于最先进的配准方法。结果证实了SRRF在处理受损点云方面的鲁棒性，使其成为真实场景中3D感知任务的潜在方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在现实环境中受噪声、密度失衡和几何畸变影响的三维点云配准问题。点云配准是自动驾驶、机器人导航和医学成像等领域的核心技术，准确对齐点云直接决定了后续建图、定位和识别的质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到骨架（Skeleton）作为点云的全局结构在噪声和稀疏情况下更为稳健，决定将骨架与传统点云配准相结合。方法借鉴了现有的骨架提取技术（如 Point2Skeleton）和基于深度学习的配准框架（如 DCP、PRNet），并在此基础上提出了分布距离损失来强化骨架一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过骨架表示提供全局稳定性，并将骨架配准与点云配准得到的变换融合，得到更准确的整体变换。实现流程包括：①提取源点云和目标点云的骨架；②使用深度学习模型对原始点云进行配准；③对骨架进行配准并计算骨架变换；④将两种变换加权融合；⑤使用分布距离损失约束骨架一致性，最终得到鲁棒的配准结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①提出骨架驱动的配准框架，利用骨架的全局稳定性提升鲁棒性；②设计分布距离损失，强制源骨架与目标骨架在分布上保持一致；③构建了包含密度、噪声和几何畸变的综合腐败点云基准，系统评估方法鲁棒性。与以往仅依赖原始点匹配或表面特征的配准方法不同，SRRF 在腐败场景下显著提升了配准精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SRRF 通过骨架表示和一致性损失实现了对受噪声、稀疏和畸变影响的三维点云的鲁棒且高精度配准，优于现有最先进方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration is fundamental in 3D vision applications, including autonomous driving, robotics, and medical imaging, where precise alignment of multiple point clouds is essential for accurate environment reconstruction. However, real-world point clouds are often affected by sensor limitations, environmental noise, and preprocessing errors, making registration challenging due to density distortions, noise contamination, and geometric deformations. Existing registration methods rely on direct point matching or surface feature extraction, which are highly susceptible to these corruptions and lead to reduced alignment accuracy. To address these challenges, a skeleton-based robust registration framework is presented, which introduces a corruption-resilient skeletal representation to improve registration robustness and accuracy. The framework integrates skeletal structures into the registration process and combines the transformations obtained from both the corrupted point cloud alignment and its skeleton alignment to achieve optimal registration. In addition, a distribution distance loss function is designed to enforce the consistency between the source and target skeletons, which significantly improves the registration performance. This framework ensures that the alignment considers both the original local geometric features and the global stability of the skeleton structure, resulting in robust and accurate registration results. Experimental evaluations on diverse corrupted datasets demonstrate that SRRF consistently outperforms state-of-the-art registration methods across various corruption scenarios, including density distortions, noise contamination, and geometric deformations. The results confirm the robustness of SRRF in handling corrupted point clouds, making it a potential approach for 3D perception tasks in real-world scenarios.&lt;/p&gt;</description></item><item><guid>2509.24275v1</guid><title>Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context</title><link>http://arxiv.org/abs/2509.24275v1</link><author>Yongqiang Wang, Weigang Li, Wenping Liu, Zhe Xu, Zhiqiang Tian</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于全局上下文的置信度估计框架CEGC，用于解决部分点云配准中的结构歧义、部分可见性和噪声问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 部分点云配准在自动驾驶感知和三维场景理解中至关重要，但由于结构歧义、部分可见性和噪声，仍然具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种统一的、置信度驱动的框架，以实现对复杂场景中部分点云的准确对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CEGC通过共享全局上下文同时建模重叠置信度和对应可靠性。混合重叠置信度估计模块结合语义描述符和几何相似性，早期检测重叠区域并抑制离群点；上下文感知匹配策略利用全局注意力为对应关系分配软置信度，从而降低歧义；这些置信度引导可微分加权奇异值分解求解器计算精确变换，整个流程自适应地降低不确定区域的权重并强调可靠匹配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ModelNet40、ScanObjectNN和7Scenes等三大数据集上，CEGC在准确性、鲁棒性和泛化能力方面均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CEGC提供了一种可解释且可扩展的解决方案，能够在挑战性条件下实现高质量的部分点云配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 部分点云配准对于自动驾驶感知和三维场景理解至关重要，但由于结构歧义、部分可见性和噪声，仍然具有挑战性。我们通过提出基于全局上下文的置信度估计（CEGC）来解决这些问题，CEGC是一种统一的、以置信度为驱动的框架，用于实现鲁棒的部分三维配准。CEGC通过在共享的全局上下文中联合建模重叠置信度和对应可靠性，实现了在复杂场景中的精确对齐。具体而言，混合重叠置信度估计模块结合语义描述符和几何相似性，检测重叠区域并在早期抑制离群点；上下文感知匹配策略通过全局注意力为对应关系分配软置信度，降低歧义并提升鲁棒性。这些置信度引导可微分加权奇异值分解求解器计算精确变换。该紧耦合的流程自适应地降低不确定区域的权重，并强调上下文可靠的匹配。对ModelNet40、ScanObjectNN和7Scenes三大三维视觉数据集的实验表明，CEGC在准确性、鲁棒性和泛化能力方面优于现有最先进方法。总体而言，CEGC为在挑战性条件下的部分点云配准提供了一种可解释且可扩展的解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决部分重叠、噪声和结构歧义下的三维点云配准问题。该问题在自动驾驶、机器人感知和三维重建等实际应用中至关重要，因为真实场景往往只能获得不完整的点云，准确配准是后续理解和操作的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了传统ICP、FGR等几何方法以及DCP、REGTR等学习方法在部分配准中的局限，发现它们缺乏对重叠区域和对应关系的联合置信度建模。基于此，他们借鉴了OMNet、RPMNet等置信度估计思路，设计了HOCE和CAMS模块，并将其与全局注意力机制和可微SVD求解器结合，形成端到端的CEGC框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过全局上下文同时估计重叠置信度和对应置信度，并用这些置信度加权求解变换。实现流程包括：①使用AGNN提取局部几何特征并增强全局上下文；②HOCE模块融合语义和几何信息预测每个点的重叠概率；③CAMS模块利用全局注意力生成软对应关系并给出置信度；④将置信度加权后输入可微SVD求解器得到最终刚性变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①HOCE模块首次将语义与几何特征融合来预测重叠区域；②CAMS通过全局注意力为对应关系赋予软置信度，缓解结构歧义；③置信度与求解器在同一端到端体系中耦合，保证不确定性在整个流程中被一致处理；④相较于以往分阶段或仅关注对应关系的学习方法，CEGC实现了更高的鲁棒性、准确性和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CEGC提出了一种统一的置信度驱动框架，能够在全局上下文中同时估计重叠与对应置信度，并通过加权SVD实现对部分点云的高精度、鲁棒配准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Partial point cloud registration is essential for autonomous perception and 3D scene understanding, yet it remains challenging owing to structural ambiguity, partial visibility, and noise. We address these issues by proposing Confidence Estimation under Global Context (CEGC), a unified, confidence-driven framework for robust partial 3D registration. CEGC enables accurate alignment in complex scenes by jointly modeling overlap confidence and correspondence reliability within a shared global context. Specifically, the hybrid overlap confidence estimation module integrates semantic descriptors and geometric similarity to detect overlapping regions and suppress outliers early. The context-aware matching strategy smitigates ambiguity by employing global attention to assign soft confidence scores to correspondences, improving robustness. These scores guide a differentiable weighted singular value decomposition solver to compute precise transformations. This tightly coupled pipeline adaptively down-weights uncertain regions and emphasizes contextually reliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D vision datasets demonstrate that CEGC outperforms state-of-the-art methods in accuracy, robustness, and generalization. Overall, CEGC offers an interpretable and scalable solution to partial point cloud registration under challenging conditions.&lt;/p&gt;</description></item><item><guid>2509.24370v1</guid><title>DINOReg: Strong Point Cloud Registration with Vision Foundation Model</title><link>http://arxiv.org/abs/2509.24370v1</link><author>Congjia Chen, Yufu Qu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出 DINOReg 网络，利用视觉与几何信息进行点云配准，显著提升配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是 3D 计算机视觉的基础任务，传统方法主要依赖几何特征，最近的研究尝试加入 RGB‑D 颜色信息，但仍未充分利用图像纹理与语义信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种充分利用视觉与几何信息的配准网络，以提高点云配准的精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用 DINOv2 提取图像视觉特征，在补丁级别与几何特征融合，并引入混合位置编码以捕捉图像与点云空间的空间关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 RGBD‑3DMatch 与 RGBD‑3DLoMatch 数据集上，DINOReg 相比现有几何或多模态方法提升了 14.2% 的补丁内点比例和 15.7% 的配准召回率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 融合视觉与几何特征并采用混合位置编码的 DINOReg 能显著提升点云配准性能，为多模态配准提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准是 3D 计算机视觉中的一项基础任务。大多数现有方法仅依赖几何信息进行特征提取和匹配。最近，一些研究将 RGB‑D 数据中的颜色信息纳入特征提取。虽然这些方法取得了显著改进，但它们并未充分利用图像中丰富的纹理和语义信息，且特征融合以图像失真方式进行，限制了性能。在本文中，我们提出 DINOReg，一种充分利用视觉和几何信息解决点云配准问题的网络。受视觉基础模型进展的启发，我们使用 DINOv2 从图像中提取信息丰富的视觉特征，并在补丁级别融合视觉和几何特征。该设计有效地将 DINOv2 提取的丰富纹理和全局语义信息与几何骨干捕获的细节几何结构信息相结合。此外，提出了一种混合位置嵌入，用于编码图像空间和点云空间的位置信息，增强了模型感知补丁间空间关系的能力。在 RGBD‑3DMatch 和 RGBD‑3DLoMatch 数据集上进行的大量实验表明，我们的方法相较于最先进的仅几何和多模态配准方法取得了显著提升，补丁内点比例提高了 14.2%，配准召回率提高了 15.7%。代码已公开发布在 https://github.com/ccjccjccj/DINOReg。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进点云配准，尤其在重叠度低或几何结构模糊时几何信息不足的场景。点云配准是三维重建、姿态估计等关键任务的基础，提升其鲁棒性和精度对实际应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法仅利用几何或仅用颜色信息，未充分利用图像的纹理与语义。借鉴了Transformer‑based注册网络（如GeoTransformer、Predator）和多模态融合思路，结合大规模视觉基础模型DINOv2，设计了在补丁级别进行视觉与几何特征融合的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是将图像补丁特征与点云补丁特征对齐并融合，随后通过混合位置编码的Transformer进行全局上下文聚合。实现流程包括：①用DINOv2提取图像补丁特征；②用KPConv‑FPN提取点云补丁特征；③通过相机标定将几何补丁投影到图像平面并聚合邻域视觉特征；④在补丁级别融合视觉与几何特征；⑤使用自注意力和交叉注意力的Transformer进行特征匹配；⑥根据匹配结果估计变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①使用视觉基础模型DINOv2获取丰富的语义与纹理信息；②在补丁级别进行视觉与几何特征的窗口聚合与融合；③提出混合位置编码同时编码图像像素位置和几何相对位置；④构建RGBD‑3DMatch与RGBD‑3DLoMatch数据集。与以往仅使用颜色或点级融合的方法不同，DINOReg充分利用图像全局语义并在补丁层面实现更精细的多模态融合，显著提升配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DINOReg通过将视觉基础模型与补丁级多模态融合及混合位置编码相结合，提供了更具辨别力的特征，显著提升了点云配准的准确性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration is a fundamental task in 3D computer vision. Most existing methods rely solely on geometric information for feature extraction and matching. Recently, several studies have incorporated color information from RGB-D data into feature extraction. Although these methods achieve remarkable improvements, they have not fully exploited the abundant texture and semantic information in images, and the feature fusion is performed in an image-lossy manner, which limit their performance. In this paper, we propose DINOReg, a registration network that sufficiently utilizes both visual and geometric information to solve the point cloud registration problem. Inspired by advances in vision foundation models, we employ DINOv2 to extract informative visual features from images, and fuse visual and geometric features at the patch level. This design effectively combines the rich texture and global semantic information extracted by DINOv2 with the detailed geometric structure information captured by the geometric backbone. Additionally, a mixed positional embedding is proposed to encode positional information from both image space and point cloud space, which enhances the model&amp;#x27;s ability to perceive spatial relationships between patches. Extensive experiments on the RGBD-3DMatch and RGBD-3DLoMatch datasets demonstrate that our method achieves significant improvements over state-of-the-art geometry-only and multi-modal registration methods, with a 14.2% increase in patch inlier ratio and a 15.7% increase in registration recall. The code is publicly available at https://github.com/ccjccjccj/DINOReg.&lt;/p&gt;</description></item><item><guid>2510.06578v1</guid><title>Novel point cloud registration approach for noninvasive patient specific estimation of leaflet strain from 3D images of heart valves</title><link>http://arxiv.org/abs/2510.06578v1</link><author>Wensi Wu, Matthew Daemer, Jeffrey A. Weiss, Alison M. Pouch, Matthew A. Jolley</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新型特征跟踪框架，用于从儿童和成人的三维超声图像中非侵入性地量化心脏瓣膜叶片应变，并验证其优于传统点基方法的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 瓣膜性心脏病普遍存在，是心力衰竭的重要原因。瓣叶应变是评估瓣膜病理机制的有前景指标，但目前缺乏可靠、可推广的非侵入性量化方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在临床获取的患者影像中，稳健、通用地量化心脏瓣膜叶片应变的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建基于特征跟踪的框架，利用儿童和成人的三维超声图像进行叶片应变计算；通过与有限元基准对比验证其准确性，并在不同瓣膜形态下无需参数调节即可跟踪跨相位变形。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在解剖变形和应变评估上比其他点基方法更准确；能够稳健跟踪不同形态瓣膜的跨相位变形；发现叶片中位应变及四分位范围大于0.5与瓣叶下垂相关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该特征跟踪框架能够准确、稳健地量化瓣膜叶片应变，为心脏瓣膜疾病的预后评估和纵向监测提供了潜在工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 瓣膜性心脏病普遍存在，是心力衰竭的重要原因。瓣叶应变是评估瓣膜病理机制的有前景指标。然而，基于临床获取的患者影像，稳健且可推广的非侵入性量化瓣膜应变的方法仍然有限。本文提出一种新型特征跟踪框架，利用儿童和成人的三维超声图像对房室瓣叶片应变进行量化。与有限元基准对比验证后，该方法在解剖变形和应变评估上比其他点基方法更准确。进一步地，该方法能够在不同瓣膜形态下，无需参数调节，稳健地跟踪跨相位的瓣膜变形。我们的分析显示，叶片第一主应变的中位数和四分位范围大于0.5与瓣叶下垂（脱垂）相关。对心脏瓣膜疾病的生物力学特征进行进一步研究，有望提升预后评估和瓣膜疾病的纵向评估。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Valvular heart disease is prevalent and a major contributor to heart failure. Valve leaflet strain is a promising metric for evaluating the mechanics underlying the initiation and progression of valvular pathology. However, robust and generalizable methods for noninvasively quantifying valvular strain from clinically acquired patient images remain limited. In this work, we present a novel feature-tracking framework for quantifying leaflet strain in atrioventricular valves using 3D echocardiographic images of pediatric and adult patients. Our method demonstrated superior accuracy in the assessment of anatomical deformation and strain of heart valves compared to other point-based approaches, as verified against a finite element benchmark. Further, our approach can robustly track inter-phase deformation of valves across highly variable morphologies without parameter tuning. Our analysis revealed that a median and interquartile range of the 1st principal strain greater than 0.5 is associated with leaflet billow (prolapse). Further investigation of the biomechanical signatures of heart valve disease has the potential to enhance prognostic assessment and longitudinal evaluation of valvular disease.&lt;/p&gt;</description></item><item><guid>2510.06582v2</guid><title>Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation</title><link>http://arxiv.org/abs/2510.06582v2</link><author>Fei Zhang, Rob Chancia, Josie Clapp, Amirhossein Hassanzadeh, Dimah Dera, Richard MacKenzie, Jan van Aardt</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种半自动、基于不确定性感知的地面激光扫描点云语义分割流程，结合球面投影、特征增强、集成学习和目标标注，显著降低人工标注成本并保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统TLS点云语义分割需要大量人工标注，成本高昂。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种高效、低成本的半自动标注管线，并评估所需标注数据量与特征重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将3D点投影到2D球面网格，使用多源特征增强像素，训练集成分割网络生成伪标签和不确定性图，利用不确定性图指导标注；随后将2D结果反投影回3D，配合三层可视化工具进行快速审核。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 性能在约12个标注扫描后趋于饱和；几何特征贡献最大；九通道特征堆叠已能捕获大部分判别力，平均交并比约为0.76；方法在其他数据集上也能泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该管线可实现可扩展、高质量的TLS点云语义分割，适用于生态监测等场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确的地面激光扫描（TLS）点云语义分割受限于昂贵的人工标注。我们提出一种半自动、基于不确定性的管线，集成球面投影、特征增强、集成学习和目标标注，以降低标注工作量，同时保持高精度。我们的方案将3D点投影到2D球面网格，用多源特征增强像素，并训练一组分割网络生成伪标签和不确定性图，后者指导对模糊区域的标注。2D输出再反投影回3D，得到密集标注的点云，并配备三层可视化工具（2D特征图、3D彩色点云、紧凑虚拟球体），用于快速筛选和审阅。利用该管线，我们构建了Mangrove3D，针对红树林的TLS语义分割数据集。我们进一步评估数据效率和特征重要性，回答两个关键问题：（1）需要多少标注数据；（2）哪些特征最重要。结果显示，性能在约12个标注扫描后饱和，几何特征贡献最大，紧凑的九通道堆叠几乎捕获了所有判别力，平均交并比在0.76左右稳定。最后，我们通过在ForestSemantic和Semantic3D数据集上的交叉测试确认了特征增强策略的泛化能力。我们的贡献包括：（i）一个稳健、基于不确定性的TLS标注管线及可视化工具；（ii）Mangrove3D数据集；（iii）关于数据效率和特征重要性的经验指导，从而实现可扩展、高质量的TLS点云分割，适用于生态监测及其他领域。数据集和处理脚本公开可在 https://fz-rit.github.io/through-the-lidars-eye/ 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在降低地面激光扫描（TLS）点云语义分割的人工标注成本，并提升在复杂生态环境（如红树林）中的分割精度。准确的分割是森林生物量估算、碳汇评估等生态监测任务的基础，而现有数据集稀缺且标注工作耗时，限制了深度学习方法的广泛应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已有的球面投影、特征融合、集成学习和不确定性分析技术，提出了一个半自动、基于不确定性的标注流程。该方法借鉴了自动驾驶领域的 RangeNet、SalsaNext 等投影网络，以及主动学习和自训练的策略，并将其迁移到生态 TLS 场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 3D 点云投影到 2D 球面图像，使用多源特征构建九通道输入，训练集成网络生成伪标签和不确定性图，利用不确定性引导人工标注，最后将标注结果反投影回 3D。整个流程包括：① 球面投影与特征堆叠；② 训练集成网络得到伪标签与不确定性；③ 通过不确定性聚焦人工纠正；④ 反投影生成完整 3D 标注，并提供三层可视化工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 提供了一个完整的不确定性驱动的半自动标注管线；② 在 TLS 上实现了多通道特征堆叠的球面投影，显著提升分割性能；③ 通过实验给出了数据量与特征重要性的经验指导；④ 通过跨数据集测试验证了方法的通用性；⑤ 发布了首个红树林 TLS 分割数据集 Mangrove3D。与以往主要针对城市或室内数据、缺乏不确定性引导的工作不同，本文针对生态复杂场景提供了可扩展的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于球面投影、特征融合和不确定性引导的半自动标注管线，显著降低了红树林等生态 TLS 点云分割的人工成本，并通过 Mangrove3D 数据集验证了其高效性与通用性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D.   Our contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.&lt;/p&gt;</description></item><item><guid>2510.10365v1</guid><title>PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion</title><link>http://arxiv.org/abs/2510.10365v1</link><author>Linlian Jiang, Rui Ma, Li Gu, Ziqiang Wang, Xinxin Zuo, Yang Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为 PointMAC 的元学习框架，用于点云完成任务的测试时自适应。通过自监督辅助目标和基于 MAML 的元学习，模型能够在推理时对每个样本进行特定细化，从而显著提升完成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云完成在机器人和增强现实等安全关键应用中至关重要，但现有模型在推理时是静态的，过度依赖训练时学习到的归纳偏置，难以适应测试时的新结构模式和传感器失真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在测试时自适应的点云完成方法，使模型能够针对每个样本进行细化，而无需额外监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PointMAC 在推理时通过优化两种自监督辅助目标（模拟结构和传感器缺失）来调整共享编码器，解码器保持不变。采用基于 MAML 的元辅助学习策略保证辅助目标的优化与主任务一致，并引入 Adaptive λ-Calibration 机制平衡主任务与辅助目标的梯度，提升自适应稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成、模拟和真实数据集上的实验表明，PointMAC 能够逐样本细化点云，取得领先的完成效果，显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文首次将元辅助测试时自适应应用于点云完成，证明了该策略在提升模型鲁棒性和完成质量方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云完成对于机器人和增强现实等安全关键应用中的鲁棒三维感知至关重要。然而，现有模型在推理时是静态的，并且过度依赖训练期间学习到的归纳偏置，限制了它们在测试时适应新结构模式和传感器引起的失真。为了解决这一限制，我们提出了 PointMAC，一种用于点云完成的元学习框架，可实现稳健的测试时自适应。它能够在不需要额外监督的情况下对每个样本进行特定细化。我们的方法在两个自监督辅助目标下优化完成模型，这些目标模拟结构和传感器级的不完整性。基于 Model-Agnostic Meta-Learning（MAML）的元辅助学习策略确保由辅助目标驱动的自适应始终与主要完成任务保持一致。在推理过程中，我们通过优化辅助损失实时调整共享编码器，而解码器保持固定。为进一步稳定自适应，我们引入 Adaptive λ-Calibration，一种元学习机制，用于平衡主任务与辅助目标之间的梯度。对合成、模拟和真实数据集的广泛实验表明，PointMAC 通过对每个样本进行细化，取得了最先进的结果，生成高质量的完成点云。据我们所知，这是首次将元辅助测试时自适应应用于点云完成。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成模型在测试时缺乏自适应能力的问题，使其无法针对不同结构缺失和传感器噪声进行动态调整，从而产生泛化不足的通用补全结果。该问题在机器人、自动驾驶和增强现实等安全关键场景中尤为重要，因为准确的三维感知直接影响决策与操作安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过将测试时自适应（TTA）与自监督辅助任务相结合，提出了Bi‑Aux Units来模拟结构缺失和噪声扰动，并采用MAML框架使辅助任务的梯度与主任务保持一致。该设计借鉴了Transformer编码器、双向自监督重建、噪声去除以及梯度平衡技术等已有工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用元学习驱动的自监督辅助任务，在测试时对共享编码器进行样本特定的微调，从而提升完成质量。实现流程包括：①训练阶段，使用MAML内循环对编码器进行辅助任务微调，外循环优化主完成损失；②推理阶段，冻结解码器，仅通过Bi‑Aux产生的自监督损失更新编码器；③得到适配后的编码器后生成最终补全点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①首次将元学习驱动的测试时自适应应用于点云完成；②设计Bi‑Aux Units提供结构遮挡与噪声去除两种自监督信号；③引入Adaptive λ‑Calibration平衡主任务与辅助任务梯度；④通过MAML确保辅助任务与主任务一致性。与以往静态推理或未对齐辅助任务的工作相比，PointMAC实现了更稳健、样本特定的补全效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointMAC通过元学习驱动的自监督辅助任务和自适应梯度平衡，实现了点云完成的测试时自适应，显著提升了补全质量并达到最先进水平。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is essential for robust 3D perception in safety-critical applications such as robotics and augmented reality. However, existing models perform static inference and rely heavily on inductive biases learned during training, limiting their ability to adapt to novel structural patterns and sensor-induced distortions at test time. To address this limitation, we propose PointMAC, a meta-learned framework for robust test-time adaptation in point cloud completion. It enables sample-specific refinement without requiring additional supervision. Our method optimizes the completion model under two self-supervised auxiliary objectives that simulate structural and sensor-level incompleteness. A meta-auxiliary learning strategy based on Model-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary objectives is consistently aligned with the primary completion task. During inference, we adapt the shared encoder on-the-fly by optimizing auxiliary losses, with the decoder kept fixed. To further stabilize adaptation, we introduce Adaptive $λ$-Calibration, a meta-learned mechanism for balancing gradients between primary and auxiliary objectives. Extensive experiments on synthetic, simulated, and real-world datasets demonstrate that PointMAC achieves state-of-the-art results by refining each sample individually to produce high-quality completions. To the best of our knowledge, this is the first work to apply meta-auxiliary test-time adaptation to point cloud completion.&lt;/p&gt;</description></item><item><guid>2510.10471v2</guid><title>DAGLFNet: Deep Feature Attention Guided Global and Local Feature Fusion for Pseudo-Image Point Cloud Segmentation</title><link>http://arxiv.org/abs/2510.10471v2</link><author>Chuang Chen, Yi Lin, Bo Wang, Jing Hu, Xi Wu, Wenyi Ge</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于伪图像的语义分割框架 DAGLFNet，旨在解决 LiDAR 点云与二维网格融合时的特征不一致问题。通过全局-局部特征融合编码、多分支特征提取和深度特征引导注意力机制，显著提升了点云语义分割的特征辨别能力，并在两个公开数据集上取得了优异的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 环境感知系统在高精度地图构建和自动驾驶中起着关键作用，LiDAR 作为核心传感器提供精确的三维点云数据。然而，如何高效处理无结构点云并提取结构化语义信息仍是一个重要挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够在保持效率的同时，提升伪图像与原始三维信息融合质量的语义分割方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DAGLFNet 由三大模块组成：全局-局部特征融合编码增强局部相关性并捕获全局上下文；多分支特征提取网络获取更丰富的邻域信息，提升轮廓特征辨别；深度特征引导注意力机制细化跨通道特征融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 SemanticKITTI 验证集上获得 69.9% 的平均交并比，在 nuScenes 验证集上获得 78.7%，显示出在准确率与效率之间取得了良好平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DAGLFNet 通过改进特征融合策略，有效解决了伪图像与三维点云不一致导致的特征辨别不足问题，证明了其在高精度环境感知中的可行性和优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 环境感知系统对于高精度地图构建和自动驾驶至关重要，LiDAR 作为核心传感器提供准确的三维点云数据。高效处理无结构点云并提取结构化语义信息仍是一个重大挑战。近年来，出现了许多基于伪图像的表示方法，通过将三维点云与二维网格融合来平衡效率和性能。然而，伪图像表示与原始三维信息之间的根本不一致严重削弱了二维-三维特征融合，成为统一信息融合的主要障碍，并导致特征辨别能力差。本文提出 DAGLFNet，一种基于伪图像的语义分割框架，旨在提取具有辨别力的特征。它包含三个关键组件：首先，Global-Local Feature Fusion Encoding（GL-FFE）模块增强同一集合内的局部特征相关性并捕获全局上下文信息；其次，多分支特征提取（MB-FE）网络捕获更丰富的邻域信息，提升轮廓特征的辨别力；第三，Feature Fusion via Deep Feature-guided Attention（FFDFA）机制细化跨通道特征融合精度。实验评估表明，DAGLFNet 在 SemanticKITTI 和 nuScenes 验证集上分别取得 69.9% 和 78.7% 的平均交并比，方法在准确率与效率之间实现了优异的平衡。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文想解决在将 LiDAR 点云投影成伪图像时，深度冲突和边界模糊导致的特征信息丢失问题。把三维点云转换成二维图像可以提高计算效率，但会破坏空间结构，影响分割精度。准确地保留点云的几何和语义信息对自动驾驶、地图构建等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有伪图像方法的缺陷，发现它们忽略了子集内部的特征关联和距离信息。随后借鉴了 FRNet、FARVNet、RangeNet++ 等工作中的投影与特征融合思路，提出了 GL-FFE、MB-FE 和 FFDFA 三个模块来补足这些不足。设计时结合了多分支卷积、注意力机制和距离加权融合等技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过全局-局部特征融合、分支特征提取和深度引导注意力，构建更具判别力的伪图像特征。实现流程包括：①将点云按激光束分组并编码为多维特征；②将组特征映射到二维图像；③使用 GL-FFE 捕获全局上下文和局部几何；④用 MB-FE 扩大感受野并强化边界特征；⑤通过 FFDFA 用距离信息加权跨通道融合；⑥最后的融合头输出每点语义标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①GL-FFE 模块同时建模全局依赖和局部几何；②MB-FE 网络通过多分支扩展感受野，提升边界识别；③FFDFA 采用距离加权的深度引导注意力，精细化特征融合。与之前的工作相比，DAGLFNet 更好地保留了三维结构信息，解决了深度冲突和边界模糊问题，并在 SemanticKITTI 和 nuScenes 上取得更高的 mIoU。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAGLFNet 通过全局-局部特征融合、多分支提取和距离引导注意力，显著提升了伪图像 LiDAR 点云分割的精度，实现了 SemanticKITTI 和 nuScenes 上的领先 mIoU。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Environmental perception systems are crucial for high-precision mapping and autonomous navigation, with LiDAR serving as a core sensor providing accurate 3D point cloud data. Efficiently processing unstructured point clouds while extracting structured semantic information remains a significant challenge. In recent years, numerous pseudo-image-based representation methods have emerged to balance efficiency and performance by fusing 3D point clouds with 2D grids. However, the fundamental inconsistency between the pseudo-image representation and the original 3D information critically undermines 2D-3D feature fusion, posing a primary obstacle for coherent information fusion and leading to poor feature discriminability. This work proposes DAGLFNet, a pseudo-image-based semantic segmentation framework designed to extract discriminative features. It incorporates three key components: first, a Global-Local Feature Fusion Encoding (GL-FFE) module to enhance intra-set local feature correlation and capture global contextual information; second, a Multi-Branch Feature Extraction (MB-FE) network to capture richer neighborhood information and improve the discriminability of contour features; and third, a Feature Fusion via Deep Feature-guided Attention (FFDFA) mechanism to refine cross-channel feature fusion precision. Experimental evaluations demonstrate that DAGLFNet achieves mean Intersection-over-Union (mIoU) scores of 69.9% and 78.7% on the validation sets of SemanticKITTI and nuScenes, respectively. The method achieves an excellent balance between accuracy and efficiency.&lt;/p&gt;</description></item><item><guid>2510.10876v1</guid><title>rareboost3d: a synthetic lidar dataset with enhanced rare classes</title><link>http://arxiv.org/abs/2510.10876v1</link><author>Shutong Lin, Zhengkang Xiang, Jianzhong Qi, Kourosh Khoshelham</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一个名为RareBoost3D的合成点云数据集，并提出了CSC loss跨域语义对齐方法，以解决真实点云数据中稀有类别的长尾问题，并显著提升了基于LiDAR的点云分割模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 真实点云数据集在LiDAR感知技术发展中起重要作用，但稀有类别样本不足导致长尾问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过合成数据补充稀有类别样本，并通过跨域对齐提升模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 创建RareBoost3D合成点云数据集；提出CSC loss跨域语义对齐方法，将不同域中同一类别的特征表示对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CSC loss对齐显著提升了LiDAR点云分割模型在真实数据上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 合成数据与跨域对齐方法有效缓解长尾问题，提升了点云分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 真实世界的点云数据集为基于LiDAR的感知技术（如自动驾驶中的目标分割）做出了重要贡献。然而，由于某些稀有类别实例数量有限，长尾问题仍是现有数据集面临的主要挑战。为解决此问题，我们提出了一个名为RareBoost3D的合成点云数据集，该数据集通过提供大量稀有类别实例来补充现有真实数据集。为有效利用合成与真实数据，我们进一步提出了名为CSC loss的跨域语义对齐方法，该方法将不同域中同一类别的特征表示对齐。实验结果表明，该对齐方法显著提升了基于LiDAR的点云分割模型在真实数据上的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 LiDAR 点云语义分割中稀有类别样本不足导致的长尾问题，这一问题在自动驾驶等实际应用中会削弱模型对少见物体的识别能力，影响安全与性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先利用 CARLA 仿真器生成大量带有稀有类别的合成点云，随后采用对比学习中的跨域语义一致性（CSC）损失，将合成与真实数据的特征对齐；这一思路借鉴了 SynLiDAR、PointAug 等合成数据增强方法以及 PointDR 的对比学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是通过合成数据补充稀有类别样本，并用对比学习把不同域同类别的特征聚集在一起。实现流程包括：1）使用 CARLA 生成 RareBoost3D 数据；2）将合成与真实数据统一标签；3）构建每个类别的特征原型记忆库；4）在训练时计算 CSC 损失与分割损失的总和；5）在 MinkUNet 或 Point Transformer 等骨干网络上训练并评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）RareBoost3D 数据集专门提升稀有类别样本量；2）基于原型的 CSC 损失实现跨域对比学习，无需复杂的对抗训练；3）在多种骨干网络上验证了对稀有类别分割的显著提升。与以往仅做数据增强或使用对抗域适配的工作不同，本文提供了更易实现且效果更好的对比学习方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一个稀有类别样本丰富的合成 LiDAR 数据集和一种基于对比学习的跨域语义一致性损失，显著提升了真实场景下稀有类别的点云分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Real-world point cloud datasets have made significant contributions to the development of LiDAR-based perception technologies, such as object segmentation for autonomous driving. However, due to the limited number of instances in some rare classes, the long-tail problem remains a major challenge in existing datasets. To address this issue, we introduce a novel, synthetic point cloud dataset named RareBoost3D, which complements existing real-world datasets by providing significantly more instances for object classes that are rare in real-world datasets. To effectively leverage both synthetic and real-world data, we further propose a cross-domain semantic alignment method named CSC loss that aligns feature representations of the same class across different domains. Experimental results demonstrate that this alignment significantly enhances the performance of LiDAR point cloud segmentation models over real-world data.&lt;/p&gt;</description></item><item><guid>2510.11017v1</guid><title>High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation</title><link>http://arxiv.org/abs/2510.11017v1</link><author>Runyang Feng, Hyung Jin Chang, Tze Ho Elden Tse, Boeun Kim, Yi Chang, Yixing Gao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的框架，利用扩展的 Mamba 模型分别学习视频人类姿态估计中的全局和局部高分辨率时空表示，并在四个基准数据集上实现了优于现有方法的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频人类姿态估计需要同时捕捉全局动态上下文和局部运动细节，但现有方法往往使用单一的卷积或注意力结构，难以平衡两者，并且在处理高分辨率序列时会出现二次复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够在保持线性复杂度的同时，分别高效提取全局和局部时空特征的模型，以提升视频姿态估计的精度和计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先设计全局时空 Mamba，通过六维选择性时空扫描和空间时间调制扫描合并来提取全局表示；随后引入基于窗口的时空扫描局部细化 Mamba，用于增强局部关键点运动的高频细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该模型在四个基准数据集上均优于现有最先进方法，并在计算成本与性能之间取得更好的平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 扩展的 Mamba 框架能够有效解决高分辨率视频姿态估计中全局与局部建模的矛盾，并提供了低复杂度的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要：对高分辨率时空表示进行建模，包括全局动态上下文（例如整体人类运动趋势）和局部运动细节（例如关键点的高频变化），对于基于视频的人类姿态估计至关重要。当前最先进的方法通常在单一的建模结构（卷积或基于注意力的块）内统一时空学习，这本质上难以平衡全局和局部动态建模，可能使网络偏向其中之一，导致性能不佳。此外，现有的基于视频的人类姿态估计模型在捕捉全局依赖时会出现二次复杂度，限制了其在高分辨率序列中的适用性。最近，状态空间模型（称为 Mamba）在以线性复杂度建模长程上下文方面表现出显著潜力；然而，它们仅限于 1D 序列数据。在本文中，我们提出了一种新框架，从两个方面扩展 Mamba，以分别学习全局和局部高分辨率时空表示用于基于视频的人类姿态估计。具体而言，我们首先提出了全局时空 Mamba，它执行六维选择性时空扫描和空间与时间调制扫描合并，以高效提取高分辨率序列的全局表示。我们进一步引入基于窗口的时空扫描局部细化 Mamba，以增强局部关键点运动的高频细节。对四个基准数据集的广泛实验表明，所提出的模型在实现更好计算权衡的同时，优于最先进的基于视频的人类姿态估计方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Modeling high-resolution spatiotemporal representations, including both global dynamic contexts (e.g., holistic human motion tendencies) and local motion details (e.g., high-frequency changes of keypoints), is essential for video-based human pose estimation (VHPE). Current state-of-the-art methods typically unify spatiotemporal learning within a single type of modeling structure (convolution or attention-based blocks), which inherently have difficulties in balancing global and local dynamic modeling and may bias the network to one of them, leading to suboptimal performance. Moreover, existing VHPE models suffer from quadratic complexity when capturing global dependencies, limiting their applicability especially for high-resolution sequences. Recently, the state space models (known as Mamba) have demonstrated significant potential in modeling long-range contexts with linear complexity; however, they are restricted to 1D sequential data. In this paper, we present a novel framework that extends Mamba from two aspects to separately learn global and local high-resolution spatiotemporal representations for VHPE. Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D selective space-time scan and spatial- and temporal-modulated scan merging to efficiently extract global representations from high-resolution sequences. We further introduce a windowed space-time scan-based Local Refinement Mamba to enhance the high-frequency details of localized keypoint motions. Extensive experiments on four benchmark datasets demonstrate that the proposed model outperforms state-of-the-art VHPE approaches while achieving better computational trade-offs.&lt;/p&gt;</description></item><item><guid>2510.11565v1</guid><title>SNAP: Towards Segmenting Anything in Any Point Cloud</title><link>http://arxiv.org/abs/2510.11565v1</link><author>Aniket Gupta, Hanhui Wang, Charles Saunders, Aruni RoyChowdhury, Hanumant Singh, Huaizu Jiang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SNAP 是一种统一的交互式 3D 点云分割模型，支持点和文本提示，跨室内、室外和空中等多域训练，利用域自适应归一化避免负迁移，自动生成掩码并与 CLIP 嵌入匹配，实现全景和开放词汇分割，实验表明在多项零样本基准上取得领先或竞争性表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法往往仅限于单一域（室内或室外）和单一交互方式（空间点击或文本提示），多数据集训练会导致负迁移，缺乏通用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够在多域、多交互方式下统一工作的 3D 分割模型，克服现有局限并提升跨域泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在七个不同域的数据集上训练 SNAP，使用域自适应归一化防止负迁移；对文本提示自动生成掩码候选并与 CLIP 文本嵌入匹配，实现全景和开放词汇分割；支持点提示和文本提示两种交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SNAP 在 9 个零样本空间提示基准中 8 个获得最优成绩，在 5 个文本提示基准中表现竞争性；整体分割质量高，证明统一模型可匹配或超越专用域模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 统一模型能够在多域、多交互方式下实现高质量分割，为可扩展的 3D 注释提供实用工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 交互式 3D 点云分割通过用户引导提示实现对复杂 3D 场景的高效标注。然而，当前方法通常仅限于单一域（室内或室外）和单一交互形式（空间点击或文本提示）。此外，在多个数据集上训练往往导致负迁移，产生缺乏通用性的域特定工具。为解决这些局限，我们提出 SNAP（Segment Anything in Any Point cloud），一种支持点基和文本基提示、跨多域的统一交互式 3D 分割模型。我们的方案通过在覆盖室内、室外和空中环境的七个数据集上训练，并采用域自适应归一化来防止负迁移，从而实现跨域泛化。对于文本提示分割，我们自动生成掩码候选并与 CLIP 文本嵌入匹配，支持全景和开放词汇分割。大量实验表明，SNAP 一直提供高质量分割结果，在 9 个零样本空间提示基准中 8 个实现了最先进性能，在 5 个文本提示基准中表现竞争。结果表明，统一模型可以匹配或超过专用域特定方法，为可扩展 3D 注释提供实用工具。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在构建一个能够在室内、室外和空中三种不同点云域中统一工作、同时支持点和文本两种提示方式的交互式点云分割模型。该问题重要，因为手工标注三维点云既耗时又昂贵，缺乏通用工具会限制大规模数据集的构建和研究进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 2D 领域的 SAM 模型和 CLIP 的跨模态嵌入，结合 3D 点云编码器 PTv3，并在此基础上引入域归一化、自动提示点生成和文本匹配等技术。通过对七个不同域的数据集进行联合训练，作者解决了跨域负迁移问题，并实现了多模态交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用统一的点云编码器和掩码解码器，同时处理空间提示和文本提示。流程为：①用 PTv3 + 域归一化编码点云；②对空间提示点进行编码并与点云特征融合；③解码器生成掩码、置信度和 CLIP 嵌入；④若为文本提示，先用自动提示点生成掩码候选，再将候选的 CLIP 嵌入与文本嵌入匹配，得到最终分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①域归一化（而非数据集归一化）以缓解多域负迁移；②统一模型支持室内、室外、空中三大域；③同时支持点提示和文本提示；④自动提示点生成实现无人工交互的全景分割；⑤预测掩码的 CLIP 嵌入实现开放词汇分割。与以往仅针对单一域或单一提示方式的模型相比，SNAP 在通用性和多模态交互上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SNAP 提供了一个跨域、支持点与文本双模态交互的统一点云分割框架，在室内、室外和空中场景中实现了领先的零样本性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Interactive 3D point cloud segmentation enables efficient annotation of complex 3D scenes through user-guided prompts. However, current approaches are typically restricted in scope to a single domain (indoor or outdoor), and to a single form of user interaction (either spatial clicks or textual prompts). Moreover, training on multiple datasets often leads to negative transfer, resulting in domain-specific tools that lack generalizability. To address these limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in \textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D segmentation that supports both point-based and text-based prompts across diverse domains. Our approach achieves cross-domain generalizability by training on 7 datasets spanning indoor, outdoor, and aerial environments, while employing domain-adaptive normalization to prevent negative transfer. For text-prompted segmentation, we automatically generate mask proposals without human intervention and match them against CLIP embeddings of textual queries, enabling both panoptic and open-vocabulary segmentation. Extensive experiments demonstrate that SNAP consistently delivers high-quality segmentation results. We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for spatial-prompted segmentation and demonstrate competitive results on all 5 text-prompted benchmarks. These results show that a unified model can match or exceed specialized domain-specific approaches, providing a practical tool for scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/&lt;/p&gt;</description></item><item><guid>2510.13245v2</guid><title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title><link>http://arxiv.org/abs/2510.13245v2</link><author>Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; • 研究目标是生成逼真且语义丰富的户外 3D 场景，用于城市仿真和自动驾驶等应用。  • 现有进展受限于缺乏公开、标注完善的数据集。  • 提出 SketchSem3D，首个大规模基于手绘草图和卫星图像伪标签的户外 3D 语义场景生成基准。  • SketchSem3D 包含 Sketch-based SemanticKITTI 与 Sketch-based KITTI-360 两个子集，提供 LiDAR 体素、草图及标注卫星图像。  • 设计 Cylinder Mamba Diffusion（CymbaDiff）模型，强化空间连贯性，捕捉圆柱连续性与垂直层级，保持物理邻域关系与全局上下文。  • 在 SketchSem3D 上的大量实验表明，CymbaDiff 在语义一致性、空间逼真度和跨数据集泛化方面均优于现有方法。  • 代码与数据集将公开发布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 户外 3D 语义场景生成需要逼真且语义丰富的环境，但缺乏公开且标注完善的数据集限制了研究进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个基于手绘草图和卫星图像伪标签的大规模户外 3D 语义场景生成基准 SketchSem3D，并提出 Cylinder Mamba Diffusion（CymbaDiff）模型以提升空间连贯性和生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SketchSem3D 包含两个子集：Sketch-based SemanticKITTI 与 Sketch-based KITTI-360，分别提供 LiDAR 体素、对应草图和伪标注卫星图像；CymbaDiff 在扩散模型中加入圆柱结构化空间排序，显式捕捉圆柱连续性与垂直层级，保持物理邻域关系与全局上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CymbaDiff 在 SketchSem3D 上实现了更高的语义一致性、更逼真的空间表现以及更好的跨数据集泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SketchSem3D 与 CymbaDiff 为户外 3D 语义场景生成提供了新的数据基准和方法，显著提升了生成质量，并将代码与数据集公开，促进后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 户外 3D 语义场景生成能够为城市仿真和自动驾驶等应用提供逼真且语义丰富的环境。然而，在该方向上的进展受到缺乏公开、标注完善的数据集的限制。我们提出 SketchSem3D，这是首个基于抽象手绘草图和卫星图像伪标签的大规模户外 3D 语义场景生成基准。SketchSem3D 包含两个子集：Sketch-based SemanticKITTI 和 Sketch-based KITTI-360（包含 LiDAR 体素以及对应的草图和标注卫星图像），以实现标准化、严格且多样化的评估。我们还提出 Cylinder Mamba Diffusion（CymbaDiff），显著提升户外 3D 场景生成的空间连贯性。CymbaDiff 强制执行结构化空间排序，显式捕捉圆柱连续性和垂直层级，并在生成的场景中保持物理邻域关系和全局上下文。对 SketchSem3D 的广泛实验表明，CymbaDiff 在语义一致性、空间逼真度和跨数据集泛化方面表现优异。代码和数据集将公开发布在 https://github.com/Lillian-research-hub/CymbaDiff。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在从自由手绘草图和伪标注的卫星图像中生成逼真且语义丰富的三维户外城市场景。此类生成在城市仿真、自动驾驶等应用中至关重要，但受限于缺乏公开、规模大且标注完善的数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先提出“基于草图的三维户外场景生成”这一新任务，并构建了SketchSem3D数据集。方法借鉴了状态空间模型（SSM）在长距离依赖建模中的优势，以及扩散模型在三维生成中的稳定性，结合了先前的BEV条件生成和多尺度策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将草图和伪标注的卫星图像作为条件，利用场景结构估计网络（SSEN）得到粗略结构，再通过潜在映射网络（LMN）压缩为潜在表示，随后使用带有圆柱形Mamba块（CylMa）的扩散去噪器（CymbaDiff）逐步生成完整的三维体素网格。整体流程为：输入草图+卫星图 → SSEN → LMN → CymbaDiff去噪 → 3D体素输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①提出基于草图的三维户外场景生成任务；②首个公开的SketchSem3D大规模基准数据集；③设计圆柱形Mamba块以强化空间连贯性和垂直层次；④在扩散框架中引入结构化空间排序。与以往仅使用BEV或多尺度室内场景的工作不同，本文实现了更自然的用户交互和更高的语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一个基于草图和卫星图像的三维户外场景生成框架，并通过圆柱形Mamba扩散模型实现了更真实、更语义一致的三维城市场景，同时发布了首个大规模SketchSem3D数据集。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Outdoor 3D semantic scene generation produces realistic and semantically rich environments for applications such as urban simulation and autonomous driving. However, advances in this direction are constrained by the absence of publicly available, well-annotated datasets. We introduce SketchSem3D, the first large-scale benchmark for generating 3D outdoor semantic scenes from abstract freehand sketches and pseudo-labeled annotations of satellite images. SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based KITTI-360 (containing LiDAR voxels along with their corresponding sketches and annotated satellite images), to enable standardized, rigorous, and diverse evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that significantly enhances spatial coherence in outdoor 3D scene generation. CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical continuity and vertical hierarchy, and preserves both physical neighborhood relationships and global context within the generated scenes. Extensive experiments on SketchSem3D demonstrate that CymbaDiff achieves superior semantic consistency, spatial realism, and cross-dataset generalization. The code and dataset will be available at https://github.com/Lillian-research-hub/CymbaDiff&lt;/p&gt;</description></item><item><guid>2510.13307v2</guid><title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title><link>http://arxiv.org/abs/2510.13307v2</link><author>Yang Li, Aming Wu, Zihao Zhang, Yahong Han</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种基于因果关系的点云分割新类别发现方法，利用已标注的基础类别信息来学习对未标注新类别的分割。通过结构因果模型和因果表示原型，消除隐藏混杂因素，实现从基础类别到新类别的因果推理，实验表明方法优于现有技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的新类别发现方法在点云分割中往往依赖粗糙或统计相关性，容易导致新类别推断混淆。需要更精确的点表示与类别标签之间的关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入因果关系约束，学习能够准确对应类别的点云表示，从而实现仅凭基础类别监督即可对新类别进行分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用结构因果模型分析基础类别表示中的隐藏混杂因素，构建消除混杂的因果表示原型；利用图结构建模基础类别与新类别原型之间的因果关系，实现因果推理；整体方法称为联合学习因果表示与推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验和可视化结果表明，该方法在3D和2D新类别发现语义分割任务中表现出更优的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过因果关系建模和因果表示学习，可以显著提升点云分割中新类别发现的效果，为相关任务提供了有效的技术方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文聚焦于点云分割的新类别发现（3D-NCD），旨在学习一种仅利用已标注基础类别监督即可对未标注（新）3D类别进行分割的模型。该任务的关键在于建立点表示与其基础类别标签之间的精确关联，以及基础类别与新类别点之间的表示关联。粗糙或统计相关性学习可能导致新类别推断混淆。如果我们在学习过程中强制施加因果关系作为一种强关联约束，则应能揭示准确对应类别的本质点云表示。为此，我们引入结构因果模型（SCM）重新表述3D-NCD问题，并提出一种新方法，即联合学习因果表示与推理。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别与新类别之间的因果关系。随后设计了消除混杂的因果表示原型，以捕捉基础类别的因果表示。随后使用图结构来建模基础类别因果表示原型与新类别原型之间的因果关系，从而实现从基础到新类别的因果推理。对3D和2D NCD语义分割的广泛实验与可视化结果表明，我们的方法具有优越性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在点云语义分割中，如何仅利用已标注的基类信息来发现并分割未标注的新类。该问题在自动驾驶、机器人等实际场景中至关重要，因为真实环境中会出现未知物体，传统的闭域假设无法满足需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先发现基类分类器往往学习到快捷特征（混淆因子），并认为新类是基类通过因果机制产生的变体。基于此，他们借鉴了结构因果模型、因果对抗去混淆以及 2D NCD 的聚类思路，设计了基于因果表示原型学习和图结构因果推理的完整框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先通过因果对抗去除基类特征中的混淆因子，得到干净的因果表示；随后为基类和新类分别学习原型，并将这些原型作为图节点构建因果图；通过因果剪枝和方向一致性约束优化图结构；最后将优化后的图输入图卷积网络，得到基类标签和新类伪标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次将因果学习引入 3D 新类发现；2) 提出因果表示原型学习和图结构因果推理；3) 通过因果对抗去混淆、因果剪枝和方向一致性约束实现更可靠的推理。与以往仅基于统计相似度的 3D NCD 方法不同，该方法显式建模因果关系并消除快捷特征，提升了新类分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种因果驱动的框架，学习无混淆因子表示并建模基类与新类之间的因果关系，从而实现对 3D 点云中未知类别的准确发现与分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation (3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes using only the supervision from labeled (base) 3D classes. The key to this task is to setup the exact correlations between the point representations and their base class labels, as well as the representation correlations between the points from base and novel classes. A coarse or statistical correlation learning may lead to the confusion in novel class inference. lf we impose a causal relationship as a strong correlated constraint upon the learning process, the essential point cloud representations that accurately correspond to the classes should be uncovered. To this end, we introduce a structural causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method, i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we first analyze hidden confounders in the base class representations and the causal relationships between the base and novel classes through SCM. We devise a causal representation prototype that eliminates confounders to capture the causal representations of base classes. A graph structure is then used to model the causal relationships between the base classes&amp;#x27; causal representation prototypes and the novel class prototypes, enabling causal reasoning from base to novel classes. Extensive experiments and visualization results on 3D and 2D NCD semantic segmentation demonstrate the superiorities of our method.&lt;/p&gt;</description></item><item><guid>2510.15018v1</guid><title>UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</title><link>http://arxiv.org/abs/2510.15018v1</link><author>Mingxuan Liu, Honglin He, Elisa Ricci, Wayne Wu, Bolei Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; UrbanVerse 是一种基于真实城市视频的仿真系统，能够生成高质量、物理感知的交互式城市场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市中出现了许多具身 AI 代理，如送货机器人和四足机器人，它们需要在多样且真实的城市环境中训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有人工或程序生成的仿真场景缺乏可扩展性或无法捕捉真实复杂性的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; UrbanVerse 包含 100k+ 注释的 3D 资产库 UrbanVerse-100K，以及自动化管道 UrbanVerse-Gen，能够从城市旅游视频中提取布局并实例化度量级 3D 仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 UrbanVerse 场景在语义和布局上与真实世界保持一致，且在仿真和零样本仿真到真实迁移中，训练的导航策略比之前方法提升了 6.3% 和 30.1%，并在真实 300 米任务中仅需两次干预。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UrbanVerse 提供了可扩展、真实感强的城市仿真环境，显著提升了具身 AI 代理的训练效果和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本论文旨在解决如何为城市导航机器人提供规模化、真实感强的训练环境。传统的手工或程序生成的模拟场景缺乏真实世界的复杂性或无法大规模扩展，而真实城市视频却包含丰富多样的布局与细节。通过将这些视频转化为可交互的物理模拟，可大幅提升机器人在真实城市中的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先构建了一个包含10万+物理属性标注的3D资产库UrbanVerse‑100K，并结合开源的数字孪生技术与现有的城市模拟平台。随后设计了UrbanVerse‑Gen流水线，利用开源的视觉基础模型（如YoloWorld、SAM 2）和SfM技术从未标注的视频中提取语义与几何信息。该方法借鉴了数字孪生、3D Gaussian Splatting以及之前的UrbanSim、MetaUrban等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是把城市旅游视频“数字孪生”为物理可交互的模拟场景。实现流程分为三步：①从视频中通过深度估计和语义检测生成包含物体、地面和天空的场景图；②在UrbanVerse‑100K中检索与场景图匹配的资产并进行多样化；③将匹配的资产按空间信息拼装到IsaacSim中，得到完整的交互式模拟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①构建了规模化、物理属性丰富的UrbanVerse‑100K资产库；②提出了自动化的UrbanVerse‑Gen流水线，可从未标注视频中生成完整的交互式场景；③生成了160个跨24国的真实分布场景，并在导航任务中展示了规模化学习的功效和显著的零样本转移。与以往手工或程序化生成的模拟器不同，本工作直接利用真实视频并保持物理真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanVerse通过将城市旅游视频转化为物理真实、可交互的模拟场景，并提供大规模资产库，显著提升了城市机器人训练的真实性与泛化能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.&lt;/p&gt;</description></item><item><guid>2510.16555v1</guid><title>Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence</title><link>http://arxiv.org/abs/2510.16555v1</link><author>Qiongyan Wang, Xingchen Zou, Yutian Jiang, Haomin Wen, Jiaheng Wei, Qingsong Wen, Yuxuan Liang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出Urban-R1框架，利用强化学习对多模态大模型进行后训练，解决地理偏差问题，提升跨地区泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 快速城市化导致对城市通用智能需求增加，现有基于监督微调的模型存在地理偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过强化学习对MLLM进行后训练，使其更好地满足UGI目标，减少地理偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用Group Relative Policy Optimization优化不同地理组的推理，并使用城市区域剖析作为代理任务提供多模态奖励。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明Urban-R1显著降低地理偏差，提升跨地区泛化，优于SFT和闭源模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 强化学习对齐是实现公平可信城市智能的有前景路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 快速城市化加剧了对城市通用智能（UGI）的需求，UGI指能够理解和推理复杂城市环境的人工智能系统。近期研究通过对大型语言模型（LLM）和多模态大型语言模型（MLLM）进行监督微调（SFT）构建了城市基础模型，但这些模型仍存在持续的地理空间偏差，导致预测结果在不同地区偏斜，泛化能力有限。为此，我们提出了Urban-R1，一种基于强化学习的后训练框架，旨在使MLLM与UGI目标保持一致。Urban-R1采用组相对策略优化（GRPO）来优化不同地理组的推理，并利用城市区域剖析作为代理任务，从多模态城市数据中提供可测量的奖励。通过在不同地区和任务上的广泛实验，结果表明Urban-R1有效缓解了地理偏差并提升了跨地区泛化性能，优于SFT训练模型和闭源模型。我们的研究结果强调了强化学习对齐作为实现公平可信城市智能的有前景路径。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Rapid urbanization intensifies the demand for Urban General Intelligence (UGI), referring to AI systems that can understand and reason about complex urban environments. Recent studies have built urban foundation models using supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit persistent geospatial bias, producing regionally skewed predictions and limited generalization. To this end, we propose Urban-R1, a reinforcement learning-based post-training framework that aligns MLLMs with the objectives of UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize reasoning across geographic groups and employs urban region profiling as a proxy task to provide measurable rewards from multimodal urban data. Extensive experiments across diverse regions and tasks show that Urban-R1 effectively mitigates geo-bias and improves cross-region generalization, outperforming both SFT-trained and closed-source models. Our results highlight reinforcement learning alignment as a promising pathway toward equitable and trustworthy urban intelligence.&lt;/p&gt;</description></item><item><guid>2510.16865v1</guid><title>Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection</title><link>http://arxiv.org/abs/2510.16865v1</link><author>Yuyang Yu, Zhengwei Chen, Xuemiao Xu, Lei Zhang, Haoxin Yang, Yongwei Nie, Shengfeng He</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D点云异常检测关键，现有记忆库方法在特征变换一致性、局部几何细节捕捉和旋转不变性方面存在局限。本文提出将点云配准与记忆库异常检测结合的注册诱导旋转不变特征提取框架，通过在配准学习中嵌入特征提取，实现对齐与表示学习的联合优化，从而获得既旋转不变又局部判别力强的特征。实验表明该方法在两个数据集上均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云异常检测在工业质量控制中至关重要，但现有基于记忆库的方法在特征转换一致性和判别能力方面存在局限，尤其在捕捉局部几何细节和实现旋转不变性方面表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将点云配准与异常检测任务结合，提出一种能够同时实现几何对齐和旋转不变、局部判别特征提取的框架，以提升异常检测的可靠性和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在配准学习过程中嵌入特征提取模块，联合优化配准目标和记忆库异常检测目标，利用局部几何结构和样本间特征相似性来学习旋转不变且判别力强的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Anomaly-ShapeNet和Real3D-AD数据集上，所提方法在效果和泛化性上均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将配准与特征学习联合优化能够显著提升3D点云异常检测的性能，尤其在旋转不变性和局部细节捕捉方面表现突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D点云数据中的异常检测对于工业质量控制至关重要，目标是以高可靠性识别结构缺陷。然而，当前基于记忆库的方法往往存在特征转换不一致和判别能力有限的问题，尤其在捕捉局部几何细节和实现旋转不变性方面。若配准失败，这些局限性会更加明显，导致检测结果不可靠。我们认为点云配准不仅在对齐几何结构方面起关键作用，还能引导特征提取朝向旋转不变且局部判别的表示。为此，我们提出了一种由配准诱导的旋转不变特征提取框架，将点云配准和基于记忆库的异常检测目标整合在一起。我们的核心见解是，这两项任务都依赖于对局部几何结构的建模以及利用样本间的特征相似性。通过将特征提取嵌入配准学习过程，我们的框架实现了对齐与表示学习的联合优化。这种整合使网络能够获得既对旋转鲁棒又高度有效的异常检测特征。在Anomaly-ShapeNet和Real3D-AD数据集上进行的大量实验表明，我们的方法在效果和泛化性方面始终优于现有方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提高3D点云异常检测的准确性，尤其是在点云旋转和局部几何细节变化下的鲁棒性。该问题在工业质量控制中至关重要，因为缺陷往往表现为细微的几何偏差，且点云采集时可能存在任意旋转。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到点云配准与基于记忆库的异常检测都需要局部几何建模和特征相似性，决定将配准任务嵌入特征学习中。方法借鉴了FPFH、RANSAC、粗细配准、Geometric Transformer以及现有的记忆库异常检测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过配准引导的特征学习获得旋转不变且局部判别力强的表示。训练阶段生成随机变换的点云对，进行多尺度采样，构造匹配关系，使用RIConv++、KPConv‑FPN和Geometric Transformer提取特征并优化配准损失；推理阶段计算测试点云与原型的配准矩阵，提取旋转不变特征，与记忆库进行归一化比较得到异常分数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 将配准任务与异常检测联合训练，形成Reg2Inv框架；2) 设计旋转不变的特征提取器，兼顾局部细节与全局结构；3) 在推理时使用配准矩阵对齐后再进行记忆库比较。与以往将配准作为预处理或使用不具备旋转不变性的编码器不同，本文通过配准学习直接提升特征鲁棒性和检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Reg2Inv通过配准引导的特征学习，生成旋转不变且局部判别力强的表示，从而显著提升3D点云异常检测的准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D anomaly detection in point-cloud data is critical for industrial quality control, aiming to identify structural defects with high reliability. However, current memory bank-based methods often suffer from inconsistent feature transformations and limited discriminative capacity, particularly in capturing local geometric details and achieving rotation invariance. These limitations become more pronounced when registration fails, leading to unreliable detection results. We argue that point-cloud registration plays an essential role not only in aligning geometric structures but also in guiding feature extraction toward rotation-invariant and locally discriminative representations. To this end, we propose a registration-induced, rotation-invariant feature extraction framework that integrates the objectives of point-cloud registration and memory-based anomaly detection. Our key insight is that both tasks rely on modeling local geometric structures and leveraging feature similarity across samples. By embedding feature extraction into the registration learning process, our framework jointly optimizes alignment and representation learning. This integration enables the network to acquire features that are both robust to rotations and highly effective for anomaly detection. Extensive experiments on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method consistently outperforms existing approaches in effectiveness and generalizability.&lt;/p&gt;</description></item><item><guid>2510.17568v3</guid><title>PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception</title><link>http://arxiv.org/abs/2510.17568v3</link><author>Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; PAGE-4D 是一种前馈模型，扩展了 VGGT 以处理动态场景，能够在不需要后处理的情况下完成相机姿态估计、深度预测和点云重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的 3D 前馈模型（如 VGGT）在静态数据集上训练，擅长推断静态场景的 3D 属性，但在包含移动人类或可变形物体（如伞）的真实动态场景中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决静态训练导致的动态场景推断困难，使模型能够在动态环境中准确估计相机姿态、深度和几何结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在 VGGT 的基础上引入 PAGE-4D，并设计了一个动态感知聚合器，该聚合器通过预测动态感知掩码来分离静态与动态信息，从而在相机姿态估计时抑制运动线索，在几何重建时放大运动线索，实现多任务 4D 重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，PAGE-4D 在动态场景中始终优于原始 VGGT，在相机姿态估计、单目和视频深度估计以及稠密点图重建方面取得了更好的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PAGE-4D 通过动态感知掩码有效解决了多任务 4D 重建中的冲突，显著提升了动态场景下的 3D 预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近的 3D 前馈模型（如 Visual Geometry Grounded Transformer，VGGT）在推断静态场景的 3D 属性方面表现出强大的能力。然而，由于它们通常在静态数据集上训练，这些模型在涉及复杂动态元素（如移动的人类或可变形物体如伞）的真实世界场景中往往表现不佳。为了解决这一限制，我们提出了 PAGE-4D，这是一种前馈模型，扩展了 VGGT 以适应动态场景，能够在不需要后处理的情况下完成相机姿态估计、深度预测和点云重建。多任务 4D 重建的核心挑战在于任务之间的固有冲突：准确的相机姿态估计需要抑制动态区域，而几何重建则需要对其建模。为了解决这一张力，我们提出了一个动态感知聚合器，通过预测动态感知掩码来分离静态和动态信息——在相机姿态估计中抑制运动线索，在几何重建中放大它们。大量实验表明，PAGE-4D 在动态场景中始终优于原始 VGGT，在相机姿态估计、单目和视频深度估计以及稠密点图重建方面取得了更好的结果。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从RGB图像序列中同时估计相机姿态、深度和点云的任务，尤其是在包含移动或变形物体的动态场景中。传统的静态场景模型在动态环境下表现不佳，导致姿态估计误差增大、几何重建不完整，这在自动驾驶、机器人导航等实际应用中是关键瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了VGGT在动态场景下的失败机制，发现姿态估计需要抑制动态区域，而几何重建则需要利用动态信息。基于此，他们借鉴VGGT的Transformer架构，加入动态感知聚合器和动态掩码预测模块，并采用针对性微调仅更新跨帧注意力层的中间层，保持模型轻量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过动态掩码将动态与静态信息解耦：对姿态估计抑制动态区域，对几何重建强调动态区域。实现流程包括：1）使用预训练的DINO编码器提取图像特征；2）三阶段聚合器（帧注意力、全局注意力、动态感知全局注意力）融合空间与时间信息；3）动态掩码预测模块生成动态掩码；4）分别通过轻量解码器得到深度、点云和姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①动态感知聚合器与动态掩码实现任务间动态信息解耦；②仅微调VGGT中间跨帧注意力层，保持模型高效；③在单一前向推理中同时输出姿态、深度和点云，避免模块化流水线。与以往需要多阶段或对静态假设的模型不同，PAGE‑4D在动态环境下保持高精度且计算开销极低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAGE‑4D通过动态掩码解耦动态与静态信息，扩展VGGT为统一的前向模型，在动态场景中实现高效准确的相机姿态、深度和点云估计。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.&lt;/p&gt;</description></item><item><guid>2510.19661v2</guid><title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title><link>http://arxiv.org/abs/2510.19661v2</link><author>Xusen Guo, Mingxing Peng, Xixuan Hao, Xingchen Zou, Qiongyan Wang, Sijie Ruan, Yuxuan Liang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了AgentSense框架，利用多代理演化系统将大型语言模型融入基于网络的参与式城市感知，提升了系统的适应性和可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统城市感知系统在多样化城市场景下泛化能力有限，且决策过程缺乏可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种训练无关、可适应多变城市环境且具可解释性的城市感知框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; AgentSense采用经典规划器生成基线方案，随后通过多代理演化系统迭代优化任务分配，并利用大型语言模型生成自然语言解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在两个大规模移动数据集和七种动态扰动下，AgentSense在适应性和可解释性方面优于传统方法，并且相较单代理LLM基线在性能和鲁棒性上更佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AgentSense为在网络上部署自适应且可解释的城市感知系统提供了重要进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Web-based participatory urban sensing has emerged as a vital approach for modern urban management by leveraging mobile individuals as distributed sensors. However, existing urban sensing systems struggle with limited generalization across diverse urban scenarios and poor interpretability in decision-making. In this work, we introduce AgentSense, a hybrid, training-free framework that integrates large language models (LLMs) into participatory urban sensing through a multi-agent evolution system. AgentSense initially employs classical planner to generate baseline solutions and then iteratively refines them to adapt sensing task assignments to dynamic urban conditions and heterogeneous worker preferences, while producing natural language explanations that enhance transparency and trust. Extensive experiments across two large-scale mobility datasets and seven types of dynamic disturbances demonstrate that AgentSense offers distinct advantages in adaptivity and explainability over traditional methods. Furthermore, compared to single-agent LLM baselines, our approach outperforms in both performance and robustness, while delivering more reasonable and transparent explanations. These results position AgentSense as a significant advancement towards deploying adaptive and explainable urban sensing systems on the web.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Web-based participatory urban sensing has emerged as a vital approach for modern urban management by leveraging mobile individuals as distributed sensors. However, existing urban sensing systems struggle with limited generalization across diverse urban scenarios and poor interpretability in decision-making. In this work, we introduce AgentSense, a hybrid, training-free framework that integrates large language models (LLMs) into participatory urban sensing through a multi-agent evolution system. AgentSense initially employs classical planner to generate baseline solutions and then iteratively refines them to adapt sensing task assignments to dynamic urban conditions and heterogeneous worker preferences, while producing natural language explanations that enhance transparency and trust. Extensive experiments across two large-scale mobility datasets and seven types of dynamic disturbances demonstrate that AgentSense offers distinct advantages in adaptivity and explainability over traditional methods. Furthermore, compared to single-agent LLM baselines, our approach outperforms in both performance and robustness, while delivering more reasonable and transparent explanations. These results position AgentSense as a significant advancement towards deploying adaptive and explainable urban sensing systems on the web.&lt;/p&gt;</description></item><item><guid>2510.22313v1</guid><title>Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis</title><link>http://arxiv.org/abs/2510.22313v1</link><author>Chen Zhiqiang, Le Gentil Cedric, Lin Fuling, Lu Minghao, Qiao Qiyuan, Xu Bowen, Qi Yuhua, Lu Peng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对动态环境下的激光雷达-惯性里程计（LIO）挑战，提出一种将动态感知直接融入点云配准的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统LIO算法基于静态世界假设，在动态物体占主导且几何稀疏的场景中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决动态LIO中需要可靠静态特征识别与精确位姿估计相互依赖的循环问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入动态感知的迭代最近点算法，利用时空法线分析和高效的空间一致性验证来构建静态地图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该方法在几何结构有限的动态环境中显著优于现有最先进的LIO系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的动态感知配准方法有效提升了动态场景下的定位精度，为LIO在复杂环境中的应用奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文解决了动态环境下激光雷达-惯性里程计（LIO）的挑战，传统方法因静态世界假设而在动态物体占主导的场景中失效，尤其在几何稀疏环境中表现不佳。当前动态LIO方法面临的根本难题是：准确定位需要可靠的静态特征识别，而区分动态物体又需要精确的位姿估计。我们的方案通过将动态感知直接集成到点云配准过程中，打破了这一循环依赖。我们提出了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并辅以高效的空间一致性验证方法来提升静态地图构建。实验评估显示，在几何结构有限的动态环境中，该方法相较于最先进的LIO系统取得了显著的性能提升。代码和数据集可在 https://github.com/thisparticle/btsa 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决激光雷达-惯性里程计（LIO）在动态环境中的定位与建图问题。传统LIO方法假设环境静止，移动物体会导致姿态估计和地图构建误差，尤其在动态物体占比高或几何结构稀疏的场景中更为严重。准确的动态环境定位对机器人导航、自动驾驶等实际应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现姿态估计与动态物体检测之间存在循环依赖，传统方法往往先检测再定位或先定位再检测。为打破这一循环，他们将动态感知直接嵌入点云配准过程，并利用时空法向量进行动态点判别。该思路借鉴了ICP、时空法向量分析（[28],[29]）以及现有的动态检测与LIO框架（如FAST‑LIO2、LOAM）等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过时空法向量对每个点进行动态/静态分类，并在ICP配准时仅使用静态点进行姿态优化。实现流程包括：①预处理并去畸变的点云；②利用时间滑动窗口地图计算时空法向量；③根据法向量的时间分量筛选稳定点；④在全局静态地图上进行点到平面ICP优化姿态；⑤更新时间与全局地图并完成下一帧处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①将动态感知与ICP配准耦合，使用时空法向量实现动态点即时剔除；②通过空间一致性验证提升静态地图质量；③在几何稀疏、动态占比高的场景中实现鲁棒定位；④公开了新的动态环境数据集和代码。与以往先预处理或后处理动态检测的方法不同，该框架在配准循环内实时完成动态点判别，消除了姿态估计与动态检测的相互依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种将时空法向量嵌入ICP的动态感知LIO框架，在高度动态、几何稀疏的环境中实现稳健的姿态估计与静态地图构建。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper addresses the challenge of Lidar-Inertial Odometry (LIO) in dynamic environments, where conventional methods often fail due to their static-world assumptions. Traditional LIO algorithms perform poorly when dynamic objects dominate the scenes, particularly in geometrically sparse environments. Current approaches to dynamic LIO face a fundamental challenge: accurate localization requires a reliable identification of static features, yet distinguishing dynamic objects necessitates precise pose estimation. Our solution breaks this circular dependency by integrating dynamic awareness directly into the point cloud registration process. We introduce a novel dynamic-aware iterative closest point algorithm that leverages spatio-temporal normal analysis, complemented by an efficient spatial consistency verification method to enhance static map construction. Experimental evaluations demonstrate significant performance improvements over state-of-the-art LIO systems in challenging dynamic environments with limited geometric structure. The code and dataset are available at https://github.com/thisparticle/btsa.&lt;/p&gt;</description></item><item><guid>2510.23416v1</guid><title>Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation</title><link>http://arxiv.org/abs/2510.23416v1</link><author>Marco Antonio Ortiz Rincon, Yihui Yang, Christoph Holst</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出一种新型工作流程，用于在城市街道场景中高效准确地将大规模移动激光扫描点云与目标模型点云进行配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市环境复杂，点云密度、噪声和遮挡差异大，传统配准方法难以满足需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够克服上述挑战、提高配准精度并降低计算时间的工作流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入半球检查预处理技术SSC，将MLS轨迹按互相垂直的平面分割；随后使用基于平面体素的通用ICP PV-GICP，仅利用体素内的平面特征进行细化配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该工作流程在慕尼黑市中心数据集上平均配准误差低于0.01米，计算时间比传统点到平面ICP减少超过50%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法显著提升了城市三维建模与更新的自动化水平，可应用于城市规划、基础设施管理和动态城市监测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究提出一种新颖的工作流程，旨在高效且准确地将大规模移动激光扫描（MLS）点云与目标模型点云在城市街道场景中进行配准。该流程专门针对城市环境的复杂性，并巧妙解决了在繁忙城市中心常见的点云密度、噪声特征和遮挡场景差异所带来的挑战。我们引入了两项方法创新。首先，提出的半球检查（SSC）预处理技术通过识别互相垂直的平面表面，最优地将MLS轨迹数据分割，从而降低MLS漂移对整个点云配准精度的影响，并确保每个碎片内具有足够的几何特征以避免局部最小值。其次，我们提出基于平面体素的通用迭代最近点（PV-GICP）细化配准方法，该方法在体素划分内选择性利用平面表面。该预处理策略不仅提升了配准精度，还将计算时间相比传统点到平面ICP方法降低了50%以上。对慕尼黑市中心真实数据集的实验表明，我们的工作流程实现了平均配准精度低于0.01米，并显著缩短了处理时间。结果强调了所提出方法在推进自动化三维城市建模与更新方面的潜力，并直接适用于城市规划、基础设施管理和动态城市监测。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在城市街道场景中将大规模移动激光扫描（MLS）点云与参考模型点云精确、快速配准的问题。该问题重要，因为城市3D模型需要频繁更新以支持规划、基础设施管理和动态监测，而传统手工或现有配准方法在大规模、噪声多、重叠少的点云中效率低、精度差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有碎片化、粗配准和细配准技术的局限，借鉴了等距/等时间碎片化、ICP、GICP、K-means聚类、图可靠性去噪等方法。基于这些工作，他们提出了半球检查（SSC）来自适应碎片化，并设计了只使用平面体素的PV-GICP来加速细配准，形成完整的工作流。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先通过SSC确保每个碎片包含足够的正交平面以减小漂移，再用ISS+FPFH+GROR做粗配准，最后用PV-GICP在平面体素上做细配准。整体流程包括预处理（重采样、去噪、语义过滤）、初始时间/空间碎片化、SSC验证、粗配准、细配准、结果合并。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) SSC自适应碎片化，保证碎片中有三正交平面，显著降低漂移；2) PV-GICP只在平面体素上执行GICP，既提高精度又将计算时间缩短超过50%；3) 通过漂移分析评估并改进MLS源点云。与以往使用等距碎片或全局ICP的做法不同，本文在碎片化和细配准上引入了几何特征筛选和体素平面选择，取得了亚厘米精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种自适应碎片化与平面体素GICP相结合的配准流程，能够在大规模城市MLS点云中实现亚厘米级精度，同时将处理时间缩短一半以上。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This study presents a novel workflow designed to efficiently and accurately register large-scale mobile laser scanning (MLS) point clouds to a target model point cloud in urban street scenarios. This workflow specifically targets the complexities inherent in urban environments and adeptly addresses the challenges of integrating point clouds that vary in density, noise characteristics, and occlusion scenarios, which are common in bustling city centers. Two methodological advancements are introduced. First, the proposed Semi-sphere Check (SSC) preprocessing technique optimally fragments MLS trajectory data by identifying mutually orthogonal planar surfaces. This step reduces the impact of MLS drift on the accuracy of the entire point cloud registration, while ensuring sufficient geometric features within each fragment to avoid local minima. Second, we propose Planar Voxel-based Generalized Iterative Closest Point (PV-GICP), a fine registration method that selectively utilizes planar surfaces within voxel partitions. This pre-process strategy not only improves registration accuracy but also reduces computation time by more than 50% compared to conventional point-to-plane ICP methods. Experiments on real-world datasets from Munich&amp;#x27;s inner city demonstrate that our workflow achieves sub-0.01 m average registration accuracy while significantly shortening processing times. The results underscore the potential of the proposed methods to advance automated 3D urban modeling and updating, with direct applications in urban planning, infrastructure management, and dynamic city monitoring.&lt;/p&gt;</description></item><item><guid>2510.23662v1</guid><title>JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing</title><link>http://arxiv.org/abs/2510.23662v1</link><author>Liangzhe Han, Leilei Sun, Tongyu Zhu, Tao Tao, Jibin Wang, Weifeng Lv</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种通用动态人类移动嵌入框架GDHME，利用自监督学习和动态图模型从大规模基站数据中学习人和区域的时空表示，并将这些表示应用于多种城市感知任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类移动性是城市感知的重要窗口，但现有方法往往针对单一任务，导致模型不足，难以在多任务中迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够从海量移动数据中自动学习通用节点嵌入，并支持多任务城市感知的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GDHME分两阶段：第一阶段将人和区域视为动态图节点，使用连续时间编码器和自回归自监督任务学习节点表示；第二阶段将学习到的表示用于支持多种下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明GDHME能自动从大规模数据中学习有价值的节点特征，并在多任务基准上表现优异；该框架已被用于部署九天川流大模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GDHME为城市感知提供了一个强大的通用时空嵌入方法，能够提升多任务的性能并具有良好的可迁移性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 作为城市感知的窗口，人类移动性包含丰富的时空信息，反映居民行为偏好和城市区域功能。人类移动性分析受到许多研究者关注。然而，现有方法往往从特定视角解决特定任务，导致对人类移动性的建模不足，所学知识在各种下游应用中的适用性有限。为解决这些挑战，本文提出将海量人类移动数据投入时空模型，发现移动行为背后的潜在语义，并支持多种城市感知任务。具体而言，通过普遍基站系统收集大规模、覆盖广泛的人类移动数据，并引入名为通用动态人类移动嵌入（GDHME）的框架。该框架遵循自监督学习思路，包含两个主要阶段。第一阶段，GDHME将人和区域视为动态图中的节点，将人-区域-时间交互统一为数据。一个在连续时间中运行的编码器动态计算节点表示，捕捉人和区域的动态状态。此外，特别设计了自回归自监督任务来指导通用节点嵌入的学习。第二阶段，利用这些表示支持多种任务。为评估GDHME框架的有效性，本文构建了多任务城市感知基准。离线实验表明GDHME能够从海量数据中自动学习有价值的节点特征。此外，本文的框架被用于部署九天川流大模型，该系统已在2023年中国移动全球合作伙伴大会上展示。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As a window for urban sensing, human mobility contains rich spatiotemporal information that reflects both residents&amp;#x27; behavior preferences and the functions of urban areas. The analysis of human mobility has attracted the attention of many researchers. However, existing methods often address specific tasks from a particular perspective, leading to insufficient modeling of human mobility and limited applicability of the learned knowledge in various downstream applications. To address these challenges, this paper proposes to push massive amounts of human mobility data into a spatiotemporal model, discover latent semantics behind mobility behavior and support various urban sensing tasks. Specifically, a large-scale and widely covering human mobility data is collected through the ubiquitous base station system and a framework named General-purpose and Dynamic Human Mobility Embedding (GDHME) for urban sensing is introduced. The framework follows the self-supervised learning idea and contains two major stages. In stage 1, GDHME treats people and regions as nodes within a dynamic graph, unifying human mobility data as people-region-time interactions. An encoder operating in continuous-time dynamically computes evolving node representations, capturing dynamic states for both people and regions. Moreover, an autoregressive self-supervised task is specially designed to guide the learning of the general-purpose node embeddings. In stage 2, these representations are utilized to support various tasks. To evaluate the effectiveness of our GDHME framework, we further construct a multi-task urban sensing benchmark. Offline experiments demonstrate GDHME&amp;#x27;s ability to automatically learn valuable node features from vast amounts of data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu Big Model, a system that has been presented at the 2023 China Mobile Worldwide Partner Conference.&lt;/p&gt;</description></item><item><guid>2511.00096v1</guid><title>Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System</title><link>http://arxiv.org/abs/2511.00096v1</link><author>Shangyu Lou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了基于大型语言模型的多智能体系统框架Urban-MAS，用于零样本设置下的以人为中心的城市预测。该框架通过三类智能体协同工作，显著降低了在东京、米兰和西雅图的跑步量预测与城市感知任务中的误差，并通过消融实验验证了预测因子引导智能体的关键作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市人工智能已推动感知预测和人类动态等以人为中心的城市任务，但大型语言模型在领域特定任务上往往表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 引入Urban-MAS框架，以零样本方式提升城市预测的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建三类智能体：预测因子引导智能体、可靠城市信息提取智能体和多城市信息推理智能体，利用LLM整合多模态输入并通过协同工作实现预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明Urban-MAS相较于单一LLM基线显著降低误差；消融实验显示预测因子引导智能体对提升预测性能最为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Urban-MAS为可扩展的以人为中心的城市AI预测范式，能够在零样本设置下实现高效准确的预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 城市人工智能（Urban AI）已推动以人为中心的城市任务，如感知预测和人类动态。大型语言模型（LLM）能够整合多模态输入来处理复杂城市系统中的异构数据，但在领域特定任务上往往表现不佳。本文提出了基于LLM的多智能体系统框架Urban-MAS，用于零样本设置下的以人为中心的城市预测。该框架包含三类智能体：预测因子引导智能体，优先考虑关键预测因子以指导知识提取并提升LLM中压缩城市知识的有效性；可靠城市信息提取智能体，通过比较多输出、验证一致性并在冲突时重新提取来提升鲁棒性；多城市信息推理智能体，将跨维度提取的多源信息整合用于预测。对东京、米兰和西雅图的跑步量预测和城市感知进行实验，结果显示Urban-MAS相较于单一LLM基线显著降低误差。消融研究表明预测因子引导智能体对提升预测性能最为关键，进一步将Urban-MAS定位为可扩展的以人为中心的城市AI预测范式。代码已在项目网站公开。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban Artificial Intelligence (Urban AI) has advanced human-centered urban tasks such as perception prediction and human dynamics. Large Language Models (LLMs) can integrate multimodal inputs to address heterogeneous data in complex urban systems but often underperform on domain-specific tasks. Urban-MAS, an LLM-based Multi-Agent System (MAS) framework, is introduced for human-centered urban prediction under zero-shot settings. It includes three agent types: Predictive Factor Guidance Agents, which prioritize key predictive factors to guide knowledge extraction and enhance the effectiveness of compressed urban knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve robustness by comparing multiple outputs, validating consistency, and re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which integrate extracted multi-source information across dimensions for prediction. Experiments on running-amount prediction and urban perception across Tokyo, Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors compared to single-LLM baselines. Ablation studies indicate that Predictive Factor Guidance Agents are most critical for enhancing predictive performance, positioning Urban-MAS as a scalable paradigm for human-centered urban AI prediction. Code is available on the project website:https://github.com/THETUREHOOHA/UrbanMAS&lt;/p&gt;</description></item><item><guid>2511.00260v1</guid><title>MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba</title><link>http://arxiv.org/abs/2511.00260v1</link><author>Linzhe Jiang, Jiayuan Huang, Sophia Bano, Matthew J. Clarkson, Zhehua Mao, Mobarak I. Hoque</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对内镜导航的3D点云配准方法，并构建了大规模临床数据集，用以评估和提升配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确的3D点云配准是可靠的影像引导结肠镜检查的基础，直接影响病变定位、切缘评估和导航安全。然而，生物组织的重复纹理和局部几何同质性导致特征退化，术前解剖与术中观测之间的显著域差进一步削弱配准稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述临床关键挑战，提出一种新型的无对应关系3D配准框架，并提供高质量、临床基准的数据集，以实现严格可重复的评测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建C3VD-Raycasting-10k数据集，包含一万零十四对几何对齐的点云；提出MambaNetLK框架，在PointNetLK基础上加入Mamba状态空间模型作为跨模态特征提取器，利用线性时间复杂度捕获长程依赖，并通过Lucas‑Kanade迭代实现配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在C3VD-Raycasting-10k数据集上，MambaNetLK相较于现有最优方法，旋转误差中位数降低56.04%，平移误差RMSE降低26.19%；在ModelNet40上表现出良好泛化能力，并对初始姿态扰动具有更强鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MambaNetLK为外科导航中的3D配准提供了稳健的基础，其基于SSM的全局特征提取器与大规模临床数据集的结合，可显著提升微创手术如结肠镜检查的引导系统的准确性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在结肠镜手术中将实时内镜点云与预手术CT模型对齐的难题。准确的三维配准直接影响病变定位、切除边缘评估和导航安全，因而在临床和研究中至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在PointNetLK的基础上引入Mamba状态空间模型，以捕捉全局几何依赖，并将其嵌入逆式Lucas‑Kanade迭代求解器。该设计借鉴了PointNetLK、Mamba、IC‑LK以及现有的医学点云配准方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用无对应关系的全局特征提取器（MambaNet）与IC‑LK求解器配合，迭代最小化源点云与目标点云特征向量的差异。流程包括：1）用共享权重的MambaNet分别编码源点云和目标点云；2）在目标点云上预计算雅可比矩阵；3）在特征空间中迭代求解增量变换并更新源点云姿态，直至收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）MambaNetLK框架，将Mamba状态空间模型用于点云编码；2）无对应关系的IC‑LK配准实现；3）新建C3VD‑Raycasting‑10k临床基准数据集；4）在临床数据上实现显著的旋转误差和位移误差下降，并在大初始旋转下保持鲁棒性。与以往方法相比，它在特征提取上突破了MLP局部感受野的限制，并提供了更高质量的评估基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MambaNetLK通过将状态空间模型编码器与Lucas‑Kanade迭代求解器相结合，实现了无对应关系的高精度结肠镜点云配准，并提供了新的临床基准数据集。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.&lt;/p&gt;</description></item><item><guid>2511.00635v1</guid><title>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</title><link>http://arxiv.org/abs/2511.00635v1</link><author>Hyungtae Lim, Daebeom Kim, Hyun Myung</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的多会话同步定位与地图构建框架 Multi-Mapcher，利用大规模地图对地图配准实现跨会话初始对齐，并通过锚点姿态图优化构建一致的全局地图，显著提升不同 LiDAR 传感器下的性能并加快速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着多种 3D LiDAR 传感器的出现，针对异构 LiDAR 的多会话同步定位与地图构建（MSS）研究日益活跃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 挑战传统依赖循环闭合检测的 MSS 方法，提出一种不依赖循环检测模块的框架，以提高跨会话对齐的鲁棒性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先使用鲁棒的 3D 点云配准实现大规模地图对地图的初始对齐；随后在初始对齐足够精确的前提下，通过半径搜索寻找跨会话循环；最后采用基于锚点的鲁棒姿态图优化构建一致的全局地图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该方法在使用不同 LiDAR 传感器捕获的会话中，MSS 性能显著优于现有方法，并且速度更快。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 利用大规模地图对地图配准和锚点姿态图优化，可有效提升异构 LiDAR 多会话同步定位与地图构建的性能与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着多种 3D 激光雷达（LiDAR）传感器的上市，针对异构 LiDAR 传感器的多会话同步定位与地图构建（MSS）研究已得到积极开展。现有的 MSS 方法大多依赖循环闭合检测来实现跨会话对齐；然而，由于不同会话中使用的传感器在点云密度和视场（FoV）上的差异，循环闭合检测的性能可能会受到影响。本文挑战了过度依赖循环检测模块的传统范式，提出了一种名为 Multi-Mapcher 的新型 MSS 框架，该框架利用大规模地图对地图配准来完成跨会话的初始对齐，尽管这通常被认为不可行，但通过使用鲁棒的 3D 点云配准实现。随后，在假设跨会话初始对齐足够精确的前提下，采用半径搜索寻找跨会话循环；随后采用基于锚点的鲁棒姿态图优化构建一致的全局地图。实验表明，我们的方法在使用不同 LiDAR 传感器捕获的会话中，MSS 性能显著优于现有方法，并且速度更快。我们的代码可在 https://github.com/url-kaist/multi-mapcher 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了多会话SLAM中不同类型LiDAR传感器之间的地图对齐问题。传统方法依赖循环闭合检测，而不同传感器的点云密度和视场差异会导致检测失效，影响地图一致性。对于需要长期更新地图的自动驾驶系统，这一问题尤为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为循环闭合检测不适用于异构LiDAR，提出利用鲁棒点云配准实现大规模地图对齐。方法借鉴了anchor node姿态图优化、outlier-robust registration以及之前的多会话框架（如Kim &amp;amp; Kim、Hydra-Multi等）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过鲁棒配准在地图层面完成初始对齐，然后在扫描层面检测并筛选跨会话循环。实现流程包括：单会话SLAM、地图对齐、半径搜索循环、误循环剔除、anchor node姿态图优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①完全去除循环闭合检测依赖；②在地图层面使用鲁棒配准实现异构LiDAR的初始对齐；③anchor node姿态图优化提升全局一致性；④在大规模、动态环境下保持鲁棒性并加速计算。与以往方法相比，它更适用于异构传感器且更高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Multi-Mapcher 提供了一种无循环闭合检测、鲁棒配准驱动的多会话SLAM框架，能够高效合并异构LiDAR地图并生成一致的全局地图。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As various 3D light detection and ranging (LiDAR) sensors have been introduced to the market, research on multi-session simultaneous localization and mapping (MSS) using heterogeneous LiDAR sensors has been actively conducted. Existing MSS methods mostly rely on loop closure detection for inter-session alignment; however, the performance of loop closure detection can be potentially degraded owing to the differences in the density and field of view (FoV) of the sensors used in different sessions. In this study, we challenge the existing paradigm that relies heavily on loop detection modules and propose a novel MSS framework, called Multi-Mapcher, that employs large-scale map-to-map registration to perform inter-session initial alignment, which is commonly assumed to be infeasible, by leveraging outlier-robust 3D point cloud registration. Next, after finding inter-session loops by radius search based on the assumption that the inter-session initial alignment is sufficiently precise, anchor node-based robust pose graph optimization is employed to build a consistent global map. As demonstrated in our experiments, our approach shows substantially better MSS performance for various LiDAR sensors used to capture the sessions and is faster than state-of-the-art approaches. Our code is available at https://github.com/url-kaist/multi-mapcher.&lt;/p&gt;</description></item><item><guid>2511.02205v1</guid><title>OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning</title><link>http://arxiv.org/abs/2511.02205v1</link><author>Kevin Valencia, Thilina Balasooriya, Xihaier Luo, Shinjae Yoo, David Keetae Park</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了OmniField框架，解决多模态时空学习中测量稀疏、噪声大且模态不完整的问题。通过连续神经场和跨模态上下文迭代融合，OmniField实现了统一的重建、插值、预测和跨模态预测，并在多项基准测试中表现优异，且对噪声具有鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态时空学习面临两大挑战：1）单模态测量稀疏、间歇且受噪声影响，但不同模态之间存在相关性；2）可用模态随空间和时间变化，导致记录被压缩，模型需适应任意子集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在任意可用模态下学习连续神经场并迭代融合跨模态信息的框架，以实现高质量的重建、插值、预测和跨模态预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建多模态互调块架构，配合迭代跨模态细化，在解码器前对信号进行对齐；使用连续神经场对可用模态进行条件化学习，避免网格化或代理预处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; OmniField在八个强大基准模型中持续表现更好；在模拟强噪声条件下，性能仍接近无噪声输入，显示出对受损测量的强鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OmniField提供了一种高效、鲁棒的多模态时空学习方法，能够在模态缺失和噪声干扰下保持优异性能，适用于真实实验数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态时空学习在真实实验数据上受到两个挑战的限制：单模态测量稀疏、间歇且噪声大（QA/QC伪影），但跨模态相关；可用模态在空间和时间上变化，除非模型能够在训练和测试时适应任意子集，否则可用记录会被压缩。我们提出了OmniField，一种连续性感知框架，学习一个以可用模态为条件的连续神经场，并迭代融合跨模态上下文。多模态互调块架构与迭代跨模态细化相结合，在解码器之前对信号进行对齐，使得统一的重建、插值、预测和跨模态预测无需网格化或代理预处理。广泛评估表明，OmniField始终优于八个强大的多模态时空基线。在重度模拟传感器噪声下，性能仍接近干净输入水平，突显了对受损测量的鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal spatiotemporal learning on real-world experimental data is constrained by two challenges: within-modality measurements are sparse, irregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set of available modalities varies across space and time, shrinking the usable record unless models can adapt to arbitrary subsets at train and test time. We propose OmniField, a continuity-aware framework that learns a continuous neural field conditioned on available modalities and iteratively fuses cross-modal context. A multimodal crosstalk block architecture paired with iterative cross-modal refinement aligns signals prior to the decoder, enabling unified reconstruction, interpolation, forecasting, and cross-modal prediction without gridding or surrogate preprocessing. Extensive evaluations show that OmniField consistently outperforms eight strong multimodal spatiotemporal baselines. Under heavy simulated sensor noise, performance remains close to clean-input levels, highlighting robustness to corrupted measurements.&lt;/p&gt;</description></item><item><guid>2511.05570v1</guid><title>Do Street View Imagery and Public Participation GIS align: Comparative Analysis of Urban Attractiveness</title><link>http://arxiv.org/abs/2511.05570v1</link><author>Milad Malekzadeh, Elias Willberg, Jussi Torkko, Silviya Korpilo, Kamyar Hasanzadeh, Olle Järv, Tuuli Toivonen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究比较了街景图像（SVI）与公众参与GIS（PPGIS）在评估赫尔辛基城市吸引力方面的相符程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 数字工具在空间规划中的重要性日益增强，街景图像和PPGIS是两种主要的基于地点的感知捕捉方法，但它们的可比性尚未得到充分探讨。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究基于街景图像预测的感知吸引力与PPGIS调查结果之间的一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用参与者评分的街景图像和语义图像分割训练机器学习模型预测吸引力；将预测结果与PPGIS标记的吸引/不吸引地点进行比较，采用严格和中等阈值计算一致性；分析噪音、交通、人口、土地利用等上下文变量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 两种数据源仅部分一致；中等阈值下吸引地点一致率67%，不吸引77%；严格阈值下仅27%和29%；非视觉线索导致不匹配，模型未能捕捉活动水平和环境压力等体验维度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 街景图像可作为可扩展的视觉代理，但无法完全替代PPGIS捕捉的体验丰富性；两种方法各有价值，需整合以全面捕捉人们对城市环境的感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着数字工具在空间规划实践中的日益普及，了解不同数据源如何反映人们对城市环境的体验变得至关重要。街景图像（SVI）和公众参与GIS（PPGIS）是两种主要的基于地点的感知捕捉方法，可支持城市规划决策，但它们的可比性尚未得到充分探讨。本研究调查了基于SVI的感知吸引力与芬兰赫尔辛基市通过全市PPGIS调查收集的居民报告体验之间的一致性。研究使用参与者评分的SVI数据和语义图像分割训练机器学习模型，以视觉特征预测感知吸引力。随后将这些预测与PPGIS标记为吸引或不吸引的位置进行比较，并使用严格和中等两个阈值计算一致性。结果显示，两种数据集仅部分一致。使用中等阈值时，吸引地点的一致率为67%，不吸引地点为77%；使用严格阈值时，一致率分别降至27%和29%。通过分析噪音、交通、人口存在和土地利用等一系列上下文变量，发现非视觉线索显著导致不匹配。模型未能考虑活动水平和环境压力等体验维度，这些因素在图像中不可见但会影响感知。研究结果表明，虽然SVI提供了可扩展且可视化的城市感知代理，但无法完全替代PPGIS捕捉的体验丰富性。作者认为两种方法各有价值，但服务于不同目的，因此需要更综合的方法来全面捕捉人们对城市环境的感知。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As digital tools increasingly shape spatial planning practices, understanding how different data sources reflect human experiences of urban environments is essential. Street View Imagery (SVI) and Public Participation GIS (PPGIS) represent two prominent approaches for capturing place-based perceptions that can support urban planning decisions, yet their comparability remains underexplored. This study investigates the alignment between SVI-based perceived attractiveness and residents&amp;#x27; reported experiences gathered via a city-wide PPGIS survey in Helsinki, Finland. Using participant-rated SVI data and semantic image segmentation, we trained a machine learning model to predict perceived attractiveness based on visual features. We compared these predictions to PPGIS-identified locations marked as attractive or unattractive, calculating agreement using two sets of strict and moderate criteria. Our findings reveal only partial alignment between the two datasets. While agreement (with a moderate threshold) reached 67% for attractive and 77% for unattractive places, agreement (with a strict threshold) dropped to 27% and 29%, respectively. By analysing a range of contextual variables, including noise, traffic, population presence, and land use, we found that non-visual cues significantly contributed to mismatches. The model failed to account for experiential dimensions such as activity levels and environmental stressors that shape perceptions but are not visible in images. These results suggest that while SVI offers a scalable and visual proxy for urban perception, it cannot fully substitute the experiential richness captured through PPGIS. We argue that both methods are valuable but serve different purposes; therefore, a more integrated approach is needed to holistically capture how people perceive urban environments.&lt;/p&gt;</description></item><item><guid>2511.05853v1</guid><title>Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology</title><link>http://arxiv.org/abs/2511.05853v1</link><author>Bingyang Guo, Qiang Zuo, Ruiyun Yu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 3D数据分割对工业应用至关重要，尤其在集成电路缺陷检测中。 2. 陶瓷封装基板（CPS）因其优异的物理化学特性在IC封装中占据重要位置，但其复杂结构和细微缺陷以及缺乏公开数据集阻碍了缺陷检测研究。 3. 本研究构建了高质量点云数据集CPS3D‑Seg，包含1300个样本、20个产品类别，并提供精确的点级标注。 4. 对现有最先进点云分割算法进行了全面基准测试。 5. 提出了基于因果推断的3D分割方法CINet，利用结构细化（SR）和质量评估（QA）模块量化点云中的潜在混杂因素。 6. 大量实验表明CINet在mIoU和准确率上显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D分割在工业领域，尤其是IC缺陷检测中具有关键作用；陶瓷封装基板因其优异特性被广泛使用，但其复杂结构和细微缺陷以及缺乏公开数据集限制了缺陷检测技术的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建高质量的CPS点云分割数据集CPS3D‑Seg，并通过基准测试验证其有效性，同时提出一种新的分割方法CINet以提升分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①收集并标注1300个点云样本，覆盖20个产品类别，形成CPS3D‑Seg数据集；②对现有最先进点云分割算法进行全面基准评估；③设计CINet模型，结合因果推断思想，使用结构细化（SR）和质量评估（QA）模块来量化并消除点云中的潜在混杂因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CINet在mIoU和准确率指标上显著优于现有算法，验证了因果推断方法在点云分割中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CPS3D‑Seg数据集为CPS表面缺陷检测提供了高质量的基准资源，CINet方法通过因果推断显著提升了3D分割性能，为工业缺陷检测提供了新的技术路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 有效的3D数据分割对于广泛的工业应用至关重要，尤其是在集成电路领域检测细微缺陷。陶瓷封装基板（CPS）作为重要的电子材料，因其优异的物理和化学特性在IC封装中必不可少。然而，CPS的复杂结构和细微缺陷以及缺乏公开数据集，严重阻碍了CPS表面缺陷检测的发展。本研究构建了高质量的点云数据集CPS3D‑Seg，用于CPS表面缺陷的3D分割，该数据集在点分辨率和精度方面优于现有的3D工业数据集。CPS3D‑Seg包含1300个点云样本，涵盖20个产品类别，每个样本都提供精确的点级标注。同时，我们基于最先进的点云分割算法进行了全面基准测试，以验证CPS3D‑Seg的有效性。此外，我们提出了一种基于因果推断的3D分割方法CINet，通过结构细化（SR）和质量评估（QA）模块量化点云中的潜在混杂因素。大量实验表明，CINet在mIoU和准确率上显著优于现有算法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决集成电路陶瓷封装基板（CPS）表面缺陷的三维点云分割问题。缺陷会导致器件失效，且传统二维方法缺乏深度信息，难以准确捕捉微小缺陷；因此需要高分辨率的三维数据和有效的分割算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先构建了高精度点云数据集 CPS3D‑Seg，采用多头激光扫描和精细标注；随后对现有最先进的点云分割算法进行基准测试。为克服点云中的潜在混杂因素，作者借鉴因果推断理论，提出基于结构因果模型的 CINet，利用结构细化（SR）和质量评估（QA）模块实现后门调整。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将因果推断引入点云分割，通过学习 SR 和 QA 模块来量化并消除潜在混杂因素，从而提升分割精度。实现流程包括：数据采集与配准 → 网格滤波去冗余 → 点云标注 → 训练 CINet（包含 SR、QA 与分割头） → 在 CPS3D‑Seg 上评估并与其他算法比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 提供了最高分辨率、最高精度的 CPS 点云分割数据集 CPS3D‑Seg；2) 建立了完整的基准评估框架；3) 首次将结构因果模型应用于三维点云分割，提出 CINet 并显著提升 mIoU 与准确率。与以往仅关注二维或通用工业数据集的工作不同，本文聚焦于陶瓷基板的细微缺陷，并通过因果推断解决点云噪声与结构误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提供了高精度陶瓷基板点云缺陷分割数据集，并提出基于因果推断的 CINet，显著提升了三维缺陷分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The effective segmentation of 3D data is crucial for a wide range of industrial applications, especially for detecting subtle defects in the field of integrated circuits (IC). Ceramic package substrates (CPS), as an important electronic material, are essential in IC packaging owing to their superior physical and chemical properties. However, the complex structure and minor defects of CPS, along with the absence of a publically available dataset, significantly hinder the development of CPS surface defect detection. In this study, we construct a high-quality point cloud dataset for 3D segmentation of surface defects in CPS, i.e., CPS3D-Seg, which has the best point resolution and precision compared to existing 3D industrial datasets. CPS3D-Seg consists of 1300 point cloud samples under 20 product categories, and each sample provides accurate point-level annotations. Meanwhile, we conduct a comprehensive benchmark based on SOTA point cloud segmentation algorithms to validate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3D segmentation method based on causal inference (CINet), which quantifies potential confounders in point clouds through Structural Refine (SR) and Quality Assessment (QA) Modules. Extensive experiments demonstrate that CINet significantly outperforms existing algorithms in both mIoU and accuracy.&lt;/p&gt;</description></item><item><guid>2511.05965v1</guid><title>Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration</title><link>http://arxiv.org/abs/2511.05965v1</link><author>Zhixin Cheng, Xiaotian Yin, Jiacheng Deng, Bohao Liao, Yujia Chen, Xu Zhou, Baoqun Yin, Tianzhu Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新的跨模态配准框架，旨在解决传统基于Transformer的检测自由方法在噪声环境下的匹配不准问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法在噪声干扰下难以准确计算相似度，且缺乏有效的跨模态信息选择机制，导致配准鲁棒性和精度受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过设计迭代代理选择和可靠代理交互两大模块，提升结构特征感知和跨模态交互的可靠性，从而提高配准的鲁棒性和准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 迭代代理选择（IAS）模块利用相位图增强结构特征感知，并采用强化学习原理高效挑选可靠代理。2) 可靠代理交互（RAI）模块利用已选代理引导跨模态交互，减少匹配错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在RGB-D Scenes v2和7-Scenes基准上，所提方法在多种挑战条件下均实现了持续领先的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架通过有效的代理选择与交互显著提升了检测自由图像到点云配准的鲁棒性和精度，达到了当前最先进水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Typical detection-free methods for image-to-point cloud registration leverage transformer-based architectures to aggregate cross-modal features and establish correspondences. However, they often struggle under challenging conditions, where noise disrupts similarity computation and leads to incorrect correspondences. Moreover, without dedicated designs, it remains difficult to effectively select informative and correlated representations across modalities, thereby limiting the robustness and accuracy of registration. To address these challenges, we propose a novel cross-modal registration framework composed of two key modules: the Iterative Agents Selection (IAS) module and the Reliable Agents Interaction (RAI) module. IAS enhances structural feature awareness with phase maps and employs reinforcement learning principles to efficiently select reliable agents. RAI then leverages these selected agents to guide cross-modal interactions, effectively reducing mismatches and improving overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes benchmarks demonstrate that our method consistently achieves state-of-the-art performance.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注图像与点云之间的配准，解决在噪声、重复结构和光照变化等挑战下的特征匹配错误。该问题在三维重建、SLAM和视觉定位等应用中至关重要，因为精确的配准是获取可靠三维信息的前提。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在检测自由方法的基础上，借鉴了Transformer跨模态特征聚合、2D3D-MATR的粗细匹配思路，并引入傅里叶相位增强、强化学习奖励机制和三阶段代理优化，以更好地选择可靠特征并降低噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用相位图提升图像的结构感知，再通过三阶段代理优化（预热、奖励引导、软掩码）挑选可靠代理，随后用这些代理引导跨模态特征交互，逐步生成稠密对应并用PnP+RANSAC估计位姿。实现流程包括特征提取、相位增强、代理初始化与训练、代理选择、代理交互、细化匹配与位姿求解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 相位图增强图像结构特征；2) 三阶段代理优化策略，结合局部/全局奖励动态平衡；3) 代理引导的交互替代传统Transformer，降低噪声并提升效率。与以往固定代理或纯Transformer融合的做法不同，A2SI实现了自适应代理选择和更稳健的特征聚合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; A2SI通过相位增强与强化学习驱动的自适应代理选择，提出了一种高效、鲁棒的图像-点云配准框架，刷新了相关基准的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Typical detection-free methods for image-to-point cloud registration leverage transformer-based architectures to aggregate cross-modal features and establish correspondences. However, they often struggle under challenging conditions, where noise disrupts similarity computation and leads to incorrect correspondences. Moreover, without dedicated designs, it remains difficult to effectively select informative and correlated representations across modalities, thereby limiting the robustness and accuracy of registration. To address these challenges, we propose a novel cross-modal registration framework composed of two key modules: the Iterative Agents Selection (IAS) module and the Reliable Agents Interaction (RAI) module. IAS enhances structural feature awareness with phase maps and employs reinforcement learning principles to efficiently select reliable agents. RAI then leverages these selected agents to guide cross-modal interactions, effectively reducing mismatches and improving overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes benchmarks demonstrate that our method consistently achieves state-of-the-art performance.&lt;/p&gt;</description></item><item><guid>2511.06378v1</guid><title>ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects</title><link>http://arxiv.org/abs/2511.06378v1</link><author>Prajval Kumar Murali, Mohsen Kaboli</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出一种基于视觉与触觉的跟踪方法，用于在机器人交互过程中识别并跟踪未知的单个、多重或关节化物体。该方法不需要预先了解物体的几何形状或运动学特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 机器人在现实环境中经常遇到结构复杂且带有关节的未知物体，如门、抽屉、橱柜和工具。如何在没有先验知识的情况下感知、跟踪并操作这些物体是机器人学中的核心挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在机器人交互过程中实时识别并跟踪未知关节化物体姿态的方法，并利用该信息实现目标驱动的关节化物体操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出名为 ArtReg 的姿态跟踪框架，将视觉与触觉点云融合到无迹卡尔曼滤波器中，并在 SE(3) 群上进行点云配准。通过两机器人团队的推动或拉扯等有目的的操作来检测可能的关节，并基于 ArtReg 开发闭环控制器实现关节化物体的姿态调整。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多种未知物体的真实机器人实验中，方法在不同重心、低光照和复杂视觉背景下表现出鲁棒性；在标准关节化物体数据集上与现有方法相比，姿态精度得到提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 利用视觉与触觉信息实现的鲁棒且精确的姿态跟踪，使机器人能够感知并与未知的复杂关节化物体进行交互，适用于旋转或滑动关节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 机器人在现实环境中经常遇到未知的、结构复杂且带有关节的物体，例如门、抽屉、橱柜和工具。如何在没有先验几何或运动学知识的情况下感知、跟踪并操作这些物体仍是机器人学的根本挑战。本文提出了一种新颖的基于视觉-触觉的跟踪方法，用于在机器人交互过程中跟踪未见过的物体（单个、多重或关节化），不假设任何关于物体形状或动力学的先验知识。该方法称为 ArtReg（关节化配准），将视觉-触觉点云整合到无迹卡尔曼滤波器的 SE(3) Lie 群中进行点云配准。ArtReg 用于通过有目的的操作（如推动或拉扯）检测物体中的可能关节，使用两机器人团队完成。随后，我们利用 ArtReg 开发了一个闭环控制器，实现目标驱动的关节化物体操作，将物体移动到期望姿态。我们在多种未知物体上通过真实机器人实验广泛评估了该方法，并通过评估不同重心、低光照条件和具有挑战性的视觉背景的物体来展示其鲁棒性。此外，我们在标准关节化物体数据集上对该方法进行了基准测试，并在姿态精度方面优于现有方法。实验表明，利用视觉-触觉信息实现的鲁棒且精确的姿态跟踪，使机器人能够感知并与未见过的复杂关节化物体（具有旋转或滑动关节）进行交互。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).&lt;/p&gt;</description></item><item><guid>2511.06925v1</guid><title>DTTNet: Improving Video Shadow Detection via Dark-Aware Guidance and Tokenized Temporal Modeling</title><link>http://arxiv.org/abs/2511.06925v1</link><author>Zhicheng Li, Kunyang Sun, Rui Yao, Hancheng Zhu, Fuyuan Hu, Jiaqi Zhao, Zhiwen Shao, Yong Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的视频阴影检测方法，结合视觉-语言匹配模块、暗色感知语义块以及时间令牌化块，显著提升阴影与暗物体区分和动态阴影形变建模的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频阴影检测面临两个主要挑战：在复杂背景中区分阴影与暗物体，以及在不同照明下建模阴影的动态形变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决阴影与背景的模糊性，并有效捕捉阴影随时间变化的形状。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 通过视觉-语言匹配模块和暗色感知语义块提取文本引导特征，区分阴影与暗物体；2) 引入自适应掩码重加权，降低半影区域影响，并在解码器末端使用边缘掩码加强监督；3) 设计令牌化时间块，将跨帧阴影语义压缩为可学习的时间令牌，实现高效序列编码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个基准数据集上实验表明，该方法在阴影检测精度上达到最先进水平，并实现实时推理速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结合视觉-语言先验和时间令牌化的阴影检测框架能够有效解决阴影与暗物体区分和动态形变建模问题，具有高精度和实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Video shadow detection confronts two entwined difficulties: distinguishing shadows from complex backgrounds and modeling dynamic shadow deformations under varying illumination. To address shadow-background ambiguity, we leverage linguistic priors through the proposed Vision-language Match Module (VMM) and a Dark-aware Semantic Block (DSB), extracting text-guided features to explicitly differentiate shadows from dark objects. Furthermore, we introduce adaptive mask reweighting to downweight penumbra regions during training and apply edge masks at the final decoder stage for better supervision. For temporal modeling of variable shadow shapes, we propose a Tokenized Temporal Block (TTB) that decouples spatiotemporal learning. TTB summarizes cross-frame shadow semantics into learnable temporal tokens, enabling efficient sequence encoding with minimal computation overhead. Comprehensive Experiments on multiple benchmark datasets demonstrate state-of-the-art accuracy and real-time inference efficiency. Codes are available at https://github.com/city-cheng/DTTNet.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Video shadow detection confronts two entwined difficulties: distinguishing shadows from complex backgrounds and modeling dynamic shadow deformations under varying illumination. To address shadow-background ambiguity, we leverage linguistic priors through the proposed Vision-language Match Module (VMM) and a Dark-aware Semantic Block (DSB), extracting text-guided features to explicitly differentiate shadows from dark objects. Furthermore, we introduce adaptive mask reweighting to downweight penumbra regions during training and apply edge masks at the final decoder stage for better supervision. For temporal modeling of variable shadow shapes, we propose a Tokenized Temporal Block (TTB) that decouples spatiotemporal learning. TTB summarizes cross-frame shadow semantics into learnable temporal tokens, enabling efficient sequence encoding with minimal computation overhead. Comprehensive Experiments on multiple benchmark datasets demonstrate state-of-the-art accuracy and real-time inference efficiency. Codes are available at https://github.com/city-cheng/DTTNet.&lt;/p&gt;</description></item><item><guid>2511.07978v2</guid><title>DANCE: Density-agnostic and Class-aware Network for Point Cloud Completion</title><link>http://arxiv.org/abs/2511.07978v2</link><author>Da-Yeong Kim, Yeong-Jun Cho</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; DANCE是一种密度无关、类别感知的点云补全框架，利用多视角射线采样生成候选点，变压器解码器细化位置并预测不透明度，轻量级分类头提供语义指导，最终在PCN和MVP基准上实现了更高的准确性和结构一致性，并对输入密度和噪声具有鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全需要从不完整的3D扫描中恢复缺失几何结构，现有方法往往假设固定密度或依赖图像表示，难以适应真实场景中的可变稀疏度和有限监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种既能补全缺失区域又能保留已观测几何的框架，兼顾密度无关性和类别一致性，并在无外部图像监督的条件下实现高质量补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DANCE通过多视角射线采样生成候选点，变压器解码器细化点位并预测不透明度分数，轻量级分类头在几何特征上训练以提供语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在PCN和MVP基准上，DANCE在准确性和结构一致性方面优于现有最先进方法，并对不同输入密度和噪声水平保持鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DANCE提供了一种密度无关、类别感知的点云补全解决方案，能够在多变的真实场景中实现高质量、鲁棒的补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从不完整的3D扫描中恢复缺失的几何结构，这些扫描往往受到遮挡或有限的传感器视角影响。现有方法通常假设输入/输出密度固定或依赖基于图像的表示，使其在真实场景中对可变稀疏度和有限监督不够适用。本文提出了密度无关且类别感知的网络DANCE，一种新框架，只补全缺失区域，同时保留已观测的几何。DANCE通过从多个视角进行射线采样生成候选点。随后，变压器解码器细化它们的位置并预测不透明度分数，决定每个点是否包含在最终表面中。为引入语义指导，轻量级分类头直接在几何特征上训练，实现类别一致的补全，而不需要外部图像监督。在PCN和MVP基准上进行的大量实验表明，DANCE在准确性和结构一致性方面优于最先进的方法，并且对不同输入密度和噪声水平保持鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成中输入密度可变且缺乏语义指导的问题。现实场景中的扫描往往稀疏且视角受限，固定密度或缺乏类别信息的完成方法会导致几何失真或不一致，影响后续任务如机器人导航和三维重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 NeRF 的射线采样思想来生成候选点，并结合 Transformer 的跨视角注意力与自注意力来细化几何。方法中还引入了分类头和融合网络，以纯几何方式学习类别先验，避免了对图像监督的依赖。整体设计参考了 PCN、GenPC、PF‑Net 等现有完成框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用多视角射线采样产生候选点，然后通过共享编码器提取特征，利用面向视角的 Transformer 进行跨视角与局部自注意力，最后融合分类信息预测每个候选点的偏移量和不透明度，筛选出有效点并与原始点云合并得到完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 采用射线采样实现密度无关的候选点生成；② 通过分类头学习类别先验，实现类感知完成；③ 使用 Transformer 进行视角分组的跨视角与自注意力，提升局部一致性；④ 通过不透明度阈值动态控制输出密度，避免固定点数。与以往固定密度或依赖图像监督的方法不同，DANCE 能在多样化输入密度下保持高质量、语义一致的完成结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DANCE 提出了一种密度无关、类感知的点云完成框架，利用射线采样与 Transformer 细化，能够在不依赖图像监督的情况下生成高质量、语义一致的完整点云。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to recover missing geometric structures from incomplete 3D scans, which often suffer from occlusions or limited sensor viewpoints. Existing methods typically assume fixed input/output densities or rely on image-based representations, making them less suitable for real-world scenarios with variable sparsity and limited supervision. In this paper, we introduce Density-agnostic and Class-aware Network (DANCE), a novel framework that completes only the missing regions while preserving the observed geometry. DANCE generates candidate points via ray-based sampling from multiple viewpoints. A transformer decoder then refines their positions and predicts opacity scores, which determine the validity of each point for inclusion in the final surface. To incorporate semantic guidance, a lightweight classification head is trained directly on geometric features, enabling category-consistent completion without external image supervision. Extensive experiments on the PCN and MVP benchmarks show that DANCE outperforms state-of-the-art methods in accuracy and structural consistency, while remaining robust to varying input densities and noise levels.&lt;/p&gt;</description></item><item><guid>2511.09866v1</guid><title>IPCD: Intrinsic Point-Cloud Decomposition</title><link>http://arxiv.org/abs/2511.09866v1</link><author>Shogo Sato, Takuhiro Kaneko, Shoichiro Takeda, Tomoyasu Shimada, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida, Akisato Kimura</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种针对点云的内在分解方法，能够直接将彩色点云分解为反照率和阴影两部分，并通过点级特征聚合和投影光照分布技术解决传统图像分解模型在非网格结构和全局光方向考虑不足的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云在增强现实和机器人等领域被广泛使用，逼真可视化需要对点云进行重新照明和纹理编辑，而这需要准确分离反照率与阴影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在点云上实现高质量反照率与阴影分离的模型，并验证其在多种光照条件下的实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出IPCD-Net，利用点级特征聚合扩展图像分解模型；引入投影光照分布（PLD）并进行层次特征细化，通过多视角投影捕捉全局光信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明IPCD-Net能显著减少反照率中的投射阴影，并提升阴影部分的色彩准确度；在纹理编辑、重新照明和点云配准等应用中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; IPCD-Net在合成与真实场景中均能实现可靠的点云内在分解，为点云的可视化与配准提供了有效工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云在增强现实（AR）和机器人等多个领域得到广泛应用，其中重新照明和纹理编辑对于实现逼真可视化至关重要。完成这些任务需要准确地将反照率与阴影分离。然而，在点云上进行此类分离面临两个主要挑战：（1）点云的非网格结构使得传统基于图像的分解模型失效；（2）为其他任务设计的点云模型并未显式考虑全局光照方向，导致阴影不准确。本文提出了内在点云分解（IPCD），将图像分解扩展到彩色点云的直接分解为反照率和阴影。为克服挑战（1），我们提出了IPCD-Net，该网络通过点级特征聚合扩展了基于图像的模型，以处理非网格数据；为解决挑战（2），我们引入了基于投影的亮度分布（PLD），并通过多视角投影捕获全局光照线索，采用层次特征细化。为全面评估，我们创建了一个合成的户外场景数据集。实验结果表明，IPCD-Net能够减少反照率中的投射阴影，并提升阴影部分的色彩准确度。我们还展示了其在纹理编辑、重新照明和不同光照条件下的点云配准中的应用。最后，我们验证了IPCD-Net在真实世界中的适用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在将彩色点云直接分解为表面反射率（色度）和光照阴影，以便在增强现实、机器人视觉和点云配准等应用中实现更真实的纹理编辑、重光照和跨光照条件的配准。该问题重要，因为传统方法需要先渲染成图像再分解，容易产生伪影，且无法在非网格结构的点云上直接操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有的二维图像内在分解方法无法直接应用于点云，随后借鉴了 PoInt‑Net 的点云表示和光照估计思路，并将其改造成在三维点云空间直接监督的 IPCD‑NetBase。为补偿缺失的全局光照信息，他们设计了 Projection‑based Luminance Distribution (PLD) 并使用 SphereNet 处理球面数据，进一步提升了阴影与色度的分离效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点云特征聚合与多视角投影得到的全局光照统计（PLD）共同驱动的共享编码器，先粗略估计色度和阴影，再通过层次化特征细化得到最终结果。实现流程包括：①输入点云（位置+颜色）→ ②使用 PTv2 进行点级特征提取；③从多视角投影生成 PLD 并通过 SphereNet 提取光照特征；④将编码器输出与 PLD 特征融合，分别预测预色度和预阴影；⑤通过层次化细化网络迭代更新，得到最终的色度和阴影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①直接在点云空间进行内在分解，避免渲染导致的失真；②点级特征聚合（PTv2）替代传统 MLP，提升非网格数据处理能力；③提出 PLD 通过多视角投影捕获全局光照信息，解决光照方向缺失问题；④层次化细化实现色度与阴影的相互约束；⑤构建合成户外点云数据集用于评估。与 PoInt‑Net、NeRF 等方法相比，IPCD‑Net 不需要图像渲染或场景特定训练，且显著降低了阴影泄漏和色度不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出 IPCD‑Net，一种利用点级特征聚合和投影光照分布的点云网络，能够直接将彩色点云分解为色度和阴影，实现无场景微调的高质量重光照与纹理编辑。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point clouds are widely used in various fields, including augmented reality (AR) and robotics, where relighting and texture editing are crucial for realistic visualization. Achieving these tasks requires accurately separating albedo from shade. However, performing this separation on point clouds presents two key challenges: (1) the non-grid structure of point clouds makes conventional image-based decomposition models ineffective, and (2) point-cloud models designed for other tasks do not explicitly consider global-light direction, resulting in inaccurate shade. In this paper, we introduce \textbf{Intrinsic Point-Cloud Decomposition (IPCD)}, which extends image decomposition to the direct decomposition of colored point clouds into albedo and shade. To overcome challenge (1), we propose \textbf{IPCD-Net} that extends image-based model with point-wise feature aggregation for non-grid data processing. For challenge (2), we introduce \textbf{Projection-based Luminance Distribution (PLD)} with a hierarchical feature refinement, capturing global-light ques via multi-view projection. For comprehensive evaluation, we create a synthetic outdoor-scene dataset. Experimental results demonstrate that IPCD-Net reduces cast shadows in albedo and enhances color accuracy in shade. Furthermore, we showcase its applications in texture editing, relighting, and point-cloud registration under varying illumination. Finally, we verify the real-world applicability of IPCD-Net.&lt;/p&gt;</description></item><item><guid>2511.10060v2</guid><title>Multivariate Gaussian Representation Learning for Medical Action Evaluation</title><link>http://arxiv.org/abs/2511.10060v2</link><author>Luming Yang, Haoxian Liu, Siqing Li, Alper Yilmaz</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对医学视觉中细粒度动作评估的挑战，提出了新的数据集和方法，并展示了显著的性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 医学视觉中的细粒度动作评估面临数据缺乏、精度要求高以及快速动作时空动态建模不足等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建包含 6,372 条专家标注视频和 22 个临床标签的多视角多标签基准数据集 CPREval-6k，并提出 GaussMedAct 框架以改进医学动作分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GaussMedAct 采用多元高斯编码，将关节运动投射到时间缩放的多维空间，并将动作分解为适应性的三维高斯作为令牌；通过各向异性协方差保持运动语义，并使用笛卡尔与向量双流的空间编码充分利用骨骼关节和骨骼特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 CPREval-6k 基准上，所提方法实现了 92.1% 的 Top‑1 准确率，实时推理且仅比基线少 10% FLOPs，准确率提升 5.9%；跨数据集实验进一步验证了方法的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GaussMedAct 在医学动作评估任务中表现出更高的准确率和更好的鲁棒性，为未来研究提供了有效的工具和数据支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 医学视觉中的细粒度动作评估面临数据缺乏、精度要求高以及快速动作时空动态建模不足等挑战。为支持开发和评估，我们推出了 CPREval‑6k，这是一个包含 6,372 条专家标注视频和 22 个临床标签的多视角、多标签医学动作基准。利用该数据集，我们提出了 GaussMedAct，一种多元高斯编码框架，通过自适应时空表示学习推进医学运动分析。多元高斯表示将关节运动投射到时间缩放的多维空间，并将动作分解为自适应的三维高斯作为令牌。这些令牌通过各向异性协方差建模保持运动语义，同时对时空噪声具有鲁棒性。混合空间编码采用笛卡尔和向量双流策略，有效利用关节和骨骼特征的骨骼信息。所提出的方法在基准上实现了 92.1% 的 Top‑1 准确率，并实现实时推理，仅比基线少 10% FLOPs，准确率提升 5.9%；跨数据集实验进一步确认了我们方法的鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Fine-grained action evaluation in medical vision faces unique challenges due to the unavailability of comprehensive datasets, stringent precision requirements, and insufficient spatiotemporal dynamic modeling of very rapid actions. To support development and evaluation, we introduce CPREval-6k, a multi-view, multi-label medical action benchmark containing 6,372 expert-annotated videos with 22 clinical labels. Using this dataset, we present GaussMedAct, a multivariate Gaussian encoding framework, to advance medical motion analysis through adaptive spatiotemporal representation learning. Multivariate Gaussian Representation projects the joint motions to a temporally scaled multi-dimensional space, and decomposes actions into adaptive 3D Gaussians that serve as tokens. These tokens preserve motion semantics through anisotropic covariance modeling while maintaining robustness to spatiotemporal noise. Hybrid Spatial Encoding, employing a Cartesian and Vector dual-stream strategy, effectively utilizes skeletal information in the form of joint and bone features. The proposed method achieves 92.1% Top-1 accuracy with real-time inference on the benchmark, outperforming baseline by +5.9% accuracy with only 10% FLOPs. Cross-dataset experiments confirm the superiority of our method in robustness.&lt;/p&gt;</description></item><item><guid>2511.10209v2</guid><title>LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures</title><link>http://arxiv.org/abs/2511.10209v2</link><author>Wenzhe He, Xiaojun Chen, Ruiqi Wang, Ruihui Li, Huilong Pi, Jiapeng Zhang, Zhuo Tang, Kenli Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LiNeXt 是一种轻量级、非扩散网络，旨在快速准确地完成 LiDAR 点云场景补全，显著提升实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶感知系统中，3D LiDAR 场景补全是关键任务。传统方法多使用扩散模型实现高质量重建，但多步采样导致计算开销大，难以满足实时需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种不使用扩散模型、单步去噪和精细化的网络，以降低计算成本并保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; LiNeXt 由 Noise-to-Coarse 模块和 Refine 模块组成。Noise-to-Coarse 在单次前向传播中去噪，消除多步采样；Refine 模块利用粗点云及其中间特征进行精细化，提升结构完整性。同时引入 Distance-aware Selected Repeat 策略，生成更均匀的噪声点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 SemanticKITTI 数据集上，LiNeXt 推理速度提升 199.8 倍，Chamfer 距离降低 50.7%，参数量仅为 LiDiff 的 6.1%，验证了其高效性和有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LiNeXt 在实时场景补全方面表现出优越的效率和效果，适用于自动驾驶等需要快速点云处理的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D LiDAR 场景补全是自动驾驶感知系统的基本组成部分。以往方法主要使用扩散模型实现高保真重建，但其多步迭代采样导致显著的计算开销，限制了实时应用。为了解决这一问题，我们提出了 LiNeXt——一种轻量级、非扩散网络，优化用于快速准确的点云补全。具体而言，LiNeXt 首先使用 Noise-to-Coarse（N2C）模块在单次前向传播中去噪输入的噪声点云，从而消除了扩散方法的多步迭代采样。Refine 模块随后利用粗点云及其来自 N2C 模块的中间特征进行更精确的细化，进一步提升结构完整性。此外，我们观察到 LiDAR 点云呈现距离相关的空间分布，近距离采样密集，远距离稀疏。为此，我们提出了 Distance-aware Selected Repeat 策略，以生成更均匀分布的噪声点云。在 SemanticKITTI 数据集上，LiNeXt 的推理速度提升 199.8 倍，Chamfer 距离降低 50.7%，参数量仅为 LiDiff 的 6.1%，这些结果证明了 LiNeXt 在实时场景补全方面的卓越效率和有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 LiDAR 点云场景补全中的稀疏与遮挡问题，使得自动驾驶感知系统能够获得完整的三维环境信息，从而提升目标检测、姿态估计和地图构建等下游任务的鲁棒性与安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现传统扩散模型在补全时需要多步采样，导致计算开销大，难以满足实时需求。于是他们借鉴扩散框架的思路，但改为单步去噪，设计了 Noise‑to‑Coarse（N2C）和 Refine 模块，并引入距离感知重复、交叉点注意力（CPA）和多尺度稀疏卷积（MSSC）等技术，以提升效率与精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过距离感知的点重复与加噪得到均匀分布的噪声点云，然后用 N2C 模块一次性生成粗略补全，再用 Refine 模块细化细节。整体流程为：输入点云 → 距离感知重复 → 加噪 → N2C → 粗补全 → Refine → 完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 距离感知重复策略平衡近远点采样；② 交叉点注意力模块实现全局与局部特征的动态融合；③ 多尺度稀疏卷积高效提取多分辨率特征；④ 完全去除扩散采样，采用单步去噪。与以往扩散模型相比，LiNeXt 在推理速度提升约200倍、参数量仅占6%且 Chamfer 距离下降50%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiNeXt 提出一种轻量级、非扩散的 LiDAR 场景补全框架，通过距离感知采样、交叉点注意力和多尺度稀疏卷积，实现实时高精度补全，显著优于现有扩散方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D LiDAR scene completion from point clouds is a fundamental component of perception systems in autonomous vehicles. Previous methods have predominantly employed diffusion models for high-fidelity reconstruction. However, their multi-step iterative sampling incurs significant computational overhead, limiting its real-time applicability. To address this, we propose LiNeXt-a lightweight, non-diffusion network optimized for rapid and accurate point cloud completion. Specifically, LiNeXt first applies the Noise-to-Coarse (N2C) Module to denoise the input noisy point cloud in a single pass, thereby obviating the multi-step iterative sampling of diffusion-based methods. The Refine Module then takes the coarse point cloud and its intermediate features from the N2C Module to perform more precise refinement, further enhancing structural completeness. Furthermore, we observe that LiDAR point clouds exhibit a distance-dependent spatial distribution, being densely sampled at proximal ranges and sparsely sampled at distal ranges. Accordingly, we propose the Distance-aware Selected Repeat strategy to generate a more uniformly distributed noisy point cloud. On the SemanticKITTI dataset, LiNeXt achieves a 199.8x speedup in inference, reduces Chamfer Distance by 50.7%, and uses only 6.1% of the parameters compared with LiDiff. These results demonstrate the superior efficiency and effectiveness of LiNeXt for real-time scene completion.&lt;/p&gt;</description></item><item><guid>2511.12170v2</guid><title>Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective</title><link>http://arxiv.org/abs/2511.12170v2</link><author>Wang Luo, Di Wu, Hengyuan Na, Yinlin Zhu, Miao Hu, Guocong Quan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了点云补全的新范式——基于校正的补全，并实现了PGNet框架，显著提升了补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全面临遮挡和几何缺失问题，现有方法多采用基于填充的范式，易产生结构不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 重新思考点云补全任务，提出更稳健的Completion-by-Correction范式，并构建PGNet实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用预训练的图像到3D模型生成拓扑完整的形状先验，随后在特征空间进行校正；PGNet通过双特征编码、粗略结构生成和分层校正实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; PGNet在ShapeNetViPC数据集上相较于现有基线平均Chamfer距离下降23.5%，F-score提升7.1%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Completion-by-Correction范式和PGNet框架能实现结构一致且与观测对齐的点云补全，优于传统填充方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分观测中重建完整的三维形状，这一任务因严重遮挡和几何缺失而具有挑战性。尽管最近在多模态技术方面取得进展，利用互补的RGB图像来补偿缺失几何，但大多数方法仍遵循基于填充的补全范式，从融合的潜在特征中合成缺失结构。我们通过实验表明，该范式往往因几何和语义约束有限而导致结构不一致和拓扑伪影。为了解决这一问题，我们重新思考任务并提出了更稳健的范式——基于校正的补全（Completion-by-Correction），该方法以预训练的图像到3D模型生成的拓扑完整形状先验为起点，并在特征空间进行校正以与部分观测对齐。该范式将补全从无约束的合成转向有指导的细化，实现结构一致且与观测对齐的重建。在此基础上，我们提出了PGNet，一种多阶段框架，进行双特征编码以锚定生成先验，合成粗略但结构对齐的支架，并通过分层校正逐步细化几何细节。对ShapeNetViPC数据集的实验表明，PGNet在平均Chamfer距离下降23.5%和F-score提升7.1%方面优于最先进的基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决多模态点云补全问题，即从部分点云和单视角RGB图像中恢复完整的三维形状。点云在LiDAR或RGB‑D传感器中常因遮挡、反射或分辨率限制而稀疏不完整，影响自动驾驶、增强现实和机器人等应用的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统的Completion‑by‑Inpainting方法在严重缺失时易产生结构不一致和拓扑伪影，随后提出Completion‑by‑Correction思路，利用预训练的图像‑&amp;gt;3D模型生成完整形状先验并在特征空间进行校正。设计中借鉴了PoinTr、DGCNN、Salient Transformer、Grounding Transformer等现有技术，并在此基础上构建了三阶段PGNet框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先得到一个完整但可能不精确的形状先验，然后通过特征空间的对齐与校正，使其与部分观测对齐，最终得到完整点云。实现流程分为三阶段：1）Corrective Dual‑Feature Encoding，对先验和部分点云进行并行编码并通过交叉注意力进行特征对齐；2）Grounded Seed Generation，利用全局融合生成粗略但结构完整的种子点云并进行几何校正；3）Hierarchical Grounded Refinement，递归细化粗点云，融合双源特征以恢复高精度几何。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① Completion‑by‑Correction范式，将补全任务从无约束生成转为先验校正；② 双特征编码与显著门控机制，实现先验与观测的自适应融合；③ 基于种子生成的结构化粗点云与层级细化策略，显著降低拓扑伪影。与以往直接在融合特征上进行补全的工作不同，PGNet通过先验校正和分阶段细化实现了更高的结构一致性和更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种Completion‑by‑Correction框架，利用图像生成的完整形状先验并通过特征校正与分阶段细化，实现了更准确、更结构一致的多模态点云补全。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).&lt;/p&gt;</description></item><item><guid>2511.13171v1</guid><title>Autonomous Sensing UAV for Accurate Multi-User Identification and Localization in Cellular Networks</title><link>http://arxiv.org/abs/2511.13171v1</link><author>Niccolò Paglierani, Francesco Linsalata, Vineeth Teeda, Davide Scazzoli, Maurizio Magarini</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自主感知框架，利用不属于5G接入网络的无人机（UAV）被动捕获上行声波参考信号，实现多用户的识别与定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在5G网络中，传统的空中接入节点需要与网络基础设施高度协同，而本研究的无人机则完全独立于网络，专注于感知任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在单次部署中自主规划并执行多用户定位与识别的无人机感知系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 无人机被动捕获上行声波参考信号，机载完成同步、用户识别与定位的完整信号处理链，并通过自主任务规划与飞行控制实现实时感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仿真与低空实测验证显示，乡村环境下定位误差低于3米，城市仿真场景下误差低于8米，且能够可靠识别每个用户。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结果证明，基础设施独立的感知无人机是低空经济（LAE）的核心要素，可在紧急或连通性受限环境中提供情境感知与快速部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种自主感知框架，利用不属于5G接入网络的无人机（UAV）被动捕获上行声波参考信号，实现多用户的识别与定位。与传统空中接入节点不同，所提出的无人机被动运行，仅专注于感知任务。它捕获上行声波参考信号（SRS），几乎不需要与网络基础设施协调。本文提出并开发了完整的信号处理链，包括同步、用户识别和定位，全部在无人机飞行期间在机载完成。该系统自主规划并适应其任务工作流程，在单次部署中估计多用户位置，将飞行控制与实时感知集成。大量仿真和全规模低空实验验证了该方法，在乡村实地测试中定位误差低于3米，在城市仿真场景中低于8米，同时可靠识别每个用户。结果证实了基础设施独立的感知无人机作为新兴低空经济（LAE）的核心要素的可行性，支持在紧急或连通性受限环境中的情境感知和快速部署。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents an autonomous sensing frame- work for identifying and localizing multiple users in Fifth Generation (5G) networks using an Unmanned Aerial Vehicle (UAV) that is not part of the serving access network. Unlike conventional aerial serving nodes, the proposed UAV operates passively and is dedicated solely to sensing. It captures Uplink (UL) Sounding Reference Signals (SRS), and requires virtually no coordination with the network infrastructure. A complete signal processing chain is proposed and developed, encompassing synchronization, user identification, and localization, all executed onboard UAV during flight. The system autonomously plans and adapts its mission workflow to estimate multiple user positions within a single deployment, integrating flight control with real-time sensing. Extensive simulations and a full-scale low- altitude experimental campaign validate the approach, showing localization errors below 3 m in rural field tests and below 8 m in urban simulation scenarios, while reliably identifying each user. The results confirm the feasibility of infrastructure-independent sensing UAVs as a core element of the emerging Low Altitude Economy (LAE), supporting situational awareness and rapid deployment in emergency or connectivity-limited environments.&lt;/p&gt;</description></item><item><guid>2511.15004v1</guid><title>IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics</title><link>http://arxiv.org/abs/2511.15004v1</link><author>Halil S. Kelebek, Linnea M. Wolniewicz, Michael D. Vergalla, Simone Mestici, Giacomo Acciarini, Bala Poduval, Olga Verkhoglyadova, Madhulika Guhathakurta, Thomas E. Berger, Frank Soboczenski, Atılım Güneş Baydin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; IonCast 是一套基于深度学习的模型，专门用于预测电离层的总电子含量（TEC），通过图网络和时空学习方法整合多种物理驱动和观测数据，提升了在风暴和静止条件下的预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 电离层对全球导航卫星系统、HF 通信和航空运营至关重要，准确的预测和建模对提升这些系统的可靠性和安全性具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够更精确地预测电离层变化的深度学习工具，以支持空间天气的操作性预警。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建了受 GraphCast 启发的图网络模型，利用时空学习技术对全球 TEC 进行预测，并将多源物理驱动和观测数据统一输入模型，随后在保留的风暴和静止条件下进行验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在风暴和静止两种条件下，IonCast 的预测精度均优于简单的持久性模型，显示出显著的技能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过可扩展的图网络时空学习，IonCast 展示了机器学习在增强对电离层变异的物理理解和提升空间天气操作韧性方面的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 电离层是近地空间的关键组成部分，影响全球导航卫星系统（GNSS）精度、高频通信和航空运营。鉴于此，准确预测和建模电离层变异变得越来越重要。为了解决这一空白，我们提出了 IonCast，一套深度学习模型，其中包括一个受 GraphCast 启发、专为电离层动力学设计的模型。IonCast 利用时空学习来预测全球总电子含量（TEC），整合多种物理驱动和观测数据集。在对保留的风暴时段和静止条件进行验证时，显示出相较于持久性模型的更高技能。通过统一异构数据并采用可扩展的图网络时空学习，IonCast 展示了机器学习如何增强对电离层变异的物理理解，并推动操作性空间天气韧性的提升。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.&lt;/p&gt;</description></item><item><guid>2511.16161v1</guid><title>Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion</title><link>http://arxiv.org/abs/2511.16161v1</link><author>Lirui Zhang, Zhengkai Zhao, Zhi Zuo, Pan Gao, Jie Qin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为Simba的新框架，用于点云补全，旨在同时保留细节并保持整体结构完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全任务需要在保持输入细节的同时保证完成形状的全局结构完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统基于回归的局部对称变换方法易过拟合且对噪声敏感的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将点级变换回归改为分布学习，结合对称先验与扩散模型的生成能力，并采用分层Mamba架构实现高质量上采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在PCN、ShapeNet和KITTI基准上，Simba实现了领先的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过分布学习和生成模型，Simba克服了过拟合和噪声敏感，显著提升了点云补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全是3D视觉中的一项基础任务。该领域的一个持续挑战是同时保留输入中存在的细粒度细节，并确保完成形状的全局结构完整性。最近利用局部对称变换通过直接回归的方法显著提升了几何结构细节的保留，但这些方法存在两个主要局限：（1）基于回归的方法易过拟合，倾向于记忆瞬时特定的变换，而不是学习可泛化的几何先验；（2）它们依赖点级变换回归，对输入噪声高度敏感，严重削弱了鲁棒性和泛化能力。为了解决这些挑战，我们提出了Simba，一种将点级变换回归重新表述为分布学习问题的新框架。我们的方法将对称先验与扩散模型的强大生成能力相结合，避免了实例特定的记忆，同时捕捉到稳健的几何结构。此外，我们引入了分层Mamba架构以实现高保真上采样。在PCN、ShapeNet和KITTI基准上的广泛实验验证了我们方法的领先性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成中同时保持细节与全局结构的问题，尤其是避免传统基于对称变换回归方法的过拟合和噪声敏感性。该问题在自动驾驶、机器人和增强现实等实际应用中至关重要，因为缺失的点云会导致感知错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为扩散模型能够生成多样化的几何先验，因而将点对点变换回归改为分布学习。借鉴了SymmCompletion的局部对称变换思想、Diffusion模型以及Mamba架构，设计了两阶段框架：先用SymmGT预训练目标变换，再用Sym-Diffuser学习条件分布，最后用MBA-Refiner细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型学习点云关键点的仿射变换场，而不是直接回归点坐标；先用SymmGT生成目标变换；Sym-Diffuser在条件下从噪声中恢复完整变换场；将变换应用于关键点得到粗完整点云；MBA-Refiner采用层级Mamba网络逐步细化并上采样，得到高精度完成结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 将点云完成视为条件生成变换场的任务；② 使用扩散模型学习仿射变换分布，避免过拟合；③ 设计MBA-Refiner的级联Mamba结构实现高效细化；④ 在合成到真实数据迁移上表现出色。与以往直接回归变换或点坐标的方法不同，Simba通过分布学习和Mamba加速实现更鲁棒、更细致的完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Simba提出一种基于扩散模型的对称变换场生成与Mamba细化的点云完成框架，兼顾细节保留与全局一致性，并在合成与真实数据上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method&amp;#x27;s state-of-the-art (SOTA) performance.&lt;/p&gt;</description></item><item><guid>2511.16807v1</guid><title>Mesh RAG: Retrieval Augmentation for Autoregressive Mesh Generation</title><link>http://arxiv.org/abs/2511.16807v1</link><author>Xiatao Sun, Chen Liang, Qian Wang, Daniel Rakita</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文提出一种名为 Mesh RAG 的无训练、即插即用框架，用于改进自回归 3D 网格生成模型。通过检索点云分割、空间变换和配准技术，解耦了严格的顺序依赖，使生成过程更快、更高质量，并支持增量编辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D 网格在工业设计、游戏、仿真和机器人等领域至关重要，但传统手工制作耗时且难以扩展。自回归模型已成为自动生成网格的有效方法，但其顺序生成导致速度慢且难以编辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决自回归网格生成中质量与速度的权衡以及增量编辑困难的问题，提供一种更高效、可并行化且无需重新训练的改进方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用检索增强生成（RAG）思路，利用点云分割、空间变换和点云配准来检索、生成并整合网格组件，从而打破顺序限制，实现并行推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Mesh RAG 在多种基础自回归网格生成模型上均表现出显著提升网格质量、加速生成速度，并实现增量编辑，且不需要模型重新训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 检索增强的 Mesh RAG 为自回归网格生成提供了一种高效、灵活且易于集成的解决方案，可在保持或提升质量的同时显著提高生成速度，并支持增量编辑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 网格是工业设计、游戏、仿真和机器人等应用的关键构件。传统上，网格由艺术家手工制作，耗时且难以扩展。为自动化和加速资产创建，已出现自回归模型作为强大的艺术网格生成范式。然而，提升质量的现有方法通常依赖更大的模型或更长的序列，导致生成时间更长，其固有的顺序特性也带来严重的质量-速度权衡。顺序依赖还显著复杂化增量编辑。为克服这些限制，我们提出 Mesh RAG，一种新颖的、无训练、即插即用框架，用于自回归网格生成模型。受语言模型 RAG 的启发，我们的方法通过利用点云分割、空间变换和点云配准来检索、生成并整合网格组件，从而增强生成过程。该检索式方法将生成与严格的顺序依赖解耦，促进高效且可并行化的推理。我们展示了 Mesh RAG 在多种基础自回归网格生成模型上的广泛适用性，证明它显著提升网格质量、加速生成速度，并实现增量编辑，且无需模型重新训练。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在解决自回归网格生成模型的顺序依赖导致的生成速度慢、质量与效率难以平衡以及增量编辑困难的问题。3D网格是游戏、工业设计、仿真等领域的核心资产，手工建模耗时且难以扩展，提升自动化生成的效率和可编辑性具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了语言模型中的检索增强生成（RAG）思路，将其迁移到3D网格域。通过使用P3‑SAM和Sonata进行点云分割，并结合粗略对齐与ICP注册来检索空间变换，形成一个训练‑free、可插拔的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将网格生成拆分为“分割-生成-检索-合并”四步：先把输入点云分割成若干部件；用现有自回归模型为每个部件生成归一化网格；通过AABB粗对齐和ICP细化检索每个部件的尺度、位置和姿态；最后将检索到的变换应用于生成的网格并拼接成完整模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新在于：①提出训练‑free、可插拔的检索增强框架；②通过空间变换检索解耦顺序依赖，实现并行生成和局部增量编辑；③在不改动原模型的前提下显著提升质量和速度。与以往关注标记化或模型训练的工作不同，Mesh RAG侧重于推理过程的改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Mesh RAG通过检索空间变换，将自回归网格生成拆分为并行的部件生成与合成，实现在不重新训练模型的情况下显著提升生成质量、速度和可编辑性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D meshes are a critical building block for applications ranging from industrial design and gaming to simulation and robotics. Traditionally, meshes are crafted manually by artists, a process that is time-intensive and difficult to scale. To automate and accelerate this asset creation, autoregressive models have emerged as a powerful paradigm for artistic mesh generation. However, current methods to enhance quality typically rely on larger models or longer sequences that result in longer generation time, and their inherent sequential nature imposes a severe quality-speed trade-off. This sequential dependency also significantly complicates incremental editing. To overcome these limitations, we propose Mesh RAG, a novel, training-free, plug-and-play framework for autoregressive mesh generation models. Inspired by RAG for language models, our approach augments the generation process by leveraging point cloud segmentation, spatial transformation, and point cloud registration to retrieve, generate, and integrate mesh components. This retrieval-based approach decouples generation from its strict sequential dependency, facilitating efficient and parallelizable inference. We demonstrate the wide applicability of Mesh RAG across various foundational autoregressive mesh generation models, showing it significantly enhances mesh quality, accelerates generation speed compared to sequential part prediction, and enables incremental editing, all without model retraining.&lt;/p&gt;</description></item><item><guid>2511.17054v1</guid><title>RL-AD-Net: Reinforcement Learning Guided Adaptive Displacement in Latent Space for Refined Point Cloud Completion</title><link>http://arxiv.org/abs/2511.17054v1</link><author>Bhanu Pratap Paregi, Vaibhav Kumar</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于强化学习的点云补全细化框架 RL-AD-Net，能够在预训练自编码器的潜在空间中对补全结果进行局部几何细化，并通过轻量级选择器挑选最优重建。实验表明该方法在不同裁剪场景下均能显著提升补全质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有点云补全模型能生成整体合理的形状，但常出现局部几何不一致的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过强化学习实现对补全结果的局部几何细化，提升整体几何精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用自编码器将补全结果编码为全局特征向量；随后强化学习代理在潜在空间中对特征进行选择性调整；使用轻量级非参数选择器比较原始与细化结果的几何一致性，保留更优的重建；若有真实标签，则用距离和一致性指标引导细化；训练按类别独立完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ShapeNetCore-2048 数据集上，RL-AD-Net 在随机裁剪和训练裁剪两种场景下均能持续提升补全效果，优于基线网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RL-AD-Net 轻量、模块化且与模型无关，可直接应用于多种补全网络，无需重新训练，且未来可扩展到多类别细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文介绍了最近的点云补全模型，包括基于变压器、去噪以及其他先进方法，它们能够从部分输入生成整体合理的形状，但往往留下局部几何不一致。我们提出了 RL-AD-Net，一种基于强化学习的细化框架，工作于预训练点云自编码器的潜在空间。自编码器将补全结果编码为紧凑的全局特征向量，随后强化学习代理在该空间中对特征进行选择性调整，以提升几何精度。为保证鲁棒性，轻量级非参数 PointNN 选择器会评估原始补全与 RL 细化结果的几何一致性，保留更优的重建。当有真实标签时，Chamfer 距离和几何一致性指标共同指导细化。由于强化学习的无监督和动态特性，训练在每个类别上单独进行，跨类别收敛较难，但未来可扩展到多类别细化。实验在 ShapeNetCore-2048 上表明，虽然基线补全网络在其训练裁剪方式下表现合理，但在随机裁剪场景下表现不佳；相比之下，RL-AD-Net 在两种设置下均能持续提升效果，凸显了基于强化学习的集成细化的有效性。该方法轻量、模块化且与模型无关，可应用于多种补全网络，无需重新训练。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进点云完成模型产生的局部几何不一致问题。准确的三维几何对于机器人抓取、自动驾驶感知和虚拟现实等应用至关重要。现有的强大模型往往在细节上出现模糊或缺失，影响后续任务的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到基线完成网络在局部细节上易出错，于是提出在预训练的自编码器潜在空间中进行细化。该思路借鉴了 RL-GAN-Net 的 RL 引导生成以及 Point-Patch RL 的局部补全经验，但区别在于不需要重新训练生成器，而是仅在潜在空间中调整。作者还参考了类别特定训练的优势，采用每个类别单独的 RL 代理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将完成结果编码为 128 维全局特征向量，使用强化学习代理预测一个微调向量，再将调整后的特征解码为改进的点云。实现流程为：①使用任意完成网络得到基线点云；②用预训练自编码器编码得到 GFV；③RL 代理输出增量并更新 GFV；④冻结解码器生成细化点云；⑤用 PointNN 评估基线与细化结果，选取质量更高的作为最终输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①在潜在空间中进行 RL 细化，保持全局结构不变；②采用 128 维轻量级潜在表示，便于高维连续控制；③每个类别单独训练 RL 代理，提升对类别几何先验的利用；④使用非参数 PointNN 进行无监督质量选择，保证细化不会退化；⑤整个框架模型无关、无需重新训练基线网络。与以往需要重新训练生成器或仅使用确定性滤波的工作不同，RL-AD-Net 通过学习的策略实现实例化、可迁移的细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RL-AD-Net 提供了一种轻量级、类别特定的强化学习细化模块，可在不改动任何完成网络的前提下，显著提升点云完成结果的局部几何质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent point cloud completion models, including transformer-based, denoising-based, and other state-of-the-art approaches, generate globally plausible shapes from partial inputs but often leave local geometric inconsistencies. We propose RL-AD-Net, a reinforcement learning (RL) refinement framework that operates in the latent space of a pretrained point autoencoder. The autoencoder encodes completions into compact global feature vectors (GFVs), which are selectively adjusted by an RL agent to improve geometric fidelity. To ensure robustness, a lightweight non-parametric PointNN selector evaluates the geometric consistency of both the original completion and the RL-refined output, retaining the better reconstruction. When ground truth is available, both Chamfer Distance and geometric consistency metrics guide refinement. Training is performed separately per category, since the unsupervised and dynamic nature of RL makes convergence across highly diverse categories challenging. Nevertheless, the framework can be extended to multi-category refinement in future work. Experiments on ShapeNetCore-2048 demonstrate that while baseline completion networks perform reasonable under their training-style cropping, they struggle in random cropping scenarios. In contrast, RL-AD-Net consistently delivers improvements across both settings, highlighting the effectiveness of RL-guided ensemble refinement. The approach is lightweight, modular, and model-agnostic, making it applicable to a wide range of completion networks without requiring retraining.&lt;/p&gt;</description></item><item><guid>2511.20253v1</guid><title>Zoo3D: Zero-Shot 3D Object Detection at Scene Level</title><link>http://arxiv.org/abs/2511.20253v1</link><author>Andrey Lemeshko, Bulat Gabdullin, Nikita Drozdov, Anton Konushin, Danila Rukhovich, Maksim Kolodiazhnyi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Zoo3D 是首个无训练的 3D 物体检测框架，利用二维实例掩码的图聚类构建 3D 边界框，并通过开放词汇模块进行语义标注。它提供零样本和自监督两种模式，在 ScanNet200 和 ARKitScenes 基准上实现了最先进的开放词汇检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的 3D 检测方法受限于封闭类别，难以识别未见过的物体；现有开放词汇检测器虽然降低了标注需求，但仍需训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种完全不需要训练的 3D 检测框架，以实现对多样且未知物体的识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过对二维实例掩码进行图聚类生成 3D 边界框，随后使用最佳视角选择和视角一致掩码生成的开放词汇模块为框分配语义标签；提供零样本模式和自监督模式，并支持直接处理姿态化或未姿态化图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Zoo3D_0（零样本）和 Zoo3D_1（自监督）在 ScanNet200 和 ARKitScenes 上均取得了最先进的开放词汇检测结果，且零样本模式超过了所有现有自监督方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 训练免费、即插即用的 Zoo3D 展示了在现实 3D 理解任务中的强大适应性和性能，证明了无训练方法的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D物体检测是空间理解的基础。现实环境需要能够识别多样且以前未见过物体的模型，而封闭集方法仍然存在重大限制。现有的开放词汇3D检测器放宽了标注要求，但仍依赖训练场景，无论是点云还是图像。我们进一步提出Zoo3D，首个无训练的3D物体检测框架。该方法通过对二维实例掩码进行图聚类构建3D边界框，然后使用新颖的开放词汇模块进行语义标注，模块包括最佳视角选择和视角一致掩码生成。Zoo3D有两种模式：零样本Zoo3D_0完全不需要训练，和自监督Zoo3D_1通过在Zoo3D_0生成的伪标签上训练一个类别无关检测器来细化3D框预测。我们还将Zoo3D扩展到点云之外，直接处理已姿态化甚至未姿态化的图像。在ScanNet200和ARKitScenes基准上，Zoo3D_0和Zoo3D_1在开放词汇3D检测任务中均取得了最先进的结果。值得注意的是，零样本Zoo3D_0超过了所有现有自监督方法，证明了训练免费、即插即用方法在现实3D理解中的强大适应性。代码可在 https://github.com/col14m/zoo3d 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现零训练、开放词汇的室内 3D 目标检测，能够在不见过任何类别的场景中定位并识别物体。此问题在实际应用中至关重要，因为真实环境中存在大量未标注或新出现的物体，传统闭集方法难以泛化，且收集 3D 标注成本高昂。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 MaskClustering、SAM、CLIP 等基础模型，先用 2D 分割生成实例掩码，再通过图聚类得到 3D 盒子；随后利用视角一致性和 CLIP 语义对齐为盒子赋予标签。该思路在保持无训练的前提下，融合了现有的 2D‑&amp;gt;3D 迁移技术和开放词汇检索方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 3D 检测拆分为无类别 3D 盒子预测和开放词汇标签分配。流程为：输入点云与图像 → 生成 2D 掩码 → 构建掩码图并聚类 → 输出 3D 盒子 → 对每个盒子裁剪点云并投影到最佳视角 → 用 SAM 细化掩码 → 通过 CLIP 计算与文本标签的相似度 → 赋予语义标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 第一次实现完全训练‑free 的 3D 检测；② 通过 MaskClustering 直接生成 3D 盒子；③ 引入最佳视角选择和视角一致性掩码生成的开放词汇模块；④ 在无姿态图像上也能工作（利用 DUSt3R）。与以往需要训练或仅在点云/姿态图像上工作的自监督方法不同，Zoo3D 在零训练条件下即可达到甚至超过自监督性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Zoo3D 展示了一个完全训练‑free、基于基础模型的零训练开放词汇 3D 目标检测框架，在室内场景中实现了最先进的性能，并在无姿态图像上也能直接工作。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .&lt;/p&gt;</description></item><item><guid>2511.20257v1</guid><title>Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling</title><link>http://arxiv.org/abs/2511.20257v1</link><author>Zhiguo Zhang, Xiaoliang Ma, Daniel Schlesinger</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种物理引导、可解释的时空学习框架，用于空气污染预测，模型由物理驱动的传输核和可解释注意力机制组成，在斯德哥尔摩数据集上表现优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 空气污染预测对公共健康至关重要，但现有模型在性能与可解释性之间存在权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种兼顾高预测性能和可解释性的空气污染预测模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建物理引导的传输核，权重受风向和地理条件影响；加入可解释注意力机制，学习局部响应并将未来浓度归因于历史时滞和外部驱动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在斯德哥尔摩地区的综合数据集上，该模型在多个预测时段均优于最先进基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型将高预测性能与时空可解释性结合，为实际空气质量管理提供更可靠的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model&amp;#x27;s integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model&amp;#x27;s integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.&lt;/p&gt;</description></item><item><guid>2511.20278v1</guid><title>DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion</title><link>http://arxiv.org/abs/2511.20278v1</link><author>Yinghui Li, Qianyu Zhou, Di Shao, Hao Yang, Ye Zhu, Richard Dazeley, Xuequan Lu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了首个将状态空间模型（SSM）应用于域自适应点云补全（DA PCC）的研究，并针对其面临的挑战设计了新框架DAPointMamba。该框架通过三种跨域对齐模块实现了对几何和语义差异的有效补偿，最终在合成与真实数据集上均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 域自适应点云补全旨在弥合标注源域与无标签目标域之间的几何与语义差异。传统方法使用CNN或视觉Transformer，受限于感受野或二次复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究SSM在DA PCC中的适应性，并提出一种高效、全局感受野的框架，以提升跨域补全性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DAPointMamba包含三大模块：1）跨域补丁级扫描，建立补丁几何对应；2）跨域空间SSM对齐，基于相似度调制补丁特征；3）跨域通道SSM对齐，交错对齐特征通道以弥合语义差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 直接将3D点云序列化为1D序列会破坏空间拓扑；忽视域无关表征会削弱适应效果。DAPointMamba通过上述三模块克服这些问题，在多种基准上实现了更低的计算复杂度和推理延迟，并取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DAPointMamba在域自适应点云补全任务中展现出强大的适应性和高效性，证明了SSM在此类任务中的可行性与优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 域自适应点云补全（DA PCC）旨在缩小标注源域与无标签目标域之间的几何和语义差异。现有方法要么受限于感受野，要么因使用CNN或视觉Transformer而导致二次复杂度。本文提出了首个研究状态空间模型（SSM）在DA PCC中适应性的工作，并发现直接将SSM应用于DA PCC会遇到若干挑战：将3D点云序列化为1D序列往往会破坏目标域的空间拓扑和局部几何特征；忽视学习域无关表征的设计会阻碍适应性能。为解决这些问题，我们提出了新框架DAPointMamba，具有跨域强适应性、全局感受野和线性复杂度优势。该框架包含三个新模块。具体而言，跨域补丁级扫描引入补丁级几何对应，能够实现有效的局部对齐；跨域空间SSM对齐通过基于跨域相似度调制补丁特征，进一步加强空间一致性，有效缓解细粒度结构差异；跨域通道SSM对齐通过交错和对齐特征通道，主动解决全局语义差距。对合成和真实世界基准的广泛实验表明，DAPointMamba在保持更低计算复杂度和推理延迟的同时，优于现有最先进方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决域自适应点云完成（DA PCC）中的几何和语义差异，使在源域训练的模型能够在未标记的目标域上保持高质量的重建。该问题重要，因为不同传感器、场景或数据集导致的分布偏移会显著降低模型泛化能力，影响自动驾驶、机器人和虚拟现实等实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统 CNN 受限于感受野、Transformer 受限于二次复杂度，随后关注能够提供全局感受野且线性复杂度的状态空间模型（SSM）Mamba。通过分析直接序列化点云会破坏空间拓扑，作者借鉴了基于 Z‑order 曲线的局部扫描和相似度引导的特征调制技术，结合 Mamba 的优势设计了跨域补丁扫描、空间和通道对齐模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 Mamba 的全局建模能力，并通过三步跨域对齐提升域适应性：1）跨域补丁扫描（CDPS）使用共享归一化和 Z‑order 序列化，使源域和目标域的补丁在空间上对应；2）跨域空间 SSM 对齐（CDSA）根据补丁间相似度调制特征，细化局部对齐；3）跨域通道 SSM 对齐（CDCA）通过全局特征混合和通道调制解决语义差异。实现流程为：输入点云 → CDPS → Mamba 块（含 CDSA、CDCA）→ 解码器生成完整点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1）首次将 Mamba 引入域自适应点云完成；2）提出跨域补丁扫描保证空间对应；3）设计空间和通道对齐模块实现细粒度与全局语义一致；4）保持线性复杂度和全局感受野，显著降低计算成本。与以往基于 CNN 或 Transformer 的方法相比，DAPointMamba 在保持高性能的同时实现了更高的效率和更强的跨域泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAPointMamba 提出一种基于 Mamba 的线性复杂度框架，通过跨域补丁扫描、空间和通道对齐实现了优于现有 CNN/Transformer 方法的域自适应点云完成。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.&lt;/p&gt;</description></item><item><guid>2511.21925v1</guid><title>OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving</title><link>http://arxiv.org/abs/2511.21925v1</link><author>Alex Richardson, Jonathan Sprinkle</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 OpenTwinMap，一个基于 Python 的开源框架，用于从 LiDAR 扫描和 OpenStreetMap 数据生成高保真 3D 城市数字孪生。该框架解决了现有工具与特定仿真器耦合、难以扩展和技术负担大的问题，并支持将生成的资产导出到 Unreal Engine 进行自动驾驶仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 数字孪生在自动驾驶研究中起着关键作用，可用于仿真、验证和与生成式世界模型集成。然而，现有公开工具往往与特定仿真器紧耦合，难以扩展，且技术开销大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一个可扩展、并行化的开源框架，降低研究者在不同城市环境中适配和扩展数字孪生生成流程的门槛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; OpenTwinMap 通过 Python 处理 LiDAR 和 OSM 数据，完成预处理、道路网格和地形生成，并支持将结果导出到 Unreal Engine；同时提供对 CARLA 的初步集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 目前框架已实现 OSM 与 LiDAR 数据的预处理、基本道路网格和地形生成，并具备对 CARLA 的初步支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenTwinMap 通过强调可扩展性和并行化，提供了一个低门槛、可适配多种城市场景的数字孪生生成管线，为自动驾驶研究提供了有价值的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 数字孪生在城市环境中对推进自动驾驶（AV）研究起着关键作用，它们通过仿真、验证和与新兴生成式世界模型的集成来实现。虽然现有工具已显示出价值，但许多公开可用的解决方案与特定仿真器紧密耦合，难以扩展，或引入显著的技术开销。例如，最广泛使用的开源 AV 仿真器 CARLA 提供的数字孪生框架完全实现为 Unreal Engine 的 C++ 插件，限制了灵活性和快速原型设计。在本研究中，我们提出了 OpenTwinMap，一个基于 Python 的开源框架，用于生成高保真 3D 城市数字孪生。完成的框架将摄取 LiDAR 扫描和 OpenStreetMap（OSM）数据，生成语义分割的静态环境资产，包括道路网络、地形和城市结构，并可导出到 Unreal Engine 进行 AV 仿真。OpenTwinMap 强调可扩展性和并行化，降低了研究者将管线适配和扩展到多样化城市环境的门槛。我们描述了 OpenTwinMap 的当前功能，包括 OSM 和 LiDAR 数据的预处理、基本道路网格和地形生成，以及对 CARLA 的初步支持。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决缺乏灵活、高精度城市数字孪生生成工具的问题。数字孪生对于自动驾驶研究至关重要，因为它们支持真实感仿真、验证和与生成式世界模型的集成，但现有工具往往与特定仿真器紧耦合、难以扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者利用公开的 OSM 与 LiDAR 数据，并借鉴 CARLA、OpenDRIVE、Open3D 等现有工具，构建了一个 Python 框架。该框架模仿 CARLA 的功能，但将其与 Unreal Engine 解耦，便于快速原型和扩展。作者还参考了近期生成式模型研究，以支持未来的集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 LiDAR 点云与 OSM 道路数据融合，先细化几何形状，再转换为 OpenDRIVE 描述，随后生成道路和静态对象的 3D 网格，最后导出到 Unreal Engine 或 CARLA。整个流程由数据摄取、几何细化、转换、网格生成和导出等模块组成，并支持并行化处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：开源、Python 版、可扩展的管线；模块化的 OpenDRIVE 1.4 实现和 OSM‑to‑OpenDRIVE 转换器；对桥梁和高架桥的支持；以及可扩展到其他仿真器的 CARLA stub。与 CARLA 的 Unreal 插件相比，OpenTwinMap 解除耦合、易于定制且支持并行化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenTwinMap 提供了一个灵活、Python 版的框架，将 LiDAR 与 OSM 数据转换为高精度、语义分割的 3D 城市数字孪生，兼容 CARLA 及其他仿真器。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Digital twins of urban environments play a critical role in advancing autonomous vehicle (AV) research by enabling simulation, validation, and integration with emerging generative world models. While existing tools have demonstrated value, many publicly available solutions are tightly coupled to specific simulators, difficult to extend, or introduce significant technical overhead. For example, CARLA-the most widely used open-source AV simulator-provides a digital twin framework implemented entirely as an Unreal Engine C++ plugin, limiting flexibility and rapid prototyping. In this work, we propose OpenTwinMap, an open-source, Python-based framework for generating high-fidelity 3D urban digital twins. The completed framework will ingest LiDAR scans and OpenStreetMap (OSM) data to produce semantically segmented static environment assets, including road networks, terrain, and urban structures, which can be exported into Unreal Engine for AV simulation. OpenTwinMap emphasizes extensibility and parallelization, lowering the barrier for researchers to adapt and scale the pipeline to diverse urban contexts. We describe the current capabilities of the OpenTwinMap, which includes preprocessing of OSM and LiDAR data, basic road mesh and terrain generation, and preliminary support for CARLA integration.&lt;/p&gt;</description></item><item><guid>2511.22181v1</guid><title>MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction</title><link>http://arxiv.org/abs/2511.22181v1</link><author>Maitrayee Keskar, Mohan Trivedi, Ross Greer</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 提出一种基于视觉的轨迹规划方法，利用ViT编码器生成与运动预测框架和意图输入对齐的上下文嵌入。 2. 通过将地图特征替换为学习得到的视觉表示，构建MTR‑VP（Motion Transformer for Vision‑based Planning）模型。 3. 在Waymo数据集上进行评估，预测未来5秒的轨迹，并通过消融实验验证模型设计。 4. 发现传统Transformer在融合视觉与运动特征时效果有限，即使加入CLIP和DINOv2的场景表示也无明显提升；而预测多条未来轨迹的分布能显著提升规划性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自动驾驶系统需要准确预测车辆未来轨迹，以实现安全与高效行驶。传统方法往往依赖地图信息和运动历史，而视觉信息在轨迹规划中的作用尚未充分挖掘。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够利用摄像头图像生成有效场景上下文嵌入，并与运动预测与意图输入协同工作的轨迹规划框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用ViT编码器处理原始图像和过去的运动状态，生成上下文嵌入；将其与意图嵌入通过交叉注意力结合；采用MTR的多模态预测机制，形成MTR‑VP模型；在Waymo End‑to‑End Driving Dataset上进行实验，并通过消融研究验证输入图像和多轨迹输出的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1. 传统Transformer在融合视觉与运动特征时效果不佳；2. 即使使用CLIP和DINOv2等基础模型增强场景表示，也未能显著提升上下文嵌入质量；3. 预测多条未来轨迹的概率分布比单一轨迹预测更能提升规划性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于视觉的轨迹规划方法MTR‑VP能够在缺少地图信息的情况下，通过交叉注意力有效整合视觉与运动信息；然而，仅靠视觉特征与运动特征的简单融合不足以产生有用的场景上下文嵌入；预测多未来轨迹分布是提升规划效果的关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种用于自动驾驶轨迹规划的方法，该方法学习基于图像的上下文嵌入，使其与运动预测框架和基于规划的意图输入保持一致。在我们的方法中，ViT编码器接受原始图像和过去的运动状态作为输入，并被训练以产生上下文嵌入，灵感来自最近的MTR（Motion Transformer）编码器，有效地用学习得到的视觉表示替代基于地图的特征。MTR为多模态轨迹预测提供了强大的基础，通过定位代理意图并通过运动查询对进行迭代细化；我们将该方法命名为MTR‑VP（Motion Transformer for Vision‑based Planning），并且在MTR解码器中使用可学习的意图查询的地方，我们使用交叉注意力对意图和上下文嵌入进行处理，这些嵌入反映了从驾驶场景和过去车辆状态编码的信息组合。我们在Waymo End‑to‑End Driving Dataset上评估了我们的方法，该数据集要求使用先前的摄像头图像、代理姿态历史和路径目标，在鸟瞰坐标系中预测代理未来5秒的轨迹。我们通过消融研究分析了我们的架构，去除了输入图像和多轨迹输出。我们的结果表明，用于将视觉特征与运动特征（如过去轨迹特征）结合的Transformer方法并不有效，即使使用CLIP和DINOv2等基础模型的场景上下文表示来增强意图嵌入，仍然无法产生有用的场景上下文嵌入，但预测多条未来轨迹的分布而不是单一未来轨迹能提升规划性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent&amp;#x27;s future 5-second trajectory in bird&amp;#x27;s-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.&lt;/p&gt;</description></item><item><guid>2511.22275v1</guid><title>RecToM: A Benchmark for Evaluating Machine Theory of Mind in LLM-based Conversational Recommender Systems</title><link>http://arxiv.org/abs/2511.22275v1</link><author>Mengfan Li, Xuanhua Shi, Yang Deng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文讨论了大型语言模型在对话式推荐系统中的应用，并指出其核心在于推断和推理用户的心理状态（即心智理论）。现有评测主要基于人工情境，缺乏对真实对话中心理状态推断和行为预测的考察。为此，作者提出了RecToM基准，涵盖认知推断和行为预测两个维度，并在多种先进模型上进行实验，发现模型在识别心理状态方面有一定能力，但在动态对话中保持连贯的心智推理和策略选择仍面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型在对话式推荐系统中表现出色，但有效推荐对话需要推断用户的心理状态。现有评测多依赖Sally-Anne测试等人工情境，忽视了真实对话中的复杂性和行为预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出RecToM基准，以更贴近人类社会推理的方式评估LLM在推荐对话中的心智理论能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; RecToM基准聚焦认知推断和行为预测两维度，评估模型对心理状态的理解以及基于此进行对话策略预测与选择的能力，并在多种先进LLM上进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明RecToM对LLM构成显著挑战；模型在识别心理状态方面表现部分，但在动态推荐对话中难以持续保持连贯、战略性的心智推理，尤其在跟踪意图演变和将推断的心理状态与对话策略对齐方面存在困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RecToM揭示了当前LLM在真实对话情境下的心智推理局限，提示未来研究需进一步提升模型在行为预测和策略选择方面的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Large Language models are revolutionizing the conversational recommender systems through their impressive capabilities in instruction comprehension, reasoning, and human interaction. A core factor underlying effective recommendation dialogue is the ability to infer and reason about users&amp;#x27; mental states (such as desire, intention, and belief), a cognitive capacity commonly referred to as Theory of Mind. Despite growing interest in evaluating ToM in LLMs, current benchmarks predominantly rely on synthetic narratives inspired by Sally-Anne test, which emphasize physical perception and fail to capture the complexity of mental state inference in realistic conversational settings. Moreover, existing benchmarks often overlook a critical component of human ToM: behavioral prediction, the ability to use inferred mental states to guide strategic decision-making and select appropriate conversational actions for future interactions. To better align LLM-based ToM evaluation with human-like social reasoning, we propose RecToM, a novel benchmark for evaluating ToM abilities in recommendation dialogues. RecToM focuses on two complementary dimensions: Cognitive Inference and Behavioral Prediction. The former focus on understanding what has been communicated by inferring the underlying mental states. The latter emphasizes what should be done next, evaluating whether LLMs can leverage these inferred mental states to predict, select, and assess appropriate dialogue strategies. Extensive experiments on state-of-the-art LLMs demonstrate that RecToM poses a significant challenge. While the models exhibit partial competence in recognizing mental states, they struggle to maintain coherent, strategic ToM reasoning throughout dynamic recommendation dialogues, particularly in tracking evolving intentions and aligning conversational strategies with inferred mental states.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large Language models are revolutionizing the conversational recommender systems through their impressive capabilities in instruction comprehension, reasoning, and human interaction. A core factor underlying effective recommendation dialogue is the ability to infer and reason about users&amp;#x27; mental states (such as desire, intention, and belief), a cognitive capacity commonly referred to as Theory of Mind. Despite growing interest in evaluating ToM in LLMs, current benchmarks predominantly rely on synthetic narratives inspired by Sally-Anne test, which emphasize physical perception and fail to capture the complexity of mental state inference in realistic conversational settings. Moreover, existing benchmarks often overlook a critical component of human ToM: behavioral prediction, the ability to use inferred mental states to guide strategic decision-making and select appropriate conversational actions for future interactions. To better align LLM-based ToM evaluation with human-like social reasoning, we propose RecToM, a novel benchmark for evaluating ToM abilities in recommendation dialogues. RecToM focuses on two complementary dimensions: Cognitive Inference and Behavioral Prediction. The former focus on understanding what has been communicated by inferring the underlying mental states. The latter emphasizes what should be done next, evaluating whether LLMs can leverage these inferred mental states to predict, select, and assess appropriate dialogue strategies. Extensive experiments on state-of-the-art LLMs demonstrate that RecToM poses a significant challenge. While the models exhibit partial competence in recognizing mental states, they struggle to maintain coherent, strategic ToM reasoning throughout dynamic recommendation dialogues, particularly in tracking evolving intentions and aligning conversational strategies with inferred mental states.&lt;/p&gt;</description></item><item><guid>2511.22404v1</guid><title>UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data</title><link>http://arxiv.org/abs/2511.22404v1</link><author>Longkun Zou, Jiale Wang, Rongqin Liang, Hai Wu, Ke Chen, Yaowei Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了UAV-MM3D合成数据集，包含400K帧多模态数据，支持无人机感知与运动理解，并提供LGFusionNet融合模型和轨迹预测基线，旨在推动低空无人机3D感知研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 低空无人机感知对空域安全和智能系统至关重要，但真实数据收集受限，手工标注成本高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服数据收集与标注难题，构建大规模、多模态、精确标注的合成数据集，以支持无人机感知与运动理解研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 创建UAV-MM3D合成数据集，包含多场景、多天气、多无人机型号和五种模态；每帧提供2D/3D框、6自由度姿态和实例级注释；提出LGFusionNet基于LiDAR的多模态融合模型和轨迹预测基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 数据集覆盖广泛场景与天气，提供丰富注释，LGFusionNet和轨迹预测基线为评测提供基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UAV-MM3D为低空无人机3D感知提供了可控、全面的公开基准，促进相关技术进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确感知低空复杂环境中的无人机对于空域安全和相关智能系统至关重要。开发可靠解决方案需要大规模、精确标注且多模态的数据。然而，现实世界的无人机数据收集受到空域法规、隐私关注和环境变化的限制，手工标注3D姿态和跨模态对应关系既耗时又昂贵。为克服这些挑战，我们提出了UAV-MM3D，一套高保真多模态合成数据集，用于低空无人机感知和运动理解。该数据集包含400K同步帧，覆盖城市、郊区、森林、沿海等多样场景以及晴朗、多云、雨天、雾天等天气条件，包含微型、小型、中型多种无人机型号，并提供RGB、红外、LiDAR、雷达和动态视觉传感器（DVS）五种模态。每帧提供二维/三维边界框、6自由度姿态和实例级注释，支持无人机相关的核心任务，如三维检测、姿态估计、目标跟踪和短期轨迹预测。我们进一步提出了基于LiDAR的多模态融合基线LGFusionNet，以及专门的无人机轨迹预测基线，以便进行基准测试。凭借可控的仿真环境、全面的场景覆盖和丰富的注释，UAV-MM3D为推进无人机三维感知提供了公开基准。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 缺乏大规模、多模态、精确标注的无人机感知数据，限制了低空无人机检测、姿态估计、跟踪和轨迹预测模型的研发。准确的无人机感知对空域安全、基础设施保护和公众隐私至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者利用Unreal Engine 4与CARLA的仿真平台，构建了可控的多天气、多场景、多无人机轨迹环境，并通过Python客户端实现天气、帧同步和坐标转换。设计参考了现有的多模态无人机数据集（如Anti-UAV-RGBT、MMDrone）和自动驾驶多模态数据集（nuScenes、Waymo），并在此基础上扩展到五种传感器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过高保真仿真生成同步的RGB、IR、DVS、LiDAR和雷达数据，并为每帧提供2D/3D框、6-DoF姿态和实例ID。实现流程包括：①在UE4-CARLA服务器中设置场景、天气和无人机轨迹；②Python客户端控制天气、帧同步和坐标转换；③多线程管道完成传感器数据采集、时空对齐和坐标变换；④生成标注并存储为统一的相机坐标系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提供400K帧、5模态、完整3D框和6-DoF标注的合成数据集；②覆盖城市、郊区、森林、海岸等八大场景和多种天气；③引入LiDAR引导的多模态融合基线LGFusionNet和轨迹预测基线；④为无人机感知提供统一评测框架。与以往仅有2D框、单模态或小规模数据集不同，UAV-MM3D在规模、模态、标注深度和任务多样性上实现了突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UAV-MM3D构建了一个大规模、多模态、精确标注的合成无人机感知基准，并提供融合与轨迹预测基线，为低空无人机检测、姿态估计、跟踪和轨迹预测研究提供统一评测平台。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.&lt;/p&gt;</description></item><item><guid>2511.22908v1</guid><title>ViGG: Robust RGB-D Point Cloud Registration using Visual-Geometric Mutual Guidance</title><link>http://arxiv.org/abs/2511.22908v1</link><author>Congjia Chen, Shen Yan, Yufu Qu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于互相引导的RGB-D点云配准方法ViGG，利用视觉与几何信息的互补，提升配准鲁棒性，并在多个数据集上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是3D视觉中的基础任务，现有方法大多仅使用几何信息；RGB-D配准方法多聚焦特征融合或学习，难以充分利用图像信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够充分利用图像信息、提高配准鲁棒性的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 在视觉-几何组合形式下求解团对齐，使用几何引导抑制模糊团；2) 通过视觉引导的几何匹配，利用视觉先验确定搜索空间，提取高质量、抗噪对应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在3DMatch、ScanNet和KITTI数据集上，ViGG在学习无关和学习相关两种设置下均优于最新方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 互相引导策略显著提升了RGB-D配准的鲁棒性，方法适用于多种配准任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准是3D视觉中的基础任务。大多数现有方法仅使用几何信息进行配准。最近提出的RGB-D配准方法主要关注特征融合或改进特征学习，这限制了它们利用图像信息的能力，并阻碍了其实际应用。在本文中，我们提出了ViGG，一种使用互相引导的鲁棒RGB-D配准方法。首先，我们在视觉-几何组合形式下求解团对齐，采用几何引导设计抑制模糊团。其次，为了减轻视觉匹配噪声导致的精度下降，我们提出了一种视觉引导的几何匹配方法，利用视觉先验确定搜索空间，从而提取高质量、抗噪对应。该互相引导策略为我们的方法带来了更优的鲁棒性，使其适用于各种RGB-D配准任务。对3DMatch、ScanNet和KITTI数据集的实验表明，我们的方法在学习无关和学习相关两种设置下均优于最近的最先进方法。代码可在 https://github.com/ccjccjccj/ViGG 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 RGB‑D 点云配准中的鲁棒性问题，尤其是在低重叠、噪声或视觉信息不完整的场景下。点云配准是 3D 视觉、重建和机器人导航等领域的基础任务，缺乏鲁棒方法会直接影响后续处理的精度与可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到视觉匹配具有高质量但易受纹理模糊影响，几何匹配则受局部歧义限制。基于此，他们借鉴了 MAC 的最大团搜索、TEASER 的鲁棒估计以及传统的视觉特征匹配技术，提出了视觉与几何互相引导的配准框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用视觉匹配做粗略配准并通过几何信息消除错误团，再用得到的粗配准作为先验引导几何匹配，提取高质量对应点。实现流程为：①提取图像关键点匹配并映射到 3D；②提取几何特征；③用几何引导的视觉团对齐得到初始变换；④评估该变换的置信度；⑤在该变换的搜索空间内进行几何匹配；⑥利用得到的对应点重新估计精确变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①视觉与几何互相引导的双向策略；②几何引导的视觉团对齐，抑制视觉歧义；③视觉引导的几何匹配，利用粗配准定位搜索区间；④在保持低计算成本的同时兼顾学习‑free 与学习‑based 场景。与以往仅通过网络融合多模态特征或单纯几何匹配的方法不同，ViGG 通过显式的互导机制显著提升了鲁棒性和适用范围。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ViGG 通过视觉与几何的互导配准框架，实现了在多种 RGB‑D 任务中显著提升鲁棒性和精度的点云配准方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration is a fundamental task in 3D vision. Most existing methods only use geometric information for registration. Recently proposed RGB-D registration methods primarily focus on feature fusion or improving feature learning, which limits their ability to exploit image information and hinders their practical applicability. In this paper, we propose ViGG, a robust RGB-D registration method using mutual guidance. First, we solve clique alignment in a visual-geometric combination form, employing a geometric guidance design to suppress ambiguous cliques. Second, to mitigate accuracy degradation caused by noise in visual matches, we propose a visual-guided geometric matching method that utilizes visual priors to determine the search space, enabling the extraction of high-quality, noise-insensitive correspondences. This mutual guidance strategy brings our method superior robustness, making it applicable for various RGB-D registration tasks. The experiments on 3DMatch, ScanNet and KITTI datasets show that our method outperforms recent state-of-the-art methods in both learning-free and learning-based settings. Code is available at https://github.com/ccjccjccj/ViGG.&lt;/p&gt;</description></item><item><guid>2511.23227v2</guid><title>PointCNN++: Performant Convolution on Native Points</title><link>http://arxiv.org/abs/2511.23227v2</link><author>Lihan Li, Haofeng Zhong, Rui Bu, Mingchao Sun, Wenzheng Chen, Baoquan Chen, Yangyan Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 PointCNN++，一种新的 3D 点云学习架构，旨在解决传统点基方法的性能瓶颈和体素基方法的几何精度损失问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的 3D 点云卷积学习方法主要分为点基方法和体素基方法，前者保持几何精度但性能有限，后者通过量化实现高效但牺牲几何细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将稀疏卷积从体素推广到点，构建一种既能保持高精度又能实现高性能的点云卷积网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用以点为中心的卷积，将感受野聚焦在原始高精度点坐标上，并设计在原生点上直接执行的计算策略，将卷积转化为矩阵向量乘法与归约问题，随后实现专用的高效 GPU 核心。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 PointCNN++ 在内存占用上比典型点基方法低一个数量级，速度提升数倍；作为体素基骨干的替代方案时，显著提升点云配准精度，同时保持更低的内存使用和更快的速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 保持几何细节与实现高性能并非互斥，PointCNN++ 为高保真高效的 3D 学习开辟了新路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的 3D 点云卷积学习方法可分为两大范式：点基方法能够保持几何精度，但往往面临性能挑战；体素基方法通过量化实现高效，但牺牲了几何细节。几何精度的损失是点云配准等任务的关键瓶颈。我们提出了 PointCNN++，一种新颖的架构设计，根本上缓解了精度与性能之间的权衡。它将稀疏卷积从体素推广到点，将体素卷积视为更一般点卷积的特殊、退化情况。首先，我们引入了以点为中心的卷积，使感受野聚焦在原始高精度点坐标上。其次，为了让这种高保真操作具备良好性能，我们设计了一种在原生点上直接执行的计算策略。我们将原生点卷积表述为矩阵-向量乘法与归约（MVMR）问题，并为此开发了专用的高效 GPU 核心。实验表明，PointCNN++ 的内存占用比代表性点基方法低一个数量级，速度提升数倍。此外，当它作为体素基骨干的简单替代时，显著提升点云配准精度，同时保持更低的内存使用和更快的速度。PointCNN++ 证明了保持几何细节与实现高性能并非互斥，为高保真高效的 3D 学习开辟了新路径。我们的代码将开源。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云卷积中精度与性能之间的权衡问题。传统的体素方法速度快但会因量化失去细节，点云方法保持精度却计算量大。对于需要高精度配准等任务，这种折衷限制了应用效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了体素与点云两种主流范式的缺陷，认为两者的折衷并非不可避免。随后提出以原始点为卷积中心的点中心卷积，并在此基础上借鉴稀疏卷积和GPU优化技术，形成一种既保持几何精度又高效的计算框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将卷积直接作用于原始点云，先用精确的邻域搜索确定邻居，再在每个邻域内做局部体素化以匹配卷积核。实现上把卷积写成矩阵-向量乘法与归约（MVMR）问题，并为此开发专门的GPU核，完成高效计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①点中心卷积将体素卷积视为特殊情况；②局部自适应体素化保持精度；③MVMR形式与专用GPU核实现显著节省内存并提升速度。与以往方法相比，它消除了全局体素化导致的精度损失，同时避免了点云方法的转换开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointCNN++提出一种在原始点云上进行卷积的高效算子，既保留几何细节，又实现与体素方法相当甚至更优的速度和内存表现。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing convolutional learning methods for 3D point cloud data are divided into two paradigms: point-based methods that preserve geometric precision but often face performance challenges, and voxel-based methods that achieve high efficiency through quantization at the cost of geometric fidelity. This loss of precision is a critical bottleneck for tasks such as point cloud registration. We propose PointCNN++, a novel architectural design that fundamentally mitigates this precision-performance trade-off. It $\textbf{generalizes sparse convolution from voxels to points}$, treating voxel-based convolution as a specialized, degraded case of our more general point-based convolution. First, we introduce a point-centric convolution where the receptive field is centered on the original, high-precision point coordinates. Second, to make this high-fidelity operation performant, we design a computational strategy that operates $\textbf{natively}$ on points. We formulate the convolution on native points as a Matrix-Vector Multiplication and Reduction (MVMR) problem, for which we develop a dedicated, highly-optimized GPU kernel. Experiments demonstrate that PointCNN++ $\textbf{uses an order of magnitude less memory and is several times faster}$ than representative point-based methods. Furthermore, when used as a simple replacement for the voxel-based backbones it generalizes, it $\textbf{significantly improves point cloud registration accuracies while proving both more memory-efficient and faster}$. PointCNN++ shows that preserving geometric detail and achieving high performance are not mutually exclusive, paving the way for a new class of 3D learning with high fidelity and efficiency. Our code will be open sourced.&lt;/p&gt;</description></item><item><guid>2512.00264v1</guid><title>HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction</title><link>http://arxiv.org/abs/2512.00264v1</link><author>Zhengda Ma, Abhirup Banerjee</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了基于点云的几何深度学习框架HeartFormer，用于从心动图 MRI 数据重建三维四腔心脏模型，并在公开数据集上实现了优于现有方法的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统心动图 MRI 只能提供二维切片图像，限制了对心脏形态和功能的全面理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服二维限制，构建能够从三维点云完成四腔心脏重建的深度学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了 HeartFormer，包含语义感知双结构变换器网络（SA-DSTNet）和语义感知几何特征细化变换器网络（SA-GFRTNet），并构建了首个公开的 17,000 份高分辨率三维多类心脏网格与点云数据集 HeartCompv1。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 HeartCompv1 和 UK Biobank 数据集上，HeartFormer 在鲁棒性、精度和泛化性方面均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架能够生成高保真、几何一致的三维心脏模型，并为该研究方向提供了可验证的基准数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了首个基于点云表示的几何深度学习框架，用于从心动图 MRI 数据中重建三维四腔心脏。该工作解决了传统心动图 MRI 仅提供二维心脏切片图像的长期局限性，从而限制了对健康和病理条件下心脏形态和生理机制的全面理解。为克服这一问题，我们提出了 “HeartFormer”，一种新颖的点云补全网络，将传统的单类点云补全扩展到多类。HeartFormer 由两个关键组件组成：语义感知双结构变换器网络（SA-DSTNet）和语义感知几何特征细化变换器网络（SA-GFRTNet）。SA-DSTNet 生成包含全局几何特征和子结构几何特征的初始粗点云。借助这些语义-几何表示，SA-GFRTNet 逐步细化粗输出，有效利用全局和子结构几何先验，生成高保真且几何一致的重建结果。我们进一步构建了 “HeartCompv1”，首个公开的大规模数据集，包含 17,000 个高分辨率三维多类心脏网格和点云，用于为这一新兴研究方向建立通用基准。对 HeartCompv1 和 UK Biobank 的跨域实验表明，HeartFormer 在鲁棒性、准确性和可泛化性方面表现出色，始终优于最先进方法。代码和数据集将在接受后发布，网址为：https://github.com/10Darren/HeartFormer。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在从传统的二维心脏磁共振成像（cine MRI）中重建完整的三维四腔心脏点云和网格。二维切片限制了对心脏形态和功能的三维评估，而高质量的三维模型对于生物标志物分析、病理可视化和个体化心脏力学模拟至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先将多视角的二维切片通过分割、轮廓提取和三维配准生成稀疏、错位的点云，然后提出 HeartFormer 这一多类别点云补全网络。该网络借鉴了现有点云补全技术（如 PCN、PoinTr、PCCN 等）和语义感知方法，进一步引入双结构 Transformer（SA‑DSTNet 与 SA‑GFRTNet）来同时捕获全局与子结构信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用语义感知的双结构 Transformer 先生成粗略的多类别点云，再通过两阶段的细化 Transformer 逐步提升几何细节和解剖一致性。实现流程为：输入 cine MRI → 分割与轮廓提取 → 生成稀疏点云 → SA‑DSTNet（全局+子结构聚合）→ 粗点云与特征 → SA‑GFRTNet（两阶段细化）→ 精细点云 → Poisson 重建与 Ball Pivoting → 生成三维网格。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次提出基于点云的全自动三维四腔心脏重建管线；2) 设计了 HeartFormer，包含 SA‑DSTNet 与 SA‑GFRTNet，能够在多类别语义下进行点云补全并纠正错位；3) 构建了首个公开的大规模多类别心脏点云数据集 Heart‑Compv1；4) 在 Heart‑Compv1 与 UK Biobank 上实现了显著优于现有单类别和多类别补全方法的性能。与以往方法相比，HeartFormer 兼顾全局与子结构语义，采用 Transformer 进行上下文聚合，并在细化阶段保持解剖一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HeartFormer 提出了一种语义感知的 Transformer 点云补全框架，能够从稀疏的 cine MRI 切片中重建高保真、解剖一致的三维四腔心脏模型，并通过首个大规模心脏点云数据集验证其优越性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.&lt;/p&gt;</description></item><item><guid>2512.00345v1</guid><title>mmPred: Radar-based Human Motion Prediction in the Dark</title><link>http://arxiv.org/abs/2512.00345v1</link><author>Junqiao Fan, Haocong Rao, Jiarui Zhang, Jianfei Yang, Lihua Xie</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文首次将毫米波雷达用于人体运动预测，提出mmPred框架，结合时间域姿态细化和频域主导运动两分支，并使用全骨架关系变换器实现全局关节协作，显著提升预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统基于RGB-D摄像头的人体运动预测受光照影响大且存在隐私问题，限制了其在消防和医疗等实际场景中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索毫米波雷达作为新型感知方式，解决光照敏感和隐私问题，并克服雷达信号的镜面反射和多径干扰导致的噪声与时序不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出mmPred，采用扩散模型，使用双域历史运动表示引导生成；时间域姿态细化分支学习细节；频域主导运动分支捕捉全局趋势并抑制帧级不一致；全骨架关系变换器作为扩散骨干，建模全局关节协作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; mmPred在mmBody数据集上比现有方法提升8.6%，在mm-Fi数据集上提升22%，实现了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 毫米波雷达结合扩散模型和骨架关系变换器能有效预测人体运动，克服雷达信号噪声和时序问题，具有良好的鲁棒性和隐私保护。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Existing Human Motion Prediction methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.&lt;/p&gt;</description></item><item><guid>2512.00355v1</guid><title>SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction</title><link>http://arxiv.org/abs/2512.00355v1</link><author>Junqiao Fan, Pengfei Liu, Haocong Rao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种单阶段扩散模型 SMamDiff，用于人类运动预测，强调空间-时间一致性，并在两个关键设计上实现了更好的预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着智能室内传感和服务机器人的广泛部署，人类运动预测对于安全主动协助至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在单阶段扩散模型中确保空间-时间一致性，以提高预测的准确性和多样性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SMamDiff 采用残差-DCT 运动编码和 stickman-drawing 空间 Mamba 模块，分别通过减去最近观测姿态并进行离散余弦变换来突出高频信息，以及按顺序逐关节处理姿态以引入跨关节长程依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 Human3.6M 和 HumanEva 数据集上，SMamDiff 在单阶段概率 HMP 方法中取得了最先进的结果，并且比多阶段扩散基线在延迟和内存占用上更低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 单阶段扩散模型结合空间-时间一致性机制能够在保持低延迟和低内存的同时实现高质量的人类运动预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着智能室内传感和服务机器人的广泛部署，人类运动预测（HMP）对于安全、主动的协助至关重要。然而，许多现有的 HMP 方法要么产生单一的确定性预测，忽略不确定性，要么依赖概率模型，牺牲运动学合理性。扩散模型在准确性与多样性之间取得了更好的平衡，但往往依赖多阶段流水线，导致在边缘设备上的部署成本高。本文聚焦于如何在单阶段扩散模型中确保空间-时间一致性。我们提出 SMamDiff，一种基于 Spatial Mamba 的扩散模型，包含两个创新设计：(i) 残差-DCT 运动编码，在进行时间离散余弦变换前减去最近观测姿态，降低零频分量的主导作用，突出更高频的有用信息，使模型学习关节如何运动而非仅仅位置；(ii) stickman-drawing 空间 Mamba 模块按顺序逐关节处理姿态，使后续关节受前面关节的条件影响，从而引入跨关节的长程依赖。在 Human3.6M 和 HumanEva 数据集上，这些一致性机制在单阶段概率 HMP 方法中实现了最先进的结果，并且在延迟和内存占用上优于多阶段扩散基线。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.&lt;/p&gt;</description></item><item><guid>2512.00927v1</guid><title>LAHNet: Local Attentive Hashing Network for Point Cloud Registration</title><link>http://arxiv.org/abs/2512.00927v1</link><author>Wentao Qu, Xiaoshui Huang, Liang Xiao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 LAHNet 的局部注意哈希网络，用于点云配准，利用局部注意机制和局部敏感哈希实现更广阔的感受野，从而生成更具区分度的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的基于学习的点云描述子主要关注局部信息，缺乏足够的感受野，导致特征区分度不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入局部注意机制和局部敏感哈希，扩展点云描述子的感受野，提高特征的区分度，从而提升点云配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了 Group Transformer 捕捉长距离上下文，使用线性邻域搜索和局部敏感哈希将点云划分为不重叠窗口；采用跨窗口策略进一步扩大感受野；在此基础上引入 Interaction Transformer 计算重叠矩阵，增强窗口间的特征交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 LAHNet 能学习到鲁棒且具有区分度的特征，在室内外真实数据集上实现了显著的配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 局部注意哈希网络通过更合理的感受野和窗口交互，显著提升了点云配准的性能，为未来点云描述子研究提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注点云配准中的特征描述子学习，尤其是如何提升特征的区分度。传统方法只聚焦局部信息，导致在相似几何结构上出现匹配错误。高质量的配准对自动驾驶、机器人导航和三维重建等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 Swin Transformer 的局部注意力思想和 FCGF 的稀疏卷积 U‑Net 结构，提出使用局部敏感哈希（LSH）对点云进行线性窗口划分。随后设计 Group Transformer 以捕捉合理的长程依赖，并在瓶颈处加入 Interaction Transformer 以增强重叠区域的特征交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是通过 LSH 将无序点云划分为非重叠窗口，并在每个窗口内使用局部注意力，同时通过跨窗口交互扩大感受野。实现流程为：输入点云 → 4D 稀疏卷积下采样 → 两层 Group Transformer（U‑Net 编码器）→ Interaction Transformer（瓶颈）→ 上采样解码器恢复尺度并聚合多尺度特征 → 输出对应点的描述子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) LSH 线性窗口划分，避免体素化、KNN 或八叉树的计算开销。2) Group Transformer 结合跨窗口策略，提供合理的长程依赖。3) Interaction Transformer 通过重叠矩阵匹配窗口并进行交叉注意力，提升低重叠场景的特征区分度。与以往仅关注局部或全局注意力的工作不同，LAHNet 在保持计算效率的同时显著提升了特征区分度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LAHNet 通过局部敏感哈希划分窗口、Group Transformer 与 Interaction Transformer 的组合，学习出更具区分度的点云描述子，在室内外高低重叠场景中实现了显著的配准性能提升。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.&lt;/p&gt;</description></item><item><guid>2512.00995v1</guid><title>S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud</title><link>http://arxiv.org/abs/2512.00995v1</link><author>Han Su, Tianyu Huang, Zichen Wan, Xiaohe Wu, Wangmeng Zuo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出S2AM3D方法，结合2D分割先验与3D一致监督，设计点一致编码器和尺度感知解码器，并构建大规模点云数据集，实验表明在多种评估中表现领先，具有鲁棒性和可控性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云分割面临数据稀缺导致3D模型泛化差，2D预训练知识引入后视角不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决3D点云分割的泛化不足和视角不一致问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; S2AM3D通过点一致编码器聚合多视角2D特征，使用3D对比学习生成全局一致点特征；尺度感知提示解码器实现实时分割粒度调整；并提供100k+样本高质量数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; S2AM3D在多种评估中取得领先性能，表现出卓越的鲁棒性和可控性，能处理复杂结构和尺寸差异显著的部件。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; S2AM3D有效融合2D先验与3D监督，提升点云分割性能，并通过大规模数据集提供充分监督，展示了在复杂场景下的优越表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于部件的点云分割近年来在3D计算机视觉中受到广泛关注。然而，现有研究面临两个主要挑战：原生3D模型由于数据稀缺而缺乏泛化能力，而引入2D预训练知识往往导致不同视角下分割结果不一致。为了解决这些挑战，我们提出了S2AM3D，该方法将2D分割先验与3D一致监督相结合。我们设计了一个点一致部件编码器，通过原生3D对比学习聚合多视角2D特征，生成全局一致的点特征。随后提出了一个尺度感知提示解码器，使得通过连续尺度信号能够实时调整分割粒度。同时，我们引入了一个大规模、高质量的部件级点云数据集，包含超过10万份样本，为模型训练提供了充足的监督信号。大量实验表明，S2AM3D在多种评估设置下实现了领先性能，在处理复杂结构和尺寸差异显著的部件时表现出卓越的鲁棒性和可控性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D点云的部件级分割中数据稀缺导致的泛化不足以及基于2D预训练知识导致的视角不一致问题。部件级分割对于3D内容创作、机器人操作和逆向工程等应用至关重要，因为它能够提供细粒度的几何与语义信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了已有的2D转3D迁移方法（如多视图提升、蒸馏）与原生3D对比学习，提出点一致编码器以聚合多视图2D特征并通过3D对比监督提升全局一致性。随后设计了可调尺度的提示解码器，利用正弦嵌入和双向交叉注意力实现实时尺度控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过2D先验与3D对比学习共同构建全局一致的点特征，再用连续尺度信号调制特征并与提示点交互，最终得到每点的分割概率。实现流程包括：①点一致编码器提取点特征；②将尺度映射为正弦嵌入并通过FiLM调制特征；③双向交叉注意力将提示点与全局特征融合；④轻量化头输出分割掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 2D-3D混合训练方案，利用原生3D对比学习提升全局一致性；2) 可连续调节尺度的提示解码器，支持实时细粒度控制；3) 大规模100k+点云数据集，提供丰富监督。与以往方法相比，S2AM3D在保持高精度的同时实现了更好的泛化、尺度可控性和无需后处理的端到端推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S2AM3D提出了一种融合2D先验与3D对比学习的可尺度点云部件分割框架，凭借大规模数据集实现了领先的精度与实时可调粒度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.&lt;/p&gt;</description></item><item><guid>2512.01178v1</guid><title>VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering</title><link>http://arxiv.org/abs/2512.01178v1</link><author>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为VSRD++的弱监督单目三维目标检测框架，利用神经场体素渲染和弱二维监督，消除了对三维标注的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单目三维目标检测是三维场景理解的基础任务，但现有方法高度依赖大量三维标注，通常需要从激光雷达点云中进行繁重的人工标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建弱监督框架，解决三维标注成本高的问题，并实现高质量的单目三维目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; VSRD++采用两阶段流程：第一阶段多视角自动标注，使用签名距离场（SDF）表示物体表面，并通过实例感知体素轮廓渲染生成实例掩码；将SDF拆分为立方体SDF和残差距离场（RDF）以优化三维边界框；通过在边界框属性中加入速度并为每个伪标签赋予置信度来处理动态物体的几何不一致；使用三维属性初始化模块初始化动态边界框参数。第二阶段使用优化后的三维边界框作为伪标签训练单目三维目标检测器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在KITTI-360数据集上，VSRD++在静态和动态场景中显著优于现有弱监督方法，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; VSRD++通过弱监督方式实现了高性能的单目三维目标检测，消除了对三维标注的依赖，为实际应用提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 单目三维目标检测是三维场景理解的基础但具有挑战性。现有方法高度依赖于带有大量三维标注的监督学习，这些标注通常通过激光雷达点云的劳动密集型标注过程获得。为了解决这个问题，我们提出了VSRD++，一种新颖的弱监督框架，消除了对三维标注的依赖，并利用基于神经场的体素渲染与弱二维监督。VSRD++由两阶段流程组成：多视角三维自动标注和随后的单目三维检测器训练。在多视角自动标注阶段，物体表面以签名距离场（SDF）表示，并通过提出的实例感知体素轮廓渲染生成实例掩码。为优化三维边界框，我们将每个实例的SDF拆分为立方体SDF和捕捉与立方体偏差的残差距离场（RDF）。为了解决在动态物体上应用体素渲染方法时常见的几何不一致问题，我们通过在边界框属性中加入速度以及为每个伪标签赋予置信度来对动态物体进行建模。此外，我们还采用三维属性初始化模块来初始化动态边界框参数。在单目三维目标检测阶段，优化后的三维边界框作为伪标签用于训练单目三维目标检测器。在KITTI-360数据集上进行的广泛实验表明，VSRD++在静态和动态场景中显著优于现有弱监督方法。代码可在 https://github.com/Magicboomliu/VSRD_plus_plus 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在没有3D标注的情况下进行单目3D目标检测的问题。由于3D标注需要依赖激光雷达点云并且人工成本高昂，缺乏足够的3D数据限制了自动驾驶等场景中检测系统的普及与扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将神经场体素渲染（NeRF、NeuS）与实例分割的2D监督相结合，提出实例感知体素轮廓渲染和SDF分解技术，并在此基础上加入速度属性和置信度权重。该设计借鉴了VSRD、NeuS、D-NeRF等现有工作，并在此基础上实现了对动态物体的建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多视角的体素渲染将每个目标的表面表示为SDF（由立方体SDF和残差SDF组成），并利用实例感知渲染生成实例掩码，利用2D掩码监督优化3D边界框和速度。流程分为两阶段：第一阶段在多视角图像中自动生成3D伪标签；第二阶段使用这些伪标签（并给每个标签分配置信度）训练单目3D检测器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）实例感知体素轮廓渲染；2）SDF分解为立方体SDF和残差SDF；3）动态物体的速度属性和置信度权重；4）利用自监督深度初始化3D属性。与以往需要LiDAR或合成数据的监督方法不同，VSRD++仅依赖2D实例掩码和多视角图像即可完成3D检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VSRD++提出了一种完全弱监督的单目3D检测框架，通过实例感知体素轮廓渲染和动态SDF优化，从2D掩码生成高质量3D伪标签，实现无需3D标注的精确检测。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance&amp;#x27;s SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus&lt;/p&gt;</description></item><item><guid>2512.01352v1</guid><title>OpenBox: Annotate Any Bounding Boxes in 3D</title><link>http://arxiv.org/abs/2512.01352v1</link><author>In-Jae Lee, Mungyeom Kim, Kwonyoung Ryu, Pierre Musacchio, Jaesik Park</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; OpenBox是一种两阶段自动标注流程，利用二维视觉基础模型将二维图像中的实例信息与三维点云对齐，并根据物体的刚性与运动状态生成自适应尺寸的三维边界框，从而在不需要自训练的情况下提供高质量的三维目标检测标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶等场景中，无监督且开放词汇的三维目标检测受到关注，主要因为降低标注成本和识别未见物体对安全与可扩展性至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有方法统一标注三维边界框、忽略物体物理状态以及需要多轮自训练导致标注质量不佳和计算开销大的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 第一阶段通过跨模态实例对齐，将二维图像中由视觉基础模型提取的实例级线索与对应的三维点云关联；第二阶段对实例按刚性和运动状态分类，并利用类别特定尺寸统计生成自适应边界框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Waymo Open Dataset、Lyft Level 5 Perception dataset和nuScenes dataset上的实验表明，OpenBox在准确性和效率上均优于基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OpenBox能够在无需自训练的情况下生成高质量的三维边界框标注，并在多数据集上实现了更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; OpenBox是一种两阶段自动标注流程，利用二维视觉基础模型将二维图像中的实例信息与三维点云对齐，并根据物体的刚性与运动状态生成自适应尺寸的三维边界框，从而在不需要自训练的情况下提供高质量的三维目标检测标注。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在自动生成任意类别的 3D 边界框，减少人工标注成本并支持开放词汇检测。此问题在自动驾驶等安全关键场景中至关重要，因为高质量的 3D 目标检测直接影响路径规划与车辆控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了 2D 视觉基础模型（Grounding DINO、SAM2）与 LiDAR 点云的跨模态对齐，借鉴了无监督 3D 检测与多模态融合的思路，但避免了迭代自训练。通过上下文感知细化和物理状态分类，提升了标注质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先将 2D 目标实例投影到 3D 点云并进行上下文细化，再根据实例的刚性与运动状态生成自适应边界框。流程包括：2D 检测+分割 → 投影到点云 → 上下文感知细化 → 物理类型分类 → 依据类别尺寸统计与 SDF 过滤生成 3D 边界框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）无需自训练即可完成标注；2）使用 2D 视觉基础模型提供高质量实例信息；3）上下文感知细化融合 LiDAR 聚类与图像掩码；4）表面感知噪声过滤与基于物理状态的自适应框生成。与以往方法相比，OpenBox 更加关注物理属性，避免了多轮迭代和输出级融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenBox 通过将 2D 视觉基础模型的实例信息与 LiDAR 点云对齐，自动生成高质量、物理状态自适应的 3D 边界框，消除了迭代自训练的需求。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects&amp;#x27; physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.&lt;/p&gt;</description></item><item><guid>2512.01850v1</guid><title>Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching</title><link>http://arxiv.org/abs/2512.01850v1</link><author>Yue Pan, Tao Sun, Liyuan Zhu, Lucas Nunes, Iro Armeni, Jens Behley, Cyrill Stachniss</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种将点云配准视为条件生成的方法，通过学习连续点级速度场将噪声点迁移到已配准场景，从而直接生成配准点云并恢复各视角姿态。该方法在低重叠、不同尺度和传感器模式下表现优异，并支持多机器人SLAM等下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云配准是将多个未姿态点云对齐到公共坐标系的核心步骤，广泛用于三维重建和机器人定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新的配准框架，避免传统对应匹配和多视角变换优化，直接生成配准点云并提高鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用轻量级局部特征提取器和测试时刚性约束，学习连续点级速度场进行条件生成；通过速度场将噪声点迁移到配准场景，并从中恢复姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 与传统方法相比，该模型在配准基准上取得最先进的结果，尤其在低重叠情况下表现突出，并能跨尺度和传感器模式泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法实现了高效、鲁棒的点云配准，可直接生成配准点云，并支持多机器人SLAM、重定位和多会话地图合并等应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云配准将多个未姿态点云对齐到公共坐标系，是三维重建和机器人定位的核心步骤。在本研究中，我们将配准视为条件生成：学习得到的连续点级速度场将噪声点迁移到已配准场景，从而恢复每个视角的姿态。与以往通过对应匹配估计点云对之间变换并优化多视角变换实现配准的方法不同，我们的模型直接生成配准点云。通过轻量级局部特征提取器和测试时刚性约束，我们的方法在配准基准上取得了最先进的结果，尤其在低重叠情况下表现突出，并能跨尺度和传感器模式泛化。它还支持下游任务，包括重定位、多机器人SLAM和多会话地图合并。源代码可在 https://github.com/PRBonn/RAP 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了将多张未标定的三维点云对齐到同一坐标系的问题，这是三维重建、机器人定位和SLAM等应用的核心步骤。由于真实世界的扫描往往稀疏、噪声大、视角重叠有限，传统的两阶段配准方法在低重叠或大规模场景中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将配准视为条件生成任务，借鉴了RPF、DiffusionReg等基于流匹配和扩散的生成模型，提出单阶段流匹配网络。为保持刚性，他们在采样过程中强制投影到每个视角的SE(3)轨道，并使用刚性误差进行样本选择。整体流程还引入了关键点采样与规范化，以实现跨尺度、跨传感器的泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个神经速度场，将高斯噪声点云逐步变换为合并后的注册点云，并在每一步通过刚性投影保证每个视角的变换保持刚性。实现流程包括：① 采样稀疏关键点并提取局部特征；② 将所有视角规范化到共享相似性不变坐标系；③ 用流匹配Transformer根据条件生成完整的注册点云；④ 在采样过程中对中间结果进行刚性投影并计算刚性误差；⑤ 选取刚性误差最小的生成结果，使用SVD恢复每个视角的刚性变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 单阶段流匹配模型直接输出全局一致的点云，省去两阶段配准和图优化；2) 引入刚性强制采样和刚性误差选择，提升低重叠和大规模场景的鲁棒性；3) 通过大规模多数据集训练，模型在不同尺度、传感器和视角数下均能泛化；4) 采用关键点规范化实现跨尺度一致性。与以往的两阶段配准、RPF等仅限对象级的工作相比，该方法可处理任意数量视角、低重叠和大范围环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种单阶段、基于流匹配的神经网络，能够直接生成多视角点云的全局一致合并结果，并通过刚性强制和误差选择实现高精度、跨尺度、跨传感器的配准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.&lt;/p&gt;</description></item><item><guid>2512.02972v1</guid><title>BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2512.02972v1</link><author>Guowen Zhang, Chenhang He, Liyi Chen, Lei Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种以 LiDAR 为中心的 BEV 膨胀框架 BEVDilation，利用图像特征作为隐式引导，解决传统融合中几何误差导致的性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在 3D 目标检测中，融合 LiDAR 与相机的 BEV 表示已被证明有效，但由于两种传感器几何精度差异，直接拼接往往导致性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种优先考虑 LiDAR 信息、并通过图像隐式引导来缓解空间失配的融合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BEVDilation 通过将图像 BEV 特征视为隐式引导而非简单拼接，提出稀疏体素膨胀块以利用图像先验密化前景体素，并引入语义引导 BEV 膨胀块以增强 LiDAR 特征扩散并捕获长程上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 nuScenes 基准上，BEVDilation 在保持计算效率的同时，优于现有最先进方法，并且对深度噪声更具鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 以 LiDAR 为中心、图像隐式引导的 BEVDilation 能有效缓解传感器几何差异带来的误差，提升 3D 检测性能并增强对深度噪声的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在鸟瞰图（BEV）表示中整合 LiDAR 与相机信息已被证明在 3D 目标检测中有效。然而，由于这些传感器在几何精度上的根本差异，先前方法中无差别的融合往往导致性能下降。本文提出 BEVDilation，一种新颖的以 LiDAR 为中心的框架，优先考虑 LiDAR 信息进行融合。通过将图像 BEV 特征视为隐式引导而非简单拼接，我们的策略有效缓解了图像深度估计误差导致的空间失配。此外，图像引导还能有效帮助以 LiDAR 为中心的范式解决点云的稀疏性和语义限制。具体而言，我们提出了稀疏体素膨胀块，通过图像先验密化前景体素来缓解固有的点稀疏性。我们还引入了语义引导 BEV 膨胀块，以图像语义引导和长程上下文捕获增强 LiDAR 特征扩散处理。在具有挑战性的 nuScenes 基准上，BEVDilation 在保持竞争性计算效率的同时，取得了比最先进方法更好的性能。重要的是，我们的以 LiDAR 为中心的策略相较于简单融合表现出更强的对深度噪声的鲁棒性。源代码可在 https://github.com/gwenzhang/BEVDilation 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进 LiDAR 与相机的多模态融合，以提升 3D 目标检测的精度和鲁棒性。由于 LiDAR 提供精确的几何信息，而相机则缺乏可靠的深度估计，传统的无差别融合往往导致空间失配和性能下降，这在自动驾驶等安全关键场景中尤为重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出 LiDAR 与相机在几何精度上的差距，决定以 LiDAR 为主导并将图像特征作为指导。设计中借鉴了 BEV 融合、Mamba 的全局感受野、以及多模态可变形卷积等现有技术，并在此基础上提出了稀疏体素膨胀块和语义引导 BEV 膨胀块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是以 LiDAR 为主，利用图像特征来补充稀疏性和语义信息。实现流程包括：①分别用 LiDAR 与多视角图像提取 BEV 特征；②使用稀疏体素膨胀块（SVDB）根据前景掩码在 LiDAR 前景中插入可学习的体素并用 Mamba 进行全局细化；③使用语义引导 BEV 膨胀块（SBDB）通过多模态可变形卷积在 LiDAR BEV 上进行特征扩散；④将融合后的 BEV 送入检测头得到 3D 目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出 LiDAR‑centric 融合框架，避免了深度误差导致的空间失配；②SVDB 通过学习体素嵌入和 Mamba 细化有效稀疏化 LiDAR；③SBDB 采用多模态可变形卷积在 LiDAR 上进行语义引导的特征扩散；④整体方法在保持计算效率的同时，在 nuScenes 上实现了新的 state‑of‑the‑art。与以往无差别融合或仅在图像上进行补充的做法不同，BEVDilation 通过图像指导来增强 LiDAR 的几何主导性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BEVDilation 通过 LiDAR‑centric 融合与图像引导的稀疏体素膨胀与语义扩散，显著提升 3D 检测性能并增强对深度噪声的鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Integrating LiDAR and camera information in the bird&amp;#x27;s eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.&lt;/p&gt;</description></item><item><guid>2512.02991v1</guid><title>GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection</title><link>http://arxiv.org/abs/2512.02991v1</link><author>Md Sohag Mia, Md Nahid Hasan, Tawhid Ahmed, Muhammad Abdullah Adnan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出GraphFusion3D框架，结合多模态融合和高级特征学习，显著提升3D目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云数据稀疏、结构不完整、语义信息有限，且远距离物体间的上下文关系难以捕捉，导致3D检测面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决点云检测中的稀疏性、结构缺失和语义不足问题，并有效建模远距离物体的上下文关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) Adaptive Cross-Modal Transformer (ACMT) 适配性地将图像特征融入点云表示，丰富几何和语义信息；2) Graph Reasoning Module (GRM) 通过多尺度图注意力建模邻域关系，捕捉局部几何和全局语义；3) 级联解码器逐步细化检测结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在SUN RGB-D和ScanNetV2数据集上，GraphFusion3D分别取得70.6% AP25、51.2% AP50和75.1% AP25、60.8% AP50的成绩，明显优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GraphFusion3D通过多模态融合和图推理有效提升3D目标检测精度，证明了其在复杂点云场景中的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管在3D目标检测方面取得了显著进展，但由于点云稀疏、结构不完整以及语义信息有限，仍然面临挑战。捕捉远距离物体之间的上下文关系也带来了额外困难。为了解决这些挑战，我们提出了GraphFusion3D，一个统一的框架，结合多模态融合和先进的特征学习。我们的方法引入了自适应跨模态变换器（ACMT），它自适应地将图像特征整合到点云表示中，以丰富几何和语义信息。为了提议细化，我们引入了图推理模块（GRM），一种新机制，建模邻域关系，以同时捕捉局部几何结构和全局语义上下文。该模块采用多尺度图注意力，动态加权提议之间的空间接近度和特征相似度。我们进一步使用级联解码器，通过多阶段预测逐步细化检测。SUN RGB-D（70.6% AP25和51.2% AP50）和ScanNetV2（75.1% AP25和60.8% AP50）上的广泛实验表明，与现有方法相比，性能有显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决从稀疏点云中准确检测3D物体的问题，主要挑战是点云稀疏、结构不完整以及缺乏语义信息。该问题在机器人、自动驾驶和增强现实等领域至关重要，因为这些应用需要精确的三维感知来安全、智能地与环境交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的投票式、稀疏卷积和多模态融合方法（如ImVoteNet、EPNet++、TokenFusion等）的基础上，提出了图推理模块和自适应跨模态变压器，以捕捉提议之间的上下文关系并融合图像与点云信息。设计过程中借鉴了Transformer、Deform-DETR以及多尺度注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先通过图推理模块对提议进行多尺度图注意力聚合，随后使用自适应跨模态变压器在点云与图像特征之间进行动态加权融合，最后通过级联解码器逐步细化检测结果。实现流程包括特征提取 → 图推理 → 跨模态融合 → 级联细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 多尺度图注意力的图推理模块，用于捕获局部几何与全局语义关系；2) 自适应跨模态变压器与跨模态门控机制，实现动态权重平衡；3) 级联细化解码器，实现多阶段逐步改进。与之前工作相比，它将图推理嵌入Transformer解码器，动态调节模态贡献，并在两阶段推理中实现更丰富的上下文建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GraphFusion3D通过图推理与自适应跨模态融合相结合，并采用级联细化解码器，显著提升了稀疏点云中的3D目标检测精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.&lt;/p&gt;</description></item><item><guid>2512.03010v1</guid><title>SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting</title><link>http://arxiv.org/abs/2512.03010v1</link><author>Svenja Strobel, Matthias Innmann, Bernhard Egger, Marc Stamminger, Linus Franke</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LiDAR 点云在平坦区域精度高，但易遗漏细小结构和暗色材料；摄影测量可补充细节，但 LiDAR 在无特征区域仍优。本文提出 SurfFill，利用高斯表面元完成 LiDAR 缺失，先分析光束发散导致的伪影，采用密度变化启发式定位缺失点，随后在这些模糊区域进行点生长与重建，并用分治策略扩展到建筑规模，实验表明该方法优于以往方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; LiDAR 被视为主动 3D 重建的金标准，但在薄结构、边缘和暗色吸收材料上易失效；摄影测量可捕获细节，但 LiDAR 在无特征区域的精度仍难以匹敌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 结合 LiDAR 与相机捕获的优势，提出一种基于高斯表面元的 LiDAR 补全方案，以提高点云的完整性和精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 分析 LiDAR 光束发散导致的伪影，利用点云密度变化识别缺失区域；在这些模糊点附近进行点生长，限制高斯表面元重建聚焦于缺失区域；提取并采样高斯原语完成点云；为大规模重建引入分治策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示 SurfFill 在合成和真实场景的 LiDAR 点云补全任务中，性能优于之前的重建方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SurfFill 能有效补全 LiDAR 点云，利用密度启发式和高斯表面元重建实现精细补全，并通过分治方案实现大规模建筑级别的扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; LiDAR 捕获的点云通常被视为主动 3D 重建的金标准。虽然它们在平坦区域的精度异常出色，但捕获过程容易遗漏细小几何结构，并且在暗色、吸收性材料上可能失效。另一种方法是对场景拍摄多张照片并应用 3D 摄影测量技术，这可以推断出这些细节，因为它们往往代表特征丰富的区域。然而，LiDAR 在无特征区域的精度很少能达到。为此，我们通过引入 SurfFill：一种基于高斯表面元的 LiDAR 补全方案，建议结合 LiDAR 与基于相机的捕获优势。我们分析了 LiDAR 捕获，并将 LiDAR 光束发散视为产生伪影的主要因素，主要表现在薄结构和边缘。基于此洞察，我们通过评估点云密度变化引入了一个模糊启发式，用于识别接近缺失区域的点，从而可以从这些点开始生长额外的点以完成扫描。为此点生长，我们将高斯表面元重建（Huang 等 2024）限制在这些模糊区域，聚焦优化和密度化。最后，从模糊区域的重建中提取并采样高斯原语，以完成点云。为应对大规模重建的挑战，我们在该流程中加入了分治方案，用于建筑规模的点云补全。我们在合成和真实场景的 LiDAR 点云补全任务上进行评估，发现我们的方法优于以往的重建方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在补全 LiDAR 点云中因光束发散、暗面或反射面导致的缺失小结构和薄边缘。由于 LiDAR 是三维重建的金标准，缺失细节会影响后续地图构建、机器人导航和建筑测量等应用的精度与可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了 LiDAR 的典型误差来源，提出基于点密度的模糊性启发式来定位可能缺失的区域。随后借鉴 2D Gaussian Splatting、Gaussian surfel 以及多视图立体视觉的技术，将该启发式嵌入到受限的 Gaussian 细化过程中，以在缺失区域进行精细重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先识别点云中密度低的模糊区，然后在这些区内用受限的 Gaussian surfel 进行局部重建，最后从重建的 Gaussian 中采样点补齐原始点云。实现流程包括：① 下采样 LiDAR 并计算模糊性分数；② 用 2D Gaussian Splatting 在模糊区内优化 Gaussian；③ 过滤并采样得到新点；④ 将新点与原点云合并。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 用点密度启发式识别 LiDAR 缺失区域；② 将该启发式与 Gaussian surfel 结合，形成针对缺失区的受限重建；③ 设计了专门的约束、损失和采样策略；④ 采用分块（divide‑and‑conquer）方案处理大规模建筑级点云。与以往的形状补全或光度重建方法不同，SurfFill 直接利用 LiDAR 的高精度信息并在缺失区进行局部补全，显著提升了完整性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SurfFill 通过识别 LiDAR 点云中的模糊区并在这些区内使用受限的 Gaussian surfel 重建，提供了一种高效、精确的大规模点云补全方法，优于现有技术。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.&lt;/p&gt;</description></item><item><guid>2512.03598v1</guid><title>Memory-Guided Point Cloud Completion for Dental Reconstruction</title><link>http://arxiv.org/abs/2512.03598v1</link><author>Jianan Sun, Yukang Huang, Dongzhihan Wang, Mingyu Fan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种检索增强框架，用于完成部分牙齿点云，框架将原型记忆集成到标准的编码-解码管线中，提供结构先验，提升完成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 部分牙齿点云常因遮挡和扫描视角受限而缺失大片区域，导致仅使用编码器的全局特征偏差，迫使解码器产生假象结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用可学习的原型记忆，借助跨样本规律稳定缺失区域推断，释放解码器容量用于细节恢复，从而实现更准确、真实的牙齿点云完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先将部分输入编码为全局描述符，再从可学习记忆中检索最近的原型，并通过置信门控加权与查询特征融合，随后解码。记忆端到端优化，自组织为可复用的牙齿形状原型，无需牙齿位置标签；该模块可插拔，兼容常见完成骨干网络，保持相同训练损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在自制的 Teeth3DS 基准上实验显示，Chamfer Distance 一致下降，视觉效果显示尖锐的咬合尖、脊纹和邻牙过渡更清晰。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 检索增强框架通过利用跨样本规律，提供了简单有效的方式，实现更准确、可信的牙齿点云完成，同时保持解码器对细节的恢复能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 部分牙齿点云常因遮挡和扫描视角受限而缺失大片区域，导致仅使用编码器的全局特征偏差，迫使解码器产生假象结构。我们提出一种检索增强框架，用于牙齿完成，将原型记忆集成到标准的编码-解码管线中。将部分输入编码为全局描述符后，模型从可学习记忆中检索最近的流形原型，并通过置信门控加权与查询特征融合后再解码。记忆端到端优化，自组织为可复用的牙齿形状原型，无需牙齿位置标签，从而提供结构先验，稳定缺失区域推断，释放解码器容量用于细节恢复。该模块可插拔，兼容常见完成骨干网络，保持相同训练损失。在自制的 Teeth3DS 基准上实验显示，Chamfer Distance 一致下降，视觉效果显示尖锐的咬合尖、脊纹和邻牙过渡更清晰。我们的方案提供了一种简单而有效的方式，利用跨样本规律实现更准确、真实的牙齿点云完成。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决牙齿点云在扫描时因遮挡、视角有限而产生的大面积缺失区域，导致传统编码器-解码器网络在完成时产生偏差和细节模糊。准确恢复牙齿形状对于临床诊断、修复和数字化牙科设计至关重要，缺失细节会影响治疗效果和模型精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到编码器产生的全局特征因缺失而偏差，决定在特征形成时主动引入结构先验。借鉴了检索增强和记忆库的思想，设计了双编码器、可学习的原型记忆和置信度门控融合机制，并参考了 FoldingNet、PCN 等现有点云完成网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过检索记忆库中的最近原型来纠正偏差的全局特征，然后与原始特征按置信度加权融合，再由解码器生成完整点云。实现流程包括：①双编码器提取部分点云和完整点云的全局描述符；②在记忆库中检索与部分描述符最近的原型；③使用置信度门控将原型与部分描述符融合得到去偏特征；④解码器根据融合特征生成完成的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①可插拔的原型记忆库，能够自组织捕捉牙齿形状规律；②置信度门控融合机制，动态调节原型对特征的影响；③无需牙齿位置标签即可获得结构先验；④在保持相同训练损失的前提下显著提升 Chamfer Distance。与以往依赖更深网络或通用先验的工作不同，MEM4TEETH直接利用跨样本形状相似性进行检索增强。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MEM4TEETH通过检索并融合可学习的牙齿形状原型，去偏编码器特征，从而在无需额外标签的情况下显著提升牙齿点云完成的准确性和细节恢复。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.&lt;/p&gt;</description></item><item><guid>2512.03756v1</guid><title>Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models</title><link>http://arxiv.org/abs/2512.03756v1</link><author>Marlon Steiner, Royden Wagner, Ömer Sahin Tas, Christoph Stiller</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨将导航信息融入注意力机制的运动预测模型，以提升自动驾驶车辆与其他交通参与者的交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 将运动预测与运动规划相结合被视为提升自动驾驶交互的有前景框架，但在将预测与导航目标对齐以及保证轨迹稳定性方面存在挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究如何在注意力机制的运动预测模型中加入导航信息，以解决预测与导航目标的耦合问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将自车预期路线和目标姿态嵌入模型架构，提出并在nuPlan数据集上评估多种导航集成策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，基于预测的运动规划具有潜力，导航信息能同时提升预测和规划任务的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将导航信息融入运动预测模型能够弥合多智能体预测与基于目标规划之间的差距，并提升整体性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 将运动预测与运动规划相结合，为提升自动驾驶车辆与其他交通参与者的交互提供了有前景的框架。然而，这也带来了在将预测与导航目标对齐以及确保轨迹稳定、运动学可行方面的挑战。为解决前者问题，本文研究了将导航信息扩展到基于注意力机制的运动预测模型。通过将自车的预期路线和目标姿态整合到模型架构中，我们弥合了多智能体运动预测与基于目标的运动规划之间的差距。我们在nuPlan数据集上提出并评估了多种架构化导航集成策略。实验结果表明，基于预测的运动规划具有潜力，导航信息能够提升预测和规划任务的效果。我们的实现可在 https://github.com/KIT-MRT/future-motion 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle&amp;#x27;s intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.&lt;/p&gt;</description></item><item><guid>2512.03936v1</guid><title>Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response</title><link>http://arxiv.org/abs/2512.03936v1</link><author>Aron Distelzweig, Yiwei Wang, Faris Janjoš, Marcel Hallgarten, Mihai Dobre, Alexander Langmann, Joschka Boedecker, Johannes Betz</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; - 传统轻量级规则方法在常规场景表现几乎完美，但在拥挤的城市交通中仍然困难，尤其是需要预测和影响其他车辆的变道与合流。- 现代运动预测器能提供高度准确的预测，但在规划中的集成往往仅仅是丢弃不安全的方案。- 端到端模型提供单向集成，避免了联合预测与规划的不确定性挑战。- 游戏理论方法提供了原则性的替代方案，但在自动驾驶中的应用有限。- 本文提出 BIBeR 框架，将运动预测与游戏理论规划统一为一个交互感知过程。- BIBeR 首次将先进预测器嵌入迭代最优响应（IBR）循环，反复细化自车与周围车辆的策略，逼近纳什均衡，实现双向适应。- 通过贝叶斯置信度估计，量化预测可靠性并调节更新力度，在低置信度时更保守，高置信度时更果断。- BIBeR 与现代预测器和规划器兼容，兼具结构化规划的透明度和学习模型的灵活性。- 实验表明，BIBeR 在高度交互的 interPlan 变道场景中比最先进规划器提升 11%，并在标准 nuPlan 基准上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统规则方法在常规场景表现优异，但在拥挤城市交通中难以处理需要预测和影响其他车辆的变道与合流；现代运动预测器准确但集成方式粗糙；端到端模型缺乏联合预测与规划的双向适应；游戏理论方法虽原则性强，但应用有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 BIBeR 框架，将运动预测与游戏理论规划统一为交互感知过程，以提升自动驾驶规划在复杂交互场景中的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将先进预测器嵌入迭代最优响应循环，反复细化自车与周围车辆策略，逼近纳什均衡；使用贝叶斯置信度估计调节更新力度；兼容现代预测器和规划器，提供透明且灵活的规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; BIBeR 在高度交互的 interPlan 变道场景中比最先进规划器提升 11%，并在标准 nuPlan 基准上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BIBeR 通过将预测与游戏理论规划结合，实现了双向适应和更可靠的决策，显著提升了自动驾驶规划在复杂交互场景中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自动驾驶规划系统在常规场景中使用轻量级规则方法几乎完美，但在拥挤的城市交通中仍然困难，变道和合流需要预测和影响其他车辆。现代运动预测器提供高度准确的预测，但其与规划的集成大多是粗糙的：丢弃不安全的方案。类似地，端到端模型提供单向集成，避免了在不确定性下联合预测和规划的挑战。相比之下，游戏理论公式提供了原则性的替代方案，但在自动驾驶中的采用有限。我们提出了贝叶斯迭代最优响应（BIBeR）框架，将运动预测与游戏理论规划统一为单一的交互感知过程。BIBeR 首次将先进预测器集成到迭代最优响应（IBR）循环中，反复细化自车和周围车辆的策略。这个重复的最优响应过程逼近纳什均衡，使自车既能对他人做出反应，也能塑造他人的行为。此外，我们提出的贝叶斯置信度估计量化预测可靠性，并调节更新力度，在低置信度时更保守，高置信度时更果断。BIBeR 与现代预测器和规划器兼容，结合了结构化规划的透明度和学习模型的灵活性。实验表明，BIBeR 在高度交互的 interPlan 变道场景中比最先进规划器提升 11%，并在标准 nuPlan 基准上优于现有方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.&lt;/p&gt;</description></item><item><guid>2512.04068v1</guid><title>Learning Steerable Clarification Policies with Collaborative Self-play</title><link>http://arxiv.org/abs/2512.04068v1</link><author>Jonathan Berant, Maximillian Chen, Adam Fisch, Reza Aghajani, Fantine Huot, Mirella Lapata, Jacob Eisenstein</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究如何让AI助手在面对不完整或含糊的查询时，依据上下文决定是直接猜测意图、列举多种可能意图还是提出澄清问题。通过自我对弈训练可调节的策略，模型能根据每个澄清问题的成本来最大化最终奖励，从而实现更高的准确率和更灵活的行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在实际应用中，AI助手需要处理用户查询的不确定性。不同场景（如小屏幕、语音交互）对列举多种意图的可行性有影响，因而需要根据用户偏好和介质来调整策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种可调节的策略训练方法，使AI助手能够在不同成本约束下，合理选择猜测、枚举或澄清的回应方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用自我对弈（self‑play）生成用户与助手的对话，模型接收每个澄清问题的数值成本和生成的词语，采用强化自我训练（ReST）最大化成本惩罚后的准确率奖励。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 训练得到的策略能够根据提供的成本值可预测地改变行为，获得更高的奖励和准确率，并且对训练时未见过的成本值也能泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过ReST训练的可调节策略在处理不确定查询时表现优异，可根据成本约束灵活调整回答方式，提升整体性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 为处理未指定或含糊的查询，AI助手需要一种管理其不确定性的策略，以决定（a）何时猜测用户意图并直接回答，（b）何时枚举并回答多种可能的意图，以及（c）何时提出澄清问题。然而，这些策略在上下文上依赖于用户偏好或介质等因素。例如，在小屏幕或语音设置中枚举多种可能的用户意图会很繁琐。在本研究中，我们提出使用自我对弈训练可调节的策略来管理这种不确定性。给定两个代理，一个模拟用户，另一个模拟AI助手，我们生成对话，其中用户提出可能含糊的查询，助手需要决定如何回应。重要的是，模型以每个澄清问题的数值成本和每个生成的词为输入，并被要求采取能最大化其最终奖励的行动，该奖励是成本惩罚后的准确率。我们使用强化自我训练（ReST）来训练我们的模型以获得高奖励，并展示这导致了一个可调节的策略，在给定成本的条件下可预测地改变其行为，从而获得更高的奖励和准确率。此外，我们的程序还可以推广到训练时未观察到的数值成本值。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.&lt;/p&gt;</description></item><item><guid>2512.04996v1</guid><title>A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs</title><link>http://arxiv.org/abs/2512.04996v1</link><author>Qiong Chang, Weimin Wang, Junpei Zhong, Jun Miyazaki</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种内存高效的优化策略，使得高性能点云配准算法VANICP能够在资源受限的嵌入式GPU上轻量化运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; VANICP是一种加速框架，通过将全局最近邻搜索转化为局部过程，显著提升点云应用的计算效率，但其原始实现占用大量内存，限制了在嵌入式系统中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决VANICP在嵌入式环境下的内存占用过高问题，实现轻量化部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出面向GPU的动态内存分配策略，优化膨胀操作的内存使用，并基于该策略构建改进版VANICP框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 改进版VANICP在保持原有性能的同时，内存消耗降低了97%以上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过动态内存分配策略，VANICP可以在嵌入式GPU上实现高效、轻量化的点云配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种内存高效的优化策略，用于高性能点云配准算法VANICP，使其能够在受限硬件资源的嵌入式GPU上轻量化执行。VANICP是一种最近发布的加速框架，显著提升了基于点云的应用的计算效率。通过将全局最近邻搜索转化为通过膨胀式信息传播机制的局部过程，VANICP大幅降低了最近邻搜索的计算复杂度。然而，其原始实现需要大量内存，限制了其在嵌入式系统等资源受限环境中的部署。为解决此问题，我们提出了一种面向GPU的动态内存分配策略，优化了膨胀操作的内存使用。此外，基于该策略，我们构建了VANICP框架的增强版本，在保持原始性能的同时，内存消耗降低了97%以上。源代码已发布在：https://github.com/changqiong/VANICP4Em.git。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了 VANICP 在嵌入式 GPU 上因静态预分配导致的高内存占用问题。高内存占用限制了该算法在资源受限的移动设备和边缘计算场景中的部署，而这些场景正是 3D 感知技术快速发展的关键领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了 VANICP 的内存分配方式，发现其固定 64KB/体素的策略导致大量浪费。随后借鉴了 GPU 的统一内存、哈希表计数和前缀和技术，并参考了 VANICP、HNSW 等加速 ICP 的工作，提出在 GPU 上并行构建体素占用直方图，在 CPU 上计算地址偏移，从而实现动态分配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是根据每个体素实际点数动态分配内存，并通过地址偏移实现间接访问。实现流程包括：① 体素化并在 GPU 上并行统计每个体素的点数；② 在 CPU 上对直方图做前缀和得到每个体素的起始地址；③ 使用这些偏移在统一内存中按需写入点索引；④ 进行体素膨胀和局部最近邻搜索，所有操作均通过间接寻址完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① GPU‑面向的动态内存分配策略；② 分段指针式内存布局与间接寻址；③ 利用统一内存实现 GPU 与 CPU 的无缝协作；④ 在保持原有性能的前提下将内存占用降低 97%。与 VANICP 的静态单块分配相比，这些改进显著提升了嵌入式系统的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种基于体素占用直方图的动态内存分配策略，使得基于膨胀的 ICP 在嵌入式 GPU 上的内存使用降低 97% 而不影响计算速度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.&lt;/p&gt;</description></item><item><guid>2512.05008v1</guid><title>Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain</title><link>http://arxiv.org/abs/2512.05008v1</link><author>Haroon Hublikar</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本论文提出了一个统一的建模与仿真框架，用于分析 COBRA 蛇形机器人在刚性、柔性和颗粒地形上的侧向滑行和翻滚运动，并通过多种方法实现对摩擦、地形变形和颗粒相互作用的准确预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 研究侧向滑行和翻滚运动在不同地形（刚性、柔性、颗粒）下的表现，面临摩擦、地形变形和颗粒动力学等复杂相互作用的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一个统一的建模与仿真框架，以分析并预测 COBRA 蛇形机器人在多种地形上的运动性能，为控制设计提供依据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 使用接触隐式形式建模侧向滑行过程中的分布摩擦相互作用，并通过 MATLAB Simscape 仿真和物理实验验证。2. 将 Project Chrono 的土壤接触模型与关节多体动力学耦合，预测滑移、下沉和负载重新分配。3. 使用 Chrono DEM 引擎模拟颗粒相互作用，揭示土壤失效、间歇性抬离和能量耗散机制。4. 构建分层仿真管道，涵盖实时控制导向仿真与高保真颗粒物理学。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 刚性地面模型在短期内能提供准确的运动预测；连续体和粒子基础的地形建模在软质和高度动态环境中才是可靠的移动性分析所必需的；地形变形会降低步幅效率；颗粒相互作用揭示了刚性模型无法捕获的土壤失效和能量耗散机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 分层仿真管道能够实现对不同地形下 COBRA 蛇形机器人运动的准确预测，为在挑战性非结构化环境中实现稳健、面向地形的运动提供了重要工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本论文提出了一个统一的建模与仿真框架，用于分析 COBRA 蛇形机器人在刚性、柔性和颗粒地形上的侧向滑行和翻滚运动。采用接触隐式形式来建模侧向滑行过程中的分布摩擦相互作用，并通过 MATLAB Simscape 仿真和在刚性地面及松散沙土上的物理实验进行验证。为捕捉地形变形效应，将 Project Chrono 的土壤接触模型（SCM）与关节多体动力学耦合，实现对滑移、下沉和负载重新分配的预测，从而降低柔性基底上的步幅效率。针对陡坡上的高能滚动运动，使用 Chrono DEM 引擎模拟粒子分辨的颗粒相互作用，揭示了土壤失效、间歇性抬离和能量耗散机制，这些在刚性模型中无法捕获。上述方法涵盖了实时控制导向的仿真与高保真颗粒物理学。实验结果表明，刚性地面模型可在短期内提供准确的运动预测，而连续体和粒子基础的地形建模在软质和高度动态环境中才是可靠的移动性分析所必需的。本工作建立了一个分层仿真管道，推动了在挑战性非结构化环境中运行的机器人实现稳健、面向地形的运动。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono&amp;#x27;s Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.&lt;/p&gt;</description></item><item><guid>2512.05270v1</guid><title>XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots</title><link>http://arxiv.org/abs/2512.05270v1</link><author>Tianyi Wang, Jiseop Byeon, Ahmad Yehia, Huihai Wang, Yiming Xu, Tianyi Zeng, Ziran Wang, Junfeng Jiao, Christian Claudel</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了XR-DT框架，利用扩展现实技术构建数字孪生，帮助移动机器人与人类在共享工作空间中实现安全、高效、可解释的交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着移动机器人在共享工作空间中的应用增多，如何保证人机交互的安全、效率和可解释性成为挑战。虽然已有研究关注人类行为预测，但对人类如何感知、解释和信任机器人推理的关注不足，限制了其在安全关键和社会嵌入环境中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个桥接物理与虚拟空间的XR-DT框架，实现人机双向理解，提升人机交互的可解释性、可信度和适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用分层XR-DT架构，集成虚拟、增强和混合现实层，融合实时传感器数据、Unity游戏引擎中的仿真环境以及可穿戴AR设备收集的人类反馈；在此框架下设计统一扩散策略的移动机器人系统，实现上下文感知任务适配；引入链式思维提示机制，让多模态大型语言模型根据人类指令和环境上下文进行推理；使用AutoGen多智能体协同层提升动态任务的鲁棒性与协作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 初步实验表明，该框架能够准确预测人类和机器人的轨迹，验证了XR-DT在HRI任务中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过将人类意图、环境动态和机器人认知嵌入XR-DT框架，系统实现了可解释、可信且自适应的人机交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着移动机器人在共享工作空间中与人类共同工作，确保安全、高效且可解释的人机交互（HRI）已成为一项紧迫挑战。虽然在预测人类行为方面已取得显著进展，但对人类如何感知、解释和信任机器人推理的关注有限，阻碍了其在安全关键和社会嵌入环境中的部署。本文提出了XR-DT，即一种利用扩展现实增强的数字孪生框架，旨在桥接物理与虚拟空间，实现人机双向理解。我们的分层XR-DT架构集成了虚拟、增强和混合现实层，融合实时传感器数据、Unity游戏引擎中的仿真环境以及通过可穿戴AR设备捕获的人类反馈。在此框架下，我们设计了一个统一扩散策略的移动机器人系统，实现上下文感知的任务适配。我们进一步提出了链式思维提示机制，使多模态大型语言模型能够在考虑人类指令和环境上下文的基础上进行推理，并利用基于AutoGen的多智能体协同层提升动态任务的鲁棒性和协作能力。初步实验结果表明，该框架能够准确预测人类和机器人的轨迹，验证了XR-DT在HRI任务中的有效性。通过将人类意图、环境动态和机器人认知嵌入XR-DT框架，我们的系统实现了可解释、可信且自适应的人机交互。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决移动机器人与人类共享工作空间时的安全、高效、可解释的交互问题。该问题重要，因为缺乏对机器人推理的可理解性会限制其在安全关键或社会嵌入环境中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将数字孪生、扩展现实和代理式人工智能结合，构建了 VR、AR、MR 三层架构，并引入链式思考提示、扩散策略和 AutoGen 多智能体框架。设计借鉴了现有的数字孪生、MR HRI、LLM 推理、扩散模型和多智能体系统等工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过实时传感器融合和多模态推理，在虚拟、增强和混合现实层面实现人机双向理解。实现流程为：1）AR 设备采集人类多模态数据，机器人采集环境数据；2）Real‑World Agentic AI 预测轨迹并在 AR 层叠加提示；3）VR 层使用统一扩散策略进行仿真优化；4）MR 层将两者结果融合，生成可执行命令并反馈给机器人和人类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① XR‑DT 三层架构实现物理与虚拟双向同步；② 统一扩散策略兼顾任务通用与特定行为；③ 链式思考提示提升多模态 VLM 推理；④ AutoGen 多智能体协同提升鲁棒性。与以往仅在 VR 或单一任务场景下的 HRI 研究不同，本文提供了实时、可解释、跨模态的全流程解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XR‑DT 通过整合扩展现实、数字孪生与代理式 AI，提供实时、可解释且可信的移动机器人人机交互框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As mobile robots increasingly operate alongside humans in shared workspaces, ensuring safe, efficient, and interpretable Human-Robot Interaction (HRI) has become a pressing challenge. While substantial progress has been devoted to human behavior prediction, limited attention has been paid to how humans perceive, interpret, and trust robots&amp;#x27; inferences, impeding deployment in safety-critical and socially embedded environments. This paper presents XR-DT, an eXtended Reality-enhanced Digital Twin framework for agentic mobile robots, that bridges physical and virtual spaces to enable bi-directional understanding between humans and robots. Our hierarchical XR-DT architecture integrates virtual-, augmented-, and mixed-reality layers, fusing real-time sensor data, simulated environments in the Unity game engine, and human feedback captured through wearable AR devices. Within this framework, we design an agentic mobile robot system with a unified diffusion policy for context-aware task adaptation. We further propose a chain-of-thought prompting mechanism that allows multimodal large language models to reason over human instructions and environmental context, while leveraging an AutoGen-based multi-agent coordination layer to enhance robustness and collaboration in dynamic tasks. Initial experimental results demonstrate accurate human and robot trajectory prediction, validating the XR-DT framework&amp;#x27;s effectiveness in HRI tasks. By embedding human intention, environmental dynamics, and robot cognition into the XR-DT framework, our system enables interpretable, trustworthy, and adaptive HRI.&lt;/p&gt;</description></item><item><guid>2512.05482v1</guid><title>Concept-based Explainable Data Mining with VLM for 3D Detection</title><link>http://arxiv.org/abs/2512.05482v1</link><author>Mai Tsujimoto</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种跨模态框架，利用二维视觉语言模型识别并挖掘驾驶场景中的稀有物体，从而提升基于点云的三维目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶系统中，仅使用点云数据进行稀有物体检测仍是难题；视觉语言模型在图像理解方面表现优异，但其在三维检测中的潜力尚未充分挖掘。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索利用二维视觉语言模型进行稀有物体挖掘，以减少标注工作量并提升三维检测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将目标检测、语义特征提取、降维、Isolation Forest 与 t‑SNE 结合的多维异常检测，并通过概念过滤识别语义上有意义的稀有物体，重点提取如施工车辆、摩托车、障碍物等概念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 nuScenes 数据集上，概念引导的数据挖掘策略在仅使用少量训练数据的情况下，显著提升了三维检测模型的性能，尤其在拖车和自行车等难检测类别上优于随机采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法通过高效挖掘稀有物体概念，既降低了标注成本，又提升了安全关键自动驾驶系统的数据集构建效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 稀有物体检测在自动驾驶系统中仍是一个具有挑战性的任务，尤其当仅依赖点云数据时。尽管视觉语言模型在图像理解方面表现出强大的能力，但它们通过智能数据挖掘提升三维物体检测的潜力尚未得到充分探索。本文提出了一种新颖的跨模态框架，利用二维视觉语言模型识别并挖掘驾驶场景中的稀有物体，从而提升三维物体检测性能。我们的方法将目标检测、语义特征提取、降维以及多维异常检测等互补技术融合成一个可解释的流程，系统地识别驾驶场景中的稀有但关键物体。通过结合 Isolation Forest 与基于 t‑SNE 的异常检测方法以及基于概念的过滤，该框架能够有效识别语义上有意义的稀有物体。该方法的一个关键优势在于能够提取并标注针对性的稀有物体概念，如施工车辆、摩托车和障碍物，从而大幅降低标注负担，并仅关注最有价值的训练样本。对 nuScenes 数据集的实验表明，这种概念引导的数据挖掘策略在仅使用少量训练数据的情况下提升了三维物体检测模型的性能，尤其在拖车和自行车等具有挑战性的物体类别上相较于相同量的随机数据有显著改进。这一发现对安全关键自动驾驶系统中数据集的高效策划具有重要意义。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决自动驾驶中稀有物体检测的难题。稀有物体在训练数据中出现频率低，导致检测模型对这些安全关键场景的鲁棒性不足，影响系统的安全性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了视觉‑语言模型（如 CLIP、Qwen2‑VL）与传统的异常检测技术（t‑SNE、Isolation Forest），并参考了现有的概念瓶颈模型、数据挖掘和多模态融合方法，构建了一个跨模态的可解释数据挖掘框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 2D 图像的语义嵌入来识别稀有物体，并通过概念过滤挑选有价值的训练样本。实现流程包括：① 用 YOLOv8 检测并裁剪物体；② 用 CLIP 提取嵌入；③ 用 t‑SNE 与 Isolation Forest 检测异常；④ 用 Qwen2‑VL 生成描述并匹配概念；⑤ 过滤出稀有概念；⑥ 选取包含目标或稀有概念的场景进行 3D 注释并训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 以概念为基础的可解释数据挖掘框架；② 将 VLM 与异常检测相结合以发现语义稀有物体；③ 通过概念过滤显著降低标注成本；④ 仅使用 20% 数据即可提升稀有类别性能；⑤ 采用“Random 10% + Target 10%”采样策略。与以往仅关注几何特征或单一模态的稀有物体挖掘方法不同，本文强调语义解释与跨模态协同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种基于视觉‑语言模型的概念驱动、可解释数据挖掘管道，能够高效识别并选取稀有安全关键物体，显著提升 3D 检测性能并降低标注成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.&lt;/p&gt;</description></item><item><guid>2512.05663v1</guid><title>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</title><link>http://arxiv.org/abs/2512.05663v1</link><author>Johannes Meier, Jonathan Michel, Oussema Dhaouadi, Yung-Hsu Yang, Christoph Reich, Zuria Bauer, Stefan Roth, Marc Pollefeys, Jacques Kaiser, Daniel Cremers</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为LeAD-M3D的单目3D检测方法，结合三项关键技术，实现了在不使用激光雷达或几何先验的情况下，兼具最高精度和实时推理速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单目3D检测面临深度歧义、视角变化和高计算成本等挑战，现有方法往往依赖激光雷达或牺牲效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在保持高精度的同时，实现单目3D检测的实时推理，且不依赖额外传感器或几何假设。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) A2D2：通过质量和重要性加权的深度特征损失，将干净图像教师的几何知识迁移到混合噪声学生；2) CM3D：将3D重叠度纳入匹配分数，提升预测与真值的对齐；3) CGI3D：仅对高置信度区域执行昂贵的3D回归，提升速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LeAD-M3D在KITTI、Waymo和Rope3D数据集上均取得了最先进的精度，并且比之前高精度方法快3.6倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 高精度与实时效率可以在单目3D检测中同时实现，无需激光雷达、立体视觉或几何假设。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种名为LeAD-M3D的单目3D检测方法，结合三项关键技术，实现了在不使用激光雷达或几何先验的情况下，兼具最高精度和实时推理速度。单目3D检测面临深度歧义、视角变化和高计算成本等挑战，现有方法往往依赖激光雷达或牺牲效率。本文的目标是在保持高精度的同时，实现单目3D检测的实时推理，且不依赖额外传感器或几何假设。方法包括：A2D2通过质量和重要性加权的深度特征损失，将干净图像教师的几何知识迁移到混合噪声学生；CM3D将3D重叠度纳入匹配分数，提升预测与真值的对齐；CGI3D仅对高置信度区域执行昂贵的3D回归，提升速度。主要发现是LeAD-M3D在KITTI、Waymo和Rope3D数据集上均取得了最先进的精度，并且比之前高精度方法快3.6倍。结论是高精度与实时效率可以在单目3D检测中同时实现，无需激光雷达、立体视觉或几何假设。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现实时、无 LiDAR、无额外传感器的单目 3D 目标检测，解决单张 RGB 图像中深度不确定性导致的定位误差，并兼顾高精度与低延迟，满足自动驾驶、机器人和城市监控等场景对即时 3D 感知的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在 YOLOv10 的高效 2D 检测框架基础上，结合知识蒸馏、混合增强、3D IoU 匹配和置信度门控等技术，借鉴了 Mixup、A2D、FD3D、3D IoU 匹配和轻量化推理策略，形成了三大核心模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合增强的学生图像与清晰教师图像进行深度特征蒸馏，使用 3D 兼容匹配确定学生与教师的对应关系，并在推理时仅对高置信度区域执行 3D 回归。实现流程包括：① 训练大模型作为教师；② 用混合图像训练学生，使用质量与重要性加权的特征损失进行蒸馏；③ 在训练和推理中使用 3D‑aware Consistent Matching 进行匹配；④ 推理时采用 Confidence‑Gated 3D Inference 限制 3D 头的计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① A2D2：无 LiDAR 的异构蒸馏，利用 Mixup 产生信息不对称并用质量/重要性加权的特征损失提升深度推理；② CM3D：将 3D IoU 纳入匹配，改进预测与真值的对齐；③ CGI3D：置信度门控的 3D 回归，显著降低推理 FLOPs。与以往依赖 LiDAR 或硬编码几何先验的工作不同，LeAD‑M3D 在保持高精度的同时实现了实时推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LeAD‑M3D 通过无 LiDAR 的异构蒸馏、3D 兼容匹配和置信度门控推理，构建了一个在精度与实时性上均领先的单目 3D 检测框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.&lt;/p&gt;</description></item><item><guid>2512.05698v1</guid><title>OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning</title><link>http://arxiv.org/abs/2512.05698v1</link><author>Xusheng Guo, Wanfa Zhang, Shijia Zhao, Qiming Xia, Xiaolong Xie, Mingming Wang, Hai Wu, Chenglu Wen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为OWL的无监督3D目标检测方法，利用占据感知预热和大模型先验推理来提高伪标签质量，并通过自适应加权自训练提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶中，无监督3D目标检测通过启发式算法发现潜在目标，可降低标注成本，但现有方法生成的伪标签初期往往不准确，影响网络收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决伪标签误差对训练的负面影响，并有效过滤和细化伪标签，以提升无监督3D检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; OWL包含占据感知预热策略初始化骨干网络，实例引导推理模块利用大模型先验评估伪标签质量，权重自适应自训练策略动态重新加权伪标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Waymo Open Dataset和KITTI数据集上，OWL比现有无监督方法提升了15%以上的mAP，验证了方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 占据感知预热与大模型先验推理相结合的OWL方法显著提升了无监督3D目标检测的精度，展示了其在自动驾驶场景中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 无监督3D目标检测利用启发式算法发现潜在目标，为降低自动驾驶中的标注成本提供了一条有前景的途径。现有方法主要生成伪标签并通过自训练迭代进行细化。然而，这些伪标签在训练初期往往不准确，导致优化过程被误导。有效过滤和细化伪标签仍是一个关键挑战。本文提出OWL，采用占据感知预热和大模型先验推理进行无监督3D目标检测。OWL首先使用占据感知预热策略，以空间感知能力初始化骨干网络，减轻错误伪标签对网络收敛的干扰。随后，OWL引入实例引导推理模块，利用大模型的先验知识评估伪标签质量，实现精准过滤和细化。最后，我们设计了权重自适应自训练策略，动态重新加权伪标签，通过自训练提升性能。对Waymo Open Dataset和KITTI的广泛实验表明，OWL在无监督方法中以超过15.0%的mAP优势领先，证明了我们方法的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决无监督 3D 目标检测中的伪标签噪声和网络初始化不稳定问题，降低自动驾驶场景下昂贵的人工标注成本。无监督检测的成功可显著提升大规模点云数据的利用效率，直接关系到自动驾驶安全与可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析现有无监督方法在伪标签初始错误和过滤不当导致的性能瓶颈，借鉴 Occupancy‑MAE 的自监督占据预测、LLM 先验推理以及自训练技术，提出三步策略：占据引导预热（OGW）、实例引导推理（ICR）和权重自适应自训练（WAS）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用占据预测自监督预热网络，使其具备空间感知能力；随后利用大模型先验对伪标签进行推理、筛选和修正；最后通过动态加权的自训练进一步提升检测性能。实现流程为：① 通过动态聚类生成初始伪标签；② OGW 预热网络；③ ICR 依据实例属性和 LLM 先验对伪标签进行筛选/修正；④ WAS 以自适应权重迭代训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① OGW：占据预测自监督预热，解决初始化不稳；② ICR：利用大模型先验进行伪标签推理，超越传统阈值过滤；③ WAS：动态加权自训练，减少误标签累积。与以往仅靠阈值过滤或无预热的无监督方法不同，OWL 在每一步都加入了更鲁棒的先验与自监督机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OWL 通过占据引导预热、大模型推理和自适应自训练，显著提升无监督 3D 目标检测性能，突破了传统方法在伪标签噪声和网络初始化方面的局限。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.&lt;/p&gt;</description></item><item><guid>2512.05710v1</guid><title>Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning</title><link>http://arxiv.org/abs/2512.05710v1</link><author>Jianan Sun, Dongzhihan Wang, Mingyu Fan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种面向流形的点云补全框架，利用测地距离近似和基于流形的特征提取，显著提升了补全结果的几何一致性和语义连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云补全旨在从不完整或稀疏的3D观测中恢复几何一致的形状，现有方法多依赖欧氏距离，忽视了点云的非线性几何结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过显式引入非线性几何信息，改进点云补全的几何一致性和语义清晰度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入测地距离近似器（GDA）估计点间测地距离，并使用基于测地距离的k近邻分组和测地关系注意力机制的流形感知特征提取器（MAFE）进行层次特征提取。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在基准数据集上持续优于现有最先进方法，提升了重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 面向流形的点云补全框架通过测地距离和关系注意力显著提高了几何一致性和语义连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云补全旨在从部分或稀疏的3D观测中恢复几何一致的形状。尽管最近的方法已实现了合理的全局形状重建，但它们往往依赖欧氏邻近性，忽视了点云的内在非线性几何结构，导致几何一致性不足和语义模糊。本文提出一种面向流形的点云补全框架，在特征学习管道中显式地整合非线性几何信息。我们的方案引入了两个关键模块：测地距离近似器（GDA），用于估计点间的测地距离以捕捉潜在的流形拓扑；以及流形感知特征提取器（MAFE），利用基于测地距离的k近邻分组和测地关系注意力机制来指导层次特征提取过程。通过整合测地感知的关系注意力，我们的方法促进了重建点云的语义连贯性和结构保真度。在基准数据集上进行的广泛实验表明，我们的方法在重建质量上始终优于最先进的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云完成问题，即从不完整或稀疏的三维观测中恢复完整、几何一致的形状。该问题在现实中很重要，因为真实世界的扫描往往受遮挡、传感器限制或噪声影响，导致点云缺失；在研究中，点云完成是实现自动驾驶、机器人导航、增强现实和文化遗产数字化等应用的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有方法大多依赖欧氏距离的 k‑NN 组网，忽视了点云的非线性流形结构，导致局部语义模糊。为此，他们借鉴了 DGCNN、AdaPoinTr、PointCFormer 和 PointAttN 等工作中的图卷积和注意力机制，但改为使用基于流形的地理距离。通过引入锚点采样、Dijkstra 最短路计算和地理距离近似，作者设计了 Geodesic Distance Approximator（GDA）和 Manifold‑Aware Feature Extractor（MAFE）两大模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将流形几何信息嵌入特征学习全过程。实现流程包括：① GDA 通过锚点构建稀疏图并使用 Dijkstra 计算锚点间的地理距离，得到近似的流形距离；② MAFE 采用地理 k‑NN 组网、Geodesic‑Relational Attention Transformer（GRA‑T）和流形位置嵌入（MPE）来提取局部与全局流形特征；③ 将提取的特征送入粗略完成模块生成初始点云，再通过多阶段 Transformer 上采样细化为完整稠密点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 锚点基地理距离近似方法，既高效又能捕捉非线性拓扑；2) 地理关系注意力 Transformer，利用地理距离和特征差异生成注意力权重；3) 流形位置嵌入，将全局地理信息注入点特征；4) 将上述模块整合到层次特征学习中，显著提升几何一致性和语义连贯性。与以往仅使用欧氏 k‑NN 或全局注意力的工作不同，该方法在邻域构建和注意力计算上都充分考虑了流形结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种基于锚点地理距离近似和地理关系注意力的流形感知点云完成框架，显著提升了重建质量和语义一致性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.&lt;/p&gt;</description></item><item><guid>2512.05759v1</guid><title>Label-Efficient Point Cloud Segmentation with Active Learning</title><link>http://arxiv.org/abs/2512.05759v1</link><author>Johannes Meyer, Jasper Hoffmann, Felix Schulz, Dominik Merkle, Daniel Buescher, Alexander Reiterer, Joschka Boedecker, Wolfram Burgard</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种简单易行的主动学习策略，用于3D点云语义分割，利用二维网格划分点云并通过网络集成估计不确定性，显著降低标注成本并在多个数据集上取得与最先进方法相当或更优的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D点云语义分割需要大量人工标注，成本高昂。主动学习通过自动选择最有价值的数据进行标注，能减少总标注量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种新颖且易于实现的点云划分与样本选择方法，以提高主动学习在3D点云分割中的效率和效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用二维网格将点云划分为列，形成可标注区域；采用网络集成对网络输出的不确定性进行估计，从而挑选下一批需标注的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在S3DIS、Toronto-3D和弗莱堡大规模城市点云上实验表明，该方法的性能与复杂的最先进方法相当或更好；并且在点云场景中，标注面积比标注点数更能体现主动学习的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该策略既简单又高效，能够在保持或提升性能的同时显著降低标注成本，为3D点云主动学习提供了实用的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 语义分割3D点云数据往往伴随高昂的标注成本。主动学习自动化选择需要标注的数据，减少实现满意性能所需的总标注量。最近针对3D点云的主动学习方法通常基于复杂的启发式方法，既将点云划分为可标注区域，又选择最有利于后续神经网络训练的数据。在本工作中，我们提出了一种新颖且易于实现的策略，将点云划分为可标注区域。我们的方法利用二维网格将点云细分为列。为识别下一批需标注的数据，我们采用网络集成来估计网络输出的不确定性。我们在S3DIS数据集、Toronto-3D数据集以及我们在弗莱堡市手动标注的一个大规模城市3D点云上评估了我们的方法。广泛的评估表明，我们的方法在所有数据集上都能达到与甚至优于复杂的最先进方法的性能。此外，我们提供了结果，表明在点云的背景下，标注面积可能是主动学习算法比标注点数更有意义的度量。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在降低对三维点云语义分割的标注成本。由于城市点云规模巨大且人工标注耗时昂贵，减少标注量对于机器人、城市规划和环境监测等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用主动学习框架，先用简单的二维网格将点云划分为列，再利用深度集成网络估计不确定性来挑选待标注区域。该思路借鉴了 ReDAL、SSDR‑AL 等混合策略，但去除了复杂的预处理和启发式指标，改为纯集成不确定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是将点云分成易标注的列，并用集成模型的方差或熵衡量每列的不确定性。流程包括：① 用少量已标注数据训练模型；② 用多模型集成预测所有列；③ 计算每列的平均不确定性；④ 选取最高不确定性的列交给人工标注；⑤ 将新标注加入训练集并迭代。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 简单的列划分实现区域分离；② 纯集成不确定性选择策略，避免过度自信的 softmax；③ 用覆盖面积而非点数衡量标注工作量；④ 在 S3DIS、Toronto‑3D 和大规模弗莱堡城市点云上实现与或优于现有复杂方法的性能。与之前工作相比，省去了繁琐的超像素/超体素生成和多种启发式特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种轻量级主动学习框架，通过将点云划分为网格列并使用深度集成不确定性挑选待标注列，在保持或提升分割性能的同时显著降低标注成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.&lt;/p&gt;</description></item><item><guid>2512.06882v1</guid><title>Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion</title><link>http://arxiv.org/abs/2512.06882v1</link><author>Yu Zhu, Naoya Chiba, Koichi Hashimoto</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种分层图像引导的三维分割框架，能够在工业环境中有效处理遮挡和尺度差异问题，并通过实例级到部件级的逐步细化实现高精度分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在工业环境中，复杂场景往往存在密集布局、多尺度物体以及严重遮挡，导致几何边界模糊，传统端到端模型难以同时捕捉粗细细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种分层图像引导的三维分割方法，以提高在遮挡和尺度差异明显的场景中的分割性能，并降低对昂贵标注的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1）实例级分割：渲染俯视图，利用YOLO-World提示的SAM生成掩码，再投影回三维点云；2）部件级分割：对每个实例渲染多视图，重复二维分割与投影，并通过贝叶斯更新融合多视图语义一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实工厂数据上实验表明，该方法能有效处理遮挡和结构复杂性，取得高的每类mIoU；在公开数据集上的评估进一步验证了其泛化能力、鲁棒性、标注效率和适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该分层图像引导框架在工业三维分割任务中表现出色，具有高效、鲁棒、适应多样环境的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决工业环境中复杂、密集且多尺度物体的 3D 点云分割问题。由于遮挡严重、几何边界模糊以及尺度差异大，传统 3D 分割方法往往难以同时捕捉粗粒度实例和细粒度部件。准确的 3D 分割对于机器人操作、数字孪生和工业自动化等任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有 3D 点云分割需要大量标注且对复杂场景泛化差，随后借鉴了 2D 视觉基础模型（YOLO‑World、SAM）和多视角融合技术。通过将检测与分割分离、采用自适应渲染、以及贝叶斯更新融合，构建了一个分层的图像引导框架，既利用了 2D 模型的强大表示，又解决了跨视角不一致的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从粗到细逐步细化 3D 分割：首先用顶部视图渲染点云，YOLO‑World 检测实例并用 SAM 生成掩码，再投影回 3D 得到实例级标签；随后对每个实例采样多视角，重复检测与分割，得到部件级 2D 掩码，投影回 3D 并通过贝叶斯更新融合多视角信息，最后用 DBSCAN 去除噪声。整个流程可视为检测‑分割‑投影‑融合‑清洗的连贯链条。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 分层框架：先实例后部件，提升对大尺度与细部的兼顾；2) 基于 2D 基础模型的检测‑分割管线，显著降低 3D 标注成本；3) 贝叶斯更新融合多视角掩码，解决跨视角语义不一致；4) 自适应渲染与 DBSCAN 后处理，增强对遮挡和噪声的鲁棒性。与以往单一端到端 3D 网络或单视角 2D 引导方法相比，该方案在工业场景中实现了更高的精度与更低的标注需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 提出一种基于 2D 基础模型和贝叶斯多视角融合的分层图像引导 3D 点云分割框架，能够在工业环境中以低标注成本实现高精度、跨尺度的实例与部件级分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.&lt;/p&gt;</description></item><item><guid>2512.07124v1</guid><title>Enhancing Urban Sensing Utility with Sensor-enabled Vehicles and Easily Accessible Data</title><link>http://arxiv.org/abs/2512.07124v1</link><author>Hui Zhong, Qing-Long Lu, Qiming Zhang, Hongliang Lu, Xinhu Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了利用配备传感器的车辆进行城市感知的策略，提出了一个自适应框架来提升感知效用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市感知对于智慧城市的发展至关重要，现代车辆正从单纯的交通工具转变为重要的数据采集源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 优化车辆感知策略，以平衡空间与时间覆盖、降低冗余并满足预算限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建了一个融合异构开放数据的框架，使用时空加权来选择车辆并覆盖城市区域，并提出了基于熵的车辆选择算法——Improved OptiFleet。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在广州320辆车辆的空气质量数据实验中，该方法比基线策略提升了约5%的感知效用，同时减少了车队规模，并证明动态城市数据对优化移动感知策略至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应车辆感知框架能够在有限资源下实现更高效的数据采集，为智慧城市的交通管理和基础设施监测提供支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Urban sensing is essential for the development of smart cities, enabling monitoring, computing, and decision-making for urban management. Thanks to the advent of vehicle technologies, modern vehicles are transforming from solely mobility tools to valuable sensors for urban data collection, and hold the potential of improving traffic congestion, transport sustainability, and infrastructure inspection. Vehicle-based sensing is increasingly recognized as a promising technology due to its flexibility, cost-effectiveness, and extensive spatiotemporal coverage. However, optimizing sensing strategies to balance spatial and temporal coverage, minimize redundancy, and address budget constraints remains a key challenge. This study proposes an adaptive framework for enhancing the sensing utility of sensor-equipped vehicles. By integrating heterogeneous open-source data, the framework leverages spatiotemporal weighting to optimize vehicle selection and sensing coverage across various urban contexts. An entropy-based vehicle selection strategy, Improved OptiFleet, is developed to maximize sensing utility while minimizing redundancy. The framework is validated using real-world air quality data from 320 sensor-equipped vehicles operating in Guangzhou, China, over two months. Key findings show that the proposed method outperforms baseline strategies, providing up to 5% higher sensing utility with reduced fleet sizes, and also highlights the critical role of dynamic urban data in optimizing mobile sensing strategies.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Urban sensing is essential for the development of smart cities, enabling monitoring, computing, and decision-making for urban management.Thanks to the advent of vehicle technologies, modern vehicles are transforming from solely mobility tools to valuable sensors for urban data collection, and hold the potential of improving traffic congestion, transport sustainability, and infrastructure inspection.Vehicle-based sensing is increasingly recognized as a promising technology due to its flexibility, cost-effectiveness, and extensive spatiotemporal coverage. However, optimizing sensing strategies to balance spatial and temporal coverage, minimize redundancy, and address budget constraints remains a key challenge.This study proposes an adaptive framework for enhancing the sensing utility of sensor-equipped vehicles.By integrating heterogeneous open-source data, the framework leverages spatiotemporal weighting to optimize vehicle selection and sensing coverage across various urban contexts.An entropy-based vehicle selection strategy, \texttt{Improved OptiFleet}, is developed to maximize sensing utility while minimizing redundancy.The framework is validated using real-world air quality data from 320 sensor-equipped vehicles operating in Guangzhou, China, over two months.Key findings show that the proposed method outperforms baseline strategies, providing up to 5\% higher sensing utility with reduced fleet sizes, and also highlights the critical role of dynamic urban data in optimizing mobile sensing strategies.&lt;/p&gt;</description></item><item><guid>2512.07684v1</guid><title>When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks</title><link>http://arxiv.org/abs/2512.07684v1</link><author>Zihan Chen, Lanyu Yu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于图神经网络的框架，用于检测英文维基百科社区中的三类不文明行为，并通过动态注意机制提升检测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 网络不文明行为在数字社区中普遍存在，给用户带来社会和心理负担。现有的内容审核和自动检测方法在准确性和效率上仍有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够同时利用文本内容和评论之间关系的模型，以更准确、高效地识别毒性、攻击性和人身攻击等不文明行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将每条评论视为图节点，节点间的边由文本相似度确定；使用图神经网络联合学习语言特征和结构信息，并加入动态调整的注意机制，在信息聚合时平衡节点特征与拓扑特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该模型在多项指标上优于12种先进的大语言模型，并且推理成本显著更低，证明结构上下文在检测不文明行为中的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结构化上下文与动态注意机制的结合显著提升了在线不文明行为检测的性能，弥补了仅依赖文本的大语言模型的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在线不文明行为已成为数字社区中普遍且持续存在的问题，对用户造成了重大的社会和心理负担。虽然许多平台尝试通过审核和自动检测来遏制不文明行为，但现有方法在准确性和效率方面往往仍然有限。为了解决这一挑战，我们提出了一个图神经网络（GNN）框架，用于检测英文维基百科社区中的三种不文明行为（即毒性、攻击性和人身攻击）。我们的模型将每条用户评论视为一个节点，评论之间的文本相似度定义边，使网络能够同时学习评论的语言内容和评论之间的关系结构。我们还引入了一个动态调整的注意机制，在信息聚合过程中自适应地平衡节点特征和拓扑特征。实证评估表明，我们提出的架构在多项指标上优于12种最先进的大语言模型（LLMs），同时推理成本显著更低。这些发现突出了结构上下文在检测在线不文明行为中的关键作用，并解决了仅基于文本的LLM范式在行为预测中的局限性。所有数据集和比较结果将公开发布在我们的仓库中，以支持进一步研究和可重复性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.&lt;/p&gt;</description></item><item><guid>2512.08223v1</guid><title>SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection</title><link>http://arxiv.org/abs/2512.08223v1</link><author>Ching-Hung Cheng, Hsiu-Fu Wu, Bing-Chen Wu, Khanh-Phong Bui, Van-Tin Luu, Ching-Chun Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了在三维目标检测任务中使用提示调优方法的有效性，评估了基于大规模Waymo数据集训练的模型是否能作为基础模型并适应其他场景，并提出了场景导向提示池（SOP^2）来进一步提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型（如GPT-3）具备强大的泛化能力，通过微调和提示调优等迁移学习技术，可在自然语言处理等领域以极少的参数调整适配多种下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证提示调优方法在三维目标检测中的适用性，并探索基于Waymo数据集的基础模型在不同场景中的迁移效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 系统评估提示词和提示生成器的影响，随后提出并实验场景导向提示池（SOP^2）以提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 提示池在三维目标检测任务中表现出显著的有效性，能够提升模型在不同场景下的检测表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提示调优技术在三维目标检测领域具有潜在价值，提示池的引入为未来研究提供了新的方向和启示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着大型语言模型（如GPT-3）的崛起，这些模型展现出强大的泛化能力。通过微调和提示调优等迁移学习技术，它们可以以最小的参数调整适配各种下游任务。这种方法在自然语言处理领域尤为常见。本文旨在探讨常见提示调优方法在三维目标检测中的有效性。我们研究了在大规模Waymo数据集上训练的模型是否能作为基础模型，并适应三维目标检测领域的其他场景。本文依次考察了提示词和提示生成器的影响，并进一步提出了场景导向提示池（SOP^2）。我们展示了提示池在三维目标检测中的有效性，旨在激发未来研究者深入探讨提示在三维领域的潜力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注 3D 目标检测模型在不同数据集之间的迁移性能。由于传感器、环境和采集方式的差异，模型在一个大规模数据集（如 Waymo）上训练后往往无法在另一个数据集（如 KITTI）上保持高精度。解决这一域间差距对自动驾驶、机器人和城市规划等实际应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到 NLP 中的 Prompt Tuning、LoRA 等参数高效微调技术，并将其思想迁移到视觉任务。随后参考 VPT、DVPT 和 Prompt Pool（L2P）等工作，提出按场景划分点云并为每个划分分配专门的 Prompt Pool。整个设计借鉴了现有的 Prompt 相关方法，并结合 3D Transformer（DSVT）实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为每个点云划分（X/Y 方向窗口）维护一个 Prompt Pool，并在推理时根据当前划分的特征与池中键进行相似度匹配，挑选最合适的 Prompt 进行拼接。实现流程包括：① 用 Waymo 预训练的 DSVT 作为骨干；② 将点云划分为若干窗口；③ 对每个窗口使用 Prompt Pool 选取 Prompt 并与窗口特征拼接；④ 通过多头自注意力完成特征交互；⑤ 只训练 Prompt Pool，保持骨干冻结。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 引入 Scene‑Oriented Prompt Pool（SOP2）专门针对 3D 点云场景；2) 通过相似度匹配动态选择 Prompt，提升对不同子场景的适应性；3) 在 DSVT 结构上实现 Prompt Pool，保持参数高效。与以往仅使用固定 Prompt 或全模型微调的做法不同，SOP2 在保持骨干不变的前提下显著提升跨域检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出一种基于场景的 Prompt Pool，能够在冻结 3D Transformer 骨干的同时实现高效、参数友好的跨域目标检测迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.&lt;/p&gt;</description></item><item><guid>2512.08247v1</guid><title>Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</title><link>http://arxiv.org/abs/2512.08247v1</link><author>Haowen Zheng, Hu Zhu, Lu Deng, Weihao Gu, Yang Yang, Yanyan Liang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种稀疏查询的未来时序知识蒸馏方法，能够将离线模型利用未来帧获得的丰富信息迁移到在线模型，从而提升摄像头驱动的三维目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 摄像头基于时序的三维目标检测在自动驾驶中表现突出，离线模型通过使用未来帧显著提高精度，但现有的知识蒸馏方法往往忽略未来帧，难以让在线模型有效学习未来知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够在不严格对齐帧的情况下，将未来帧知识有效传递给在线模型的知识蒸馏框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出Future Temporal Knowledge Distillation (FTKD)，包括未来感知特征重建策略和未来引导的logit蒸馏，利用稀疏查询方式在离线教师与在线学生之间传递未来帧信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，FTKD在两个高性能基线模型上分别提升了最高1.3 mAP和1.3 NDS，并实现了最准确的速度估计，且未增加推理成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FTKD能够有效迁移未来帧知识，显著提升在线三维目标检测的精度与速度估计，证明了未来时序知识蒸馏的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher&amp;#x27;s stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在让在线 3D 目标检测模型在没有未来帧的情况下，学习到离线模型利用未来帧获得的丰富时序信息，从而提升检测精度，尤其是对远距离或被遮挡物体的识别，并改善速度估计，这在自动驾驶等实时场景中至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有知识蒸馏方法大多只关注空间特征或仅利用过去帧的时序关系，忽略了未来帧的价值；他们借鉴了掩码特征重建和 logit 蒸馏的思路，结合稀疏查询的检测框架，设计了未来感知的特征重建和未来引导的 logit 蒸馏两大模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用教师模型聚合未来帧的特征作为重建目标，然后对学生模型的特征随机掩码并用自适应生成器重建，最后通过匈牙利算法匹配教师与学生的查询进行 logit 蒸馏。实现流程包括：冻结教师、计算未来聚合特征、生成掩码、重建学生特征、计算重建损失、匹配查询并计算 logit 损失，完成训练后保持原始学生结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 未来感知的特征重建突破了严格帧对齐限制；② 未来引导的 logit 蒸馏利用背景查询信息；③ 采用稀疏查询的教师与学生，兼顾精度与效率；④ 训练后无额外推理开销。与以往只关注空间或过去时序的蒸馏方法不同，本文显式利用未来帧知识并兼顾背景信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种仅使用相机的知识蒸馏框架，使在线 3D 检测器能够从离线教师的未来帧中学习，提升检测与速度估计精度且不增加推理成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher&amp;#x27;s stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.&lt;/p&gt;</description></item><item><guid>2512.08557v2</guid><title>SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds</title><link>http://arxiv.org/abs/2512.08557v2</link><author>Alexander Dow, Manduhu Manduhu, Matheus Santos, Ben Bartlett, Gerard Dooly, James Riordan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用激光雷达连续扫描运动的技术，通过只关注帧间点云变化的区域来提高目标检测效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统稀疏卷积在处理激光雷达点云时需要对整个点云进行卷积，计算量大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不降低检测精度的前提下，减少卷积运算次数，提高计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用短步长的滑动时间窗口，存储跨帧卷积结果，忽略未变化区域；在此基础上扩展散射卷积，提出带时间数据回收的稀疏散射卷积算法（SSCATeR），仅对变化部分进行运算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法与传统稀疏卷积产生相同的特征图，处理时间可缩短至原来的约1/6.61，显著提升计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过数据重用和时间维度的利用，SSCATeR 在保持精度的同时大幅降低了计算成本，为实时激光雷达目标检测提供了可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在降低无人机 LiDAR 点云 3D 目标检测的延迟。实时检测对无人机在动态环境中的安全至关重要，尤其是在需要快速响应的场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到 LiDAR 连续扫描导致大部分点云在相邻帧中保持不变，于是提出在滑动时间窗口内仅处理变化部分，并重用先前卷积结果。该思路借鉴了之前的散射卷积和稀疏卷积技术，并在先前的流式 LiDAR 研究基础上扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是利用时间稀疏性，记录点云变化地图，存储并重用未变化区域的卷积结果。实现流程包括：滑动 100 ms 时间窗口、生成变化地图、使用稀疏散射卷积与时间数据回收（SSCATeR）处理仅变化的点，输出与传统方法相同的特征图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 在稀疏散射卷积中加入时间数据回收；2) 将活跃点数平均减少 72.8%，处理时间平均降低 59.85%（单层可达 84.88%）；3) 保持与传统卷积相同的特征图和精度。与以往提升精度或使用注意力网络的工作不同，SSCATeR 通过利用时间稀疏性显著提升效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SSCATeR 通过在 LiDAR 点云中重用未变化区域的卷积结果，将 3D 目标检测的处理时间缩短多达 6.6 倍，同时保持原有精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.&lt;/p&gt;</description></item><item><guid>2512.08957v1</guid><title>LUMOS: Large User MOdels for User Behavior Prediction</title><link>http://arxiv.org/abs/2512.08957v1</link><author>Dhruv Nigam</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于 transformer 的 LUMOS 架构，用于大规模用户行为预测，消除了任务特定模型和手工特征工程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统方法依赖任务特定模型和领域特定特征工程，耗时、计算量大且不易扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种可同时学习多任务、仅使用原始用户活动数据的模型，以提高预测性能并降低成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; LUMOS 采用跨注意力机制，将未来已知事件（如节假日、促销）作为条件；使用多模态分词，将交易、事件上下文和静态人口属性融合为丰富表示，并通过专门嵌入路径处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在包含 2.75 亿用户、275 亿个活动 token 的生产数据集上，LUMOS 在 5 个任务上平均提升 ROC‑AUC 0.025，回归任务 MAPE 降低 4.6%，在线 A/B 测试显示日活跃用户提升 3.15%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LUMOS 在多任务学习和跨注意力机制下显著优于传统方法，能够在大规模场景中实现更高效、更准确的用户行为预测，并带来可观的业务收益。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 用户行为预测在大规模在线 B2C 平台上仍然是一个关键挑战。传统方法高度依赖任务特定模型和领域特定特征工程，耗时、计算成本高且需要领域专业知识，因而难以扩展。我们提出 LUMOS（Large User MOdel Series），一种基于 transformer 的架构，通过仅使用原始用户活动数据联合学习多项任务，消除了任务特定模型和手工特征工程。LUMOS 引入了一种新颖的跨注意力机制，使预测能够基于已知的未来事件（如节假日、促销等）进行条件化，从而能够预测诸如“即将到来的节假日如何影响用户参与度”等复杂行为模式。该架构还采用多模态分词，将用户交易、事件上下文和静态用户人口属性融合为丰富的表示，并通过专门的嵌入路径进行处理。通过在包含 2.75 亿用户、275 亿个用户活动 token 的生产数据集上进行广泛实验，我们证明 LUMOS 在传统任务特定模型之上实现了更优的性能。在 5 个已建立基线的任务中，二分类任务的 ROC‑AUC 平均提升 0.025，回归任务的 MAPE 降低 4.6%。在线 A/B 测试验证了这些改进转化为可衡量的业务影响，日活跃用户提升 3.15%。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like &amp;quot;how will upcoming holidays affect user engagement?&amp;quot; The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.   Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.&lt;/p&gt;</description></item><item><guid>2512.09260v1</guid><title>From Forecast to Action: Uncertainty-Aware UAV Deployment for Ocean Drifter Recovery</title><link>http://arxiv.org/abs/2512.09260v1</link><author>Jingeun Kim, Yong-Hyuk Kim, Yourim Yoon</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种新的预测-优化框架，将轨迹预测与无人机部署优化相结合，用于海上搜救。使用大语言模型预测漂流物轨迹，并通过高斯粒子采样建模空间不确定性。与传统静态部署不同，动态调整无人机探测半径并使用元启发式算法优化位置。实验表明该方法，尤其是修复机制，显著优于随机搜索基线。该工作为智能海上救援提供了实用且稳健的集成方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统海上搜救中无人机部署多为静态方法，缺乏动态适应和轨迹预测的集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种端到端的预测-优化框架，提升海上搜救效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 用大语言模型预测漂流物轨迹；2. 用高斯粒子采样建模空间不确定性；3. 动态调整无人机探测半径；4. 用元启发式算法优化无人机位置；5. 设计修复机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法，尤其是修复机制，显著优于随机搜索基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架为智能海上救援提供了实用且稳健的轨迹预测与空间优化集成方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种新颖的预测-优化框架，用于海上搜救操作，整合了轨迹预测与无人机部署优化——这是前人未曾涉及的端到端方法。大语言模型预测漂流物轨迹，并使用基于高斯的粒子采样来建模空间不确定性。与传统的静态部署方法不同，我们根据距离动态调整无人机的探测半径，并使用元启发式算法优化其位置。对韩国海岸线真实数据的实验表明，我们的方法，特别是为此问题设计的修复机制，显著优于随机搜索基线。本工作为智能海上救援提供了实用且稳健的轨迹预测与空间优化集成方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a novel predict-then-optimize framework for maritime search operations that integrates trajectory forecasting with UAV deployment optimization-an end-to-end approach not addressed in prior work. A large language model predicts the drifter&amp;#x27;s trajectory, and spatial uncertainty is modeled using Gaussian-based particle sampling. Unlike traditional static deployment methods, we dynamically adapt UAV detection radii based on distance and optimize their placement using meta-heuristic algorithms. Experiments on real-world data from the Korean coastline demonstrate that our method, particularly the repair mechanism designed for this problem, significantly outperforms the random search baselines. This work introduces a practical and robust integration of trajectory prediction and spatial optimization for intelligent maritime rescue.&lt;/p&gt;</description></item><item><guid>2512.09407v1</guid><title>Generative Point Cloud Registration</title><link>http://arxiv.org/abs/2512.09407v1</link><author>Haobo Jiang, Jin Xie, Jian Yang, Liang Yu, Jianmin Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新颖的生成式点云配准范式，通过生成跨视角一致且与源点云和目标点云几何对齐的图像对，实现几何与颜色特征融合，从而提升三维配准的鲁棒性和性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统三维配准方法在匹配精度和鲁棒性方面存在局限，尤其在纹理缺失或几何噪声较大的情况下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用先进的二维生成模型与三维匹配任务相结合，构建一种能够生成高质量、跨视角一致图像对的配准框架，以提升配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入 Match-ControlNet，一种可控二维生成模型，利用深度条件生成保证二维-三维几何一致性，并通过耦合条件去噪与提示引导促进跨视角纹理一致性，从而生成符合要求的图像对用于配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 3DMatch 与 ScanNet 数据集上的实验表明，该生成式配准范式显著提升了配准性能，验证了方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 生成式点云配准范式具有通用性，可无缝集成到多种配准方法中，显著提升配准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在本文中，我们提出了一种新颖的三维配准范式——生成式点云配准，它将先进的二维生成模型与三维匹配任务相结合，以提升配准性能。我们的核心思路是生成跨视角一致的图像对，并使其与源点云和目标点云良好对齐，从而实现几何-颜色特征融合，促进鲁棒匹配。为确保高质量匹配，生成的图像对应同时具备二维-三维几何一致性和跨视角纹理一致性。为此，我们引入了 Match-ControlNet，一种针对匹配的可控二维生成模型。它利用 ControlNet 的深度条件生成能力，生成与点云派生的深度图几何对齐的图像，保证二维-三维几何一致性。此外，通过结合耦合条件去噪方案和耦合提示引导，Match-ControlNet 进一步促进跨视角特征交互，引导纹理一致性生成。我们的生成式三维配准范式具有通用性，可无缝集成到各种配准方法中以提升其性能。对 3DMatch 和 ScanNet 数据集的广泛实验验证了我们方法的有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在提升仅使用几何信息的点云配准精度，方法是通过生成与源点云和目标点云几何一致且纹理相互一致的 RGB 图像，为配准提供补充的颜色信息。此问题在现实中十分重要，因为实际扫描往往存在低重叠、噪声或缺失纹理，传统仅基于几何的配准方法难以获得可靠对应关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 RGB‑D 配准中颜色信息的重要性，结合 Stable Diffusion 的 ControlNet 深度条件生成框架，并在此基础上加入了耦合条件去噪和耦合提示引导，以实现跨视角纹理一致性。同时利用预训练的视觉模型进行零样本特征融合，形成一种可直接插拔的配准增强方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先将源点云和目标点云转换为深度图，再使用 Match‑ControlNet 生成与深度图几何一致且纹理相互一致的 RGB 图像对；随后提取这些图像的零样本特征并与原始几何描述子融合，得到增强的特征；最后通过对应匹配和姿态估计得到最终变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）提出生成点云配准范式，利用合成 RGB 图像补充颜色信息；2）设计 Match‑ControlNet，采用耦合去噪和提示引导实现 2D‑3D 几何一致性与跨视角纹理一致性；3）实现零样本几何‑颜色融合，利用预训练模型无需微调即可提升描述子。与以往仅使用真实 RGB 或独立生成单张图像的方法不同，该方案在不需要真实图像且无需任务特定微调的情况下，生成一致的图像对并直接增强配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过联合生成几何一致且纹理相互一致的 RGB 图像对，并将其颜色特征与几何描述子融合，作者提出了一种可直接插拔的框架，在无需真实 RGB 数据和微调的情况下显著提升点云配准精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.&lt;/p&gt;</description></item><item><guid>2512.09423v1</guid><title>FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds</title><link>http://arxiv.org/abs/2512.09423v1</link><author>Marco Pegoraro, Evan Atherton, Bruno Roy, Aliasghar Khani, Arianna Rampini</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为 FunPhase 的功能周期自编码器，用于学习人体运动的相位流形，并通过函数空间解码实现平滑、可任意时间分辨率的运动轨迹。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 学习自然人体运动因空间几何与时间动态的强耦合而具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有周期自编码器在可扩展性和适用范围上的局限，构建能够在不同骨架和数据集上通用的运动预测与生成模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; FunPhase 通过学习运动的相位流形，将离散时间解码替换为函数空间表示，支持超分辨率、局部身体运动补全等下游任务，并在单一可解释流形中统一预测与生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 与之前的周期自编码器基线相比，FunPhase 在重建误差上显著降低，同时能够实现更广泛的应用，并与最先进的运动生成方法保持相当的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FunPhase 提供了一个可扩展、可解释且通用的运动学习框架，兼顾预测与生成，适用于多种骨架和数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 学习自然人体运动仍然具有挑战性，因为空间几何与时间动态之间存在强耦合。将运动嵌入相位流形，即捕捉局部周期性的潜在空间，已被证明对运动预测有效；然而，现有方法缺乏可扩展性，且仅限于特定设置。我们提出 FunPhase，一种功能周期自编码器，它为运动学习相位流形，并用函数空间形式替代离散时间解码，从而实现可在任意时间分辨率下采样的平滑轨迹。FunPhase 支持超分辨率和局部身体运动补全等下游任务，能够跨骨架和数据集泛化，并在单一可解释流形中统一运动预测和生成。我们的模型在重建误差上显著低于以往周期自编码器基线，同时支持更广泛的应用，并与最先进的运动生成方法相当。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.&lt;/p&gt;</description></item><item><guid>2512.10046v1</guid><title>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</title><link>http://arxiv.org/abs/2512.10046v1</link><author>Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang, Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu, Tianmin Shu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SimWorld-Robotics 是一个基于Unreal Engine 5的城市级仿真平台，能够无限生成逼真的城市场景并支持多机器人控制。作者利用该平台创建了两个挑战性基准：多模态指令跟随和多机器人搜索，全面评估机器人在复杂城市环境中的感知、推理、导航与协作能力。实验表明现有视觉语言模型在这些任务中表现不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来基础模型在多模态输入下的通用机器人研究取得进展，但大多聚焦室内家庭场景，缺乏对开放式城市环境的评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个可扩展、逼真的城市仿真平台，并通过新基准评估机器人在真实城市环境中的关键能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Unreal Engine 5实现程序化生成的城市场景，加入行人和交通系统，支持多机器人控制与通信；基于此平台设计两项任务：视觉语言导航与多机器人搜索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示当前最先进的视觉语言模型在多模态指令跟随和多机器人搜索任务中表现不佳，缺乏稳健的感知、推理和规划能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SimWorld-Robotics 及其基准为评估和推动机器人在复杂城市环境中的研究提供了重要工具，现有模型需要进一步提升感知与协作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近在基础模型方面的进展表明，在多模态输入下开发通用机器人能够在开放式场景中执行多样任务具有良好前景。然而，目前的工作主要集中在室内、家庭场景。本研究提出了SimWorld-Robotics（SWR），一个用于大规模、逼真城市环境中具身人工智能的仿真平台。基于Unreal Engine 5，SWR通过程序化生成无限逼真的城市场景，并加入行人和交通系统等动态元素，超过了以往城市仿真的真实性、复杂性和可扩展性。它还支持多机器人控制与通信。凭借这些关键特性，我们构建了两个具有挑战性的机器人基准：（1）多模态指令跟随任务，机器人必须在行人和交通存在的情况下，遵循视觉语言导航指令到达目的地；（2）多智能体搜索任务，两个机器人必须通过通信合作寻找并相遇。与现有基准不同，这两个新基准全面评估了在现实场景中对机器人关键能力的广泛需求，包括多模态指令 grounding、在大环境中的三维空间推理、安全长距离导航与人车交互、多机器人协作以及 grounded communication。我们的实验结果表明，包括视觉语言模型在内的最先进模型在我们的任务中表现不佳，缺乏在城市环境中所需的稳健感知、推理和规划能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在构建一个能够生成无限、光照、天气和人车动态真实的城市环境的仿真平台，以支持机器人在大规模、真实感强的户外场景中进行多模态导航和多机器人协作。现实中现有仿真器大多局限于室内或缺乏动态交互，难以训练和评估能够在复杂城市环境中安全、长距离导航的通用机器人模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有城市仿真器（如CARLA、AirSim、MetaUrban等）的不足，发现它们缺乏光照、天气、动态人车交互和多机器人控制。随后在Unreal Engine 5基础上扩展SimWorld，加入了程序化城市生成、交通系统和四足机器人等功能，并借鉴了这些已有工作中的技术与设计理念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过程序化生成无限、光照、天气和动态人车交互的城市环境，并提供异步多代理控制与丰富感知。实现流程包括：1）道路、建筑、街道元素、交通元素四阶段生成；2）异步多代理控制框架，允许机器人、人车独立行动；3）提供RGB、深度、语义分割等观测；4）基于此平台构建多模态导航与多机器人搜索基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①无限程序化生成的光照、天气真实城市；②多模态感知与异步多代理控制；③支持四足机器人与人车的统一仿真；④提出SIMWORLD-MMNAV和SIMWORLD-MRS两大基准；⑤发布SimWorld-20K大规模训练数据。与之前工作相比，SWR在规模、真实感、动态交互和多机器人协作方面均有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SimWorld‑Robotics提供了一个可无限生成、光照、天气和动态人车交互的高真实感城市仿真平台，并通过新基准和大规模数据集，揭示了当前通用机器人模型在复杂城市环境中的局限。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.&lt;/p&gt;</description></item><item><guid>2512.10056v1</guid><title>Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens</title><link>http://arxiv.org/abs/2512.10056v1</link><author>Alireza Namazi, Amirreza Dolatpour Fathkouhi, Heman Shakeri</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自回归预测方法SoTra，旨在解决糖尿病和血流动力学管理中的预测控制问题，减少暴露偏差并提高多步预测的稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在糖尿病和血流动力学管理中，自回归预测是预测控制的核心，不同的操作区间对应不同的临床风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入软令牌轨迹预测，降低暴露偏差，学习校准的、不确定性感知的预测轨迹，并通过风险感知解码模块最小化预期临床伤害。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SoTra传播连续概率分布（软令牌），并结合风险感知解码模块进行期望临床伤害最小化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在血糖预测中，SoTra将基于区间的平均风险降低了18%；在血压预测中，降低了约15%的有效临床风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SoTra在安全关键的预测控制中表现出显著改进，支持其在临床应用中的使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自回归预测是糖尿病和血流动力学管理中预测控制的核心，不同的操作区间携带不同的临床风险。使用教师强制训练的标准模型会出现暴露偏差，导致闭环使用时多步预测不稳定。我们提出了软令牌轨迹预测（SoTra），它传播连续概率分布（“软令牌”）以减轻暴露偏差，并学习校准的不确定性感知轨迹。一个风险感知解码模块随后最小化预期临床伤害。在血糖预测中，SoTra将基于区间的平均风险降低了18%；在血压预测中，它将有效临床风险降低了约15%。这些改进支持其在安全关键的预测控制中的使用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens&amp;#x27;&amp;#x27;) to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\%. These improvements support its use in safety-critical predictive control.&lt;/p&gt;</description></item><item><guid>2512.10386v1</guid><title>Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method</title><link>http://arxiv.org/abs/2512.10386v1</link><author>Ge Zhang, Chunyang Wang, Bo Xiao, Xuelian Liu, Bin Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自适应双权重重力模型点云去噪方法，利用八叉树并行加速，结合体素占据统计和kNN密度估计快速剔除噪点，并通过密度与距离加权的重力评分函数精细区分噪点与物体点。实验表明该方法在多种噪声条件下相较现有方法在F1、PSNR、CD指标上均有提升，同时单帧处理时间更短，证明其高精度、鲁棒性和实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高质量点云数据是自动驾驶、三维重建等任务的关键基础，但基于LiDAR的点云采集易受多种干扰，产生大量噪点，影响后续目标检测与识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有点云去噪方法在高精度、边缘保持和实时性能之间难以兼顾的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用八叉树进行空间分区实现并行加速；在叶节点内使用自适应体素占据统计和kNN密度估计快速剔除孤立低密度噪点；构建结合密度权重与自适应距离权重的重力评分函数，精细区分噪点与物体点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验显示该方法在Stanford 3D扫描库、CADC数据集和实验室FMCW LiDAR点云上，在F1、PSNR、CD指标上均优于现有方法，并且单帧处理时间更短。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该自适应双权重重力模型点云去噪方法在多噪声场景下实现了高精度、强边缘保持和实时性能，具有良好的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 高质量的点云数据是自动驾驶和三维重建等任务的关键基础。然而，基于LiDAR的点云采集往往受到多种干扰的影响，导致大量噪点出现，降低后续点云目标检测和识别的准确性。此外，现有的点云去噪方法通常在追求更高去噪精度时牺牲计算效率，或者在提升处理速度时牺牲边缘保持和细节结构的保留，使得难以同时实现高去噪精度、强边缘保持和实时性能。为解决这些局限，本文提出了一种自适应双权重重力模型点云去噪方法。首先，采用八叉树对全局点云进行空间分区，实现并行加速。随后，在每个叶节点内，利用自适应体素占据统计和k近邻密度估计快速去除明显孤立且低密度的噪点，从而减少有效候选集。最后，构建结合密度权重与自适应距离权重的重力评分函数，细致区分噪点与物体点。对Stanford 3D扫描库、加拿大不利驾驶条件（CADC）数据集以及我们实验室采集的FMCW LiDAR点云进行实验，结果表明，与现有方法相比，所提出的方法在各种噪声条件下在F1、PSNR和Chamfer距离（CD）指标上均取得一致提升，同时单帧处理时间更短，从而验证了其在多噪声场景下的高精度、鲁棒性和实时性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 LiDAR 采集的点云中噪声过多导致后续任务（如自动驾驶、三维重建）精度下降的问题。噪声会破坏几何结构，影响检测与识别，因此需要一种既能高效去噪又能保留边缘细节的算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在已有的基于重力特征的去噪方法基础上，结合八叉树空间划分、体素占用统计和 kNN 密度估计，提出了自适应双权重重力评分。文中引用了学习式与非学习式去噪、双边滤波、MLS、LOP 等现有技术，说明了对现有方法的借鉴与改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用八叉树将点云分块，快速剔除明显离群点和低密度噪声，然后对剩余候选点计算结合密度与距离的双权重重力分数，决定是否保留。实现流程为：八叉树划分 → 体素占用剔除 → kNN 低密度剔除 → 计算密度–距离双权重重力分数 → 过滤噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 自适应双权重重力评分函数，兼顾局部密度与空间关系；2) 八叉树块级并行框架，显著提升速度；3) 两阶段噪声剔除（体素+kNN）压缩候选集，降低计算量。与传统重力方法相比，算法更快且更能保留边缘细节；与滤波/优化方法相比，提供了更好的全局结构建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种结合八叉树划分、体素与 kNN 预过滤以及密度–距离双权重重力评分的点云去噪框架，能够在保持边缘细节的同时实现实时高精度去噪。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.&lt;/p&gt;</description></item><item><guid>2512.11354v1</guid><title>A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection</title><link>http://arxiv.org/abs/2512.11354v1</link><author>Qinghan Hu, Haijiang Zhu, Na Sun, Lei Chen, Zhengqiang Fan, Zhiqing Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种多模态水下结构光三维成像系统，用于管道检测，结合多源信息融合，提升了检测精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 水下管道易腐蚀，传统人工检测效率低且不安全，需更可靠的实时成像技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种基于多源信息融合的多模态水下结构光三维成像系统，以实现高精度、实时的管道缺陷检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用快速畸变校正、基于因子图的外参优化、适应几何变化的多模态成像策略、适应性扩展卡尔曼滤波以及基于边缘检测的ICP算法，实现图像校正、姿态估计和点云配准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该系统在不同工作模式、速度和深度下均表现出优异的精度、适应性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该系统为自主水下管道检测提供了可靠的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 水下管道易受腐蚀，缩短使用寿命并带来安全风险。与人工检查相比，智能实时成像系统已成为更可靠、更实用的解决方案。在众多水下成像技术中，结构光三维成像能够恢复足够的空间细节，精确表征缺陷。因此，本文基于多源信息融合，开发了一种多模态水下结构光三维成像系统（UW‑SLD系统）用于管道检测。首先，采用快速畸变校正方法实现高效的水下图像校正。为克服水下传感器间外参校准的挑战，提出基于因子图的参数优化方法，估计结构光与声学传感器之间的变换矩阵。进一步引入多模态三维成像策略，以适应水下管道几何变化。鉴于水下环境中存在大量干扰，设计了多源信息融合策略和自适应扩展卡尔曼滤波，确保姿态估计稳定、测量精度高。特别地，提出了基于边缘检测的ICP算法，该算法将管道边缘检测网络与增强点云配准相结合，即使在运动条件变化下也能实现稳健、高保真度的缺陷结构重建。对不同工作模式、速度和深度下进行了大量实验，结果表明所开发的系统在精度、适应性和鲁棒性方面均优于现有方法，为自主水下管道检测奠定了坚实基础。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决水下管道腐蚀和缺陷的检测问题。传统人工检查成本高、危险，且难以及时发现细小缺陷。准确、实时的三维成像能帮助维护人员及时评估管道安全，避免泄漏、爆炸等灾害，具有重要的工业和环境意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有声学、被动视觉和结构光技术的局限，决定将结构光与声学、惯性传感器结合。借鉴了结构光快速畸变校正、因子图外参优化、自适应扩展卡尔曼滤波、ICP配准以及深度学习边缘检测等已有技术，并在此基础上提出了多源信息融合与多模式成像方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多源传感器融合和深度学习辅助，实现高精度、鲁棒的水下三维成像。实现流程包括：硬件集成、快速畸变校正、因子图外参优化、姿态估计（AEKF）、多模式成像（平移、旋转、组合）、管道边缘检测网络提取感兴趣区域、基于边缘的ICP配准，最终得到高质量的缺陷三维模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 快速高精度的畸变校正方法；2) 因子图外参优化实现结构光与DVL的统一坐标；3) 分层多频信息融合与自适应卡尔曼滤波提升姿态估计；4) 多模式成像策略与管道边缘检测网络减少噪声；5) 边缘检测ICP算法在动态环境下实现稳健配准。与以往单一技术或单一场景的研究不同，该系统实现了多源融合、实时性和多模式适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种集成结构光、声学、惯性传感器和深度学习的多模式三维成像系统，能够在动态水下环境中实现高精度、鲁棒的管道缺陷检测。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.&lt;/p&gt;</description></item><item><guid>2512.11465v1</guid><title>DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation</title><link>http://arxiv.org/abs/2512.11465v1</link><author>Mohamed Abdelsamad, Michael Ulrich, Bin Yang, Miao Zhang, Yakov Miron, Abhinav Valada</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督学习框架 DOS（Distilling Observable Softmaps），通过仅在可观测（未遮挡）点上蒸馏语义相关软图，避免信息泄漏并提供比离散标记更丰富的监督；同时引入 Zipfian 原型和改进的 Sinkhorn-Knopp 算法 Zipf-Sinkhorn，以在无监督环境下处理语义不平衡问题。实验表明，DOS 在多项基准（nuScenes、Waymo、SemanticKITTI、ScanNet、ScanNet200）上的语义分割和 3D 目标检测任务中均优于现有最先进方法，且不依赖额外数据或标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自监督学习在 3D 点云表示学习中展现出巨大潜力，但由于几何不规则、重建易出现捷径以及语义分布不均衡等挑战，仍难以充分发挥其优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够克服上述挑战、提升 3D 点云自监督学习效果的鲁棒方法，并在语义分割与目标检测任务中实现领先性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1）仅在可观测点上蒸馏软图，防止遮挡区域信息泄漏；2）使用 Zipfian 原型并通过 Zipf-Sinkhorn 算法强制原型使用遵循幂律先验，调节软图的尖锐度；3）在无监督设置下实现对不平衡语义的有效处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DOS 在 nuScenes、Waymo、SemanticKITTI、ScanNet、ScanNet200 等多项基准上，语义分割和 3D 目标检测任务中均超过当前最先进方法，且不需要额外数据或标注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可观测点软图蒸馏为学习稳健 3D 表示提供了一种可扩展且有效的范式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近在自监督学习（SSL）方面的进展显示了在没有人工注释的情况下学习 3D 点云表示的巨大潜力。然而，针对 3D 点云的 SSL 仍面临由于几何不规则、易出现捷径的重建以及语义分布不平衡等关键挑战。在本研究中，我们提出了 DOS（Distilling Observable Softmaps），一种新颖的 SSL 框架，它仅在可观测（未遮挡）点上自蒸馏语义相关软图。该策略防止了来自遮挡区域的信息泄漏，并提供了比离散标记到原型分配更丰富的监督。为了解决无监督环境下语义不平衡的挑战，我们引入了 Zipfian 原型，并通过改进的 Sinkhorn-Knopp 算法 Zipf-Sinkhorn 强制原型使用遵循幂律先验，并在训练期间调节目标软图的尖锐度。DOS 在语义分割和 3D 目标检测的多个基准（包括 nuScenes、Waymo、SemanticKITTI、ScanNet 和 ScanNet200）上均优于当前最先进的方法，且不依赖额外数据或注释。我们的结果表明，可观测点软图蒸馏为学习稳健 3D 表示提供了一种可扩展且有效的范式。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决自监督学习在3D点云中的关键挑战，包括不规则几何导致的学习不稳定、重建方法易出现捷径学习以及语义分布极度不平衡的问题。解决这些问题对于在没有人工标注的情况下构建高质量、可迁移的3D表示至关重要，直接影响自动驾驶、机器人导航等实际应用的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在现有的掩码自监督和聚类方法基础上，发现这些方法仍受位置泄漏和均匀原型使用的限制。于是他们提出只在可见点上进行监督、使用语义软图（softmap）来捕捉空间竞争，并引入 Zipf‑Sinkhorn 以匹配真实的长尾语义分布。该设计借鉴了 SwAV 的 Sinkhorn 算法、掩码自监督框架以及 Zipf 定律的概念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学生‑教师框架，仅在学生能看到的可见点上对教师生成的 Zipf‑正则化软图进行蒸馏，从而避免位置泄漏并鼓励空间竞争。实现流程包括：①对点云做两视角增强并随机掩码；②学生仅处理可见点，教师处理完整点云；③两者计算与原型的相似度并归一化得到软图；④教师软图通过 Zipf‑Sinkhorn 进行长尾正则化；⑤用 KL 散度将学生软图对齐到教师软图；⑥更新学生参数并用 EMA 更新教师。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）可见点蒸馏，消除位置泄漏；2）语义软图蒸馏，提供空间竞争的学习信号；3）Zipf‑Sinkhorn 正则化，匹配长尾语义分布。与以往的重建或聚类方法不同，DOS 不依赖重建或均匀原型分配，而是通过软图和 Zipf 先验实现更丰富、更稳健的自监督学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DOS通过在可见点上蒸馏Zipf正则化的语义软图，消除位置泄漏并处理长尾语义，提供了一种无需标注即可学习鲁棒3D点云表示的新框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.&lt;/p&gt;</description></item><item><guid>2512.11811v2</guid><title>Enhancing Geo-localization for Crowdsourced Flood Imagery via LLM-Guided Attention</title><link>http://arxiv.org/abs/2512.11811v2</link><author>Fengyi Xu, Jun Ma, Waishan Qiu, Cui Guo, Jack C. P. Cheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出一种名为VPR‑AttLLM的框架，利用大型语言模型（LLM）的语义推理和地理知识，通过注意力引导的描述符增强，提升视觉地点识别（VPR）模型在社交媒体街景图像中的检索性能。该方法无需重新训练模型或额外数据，能够识别城市环境中的位置相关区域并抑制视觉噪声。实验表明，将VPR‑AttLLM与CosPlace、EigenPlaces和SALAD等先进VPR模型结合，可在多种基准上提升召回率，提升幅度通常为1–3%，在最具挑战性的真实洪水图像上可达8%。此外，该框架展示了LLM驱动的多模态融合的通用范式，并通过嵌入城市感知理论原理，桥接了人类空间推理与现代VPR架构。其即插即用、跨源鲁棒性强、可解释性好，具有可扩展的城市监测和快速危机图像地理定位潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 社交媒体提供的实时街景图像是城市洪水等危机事件的重要视觉证据，但往往缺乏可靠的地理元数据，导致传统的图像地理定位方法在此类图像上性能显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过整合LLM的语义推理与地理知识，改进VPR模型在跨源、视觉失真严重的社交媒体图像中的检索效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建VPR‑AttLLM框架，利用LLM识别城市上下文中的位置信息区域，并通过注意力机制增强VPR描述符，抑制视觉噪声；不需要模型再训练或额外数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在SF‑XL、合成洪水场景、Mapillary照片以及新建HK‑URBAN数据集上，VPR‑AttLLM与CosPlace、EigenPlaces、SALAD三种VPR模型结合，均实现了1–3%的召回率提升，最具挑战的真实洪水图像提升可达8%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; VPR‑AttLLM提供了一种通用、即插即用的LLM驱动多模态融合方案，显著提升跨源视觉检索性能，具有广泛应用于城市监测和危机响应的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 众包街景图像来自社交媒体，为城市洪水和其他危机事件提供实时视觉证据，但往往缺乏可靠的地理元数据以供紧急响应使用。现有的图像地理定位方法，也称为视觉地点识别（VPR）模型，在应用于此类图像时，由于视觉失真和跨源场景的域迁移，性能显著下降。本文提出了VPR‑AttLLM，一种模型无关的框架，通过注意力引导的描述符增强，将大型语言模型（LLM）的语义推理和地理知识整合到既有的VPR流程中。通过利用LLM识别城市语境中的位置信息区域并抑制视觉噪声，VPR‑AttLLM在不需要模型再训练或额外数据的情况下提升检索性能。我们在扩展的基准上进行了全面评估，包括使用真实社交媒体洪水图像增强的SF‑XL、在既定查询集和Mapillary照片上进行的合成洪水场景，以及捕捉形态学不同城市景观的新HK‑URBAN数据集。将VPR‑AttLLM与三种最先进的VPR模型——CosPlace、EigenPlaces和SALAD——结合，始终提升召回率，提升幅度通常在1–3%之间，在最具挑战性的真实洪水图像上可达8%。除了可测量的检索准确性提升，本研究还确立了LLM指导的多模态融合在视觉检索系统中的通用范式。通过将城市感知理论的原则嵌入注意力机制，VPR‑AttLLM将类人空间推理与现代VPR架构桥接。其即插即用设计、强大的跨源鲁棒性和可解释性凸显了其在可扩展城市监测和快速地理定位众包危机图像方面的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Crowdsourced street-view imagery from social media provides real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing image geo-localization approaches, also known as Visual Place Recognition (VPR) models, exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geo-knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.&lt;/p&gt;</description></item><item><guid>2512.11870v2</guid><title>Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT</title><link>http://arxiv.org/abs/2512.11870v2</link><author>Mulham Fawakherji, Bruce Race, Driss Benhaddou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文聚焦休斯顿市道路交通排放，提出通过扩大零排放车辆使用、提升公共交通和城市设计来降低行驶里程，力争到2050年实现净零排放。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 全球道路交通占温室气体排放的15%，导致约38.5万例早逝。城市贡献了全球能源相关排放的75%，休斯顿的道路排放占其基线排放的48%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 建立道路排放基线，并评估利用社会经济指标与智能交通系统加速零排放车辆采用、降低行驶里程的政策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过社会经济指标与智能交通系统分析，构建Unity 3D仿真环境进行动态城市移动性建模与政策情景可视化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 智能停车、公共交通激励、数据安全系统和零排放车队管理可提升交通模式分配与系统可靠性，并为政策制定提供可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 面向2050年排放目标的低密度、汽车依赖城市可借助本文提出的指标、度量与技术实现减排与零排放车辆推广。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文旨在通过建立道路排放基线并评估利用社会经济指标与智能交通系统加速零排放车辆采用、降低行驶里程的政策，以帮助低密度、汽车依赖城市实现2050年净零排放目标。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.&lt;/p&gt;</description></item><item><guid>2512.11872v1</guid><title>WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving</title><link>http://arxiv.org/abs/2512.11872v1</link><author>Mingwang Xu, Jiahao Cui, Feipeng Cai, Hanlin Shang, Zhihao Zhu, Shan Luan, Yifang Xu, Neng Zhang, Yaoyi Li, Jia Cai, Siyu Zhu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于可掩蔽扩散的端到端视觉语言动作模型WAM-Diff，用于自动驾驶轨迹生成，并在NAVSIM仿真平台上取得优异性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的自动驾驶系统多采用自回归大型语言模型或连续扩散策略，但离散可掩蔽扩散在轨迹生成中的潜力尚未被充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索并验证离散可掩蔽扩散在自动驾驶轨迹生成中的有效性，并提出一种可支持灵活、非因果解码顺序的VLA框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用可掩蔽扩散对离散轨迹序列进行迭代细化；使用稀疏MoE架构联合训练运动预测和视觉问答任务；通过Group Sequence Policy Optimization进行在线强化学习以优化序列级驾驶奖励。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; WAM-Diff在NAVSIM-v1上获得91.0 PDMS，在NAVSIM-v2上获得89.7 EPDMS，证明了可掩蔽扩散在自动驾驶中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可掩蔽扩散为自动驾驶轨迹生成提供了一个有前景的替代方案，支持情景感知的解码策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 端到端的基于视觉-语言-动作（VLA）模型的自动驾驶系统整合多模态传感器输入和语言指令，以生成规划和控制信号。虽然自回归大型语言模型和连续扩散策略很普遍，但离散可掩蔽扩散在轨迹生成中的潜力尚未得到充分探索。本文提出了WAM-Diff，一种使用可掩蔽扩散的VLA框架，迭代细化表示未来自车轨迹的离散序列。我们的方法具有三大创新：一是系统地将可掩蔽扩散适配于自动驾驶，支持灵活、非因果的解码顺序；二是通过稀疏的MoE架构实现可扩展的模型容量，并联合训练运动预测和面向驾驶的视觉问答（VQA）任务；三是使用Group Sequence Policy Optimization（GSPO）进行在线强化学习，以优化序列级驾驶奖励。值得注意的是，我们的模型在NAVSIM-v1上获得91.0 PDMS，在NAVSIM-v2上获得89.7 EPDMS，展示了可掩蔽扩散在自动驾驶中的有效性。该方法为自回归和扩散策略提供了有前景的替代方案，支持情景感知的轨迹生成解码策略。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff&lt;/p&gt;</description></item><item><guid>2512.11926v1</guid><title>TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder</title><link>http://arxiv.org/abs/2512.11926v1</link><author>Qinghao Meng, Chenming Wu, Liangjun Zhang, Jianbing Shen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种联合补全与检测框架，利用Transformer结构提升稀疏区域的检测特征，同时保持成本不变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D目标检测在自动驾驶中至关重要，但在远距离仅有少量激光点的区域检测仍是难点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过稠密化点云来解决点云稀疏问题，并提升稀疏区域的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计TransBridge变压器上采样块，将检测网络与补全网络的特征融合；构建DSRecon模块生成稠密激光点云；利用变压器建立通道与空间关系，得到高分辨率特征图用于补全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和Waymo数据集上实验表明，该框架在多种方法上平均精度提升0.7到1.5，且在两阶段检测框架中提升至5.78点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架具有良好的泛化能力，能够持续提升端到端3D目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D目标检测在自动驾驶中至关重要，为移动物体和障碍物提供关键信息。仅凭少量激光点在远距离检测对象仍是挑战，许多策略已被开发以通过稠密化解决点云稀疏问题。本文提出一种联合补全与检测框架，在稀疏区域提升检测特征，同时保持成本不变。具体而言，我们提出TransBridge，一种基于Transformer的上采样块，融合检测网络与补全网络的特征。检测网络可通过获取来自补全网络的隐式补全特征获益。此外，我们设计了Dynamic-Static Reconstruction（DSRecon）模块，为补全网络生成稠密激光点云，满足稠密点云地面真值的需求。进一步地，我们采用Transformer机制建立通道与空间关系，得到用于补全的高分辨率特征图。对nuScenes和Waymo数据集进行的大量实验表明，所提出的框架有效。结果显示，我们的框架在多种方法上平均精度提升0.7到1.5，表明其泛化能力。对于两阶段检测框架，它还将平均精度提升至5.78点。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在稀疏 LiDAR 点云中检测远距离物体的困难，尤其是由于不可见体素导致的误检和漏检。这个问题在自动驾驶中至关重要，因为车辆需要准确感知周围环境以做出安全决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将检测网络和点云补全网络共享编码器，并在解码阶段引入基于 Transformer 的上采样桥（TransBridge）来融合两者特征。方法借鉴了 CenterPoint、VoxelNet 等检测框架和现有点云补全技术，同时改进了稀疏控制和稠密标签生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练检测和补全网络，让补全网络提供隐含的结构信息来提升检测特征。流程包括：1）共享编码器提取多尺度特征；2）TransBridge 上采样并解释特征，生成体素存在预测；3）使用 DSRecon 生成稠密点云标签；4）完成网络监督下的反向传播，最终得到更准确的检测结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）TransBridge 作为 Transformer 上采样桥，兼顾特征上采样和语义桥接；2）DSRecon 模块提供无拖尾的稠密标签；3）稀疏控制模块（SCM）保持高分辨率特征的稀疏性；4）整体框架在保持推理速度的同时提升检测精度。与以往需要额外推理成本的稠密化方法不同，该方法在训练阶段完成补全，推理时无额外开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TransBridge 通过 Transformer 上采样桥实现检测与点云补全的联合训练，在稀疏 LiDAR 数据上显著提升 3D 检测精度，而不增加推理成本。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.&lt;/p&gt;</description></item><item><guid>2512.12377v1</guid><title>INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset</title><link>http://arxiv.org/abs/2512.12377v1</link><author>Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; INDOOR-LIDAR 是一套综合性的室内 3D LiDAR 点云混合数据集，旨在推动机器人感知研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有室内 LiDAR 数据集规模有限、注释格式不统一、采集过程受人工影响导致变异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过结合仿真环境与真实机器人扫描，提供一致覆盖、可控变异的高质量点云数据，填补现有数据集的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采集密集点云并记录强度值，采用 KITTI 风格注释，包含常见室内物体类别；仿真子集支持灵活布局、点密度和遮挡配置；真实子集记录真实噪声、杂物和特定域伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该数据集兼具可配置的仿真特性和真实世界噪声，支持 3D 目标检测、鸟瞰图感知、SLAM、语义场景理解以及仿真与真实域适配等多种应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; INDOOR-LIDAR 通过桥接合成与真实数据，提供可扩展、真实、可复现的基准，推动复杂室内环境下机器人感知技术进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 INDOOR-LIDAR，一套全面的室内 3D LiDAR 点云混合数据集，旨在推动机器人感知研究。现有室内 LiDAR 数据集往往规模有限、注释格式不一致、采集过程中存在人为变异。INDOOR-LIDAR 通过将仿真环境与使用自主地面机器人获取的真实扫描相结合，提供一致的覆盖范围和在受控变异下的真实传感器行为。每个样本包含密集点云数据，配有强度测量和 KITTI 风格注释。注释方案涵盖各种场景中的常见室内物体类别。仿真子集支持灵活配置布局、点密度和遮挡，而真实子集则捕捉真实传感器噪声、杂物和特定域伪影。INDOOR-LIDAR 支持 3D 目标检测、鸟瞰图感知、SLAM、语义场景理解以及仿真与真实室内域之间的域适配等广泛应用。通过弥合合成与真实世界数据之间的差距，INDOOR-LIDAR 建立了一个可扩展、真实且可复现的基准，推动复杂室内环境下机器人感知技术的进步。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决现有室内 LiDAR 数据集规模小、注释不统一、采集方式受限（如手持导致盲区）等问题，缺乏可直接用于移动机器人 360° 全景感知的数据。此问题重要，因为高质量、完整的机器人视角数据是训练和评估 SLAM、目标检测、语义分割等关键感知算法的基础，且能有效缩小模拟与真实环境之间的差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合现有室内 LiDAR 数据集（如 LiDAR‑Net、3DSES）与模拟平台（NVIDIA Isaac Sim、Unity 等），提出将真实机器人采集与程序化生成的虚拟环境相结合的混合数据集。通过在机器人上安装全景 LiDAR，消除手持盲区，并采用统一的 KITTI‑style 注释格式，确保数据在真实与仿真之间保持一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个机器人视角的 360° LiDAR 混合数据集，包含真实扫描、合成扫描、强度信息和 3D 边界框。实现流程包括：① 在真实环境中使用自主地面机器人采集 LiDAR 点云；② 在仿真平台中生成可配置的室内场景并同步采集；③ 通过 ROS2 管道统一处理数据，生成点云、强度图、3D 边界框、轨迹等多模态信息；④ 提供标准数据划分和基线评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 混合真实与合成数据，提供可扩展的室内场景；② 机器人安装的 360° LiDAR 采集，消除盲区；③ 统一的 intensity、点级注释和 KITTI‑style 3D 边界框；④ 支持多任务（检测、BEV、SLAM 等）并提供基线。与以往仅提供手持扫描或单一真实/合成数据集的工作不同，INDOOR‑LiDAR 通过机器人视角和完整覆盖显著提升了数据的实用性和可迁移性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; INDOOR‑LiDAR 提供了一个机器人视角、360° 全景、带强度和统一 3D 注释的混合室内 LiDAR 数据集，弥合了模拟与真实之间的差距，为多任务感知研究提供了可扩展、可复现的基准。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird&amp;#x27;s-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.&lt;/p&gt;</description></item><item><guid>2512.12717v1</guid><title>HMPCC: Human-Aware Model Predictive Coverage Control</title><link>http://arxiv.org/abs/2512.12717v1</link><author>Mattia Catellani, Marta Gabbi, Lorenzo Sabattini</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种人类感知的覆盖框架HMPCC，利用MPC预测人类运动，实现机器人团队在未知环境中的安全覆盖与协同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统覆盖方法假设已知或凸环境、静态密度，难以适应包含人类的真实场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决机器人团队在未知环境中安全覆盖并避免与非合作主体碰撞的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用基于MPC的HMPCC框架，将人类运动预测融入规划；环境用高斯混合模型表示；团队成员完全去中心化，无显式通信。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 人类轨迹预测提升了覆盖效率和适应性，改善了人机协同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 人类感知的MPC框架在未知环境中实现了更高效、适应性更强的覆盖，适用于通信受限或敌对环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究了在未知环境中协调机器人团队进行覆盖，同时确保安全操作并避免与非合作主体碰撞的问题。传统覆盖策略往往依赖简化假设，如已知或凸环境以及静态密度函数，难以适应现实场景，尤其是涉及人类时。在此工作中，我们提出了基于模型预测控制的以人为中心的覆盖框架HMPCC，将人类运动预测整合到规划过程中。通过在MPC时域内预测人类轨迹，机器人能够主动协调行动，避免冗余探索，并适应动态条件。环境被建模为高斯混合模型，表示感兴趣区域。团队成员以完全去中心化的方式运作，不依赖显式通信，这在敌对或通信受限的场景中尤为重要。我们的结果表明，人类轨迹预测使覆盖更高效、更具适应性，提升了人机协同。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.&lt;/p&gt;</description></item><item><guid>2512.12884v1</guid><title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title><link>http://arxiv.org/abs/2512.12884v1</link><author>Xiangzhong Liu, Jiajie Zhang, Hao Shen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种端到端的跨层融合方法，利用Transformer将智能传感器和V2X模块产生的对象列表与原始摄像头图像结合，用于三维目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 汽车传感器融合系统常使用智能传感器和V2X模块，获取的数据通常是处理后的对象列表，而非传统传感器的原始数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 避免分别处理原始数据后再在对象层面融合，提出一种直接融合不同层次信息的方案，以提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将对象列表作为去噪查询输入Transformer，并与可学习查询一起进行特征聚合；在解码器中加入基于对象列表位置和尺寸先验的可变形高斯掩模，引导注意力并加速训练；通过模拟噪声和误检误漏生成伪对象列表，以弥补缺乏公开数据集的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在nuScenes数据集上相较于仅基于视觉的基线显著提升性能，并能在不同噪声水平和真实检测器下保持良好泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 跨层融合的Transformer框架能够有效结合高层抽象信息与低层原始图像，提升三维目标检测精度，并具有良好的适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在汽车传感器融合系统中，智能传感器和V2X模块被广泛使用。来自这些系统的传感器数据通常仅以处理后的对象列表形式提供，而不是传统传感器的原始数据。与其分别处理其他原始数据然后在对象层面进行融合，我们提出了一种基于Transformer的端到端跨层融合概念，将高度抽象的对象列表信息与原始摄像头图像结合用于三维目标检测。对象列表被作为去噪查询输入Transformer，并与可学习查询一起通过后续特征聚合过程传播。此外，基于对象列表的位置信息和尺寸先验得到的可变形高斯掩模被显式集成到Transformer解码器中，指导注意力聚焦于目标感兴趣区域并加速模型训练收敛。由于没有公开数据集包含单独的对象列表作为模态，我们提出了一种通过模拟状态噪声以及假阳性和假阴性从真实边界框生成伪对象列表的方法。作为首个进行跨层融合的工作，我们的方法在nuScenes数据集上相较于基于视觉的基线表现出显著的性能提升，并展示了对模拟对象列表不同噪声水平以及真实检测器的泛化能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在自动驾驶中，如何将仅提供对象列表的高层感知信息与原始相机图像低层数据进行融合，以提升3D目标检测的精度。由于V2X和智能传感器只能输出对象列表，传统的低层融合难以实现，故需要一种能在端到端模型中同时利用高层和低层信息的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于DETR类的3D检测框架，借鉴了DN-DETR的查询去噪技术和SMCA的空间加权注意力机制，设计了将对象列表转化为去噪查询并与可学习查询拼接的方式，并在Transformer解码器中加入可变形高斯掩码。为缺乏公开对象列表数据，作者还提出了伪对象列表生成模块来模拟不同噪声水平的检测器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将高层对象列表信息作为去噪查询和注意力掩码，直接注入Transformer解码器，实现端到端的跨层融合。实现流程为：相机图像通过骨干网络提取2D特征，生成3D坐标并编码为位置感知特征；对象列表被转换为去噪查询并与学习查询拼接；在解码器中使用去噪查询与位置感知特征进行自注意力和交叉注意力，交叉注意力中加入基于对象列表的可变形高斯掩码；最后将解码器输出的嵌入送入检测头得到3D边界框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次提出跨层融合的Transformer框架，将高层对象列表与低层图像特征在同一网络中联合学习；2) 采用查询去噪技术将对象列表直接作为去噪查询，提升鲁棒性；3) 引入可变形高斯掩码显式引导注意力，聚焦目标区域；4) 伪对象列表生成模块为无公开数据的场景提供训练数据；5) 该模块可无额外参数、无额外计算地插拔。与以往仅在高层或低层进行融合的工作不同，本文实现了端到端的跨层融合并显著提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了首个端到端Transformer框架，通过查询去噪和可变形高斯注意力将高层对象列表与原始相机图像进行跨层融合，显著提升了nuScenes上的3D目标检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.&lt;/p&gt;</description></item><item><guid>2512.13107v2</guid><title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title><link>http://arxiv.org/abs/2512.13107v2</link><author>Zhijian He, Feifei Liu, Yuwei Li, Zhanpeng Luo, Jintao Cheng, Xieyuanli Chen, Xiaoyu Tang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出DiffFusion框架，通过扩散模型恢复受天气影响的图像和点云，并使用双向自适应融合与对齐模块实现多模态融合，显著提升在恶劣天气下的三维目标检测鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态3D目标检测在机器人和自动驾驶中至关重要，但在恶劣天气下由于天气导致的失真和不同模态间的对齐问题，效果受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提升在恶劣天气条件下多模态3D目标检测的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Diffusion-IR图像恢复、PCR点云恢复、BAFAM双向自适应融合与对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个公开数据集上，DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时在清洁数据上保持强劲性能；在真实DENSE数据集的零样本测试也验证了其泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DiffFusion通过扩散模型恢复和自适应融合显著提升了恶劣天气下的检测性能，并具备良好的泛化性，且实现将开源发布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper proposes the DiffFusion framework, which uses diffusion models to restore images and point clouds affected by weather, and employs a bidirectional adaptive fusion and alignment module to achieve multimodal fusion, significantly improving the robustness of 3D object detection in adverse weather conditions.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升多模态3D目标检测在恶劣天气下的鲁棒性。恶劣天气会导致图像模糊、LiDAR点稀疏以及跨模态对齐失效，严重影响自动驾驶和机器人感知的安全性。解决这一问题有助于在真实环境中实现可靠的感知系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为扩散模型在去噪和生成方面表现优异，可用于统一处理多种天气退化。基于此，他们设计了Diffusion-IR和PCR两支分支，并引入BAFAM进行跨模态对齐。该方案借鉴了现有的扩散恢复、点云补全以及BEV融合方法，并在此基础上提出了新的模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用扩散模型恢复被天气破坏的图像和LiDAR点云，再通过自适应融合与对齐实现跨模态信息的有效整合。实现流程为：输入受损图像和点云 → Diffusion-IR恢复图像并提取特征 → 2D检测生成框 → PCR利用图像框和深度信息补全点云 → BAFAM进行特征融合与BEV对齐 → 3D检测头输出目标框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) Diffusion-IR对图像进行条件扩散恢复；2) PCR利用图像提示进行点云补全；3) BAFAM实现双向自适应融合与BEV对齐。与以往仅在单模态或简单融合的工作不同，DiffFusion在同一框架下同时解决图像去噪、点云补全和跨模态对齐，显著提升了恶劣天气下的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DiffFusion通过扩散式恢复与自适应跨模态融合，实现在恶劣天气下的鲁棒3D目标检测，并在公开数据集上刷新了性能记录。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird&amp;#x27;s-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.&lt;/p&gt;</description></item><item><guid>2512.13903v1</guid><title>PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration</title><link>http://arxiv.org/abs/2512.13903v1</link><author>Sibo Tian, Minghui Zheng, Xiao Liang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的预测-细化框架，用于实时、真实且考虑人机交互的随机人类运动预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在工业再制造的安全高效人机协作中，随机人类运动预测能够捕捉运动不确定性和多模态行为，传统确定性方法无法处理。早期研究强调多样化预测但往往产生不现实的运动；近期方法注重精度和实时性，但仍有提升空间；现有研究多忽视机器人运动对人类行为的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补上述研究空白，构建一种能够实时、真实且考虑机器人运动影响的预测方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用预训练的先进预测器生成初始预测，然后通过一个利用流匹配结构的细化模块，结合人类和机器人观测运动来修正预测，从而保留不确定性和多模态特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在HRC桌面拆解数据集上实验表明，该方法显著提升了预测精度，同时保持了不确定性和多模态性，且总推理时间仍在预算内。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提框架在保持实时性和不确定性保留的前提下，显著提升了人机协作中的运动预测质量，具有实用性和有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随机人类运动预测对于工业再制造中的安全有效人机协作至关重要，因为它能够捕捉人类运动的不确定性和多模态行为，而确定性方法无法处理。早期工作强调高度多样化的预测，但往往生成不现实的人类运动。最近的方法侧重于精度和实时性能，然而在不超过时间预算的前提下，预测质量仍有提升空间。此外，当前关于人机协作中随机人类运动预测的研究通常只考虑人类运动，忽略了机器人运动对人类行为的影响。为了解决这些研究空白并实现实时、真实且具交互感知的人类运动预测，我们提出了一种新颖的预测-细化框架，该框架将人类和机器人观测运动整合，以细化预训练的先进预测器产生的初始预测。细化模块采用流匹配结构来考虑不确定性。对HRC桌面拆解数据集的实验表明，我们的方法显著提高了预测精度，同时保留了人类运动的不确定性和多模态性。更重要的是，所提框架的总推理时间仍在时间预算内，凸显了其有效性和实用性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.&lt;/p&gt;</description></item><item><guid>2512.14161v1</guid><title>Transfer Learning-Based Surrogate Modeling for Nonlinear Time-History Response Analysis of High-Fidelity Structural Models</title><link>http://arxiv.org/abs/2512.14161v1</link><author>Keiichi Ishikawa, Yuma Matsumoto, Taro Yaoyama, Sangwon Lee, Tatsuya Itoi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于迁移学习的框架，用低精度响应模型作为预训练模型，快速构建高精度非线性时程响应的代理模型，从而在性能基础地震工程中显著降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在性能基础地震工程中，需要对大量地震动进行非线性时程响应分析以评估建筑物的地震风险，但传统数值模拟计算量大，限制了实际应用。以往的机器学习研究多聚焦于低精度单自由度模型，缺乏高精度代理模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在训练样本极少的情况下构建高精度响应分析代理模型的方法，以降低数据集构建的计算开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用迁移学习框架：先训练低精度响应代理模型，再将其知识迁移到高精度代理模型中；在20层钢框架案例中，仅用20个训练样本完成高精度代理模型的构建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 迁移学习得到的高精度代理模型在预测地震动响应时与基于现场时程的危害结果保持一致，证明了方法的可行性和高效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 迁移学习能够有效构建高精度响应分析代理模型，显著降低计算成本，为性能基础地震工程提供了实用的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在性能基础地震工程框架中，需要对大量地震动进行非线性时程响应分析以评估建筑物或土木工程结构的地震风险。然而，这类数值模拟计算量大，限制了框架的实际应用。为了解决此问题，先前的研究使用机器学习以低计算成本预测结构对地震动的响应。这些研究通常对数百个地震动进行非线性时程分析，并利用结果训练和验证代理模型。然而，大多数先前研究聚焦于计算成本低的响应分析模型，如单自由度。需要高精度响应分析的代理模型，以丰富用于PBEE损伤评估的信息数量和多样性。值得注意的是，若响应分析模型的精度提高，创建训练和验证数据集的计算成本也会增加。因此，需要能够在不使用大量训练样本的情况下实现高精度响应分析代理建模的方法。本文提出一种使用迁移学习构建高精度响应分析代理模型的框架。该框架使用低精度响应分析的代理模型作为预训练模型，并将其知识迁移以构建高精度响应分析的代理模型，显著降低计算成本。作为案例研究，使用仅20个样本作为训练数据集构建了预测20层钢框架响应的代理模型。构建的代理模型预测的地震动响应与基于现场时程的危害一致。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In a performance based earthquake engineering (PBEE) framework, nonlinear time-history response analysis (NLTHA) for numerous ground motions are required to assess the seismic risk of buildings or civil engineering structures. However, such numerical simulations are computationally expensive, limiting the real-world practical application of the framework. To address this issue, previous studies have used machine learning to predict the structural responses to ground motions with low computational costs. These studies typically conduct NLTHAs for a few hundreds ground motions and use the results to train and validate surrogate models. However, most of the previous studies focused on computationally-inexpensive response analysis models such as single degree of freedom. Surrogate models of high-fidelity response analysis are required to enrich the quantity and diversity of information used for damage assessment in PBEE. Notably, the computational cost of creating training and validation datasets increases if the fidelity of response analysis model becomes higher. Therefore, methods that enable surrogate modeling of high-fidelity response analysis without a large number of training samples are needed. This study proposes a framework that uses transfer learning to construct the surrogate model of a high-fidelity response analysis model. This framework uses a surrogate model of low-fidelity response analysis as the pretrained model and transfers its knowledge to construct surrogate models for high-fidelity response analysis with substantially reduced computational cost. As a case study, surrogate models that predict responses of a 20-story steel moment frame were constructed with only 20 samples as the training dataset. The responses to the ground motions predicted by constructed surrogate model were consistent with a site-specific time-based hazard.&lt;/p&gt;</description></item><item><guid>2512.15581v1</guid><title>IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</title><link>http://arxiv.org/abs/2512.15581v1</link><author>Shashank Mishra, Karan Patil, Didier Stricker, Jason Rambach</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于多层知识蒸馏的雷达-相机融合框架IMKD，利用雷达和相机的互补特性实现高性能3D目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的知识蒸馏方法直接将模态特征传递给各传感器，容易扭曲其独特特征，削弱各自优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不使用激光雷达推理的情况下，通过知识蒸馏提升雷达-相机3D检测性能，同时保持各模态的内在特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; IMKD采用三阶段强度感知蒸馏：1）雷达特征蒸馏增强细粒度结构信息；2）融合层蒸馏突出几何深度信息，促进模态互补；3）相机-雷达强度引导融合机制实现特征对齐与校准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes基准上，IMKD取得67.0% NDS和61.0% mAP，优于现有所有基于蒸馏的雷达-相机融合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多层强度感知蒸馏能够有效保留各模态特性并增强互补性，从而显著提升雷达-相机3D检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 通过知识蒸馏，在推理时不使用激光雷达即可实现高性能的雷达-相机3D目标检测。然而，现有的蒸馏方法通常将模态特定特征直接传递给每个传感器，这可能扭曲它们的独特特性并削弱各自的优势。为了解决这一问题，我们提出了IMKD，一种基于多层知识蒸馏的雷达-相机融合框架，能够在保留每个传感器内在特性的同时放大它们的互补优势。IMKD采用三阶段强度感知蒸馏策略，在整个架构中丰富融合表示：(1) 激光雷达到雷达的强度感知特征蒸馏，用细粒度结构线索增强雷达表示；(2) 激光雷达到融合特征的强度引导蒸馏，在融合层选择性突出有用的几何和深度信息，促进模态之间的互补，而不是强迫它们对齐；(3) 相机-雷达强度引导融合机制，促进有效的特征对齐与校准。对nuScenes基准的广泛实验表明，IMKD达到了67.0% NDS和61.0% mAP，优于所有先前基于蒸馏的雷达-相机融合方法。我们的代码和模型可在 https://github.com/dfki-av/IMKD/ 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升仅使用相机和雷达的三维目标检测性能，避免在推理时使用昂贵且范围有限的 LiDAR。此问题重要，因为成本效益高、在恶劣天气和长距离场景下仍能保持可靠感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统跨模态知识蒸馏往往强迫雷达或相机特征模仿 LiDAR，导致特征冲突。借鉴了 LiDAR‑to‑Camera、Fusion‑based KD 等现有工作，提出在多级、强度感知的框架中引入 LiDAR 作为特权教师，利用其强度信息作为置信度先验，指导雷达和融合特征的学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过强度感知的多级知识蒸馏，既保留每个传感器的固有特性，又增强其互补优势。实现流程包括：提取相机和雷达特征并升维到 BEV；使用强度感知的可变形注意力进行相机‑雷达融合；在三个阶段分别进行 LiDAR‑to‑Radar、LiDAR‑to‑Fused 以及 Camera‑Radar 的强度引导蒸馏；同时加入标签蒸馏和一致性损失；训练完成后仅使用相机和雷达进行推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 强度感知的多级蒸馏，利用 LiDAR 强度作为置信度先验；2) 在融合空间而非单一模态中进行蒸馏，提升空间推理；3) 引入强度感知的相机‑雷达融合模块；4) 通过结构化标签监督增强鲁棒性。与以往仅做模态对模态或融合对融合蒸馏的工作不同，IMKD 在保持各模态特性同时实现更高效的互补学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IMKD 通过强度感知的多级知识蒸馏，既保留相机和雷达的独特优势，又显著提升其融合性能，达成无 LiDAR 的三维检测新标杆。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor&amp;#x27;s intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.&lt;/p&gt;</description></item><item><guid>2512.15957v1</guid><title>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</title><link>http://arxiv.org/abs/2512.15957v1</link><author>Utsav Panchal, Yuchen Liu, Luigi Palmieri, Ilche Georgievski, Marco Aiello</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于视觉语言模型的多人体行为预测框架CAMP-VLM，利用视觉上下文和场景图的空间信息来提升从第三人称视角预测多人体与场景交互的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 移动机器人在有人类的环境中运行时，需要准确预测人类行为。现有研究多聚焦于单人、第一人称视角的行为预测，而多人体、第三人称视角的预测需求尚未得到充分研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够从观察者视角预测多人体行为的模型，并验证其在合成与真实数据上的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建CAMP-VLM框架，结合视觉输入的上下文特征和场景图的空间信息；使用光照逼真的模拟器生成合成多人体行为数据进行监督微调和直接偏好优化；在合成与真实序列上评估模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通过监督微调和直接偏好优化，CAMP-VLM在预测准确率上比最优基线提升高达66.9%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CAMP-VLM在第三人称视角下的多人体行为预测任务中表现优异，证明了视觉语言模型结合场景图和合成数据的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确预测人类行为对于在有人类环境中运行的移动机器人至关重要。虽然先前的研究主要关注从第一人称视角预测单人情境下的动作，但多种机器人应用需要从第三人称视角理解多人体行为。为此，我们提出了CAMP-VLM（Context-Aware Multi-human behavior Prediction）：一种基于视觉语言模型的框架，结合视觉输入的上下文特征和场景图的空间意识，以提升对人类-场景交互的预测。由于缺乏适用于从观察者视角进行多人体行为预测的数据集，我们使用光照逼真的模拟器生成合成的人体行为数据，对CAMP-VLM进行微调，并在合成和真实序列上评估模型，以检验其泛化能力。利用监督微调（SFT）和直接偏好优化（DPO），CAMP-VLM在预测准确率上比最佳基线提升高达66.9%。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.&lt;/p&gt;</description></item><item><guid>2512.16077v1</guid><title>Auto-Vocabulary 3D Object Detection</title><link>http://arxiv.org/abs/2512.16077v1</link><author>Haomeng Zhang, Kuan-Chuan Peng, Suhas Lohit, Raymond A. Yeh</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自动生成类别的3D目标检测方法AV3DOD，利用2D视觉语言模型生成语义候选，并通过图像描述、伪3D框生成和特征空间语义扩展实现高质量检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的开放词汇3D目标检测方法虽然能定位未见类别，但仍需用户在训练和推理时指定类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究一种无需用户输入即可自动生成检测对象类别的自动词汇3D目标检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入语义得分评估生成类别名称的质量，构建AV3DOD框架，利用2D视觉语言模型进行图像描述、伪3D框生成和特征空间语义扩展，生成丰富的语义候选。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ScanNetV2和SUNRGB-D数据集上，AV3DOD在定位和语义质量上均达到最先进水平，整体平均精度比CoDA高3.48，ScanNetV2上的语义质量提升24.5%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AV3DOD实现了无需用户输入即可自动生成类别的3D目标检测，并在多个数据集上显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 开放词汇3D目标检测方法能够定位训练期间未见过类别的3D框。尽管名称如此，现有方法在训练和推理时仍依赖用户指定的类别。我们提出研究自动词汇3D目标检测（AV3DOD），在检测对象时无需任何用户输入即可自动生成类别。为此，我们引入语义得分来评估生成类别名称的质量。随后，我们开发了一种新框架AV3DOD，利用2D视觉语言模型通过图像描述、伪3D框生成和特征空间语义扩展生成丰富的语义候选。AV3DOD在ScanNetV2和SUNRGB-D数据集上在定位（平均精度）和语义质量上均实现了最先进的性能。值得注意的是，它在ScanNetV2上比最先进方法CoDA整体平均精度高3.48，并在语义质量上相对提升24.5%。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 它解决了现有开放词汇3D检测仍需用户指定词汇表的问题，使检测器能够自动生成检测到物体的类别标签，从而实现真正的开放世界感知。该能力对机器人、自动驾驶和场景理解等需要识别未知物体的应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在开放词汇3D检测基础上，引入语义得分（Semantic Score）评估指标，并利用2D视觉语言模型（VLM）进行图像描述、伪3D框生成和特征空间语义扩展。框架借鉴了3DETR、CoDA、CLIP等已有技术，并结合了基于文本的语义对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合2D VLM产生的语义候选和特征空间扩展，构建丰富的词汇特征集合，然后将检测到的3D物体特征与该集合对齐以预测自由形式的类别标签。实现流程包括：①使用3DETR生成无类别的3D框和特征；②通过图像描述、伪标签和特征扩展构建超级词汇特征集合；③将每个框的特征与词汇集合匹配，得到类别预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①提出Auto‑Vocabulary 3D Object Detection任务；②设计Semantic Score评估指标；③利用2D VLM进行图像描述和伪3D框生成；④引入特征空间语义扩展以捕获非离散词汇概念；⑤在推理时不需要用户词汇表或2D输入。与以往开放词汇3D检测方法相比，AV3DOD能够自主构建词汇并仅依赖点云进行检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出AV3DOD框架，利用2D视觉语言模型和特征空间语义扩展，使3D检测器能够自动发现并命名物体，且在不需要预定义词汇表的情况下实现领先的检测与语义质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.&lt;/p&gt;</description></item><item><guid>2512.16454v1</guid><title>AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems</title><link>http://arxiv.org/abs/2512.16454v1</link><author>Tianhao Shao, Kaixing Zhao, Feng Liu, Lixin Yang, Bin Guo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了MPBS框架，利用设备行为识别与移动预测实现无人系统的任务调度，显著提升任务完成效率与资源利用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 无人机和无人地面车辆在城市感知与应急响应等领域日益重要，如何高效招募这些自主设备完成时间敏感任务成为关键挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种可扩展的任务招募框架MPBS，以更好地匹配设备与任务需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MPBS包含行为感知的KNN分类器、用于预测设备移动的时变马尔可夫模型以及考虑任务紧急度和基站性能的动态优先级调度机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在GeoLife真实数据集上的实验表明，MPBS显著提升了任务完成效率和资源利用率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MPBS提供了一种预测性、行为感知的解决方案，可实现无人系统的智能协同调度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着无人机（UAV）和无人地面车辆（UGV）在城市感知和应急响应等应用中的重要性日益提升，高效招募这些自主设备完成时间敏感任务已成为关键挑战。本文提出了MPBS（基于移动预测与行为感知的调度）框架，该框架将每个设备视为可招募的“用户”，并实现可扩展的任务招募。MPBS集成了三个关键模块：行为感知的KNN分类器、用于预测设备移动的时变马尔可夫模型，以及考虑任务紧急度和基站性能的动态优先级调度机制。通过将行为分类与时空预测相结合，MPBS能够实时将任务动态分配给最合适的设备。对真实 GeoLife 数据集的实验评估表明，MPBS显著提升了任务完成效率和资源利用率。该框架为无人系统提供了一种预测性、行为感知的智能协同调度解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable &amp;quot;user&amp;quot;. MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.&lt;/p&gt;</description></item><item><guid>2512.16784v1</guid><title>R3ST: A Synthetic 3D Dataset With Realistic Trajectories</title><link>http://arxiv.org/abs/2512.16784v1</link><author>Simone Teglia, Claudia Melis Tonti, Francesco Pro, Leonardo Russo, Andrea Alfarano, Leonardo Pentassuglia, Irene Amerini</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; R3ST 数据集通过在合成 3D 环境中嵌入真实车辆轨迹，提供了既具备高精度多模态标注又具备真实驾驶行为的合成数据，弥补了传统合成数据缺乏真实运动轨迹的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 交通分析与道路安全需要大量数据来训练和评估计算机视觉模型。真实数据虽然能捕捉真实道路对象行为，但往往缺乏精确的地面真值标注；合成数据可以无成本大量标注，但其车辆运动轨迹往往不够真实。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种既能提供真实车辆运动轨迹又能提供精确多模态标注的合成数据集，以促进道路车辆轨迹预测研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在合成 3D 环境中生成场景，并将来自 SinD 无人机视角数据集的真实轨迹嵌入其中，从而得到 R3ST 数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; R3ST 数据集成功弥合了合成数据与真实轨迹之间的差距，为道路车辆轨迹预测提供了更可靠的数据基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; R3ST 提供了准确的多模态地面真值标注和真实人类驾驶车辆轨迹，推动了轨迹预测研究的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird&amp;#x27;s-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird&amp;#x27;s-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.&lt;/p&gt;</description></item><item><guid>2512.16791v1</guid><title>KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</title><link>http://arxiv.org/abs/2512.16791v1</link><author>Shuting Zhao, Zeyu Xiao, Xinrong Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为KineST的运动学引导状态空间模型，用于在AR/VR环境中从头戴显示器获取的稀疏信号重建全身姿态。该方法通过双向扫描和混合时空表示学习，结合局部与全局姿态感知，并加入几何角速度损失，显著提升了姿态重建的准确性和时间连贯性，同时保持轻量化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 全身运动跟踪是增强现实与虚拟现实应用的关键技术，能够实现物理与虚拟交互。然而，利用头戴显示器提供的稀疏信号重建逼真多样的全身姿态十分困难，现有方法往往计算量大或需分别建模空间与时间依赖，难以兼顾精度、时间连贯性与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新型的运动学引导状态空间模型KineST，以高效、准确且时间连贯的方式从稀疏信号中重建全身姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; KineST在状态空间双重性框架下采用运动学引导的双向扫描，嵌入关节运动学先验；同时使用混合时空表示学习，将空间与时间上下文紧密耦合；并引入几何角速度损失，约束旋转变化的物理合理性，从而提升运动稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，KineST在轻量化框架下，在准确性和时间一致性方面均优于现有方法，显示出更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; KineST通过整合运动学先验、双向扫描和混合时空学习，有效平衡了精度、时间连贯性与计算效率，为AR/VR中的全身姿态重建提供了可行且高效的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/&lt;/p&gt;</description></item><item><guid>2512.16818v1</guid><title>DenseBEV: Transforming BEV Grid Cells into 3D Objects</title><link>http://arxiv.org/abs/2512.16818v1</link><author>Marius Dähling, Sebastian Krebs, J. Marius Zöllner</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用BEV特征单元直接作为锚点的端到端多摄像头3D目标检测方法，并通过两阶段锚点生成、BEV基非极大值抑制以及混合时序建模实现性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 当前多摄像头3D检测研究中，BEV基变压器被广泛使用，传统模型采用随机查询作为锚点，最近的进展则用辅助网络检测结果替代随机查询。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在提供一种更直观、高效的锚点生成方式，利用BEV网格密集查询单元直接作为潜在目标，从而简化训练流程并提升检测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出两阶段锚点生成；使用BEV基非极大值抑制减少注意力规模问题；将BEV特征直接作为对象查询，天然嵌入时序信息；进一步结合先前检测结果进行混合时序建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，DenseBEV在NDS和mAP上均显著优于基线，尤其在稀疏BEV网格下仍保持优势；对小目标（行人）检测提升3.8% mAP；在Waymo上LET-mAP提升8%；在Waymo Open数据集上达到60.7% LET-mAP，超过前一最佳5.4%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DenseBEV通过直接使用BEV特征单元作为锚点，并结合BEV基NMS与混合时序建模，能够在保持训练效率的同时显著提升多摄像头3D检测性能，尤其对小目标检测效果突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在当前研究中，基于鸟瞰图（BEV）的变压器越来越多地用于多摄像头3D目标检测。传统模型通常使用随机查询作为锚点，并逐步优化它们。最近的进展补充或替代了这些随机查询，使用辅助网络的检测结果。我们提出了一种更直观、高效的方法，直接使用BEV特征单元作为锚点。该端到端方法利用BEV查询的稠密网格，将每个单元视为最终检测任务的潜在目标。因此，我们引入了一种专门为多摄像头3D目标检测设计的两阶段锚点生成方法。为了解决大量查询时注意力的缩放问题，我们应用BEV基非极大值抑制，只让梯度通过未被抑制的对象流动，从而实现高效训练而无需后处理。通过将来自BEVFormer等编码器的BEV特征直接用作对象查询，时序BEV信息天然嵌入。基于已嵌入的时序BEV信息，我们通过整合先前检测结果引入混合时序建模，以进一步提升检测性能。在nuScenes数据集上评估我们的方法，显示出相对于基线在NDS和mAP上的持续显著提升，即使使用更稀疏的BEV网格和更少的初始锚点。它对小目标尤其有效，在nuScenes上行人检测的mAP提升了3.8%，在Waymo上LET-mAP提升了8%。将我们的DenseBEV方法应用于具有挑战性的Waymo Open数据集，取得了LET-mAP 60.7%的最先进性能，超过之前最佳5.4%。代码可在 https://github.com/mdaehl/DenseBEV 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在改进多摄像头 3D 目标检测中的锚点生成方式。传统方法使用随机锚点或辅助网络产生锚点，导致锚点质量受限、计算开销大，尤其难以检测小目标。准确检测车辆、行人等多种目标对自动驾驶和机器人安全至关重要，因此需要更高效、更可靠的锚点策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 BEVFormer 的 BEV 编码器、Deformable DETR 的两阶段锚点初始化以及 DDQ 的 NMS 过滤思想，并结合 Stream-PETR 的时间记忆机制。通过将 BEV 网格细胞直接作为锚点，并在训练中加入 BEV‑NMS，作者实现了无需辅助网络的密集锚点生成，并在此基础上融合历史检测信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是把 BEV 编码器输出的网格细胞当作初始锚点，先用辅助检测头得到候选框，再用 BEV‑NMS 去除冗余，最后将保留的锚点送入 Transformer 解码器。实现流程包括：图像 → backbone → BEVFormer encoder → BEV 网格 → 辅助检测头 → BEV‑NMS → top‑k 选取 → 解码器（含抑制块）→ 3D 检测头；若有时间信息，还会把前一帧检测结果与当前 BEV 结合，形成混合时间锚点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 直接使用 BEV 网格细胞作为锚点，消除辅助网络；2) 在训练中嵌入 BEV‑NMS，减少冗余并保持高密度锚点；3) 通过记忆队列将历史检测与当前 BEV 结合，形成混合时间建模；4) 端到端实现，参数量低、计算开销小。与以往依赖随机锚点或外部检测器的做法不同，DenseBEV 在保持密集锚点的同时显著提升小目标检测性能，并在 Waymo Open 数据集上刷新了最佳成绩。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DenseBEV 提出了基于 BEV 网格的密集锚点生成与 BEV‑NMS 过滤，并结合混合时间建模，实现高效端到端的多摄像头 3D 目标检测，在大型基准上取得领先性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In current research, Bird&amp;#x27;s-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.&lt;/p&gt;</description></item><item><guid>2512.16905v1</guid><title>Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</title><link>http://arxiv.org/abs/2512.16905v1</link><author>Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于元梯度的自动化数据筛选框架Alchemist，用于从大规模文本-图像数据集中挑选高质量样本，以提升文本到图像生成模型的视觉效果和训练效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 文本到图像生成模型在视觉质量上取得突破，但受训练数据质量限制，网络爬取和合成数据往往包含低质量或冗余样本，导致视觉质量下降、训练不稳定和计算效率低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种无需人工标注、可扩展的自动化数据选择方法，以提高数据效率并改善模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Alchemist包含两个阶段：数据评分和数据裁剪。首先训练轻量级评分器，利用梯度信息和多粒度感知评估每个样本的影响力；随后采用Shift-Gsampling策略挑选信息量大的子集进行高效训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成和网络爬取的数据集上实验表明，使用Alchemist挑选的50%数据训练的模型在视觉质量和下游任务上均优于使用完整数据集的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Alchemist是首个自动、可扩展、基于元梯度的数据选择框架，能够显著提升文本到图像生成模型的训练效果和计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose Alchemist, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample&amp;#x27;s influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample&amp;#x27;s influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.&lt;/p&gt;</description></item><item><guid>2512.17012v2</guid><title>4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</title><link>http://arxiv.org/abs/2512.17012v2</link><author>Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了4D-RGPT模型、P4D训练框架和R4D-Bench基准，提升多模态LLM在4D视频问答中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有多模态LLM在3D结构和时间动态推理方面受限，4D感知薄弱；现有4D/3D视频问答基准侧重静态场景，缺乏区域级提示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决4D感知不足和基准缺陷，提升模型对4D视频的理解和推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 4D-RGPT：专门捕捉视频4D表示并增强时间感知；2) P4D：将冻结专家模型的4D表示蒸馏到4D-RGPT；3) R4D-Bench：构建深度感知动态场景的区域级提示基准，采用自动化与人工验证相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 4D-RGPT在现有4D VQA基准和新提出的R4D-Bench上均显著提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过专用模型、蒸馏框架和新基准，显著提升了多模态LLM在4D视频问答中的表现，为未来研究提供了工具和数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管多模态大型语言模型（MLLMs）取得了进展，但它们在推理3D结构和时间动态方面的能力仍然有限，受制于弱的4D感知和时间理解。现有的3D和4D视频问答（VQA）基准也侧重于静态场景，缺乏区域级提示。我们通过引入以下方法来解决这些问题：（a）4D-RGPT，一种专门的MLLM，旨在从视频输入中捕捉4D表示，并增强时间感知；（b）感知4D蒸馏（P4D），一种训练框架，将冻结的专家模型的4D表示转移到4D-RGPT，以实现全面的4D感知；以及（c）R4D-Bench，一个针对深度感知动态场景的区域级提示基准，采用混合自动化和人工验证的管道构建。我们的4D-RGPT在现有的4D VQA基准和提出的R4D-Bench基准上都取得了显著改进。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决区域级 4D 理解问题，即在视频中同时识别深度、运动和时间变化，并将语言查询精确定位到特定区域。该能力对自动驾驶、工业检测等需要精确空间时间信息的应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有多模态 LLM 在 4D 感知和区域提示方面表现不足，借鉴了 3D/4D VQA、区域级 MLLM 以及知识蒸馏技术，提出通过冻结专家 4D 感知模型进行训练时的蒸馏，并加入时间位置编码来提升时序感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用训练时仅存在的蒸馏模块，让学生 4D‑RGPT 学习专家模型的 4D 隐层特征和显式深度/光流信息，并通过时间位置编码增强时序感知。实现流程包括：视频编码 → 视觉投影 → LLM → 训练时 4D 感知解码器和预测头提取 4D 表征；冻结专家模型生成目标特征；对齐隐层和显式特征的蒸馏损失；最终训练得到具备 4D 感知的 MLLM。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) 4D‑RGPT 结构，专门捕获 4D 表征；2) P4D 蒸馏框架，结合隐层和显式蒸馏，训练时无额外推理成本；3) 时间位置编码提升时序感知；4) 新的 R4D‑Bench 区域级 4D VQA 基准。与以往仅使用 SFT/RL 或添加额外模块的工作不同，本文通过蒸馏实现高效 4D 感知，并在区域级任务上取得显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种训练时蒸馏框架，使多模态 LLM 获得 4D 感知和区域级推理能力，并在新构建的 R4D‑Bench 上实现领先性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.&lt;/p&gt;</description></item><item><guid>2512.17185v1</guid><title>Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning</title><link>http://arxiv.org/abs/2512.17185v1</link><author>Sandeep Neela</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出系统性风险雷达框架，通过多层图模型检测金融市场早期脆弱性和崩盘转变，并在三大危机中验证其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 金融危机源于跨部门、市场和投资者行为的结构性脆弱性累积，难以预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建并评估一种基于图神经网络的系统性风险监测方法，以提供早期预警。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将金融市场建模为多层图，使用图神经网络（包括快照GNN、简化时序GNN）与传统基线模型比较，提取结构网络信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 结构网络信息比单纯特征模型更能提供早期警报，图特征在压力事件中捕捉到有意义的市场结构变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SRR框架有效，建议加入更多图层和更强的时序架构以提升对不同危机类型的处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 金融危机在结构性脆弱性在各个部门、市场和投资者行为中累积时出现。预测这些系统性转变具有挑战性，因为它们源于市场参与者之间不断演变的相互作用，而不是孤立的价格波动。我们提出系统性风险雷达（SRR），该框架将金融市场建模为多层图，以检测系统脆弱性和崩盘模式转换的早期迹象。我们在三大危机中评估了SRR：点对点泡沫破裂、全球金融危机和COVID-19冲击。我们的实验比较了快照图神经网络、简化的时序图神经网络原型以及标准基线模型（逻辑回归和随机森林）。结果表明，与仅基于特征的模型相比，结构网络信息提供了有用的早期预警信号。该基于相关性的SRR实例表明，图派生特征在压力事件期间捕捉到有意义的市场结构变化。研究结果促使将SRR扩展为额外的图层（行业/因子暴露、情绪）和更具表达力的时序架构（LSTM/GRU或Transformer编码器），以更好地处理不同类型的危机。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.   We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.   This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.&lt;/p&gt;</description></item><item><guid>2512.17620v1</guid><title>StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</title><link>http://arxiv.org/abs/2512.17620v1</link><author>Di Wu, Feng Yang, Wenhui Zhao, Jinwen Yu, Pan Liao, Benlian Xu, Dingwen Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多视角三维检测是自动驾驶感知的核心任务，稀疏查询式检测器通过可学习查询高效聚合多视角图像特征。MV2D利用二维检测结果为查询提供高质量先验，提升精度和召回率，但单帧二维检测的深度歧义限制了三维查询的准确性。本文提出StereoMV2D，将时序立体建模融入二维检测引导的多视角三维检测框架，利用相邻帧的跨时差异提升深度感知并细化查询先验，同时在二维感兴趣区域内高效计算。动态置信门控机制通过帧间匹配矩阵和外观一致性评估时序立体线索的可靠性，保证在外观变化和遮挡下的鲁棒检测。实验表明StereoMV2D在nuScenes和Argoverse 2数据集上实现了更优检测性能，且计算开销不显著增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多视角三维检测需要在检测精度与计算效率之间取得平衡。稀疏查询式检测器提供了简洁且端到端的检测范式。MV2D在此基础上利用二维检测结果为查询初始化提供高质量先验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过整合时序立体建模，解决单帧二维检测中深度歧义导致的三维查询生成精度受限的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; StereoMV2D框架在二维检测引导的多视角三维检测器中加入时序立体建模，利用相邻帧中同一目标的跨时差异提升深度感知并细化查询先验。所有计算在二维感兴趣区域内完成，并通过动态置信门控机制评估时序立体线索的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和Argoverse 2数据集上，StereoMV2D在不显著增加计算开销的前提下，取得了比现有方法更优的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; StereoMV2D通过融合时序立体信息和动态置信门控，有效克服了深度歧义问题，提供了一种高效、准确的多视角三维检测方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多视角三维目标检测是自动驾驶感知中的基础任务，如何在检测精度与计算效率之间取得平衡至关重要。稀疏查询式三维检测器通过一组可学习的查询高效聚合多视角图像中的目标相关特征，提供了简洁且端到端的检测范式。在此基础上，MV2D 利用二维检测结果为查询初始化提供高质量的目标先验，从而实现更高的精度和召回率。然而，单帧二维检测中固有的深度歧义仍限制了三维查询生成的准确性。为解决此问题，我们提出了 StereoMV2D，一种将时序立体建模融入二维检测引导的多视角三维检测器的统一框架。通过利用相邻帧中同一目标的跨时差异，StereoMV2D 提升了深度感知并细化了查询先验，同时在二维感兴趣区域内高效完成所有计算。此外，动态置信门控机制通过学习来自帧间匹配矩阵的统计模式以及外观一致性，动态评估时序立体线索的可靠性，确保在目标外观变化和遮挡下的鲁棒检测。对 nuScenes 和 Argoverse 2 数据集的广泛实验表明，StereoMV2D 在不产生显著计算开销的前提下实现了更优的检测性能。代码将发布在 https://github.com/Uddd821/StereoMV2D。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决多视角3D目标检测中单帧2D检测导致的深度不确定性问题。深度不准会导致3D查询初始化错误，影响检测精度。对自动驾驶等实时安全场景而言，既要高精度又要低延迟，解决深度模糊至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先基于MV2D的2D检测引导查询框架，发现单帧深度模糊限制性能。随后借鉴BEVStereo、SOLOFusion等工作中的跨帧几何约束，设计了两阶段查询生成器：先做运动感知软匹配，再在RoI级别构建轻量级立体匹配成本体。最后加入自适应置信门控以融合单帧与立体先验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将RoI级别的时间立体几何信息注入稀疏查询框架，以获得更准确的3D位置先验。实现流程为：多视角图像 → 图像特征提取 → 2D检测 → RoI特征提取 → 单帧与立体两路查询生成 → 运动感知软匹配 → RoI级立体匹配 → 置信门控融合 → 稀疏解码器 → 3D检测头。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1) RoI级时间立体匹配，仅在检测框内构建成本体，显著降低计算；2) 动态置信门控学习单帧与立体先验的可靠性并加权融合；3) 将上述技术嵌入稀疏查询检测器，保持高效推理。与以往使用全图或BEV稠密立体、仅基于外观的时间传播不同，StereoMV2D在保持稀疏性的同时引入几何一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; StereoMV2D通过RoI级时间立体匹配和自适应置信门控，在保持稀疏查询效率的同时显著提升多视角3D检测的深度精度与整体性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.&lt;/p&gt;</description></item><item><guid>2512.17908v1</guid><title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title><link>http://arxiv.org/abs/2512.17908v1</link><author>Ananta R. Bhattarai, Helge Rhodin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种在测试时自监督的框架 Re-Depth Anything，旨在解决单目深度估计在真实图像上的性能下降问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的基础模型如 Depth Anything V2 在训练分布之外的真实图像上表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过融合 DA-V2 与大型 2D 扩散模型的先验知识，弥合域差距并提升深度估计的准确性与真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在输入图像上进行无标签的细化，重新照明预测的深度图并增强输入；使用形状自阴影提示与 Score Distillation Sampling 替代传统的光度重建；冻结编码器，仅更新中间嵌入并微调解码器，以防止优化崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多种基准测试中，Re-Depth Anything 在深度精度和视觉真实性上显著优于 DA-V2，验证了自监督与几何推理相结合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架为利用几何推理增强自监督提供了新的思路，并展示了在单目深度估计领域的显著改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在真实世界图像上，现有基础模型（如Depth Anything V2）产生的深度估计误差。由于这些模型在训练时主要使用合成或有限的真实数据，面对分布外的图像时会出现不准确的深度预测，影响三维重建、自动驾驶、机器人导航等关键应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了自监督测试时适配、2D扩散模型作为先验以及形状与光照的关系。借鉴了DreamFusion、RealFusion等利用扩散模型进行3D重建的工作，并将其思想迁移到单图像深度估计的自监督优化中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用DA‑V2得到初始深度，然后用Blinn‑Phong模型在随机光照下重新渲染该深度，得到增强的图像；再用扩散模型的Score Distillation Sampling（SDS）对该图像进行评分，并将梯度反向传播到深度网络的中间嵌入和解码器权重（编码器保持冻结）。整个流程包括：输入图像 → DA‑V2初始深度 → 计算法向量 → 随机光照渲染 → SDS损失 → 目标嵌入/解码器优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 在测试时使用单图像自监督优化；2) 通过重新照明而非光度重建，将深度与图像关联；3) 采用SDS损失将2D扩散模型的先验引入深度优化；4) 只更新嵌入和解码器，避免过拟合。与以往多视角NeRF或全局光照重建不同，Re‑Depth Anything 在单视角下即可提升深度精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Re‑Depth Anything 在测试时通过对深度预测进行随机照明并利用扩散模型的SDS损失进行自监督优化，显著提升了基础模型在真实图像上的深度估计精度，且不需要额外标注。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.&lt;/p&gt;</description></item><item><guid>2512.17969v1</guid><title>Convolutional-neural-operator-based transfer learning for solving PDEs</title><link>http://arxiv.org/abs/2512.17969v1</link><author>Peng Fan, Guofei Pang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了卷积神经算子在少样本学习中的应用，并比较了三种参数调整策略，发现神经元线性变换策略在求解多种偏微分方程时具有最高的代理精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 卷积神经算子是一种基于CNN的架构，能够保持连续-离散等价性并实现无混叠的偏微分方程解算子学习，已在某些情况下优于DeepONet、傅里叶神经算子和Galerkin变换器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证卷积神经算子在少样本学习场景下的有效性，并寻找最优的参数微调方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先在源数据集上预训练卷积神经算子，然后仅使用少量目标数据集对其参数进行调整，比较了微调、低秩适配和神经元线性变换三种策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 神经元线性变换策略在解决库拉托夫-西凡斯基方程、布鲁塞勒反应扩散系统和Navier-Stokes方程时，取得了最高的代理精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 卷积神经算子可以通过神经元线性变换有效地适应少样本学习任务，显著提升偏微分方程求解的代理精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 卷积神经算子是一种最近提出的基于CNN的架构，旨在实现结构保持的连续-离散等价性，并实现对偏微分方程解算子的真实、无混叠学习。该算子已被证明在某些情况下在代理精度上优于基线模型，如DeepONet、傅里叶神经算子和Galerkin变换器。然而，卷积神经算子似乎尚未在少样本学习中得到验证。我们通过先在源数据集上预训练卷积神经算子，然后仅使用少量目标数据集来调整已训练算子的参数，扩展了该模型以适应少样本学习场景。我们研究了三种调整已训练算子参数的策略，包括微调、低秩适配和神经元线性变换，并发现神经元线性变换策略在求解库拉托夫-西凡斯基方程、布鲁塞勒扩散-反应系统和Navier-Stokes方程等偏微分方程时具有最高的代理精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Convolutional neural operator is a CNN-based architecture recently proposed to enforce structure-preserving continuous-discrete equivalence and enable the genuine, alias-free learning of solution operators of PDEs. This neural operator was demonstrated to outperform for certain cases some baseline models such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy. The convolutional neural operator, however, seems not to be validated for few-shot learning. We extend the model to few-shot learning scenarios by first pre-training a convolutional neural operator using a source dataset and then adjusting the parameters of the trained neural operator using only a small target dataset. We investigate three strategies for adjusting the parameters of a trained neural operator, including fine-tuning, low-rank adaption, and neuron linear transformation, and find that the neuron linear transformation strategy enjoys the highest surrogate accuracy in solving PDEs such as Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.&lt;/p&gt;</description></item><item><guid>2512.18135v1</guid><title>Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications</title><link>http://arxiv.org/abs/2512.18135v1</link><author>Cristiano da Costa Cunha, Wei Liu, Tim French, Ajmal Mian</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文综述了因果推断与强化学习结合的最新进展，指出该方法能解决传统强化学习在可解释性、鲁棒性和泛化方面的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统强化学习依赖相关性决策，难以应对分布偏移、混杂变量和动态环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 系统评估因果强化学习在不同子领域的研究进展，并识别挑战与未来方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将现有工作分为因果表示学习、反事实策略优化、离线因果强化学习、因果迁移学习和因果可解释性等类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通过结构化分析，作者指出了普遍存在的挑战，展示了在实际应用中的经验成功，并讨论了未解决的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 因果强化学习有望构建更稳健、可泛化且可解释的人工智能系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 将因果推断与强化学习相结合已成为解决传统强化学习低可解释性、缺乏鲁棒性和泛化失败等关键局限的强大范式。传统强化学习技术通常依赖相关性驱动的决策，在面对分布偏移、混杂变量和动态环境时表现不佳。因果强化学习利用因果推断的基本原理，通过显式建模因果关系，为这些挑战提供了有前景的解决方案。在本综述中，我们系统回顾了因果推断与强化学习交叉领域的最新进展。我们将现有方法分为因果表示学习、反事实策略优化、离线因果强化学习、因果迁移学习和因果可解释性等类别。通过这种结构化分析，我们识别了普遍存在的挑战，突出展示了在实际应用中的经验成功，并讨论了开放问题。最后，我们提出了未来研究方向，强调因果强化学习在构建稳健、可泛化和可解释的人工智能系统方面的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.&lt;/p&gt;</description></item><item><guid>2512.18172v1</guid><title>cardinalR: Generating Interesting High-Dimensional Data Structures</title><link>http://arxiv.org/abs/2512.18172v1</link><author>Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, Thiyanga S. Talagala</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一个新的R包cardinalR，用于生成多种高维数据结构，帮助研究者测试和改进降维、监督和无监督学习算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高维数据具有多变量相互依赖的特征，常见线性、非线性、聚类或异常模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提供新的方法生成多样化的高维结构，并提供示例数据集，以便更好地理解和改进分析方法，特别是非线性降维技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用数学函数和统计分布构建高维结构，并将其封装在R包cardinalR中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该包提供了多种高维数据生成方法和示例数据集，丰富了评估算法的基准数据集工具箱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; cardinalR为研究者提供了实用的高维数据生成工具，可用于验证和改进降维及学习算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 模拟高维数据对于测试、验证和改进用于降维、监督学习和无监督学习的算法非常有用。高维数据的特点是多个变量以某种方式相互依赖或关联，例如线性、非线性、聚类或异常。本文提供了使用数学函数和统计分布生成多种高维结构的新方法，并将其组织成R包cardinalR。还提供了若干示例数据集。这些将帮助研究者更好地理解不同分析方法的工作原理并加以改进，特别关注非线性降维方法。该包丰富了用于评估算法的基准数据集现有工具集。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Simulated high-dimensional data is useful for testing, validating, and improving algorithms used in dimension reduction, supervised and unsupervised learning. High-dimensional data is characterized by multiple variables that are dependent or associated in some way, such as linear, nonlinear, clustering or anomalies. Here we provide new methods for generating a variety of high-dimensional structures using mathematical functions and statistical distributions organized into the R package cardinalR. Several example data sets are also provided. These will be useful for researchers to better understand how different analytical methods work and can be improved, with a special focus on nonlinear dimension reduction methods. This package enriches the existing toolset of benchmark datasets for evaluating algorithms.&lt;/p&gt;</description></item><item><guid>2512.18173v1</guid><title>Transfer Learning for Analysis of Collective and Non-Collective Thomson Scattering Spectra</title><link>http://arxiv.org/abs/2512.18173v1</link><author>T. Van Hoomissen, J. Alhuthali, A. M. Ortiz, D. A. Mariscal, R. S. Dorst, S. Eisenbach, H. Zhang, J. J. Pilgram, C. G. Constantin, L. Rovige, C. Niemann, D. B. Schaeffer</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨利用深度神经网络结合迁移学习，对托马森散射光谱进行快速、准确的电子密度和温度估计，并评估其在实验数据中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 托马森散射诊断能够提供可靠、最小扰动的等离子体参数测量，但传统拟合方法在噪声强或实时分析需求下可能失效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证迁移学习在有限实验数据下提升深度网络估计托马森散射参数的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建五种结构不同的深度网络，在合成数据上预训练后，用实验光谱进行微调；比较使用与不使用迁移学习的模型在不同训练集规模下的误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 当实验光谱样本不足约200条时，迁移学习显著降低了电子密度和温度估计误差；在集体与非集体散射模式下均表现出优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 迁移学习能够在实验数据稀缺的情况下显著提升托马森散射参数估计精度，为实时等离子体诊断提供可行方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Thomson scattering diagnostics provide reliable, minimally perturbative measurements of fundamental plasma parameters, such as electron density and electron temperature. Deep neural networks can provide accurate estimates of electron density and electron temperature when conventional fitting algorithms may fail, such as when TS spectra are dominated by noise, or when fast analysis is required for real-time operation. Although deep neural networks typically require large training sets, transfer learning can improve model performance on a target task with limited data by leveraging pre-trained models from related source tasks, where select hidden layers are further trained using target data. We present five architecturally diverse deep neural networks, pre-trained on synthetic TS data and adapted for experimentally measured TS data, to evaluate the efficacy of transfer learning in estimating electron density and electron temperature in both the collective and non-collective scattering regimes. We evaluate errors in electron density and electron temperature estimates as a function of training set size for models trained with and without transfer learning, and we observe decreases in model error from transfer learning when the training set contains less than or approximately 200 experimentally measured spectra.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Thomson scattering (TS) diagnostics provide reliable, minimally perturbative measurements of fundamental plasma parameters, such as electron density ($n_e$) and electron temperature ($T_e$). Deep neural networks can provide accurate estimates of $n_e$ and $T_e$ when conventional fitting algorithms may fail, such as when TS spectra are dominated by noise, or when fast analysis is required for real-time operation. Although deep neural networks typically require large training sets, transfer learning can improve model performance on a target task with limited data by leveraging pre-trained models from related source tasks, where select hidden layers are further trained using target data. We present five architecturally diverse deep neural networks, pre-trained on synthetic TS data and adapted for experimentally measured TS data, to evaluate the efficacy of transfer learning in estimating $n_e$ and $T_e$ in both the collective and non-collective scattering regimes. We evaluate errors in $n_e$ and $T_e$ estimates as a function of training set size for models trained with and without transfer learning, and we observe decreases in model error from transfer learning when the training set contains $\lessapprox$ 200 experimentally measured spectra.&lt;/p&gt;</description></item><item><guid>2512.18187v1</guid><title>ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection</title><link>http://arxiv.org/abs/2512.18187v1</link><author>Janghyun Baek, Mincheol Chang, Seokha Moon, Seung Joon Lee, Jinkyu Kim</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 ALIGN 方法，改进 3D 目标检测中的查询初始化，提升遮挡和拥挤场景下的检测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有基于查询的 3D 检测方法在相机和 LiDAR 输入下表现良好，但随机或 BEV 热图采样的查询初始化往往低效，导致遮挡或拥挤物体的准确率下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决查询初始化低效导致的精度下降问题，提出一种对遮挡鲁棒、面向对象的查询初始化方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ALIGN 由三部分组成：Occlusion-aware Center Estimation（OCE）利用 LiDAR 几何和图像语义估计物体中心；Adaptive Neighbor Sampling（ANS）从 LiDAR 聚类生成候选物体，并在其周围采样空间和语义对齐的点；Dynamic Query Balancing（DQB）自适应平衡前景和背景查询。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 nuScenes 基准上，ALIGN 在多种先进检测器上均提升性能，最大提升 0.9 mAP 和 1.2 NDS，尤其在遮挡或密集人群场景中效果显著。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ALIGN 通过改进查询初始化显著提升 3D 检测精度，特别适用于遮挡和拥挤环境，且代码将公开。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近基于查询的 3D 目标检测方法使用相机和 LiDAR 输入已显示出强劲的性能，但现有的查询初始化策略，如随机采样或基于 BEV 热图的采样，往往导致查询使用效率低下和准确率下降，尤其是在遮挡或拥挤物体的情况下。为了解决这一限制，我们提出 ALIGN（Advanced query initialization with LiDAR and Image GuidaNce），一种针对遮挡鲁棒、面向对象的查询初始化的新方法。我们的模型由三个关键组件组成：（i）Occlusion-aware Center Estimation（OCE），它整合 LiDAR 几何和图像语义来准确估计物体中心；（ii）Adaptive Neighbor Sampling（ANS），它从 LiDAR 聚类生成物体候选，并通过在其周围采样空间和语义对齐的点来补充每个物体；（iii）Dynamic Query Balancing（DQB），它自适应地在前景和背景区域之间平衡查询。我们在 nuScenes 基准上的广泛实验表明，ALIGN 在多种最先进的检测器上持续提升性能，尤其在遮挡或密集人群的挑战性场景中，提升幅度高达 +0.9 mAP 和 +1.2 NDS。我们的代码将在发布后公开。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文解决了查询式3D目标检测中查询初始化不合理导致的效率低下和对遮挡或拥挤场景识别不佳的问题。准确的目标检测是自动驾驶和机器人感知的核心，尤其在复杂城市环境中，遮挡和密集目标的检测准确性直接影响安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了随机采样和BEV热图采样的局限，随后结合LiDAR几何信息与图像语义，提出了OCE、ANS和DQB三模块。设计中借鉴了DBSCAN聚类、Deformable DETR的邻域采样、BEV热图与图像语义融合等已有技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态信息精确估计目标中心，随后在聚类核心点周围采样邻域查询，并动态平衡前景与背景查询。实现流程为：1）将LiDAR点投影到图像，利用分割掩码和局部同伦估计中心；2）对LiDAR点云做DBSCAN聚类，采样核心点及其邻域并语义过滤；3）根据剩余查询预算按比例分配邻域查询和随机背景查询；4）将所有查询送入多模态Transformer解码器完成检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① Occlusion‑Aware Center Estimation (OCE) 通过图像分割与LiDAR投影精确定位中心；② Adaptive Neighbor Sampling (ANS) 在聚类核心点周围采样语义一致的邻域点；③ Dynamic Query Balancing (DQB) 根据场景密度动态分配前景与背景查询。与以往仅使用随机或热图采样的做法不同，ALIGN提供了显式的目标中心估计和自适应邻域采样，显著提升了遮挡和密集场景下的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALIGN通过多模态目标中心估计、邻域采样和动态查询平衡，提出了一种高效、遮挡鲁棒的查询初始化策略，显著提升了3D目标检测的准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent query-based 3D object detection methods using camera and LiDAR inputs have shown strong performance, but existing query initialization strategies,such as random sampling or BEV heatmap-based sampling, often result in inefficient query usage and reduced accuracy, particularly for occluded or crowded objects. To address this limitation, we propose ALIGN (Advanced query initialization with LiDAR and Image GuidaNce), a novel approach for occlusion-robust, object-aware query initialization. Our model consists of three key components: (i) Occlusion-aware Center Estimation (OCE), which integrates LiDAR geometry and image semantics to estimate object centers accurately (ii) Adaptive Neighbor Sampling (ANS), which generates object candidates from LiDAR clustering and supplements each object by sampling spatially and semantically aligned points around it and (iii) Dynamic Query Balancing (DQB), which adaptively balances queries between foreground and background regions. Our extensive experiments on the nuScenes benchmark demonstrate that ALIGN consistently improves performance across multiple state-of-the-art detectors, achieving gains of up to +0.9 mAP and +1.2 NDS, particularly in challenging scenes with occlusions or dense crowds. Our code will be publicly available upon publication.&lt;/p&gt;</description></item><item><guid>2512.18211v1</guid><title>LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</title><link>http://arxiv.org/abs/2512.18211v1</link><author>Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 LLaViDA，一种利用视觉语言模型进行轨迹规划的自动驾驶助手，采用两阶段训练提升场景理解和轨迹规划能力，在 NuScenes 基准上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自动驾驶轨迹规划在恶劣天气、不可预测人类行为或复杂道路布局下表现不佳，现有端到端规划器缺乏泛化和少样本能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过视觉语言模型实现更强的场景理解和轨迹规划，以提升自动驾驶的可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用视觉语言模型进行目标运动预测、语义定位和链式推理，采用监督微调和轨迹偏好优化两阶段训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 NuScenes 测试集上，LLaViDA 的平均路径误差为 0.31 米，碰撞率为 0.10%，超过了现有端到端和 VLM/LLM 基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LLaViDA 通过结合视觉语言模型和两阶段训练，显著提升了自动驾驶轨迹规划的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 轨迹规划是自动驾驶的基础但具有挑战性的组成部分。端到端规划器在恶劣天气、不可预测的人类行为或复杂道路布局下经常失效，主要是因为它们缺乏强大的泛化或少样本能力，无法超越训练数据。我们提出 LLaViDA，一种大型语言视觉驾驶助手，利用视觉语言模型（VLM）进行目标运动预测、语义定位和链式推理，以实现自动驾驶轨迹规划。一个两阶段训练流程——监督微调后跟随轨迹偏好优化（TPO）——通过注入基于回归的监督来增强场景理解和轨迹规划，产生了强大的“VLM 轨迹规划器”。在 NuScenes 基准上，LLaViDA 在开放环轨迹规划任务中超过了最先进的端到端和其他近期的 VLM/LLM 基线，平均路径误差为 0.31 米，碰撞率为 0.10%，在 NuScenes 测试集上表现最佳。本文代码可在 GitHub 上获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful &amp;quot;VLM Trajectory Planner for Autonomous Driving.&amp;quot; On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.&lt;/p&gt;</description></item><item><guid>2512.18219v1</guid><title>Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching</title><link>http://arxiv.org/abs/2512.18219v1</link><author>Mohammad Zolfaghari, Hedieh Sajedi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种增强教师-学生框架用于无监督异常检测，并通过预训练 ResNet-18 并在 MVTech-AD 数据集上微调，最终在图像级和像素级上取得了比以往方法更好的准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 异常检测是无监督学习中的一个挑战性课题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过改进教师网络来提升异常检测的性能指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先在 ImageNet 上预训练 ResNet-18，然后在 MVTech-AD 数据集上微调，构建 Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM) 模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该模型在图像级和像素级的平均准确率分别为 0.971 和 0.977，优于之前的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 增强教师-学生特征金字塔框架能够显著提升无监督异常检测的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Anomaly detection or outlier is one of the challenging subjects in unsupervised learning. This paper introduces a student-teacher framework for anomaly detection that its teacher network is enhanced for achieving high-performance metrics. For this purpose, we first pre-train the ResNet-18 network on the ImageNet and then fine-tune it on the MVTech-AD dataset. Experiment results on the image-level and pixel-level demonstrate that this idea has achieved better metrics than the previous methods. Our model, Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM), achieved 0.971 mean accuracy on the image-level and 0.977 mean accuracy on the pixel-level for anomaly detection.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Anomaly detection or outlier is one of the challenging subjects in unsupervised learning . This paper is introduced a student-teacher framework for anomaly detection that its teacher network is enhanced for achieving high-performance metrics . For this purpose , we first pre-train the ResNet-18 network on the ImageNet and then fine-tune it on the MVTech-AD dataset . Experiment results on the image-level and pixel-level demonstrate that this idea has achieved better metrics than the previous methods . Our model , Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM), achieved 0.971 mean accuracy on the image-level and 0.977 mean accuracy on the pixel-level for anomaly detection.&lt;/p&gt;</description></item><item><guid>2512.18269v1</guid><title>Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System</title><link>http://arxiv.org/abs/2512.18269v1</link><author>Se-Young Jang, Su-Yeon Yoon, Jae-Woong Jung, Dong-Hun Lee, Seong-Hun Choi, Soo-Kyung Jun, Yu-Bin Kim, Young-Seon Ju, Kyounggon Kim</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种可视化暗模式检测框架，利用自建数据集和YOLOv12x模型，实现高精度和实时检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着数字化转型加速，暗模式在在线平台中日益突出，传统监管手段反应式不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发主动、实时的暗模式检测技术，提升检测准确率和实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建4,066张UI截图数据集，标注五类UI组件；采用YOLOv12x并迁移学习优化模型；公开数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在数据集上达到92.8%的检测准确率，实时推理速度为40.5帧每秒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在准确率和实时性上均表现优异，适合在线环境部署，并为后续研究提供数据资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着数字化转型加速，暗模式在在线平台中日益突出。本文提出一种可视化暗模式检测框架，构建了4,066张UI截图数据集，标注了五类UI组件，并采用YOLOv12x模型进行迁移学习，取得92.8%的检测准确率和40.5帧每秒的实时推理速度。该数据集已公开，支持后续研究。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;With the accelerating pace of digital transformation and the widespread adoption of online platforms, both social and technical concerns regarding dark patterns-user interface designs that undermine users&amp;#x27; ability to make informed and rational choices-have become increasingly prominent. As corporate online platforms grow more sophisticated in their design strategies, there is a pressing need for proactive and real-time detection technologies that go beyond the predominantly reactive approaches employed by regulatory authorities. In this paper, we propose a visual dark pattern detection framework that improves both detection accuracy and real-time performance. To this end, we constructed a proprietary visual object detection dataset by manually collecting 4,066 UI/UX screenshots containing dark patterns from 194 websites across six major industrial sectors in South Korea and abroad. The collected images were annotated with five representative UI components commonly associated with dark patterns: Button, Checkbox, Input Field, Pop-up, and QR Code. This dataset has been publicly released to support further research and development in the field. To enable real-time detection, this study adopted the YOLOv12x object detection model and applied transfer learning to optimize its performance for visual dark pattern recognition. Experimental results demonstrate that the proposed approach achieves a high detection accuracy of 92.8% in terms of mAP@50, while maintaining a real-time inference speed of 40.5 frames per second (FPS), confirming its effectiveness for practical deployment in online environments. Furthermore, to facilitate future research and contribute to technological advancements, the dataset constructed in this study has been made publicly available at https://github.com/B4E2/B4E2-DarkPattern-YOLO-DataSet.&lt;/p&gt;</description></item><item><guid>2512.18412v1</guid><title>Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation</title><link>http://arxiv.org/abs/2512.18412v1</link><author>Mykyta Lapin, Kostiantyn Bokhan, Yurii Parzhyn</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 提出结构图方法用于少样本轮廓图像分类；2. 通过属性图编码关键点与线条，携带几何属性；3. 通过概念吸引子实现类级泛化；4. 采用少量样本（5–6个/类）形成类概念；5. 通过图匹配（近似图编辑距离）进行分类；6. 在MNIST子集上取得约82%准确率；7. 决策可追溯，误分类可解释；8. 与传统SVM、MLP、CNN及度量学习、元学习基线对比；9. 主要限制为GED计算成本与骨架化质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 少样本学习场景下，传统方法依赖大量训练样本与反向传播，缺乏可解释性；轮廓图像分类需要结构化表示与透明决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计并验证一种架构，利用结构与参数化简从少量样本形成类概念，提供透明决策并消除反向传播。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 轮廓向量化后构建双部图（点/线节点），节点属性为归一化几何属性；通过消除不稳定子结构或噪声、路径对齐进行简化；概念通过迭代组合样本形成；分类通过近似图编辑距离匹配最佳概念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在MNIST子集5–6例/类单轮训练下，准确率约82%；误分类可通过结构相似性解释；与SVM、MLP、CNN及度量学习、元学习基线进行对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结构图方案实现无反向传播的少样本学习，内置解释；限制为GED计算成本与骨架化质量；未来方向包括优化分类算法、处理静态场景及关联识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出一种结构图方法，用于在少样本环境下对轮廓图像进行分类，并且不使用反向传播。核心思想是让结构成为解释的载体：将图像编码为带属性的图（关键点和线条作为节点，携带几何属性），通过形成概念吸引子（类级概念图）实现泛化。目的在于设计并实验验证一种架构，该架构能够从每类仅 5–6 个样本通过结构和参数化简形成类概念，提供透明决策并消除反向传播。方法包括轮廓向量化后构建双部图（点/线为节点），节点属性为归一化的几何属性，如坐标、长度、角度和方向；通过消除不稳定子结构或噪声以及对关键点之间路径的对齐来进行简化；概念通过对样本的迭代组合形成；分类时通过选择最佳图-概念匹配（使用近似图编辑距离）完成。结果显示，在 MNIST 子集上，每类仅 5–6 个基准样本（单轮训练）下，我们获得约 82% 的一致准确率，并且决策具有完整可追溯性：误分类可通过显式结构相似性解释。我们还提供了与 SVM、MLP、CNN 以及度量学习和元学习基线的对比。结构图方案与概念吸引子实现了无反向传播的少样本学习，并通过显式图结构提供内置解释。局限性包括图编辑距离的计算成本和骨架化质量；未来的有前景方向包括分类算法优化、与静态场景的结合以及关联识别。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.&lt;/p&gt;</description></item><item><guid>2512.18550v1</guid><title>ShibuyaSocial: Multi-scale Model of Pedestrian Flows in Scramble Crossing</title><link>http://arxiv.org/abs/2512.18550v1</link><author>Akihiro Sakurai, Naoya Kajio, Ko Yamamoto</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于学习的行人流模型，融合多尺度行为，包括全局路径选择和局部碰撞规避，重点研究涩谷十字路口行人运动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 过度拥堵可能导致严重事故，数学建模与预测行人行为对于预防事故和提供安全舒适环境至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建能够同时考虑全局路径选择和局部碰撞规避的行人行为模型，并验证其预测准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用注意力机制将局部行为与全局路径选择融合；通过记录涩谷十字路口的视频获取行人轨迹数据，训练模型；随后进行仿真验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 训练后的模型能够定性和定量地准确预测行人行为，证明了融合全局与局部行为的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该模型能够在多尺度层面准确预测行人行为，为城市交通安全提供支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种基于学习的行人流模型，整合了多尺度行为，如全局路径选择和局部碰撞规避，重点关注涩谷十字路口的行人运动。由于过度拥堵可能导致严重事故，数学建模和预测行人行为对于预防事故并提供安全舒适的环境至关重要。虽然已有许多研究探讨了基于学习的建模方法，但大多数仅关注行人的局部行为，例如与邻居和环境物体的碰撞规避。在实际环境中，行人行为涉及更复杂的决策，包括全局路径选择。此外，还应同时考虑从停止到行走的状态转换。本研究提出的模型将局部行为与全局路径选择结合，使用注意力机制确保全局和局部行为预测的一致性。我们记录了涩谷十字路口行人的视频数据，并使用从视频中获取的行人行走轨迹训练了该模型。基于训练模型的行人行为仿真在定性和定量上验证了该模型能够适当地预测行人行为。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents a learning-based model of pedestrian flows that integrates multi scale behaviors such as global route selection and local collision avoidance in urban spaces, particularly focusing on pedestrian movements at Shibuya scramble crossing. Since too much congestion of pedestrian flows can cause serious accidents, mathematically modeling and predicting pedestrian behaviors is important for preventing such accidents and providing a safe and comfortable environment. Although numerous studies have investigated learning-based modeling methods, most of them focus only on the local behavior of pedestrians, such as collision avoidance with neighbors and environmental objects. In an actual environment, pedestrian behavior involves more complicated decision making including global route selection. Moreover, a state transition from stopping to walking at a traffic light should be considered simultaneously. In this study, the proposed model integrates local behaviors with global route selection, using an Attention mechanism to ensure consistent global and local behavior predictions. We recorded video data of pedestrians at Shibuya scramble crossing and trained the proposed model using pedestrian walking trajectory data obtained from the video. Simulations of pedestrian behaviors based on the trained model qualitatively and quantitatively validated that the proposed model can appropriately predict pedestrian behaviors.&lt;/p&gt;</description></item><item><guid>2512.18661v1</guid><title>ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting</title><link>http://arxiv.org/abs/2512.18661v1</link><author>Hafiz Saif Ur Rehman, Ling Liu, Kaleem Ullah Qasim</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出ASTIF系统，利用自适应语义-时间融合方法提升加密货币和科技股价格预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统金融时间序列预测模型多采用静态结构，难以融合多源信息并快速适应市场变化，且往往忽视政策不确定性和市场叙事等语义驱动因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够实时根据置信度自适应调整预测策略的混合智能系统，以更好地融合定量与定性信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ASTIF由三部分组成：1）双通道小语言模型（MirrorPrompt）提取语义市场线索和数值趋势；2）混合LSTM-随机森林模型捕捉时间序列依赖；3）置信度感知元学习器根据实时不确定性调节各预测器贡献。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明ASTIF在2020-2024年AI加密货币和主要科技股数据集上优于Informer、TFT等基准；消融实验验证自适应元学习机制在市场动荡时通过切换语义与时间通道有效降低风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ASTIF提供了一种可扩展、基于知识的解决方案，能够在非平稳环境中融合定量与定性数据，提升金融时间序列预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 金融时间序列预测本质上是信息融合挑战，但现有大多数模型依赖静态架构，难以整合异构知识来源或适应快速的制度变迁。传统方法仅依赖历史价格序列，往往忽视政策不确定性和市场叙事等波动的语义驱动因素。为解决这些局限，我们提出ASTIF（Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting），一种混合智能系统，通过置信度驱动的元学习实时调整预测策略。该框架集成了三种互补组件：使用MirrorPrompt的双通道小语言模型提取语义市场线索和数值趋势；混合LSTM随机森林模型捕捉序列时间依赖；置信度感知元学习器作为自适应推理层，根据实时不确定性调节各预测器的贡献。对2020-2024年AI聚焦加密货币和主要科技股的多样化数据集进行实验，结果显示ASTIF优于领先的深度学习和Transformer基线（如Informer、TFT）。消融研究进一步确认自适应元学习机制的关键作用，它通过在市场动荡期间在语义和时间通道之间切换，成功降低风险。该研究为在非平稳环境中融合定量与定性数据提供了可扩展的基于知识的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor&amp;#x27;s contribution based on its real-time uncertainty.   Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.&lt;/p&gt;</description></item><item><guid>2512.18684v1</guid><title>A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</title><link>http://arxiv.org/abs/2512.18684v1</link><author>Huimin Wu, Kwang-Ting Cheng, Stephen Lin, Zhirong Wu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了将视频预训练的视觉变换器用于多视角几何任务，如光流估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多视角几何任务需要同时处理时空信息，传统方法往往需要定制网络结构和任务特定的预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究通用视频预训练模型在多视角几何任务中的迁移效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在视频预训练的Transformer骨干上添加线性解码器，并通过迭代细化提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通用模型的注意力机制能学习时空信息，最小适配即可迁移到多视角任务；线性解码器足以获得良好结果；迭代细化可达到最先进水平；在光流估计上实现跨数据集最佳泛化，并在在线测试基准上创下新纪录；在3D深度估计和立体匹配上也表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 视频预训练模型在几何视觉任务中具有高度通用性和卓越性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了对多视角几何任务（如光流估计）进行视觉变换器学习的研究，通过微调视频基础模型实现。与以往需要定制架构和任务特定预训练的方法不同，我们的研究发现，通用的、在视频上预训练的模型可以在最小适配下直接迁移到多视角问题。核心见解是，通用的注意力机制在补丁之间学习时空信息，从而实现几何推理。我们证明，在Transformer骨干后添加线性解码器即可获得令人满意的结果，迭代细化进一步将性能提升到最先进水平。该概念上简单的方法在光流估计的跨数据集泛化上取得了顶尖成绩，端点误差（EPE）分别为0.69、1.78和3.15（Sintel clean、Sintel final和KITTI）。我们的方案在在线测试基准上也创下了新纪录，EPE分别为0.79、1.88，F1值为3.79。对3D深度估计和立体匹配的应用也显示出强劲表现，说明视频预训练模型在解决几何视觉任务方面的多样性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在研究如何将通用的视频预训练视觉变压器（ViT）迁移到多视角几何任务（如光流、立体匹配和深度估计）中，并实现高性能。该问题重要，因为这些几何任务是计算机视觉的基础，现有方法往往需要复杂的专用网络和任务特定的预训练，限制了模型的通用性和部署效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为跨帧自注意力能够捕捉匹配信息，决定将预训练的 3D ViT 适配为两帧任务，并在其上添加线性解码器和迭代细化。设计借鉴了 RAFT 的迭代细化思想、Flowformer 的 Transformer 结构以及 CroCo 的适配模块，但避免了专门的预训练任务和复杂的成本体积。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视频预训练的 Transformer 编码器的跨帧注意力来提取几何特征，并通过一个简单的线性头直接回归几何量，再用 ConvGRU 进行迭代细化。实现流程包括：1）调整空间和时间位置编码以适配两帧输入；2）将图像对（或已 warp 的图像）送入预训练 ViT 生成特征；3）线性头输出初始预测；4）在每一步迭代中将上一步预测 warp 目标图像，重新编码并用 ConvGRU 预测残差，累加得到更精细的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）证明通用视频预训练 ViT 能直接迁移到几何任务；2）仅使用线性头即可获得强性能；3）提出不依赖成本体积的迭代细化框架；4）在光流、立体匹配和深度估计上实现新的 state‑of‑the‑art。与以往需要专门预训练、复杂管线或成本体积的工作不同，GeoViT 只需最小化改动即可获得高性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们展示了一个通用的视频预训练 Transformer，配合线性解码器和迭代细化即可在光流、立体匹配和深度估计等多视角几何任务中实现 state‑of‑the‑art 性能，提供了一种简单而通用的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.&lt;/p&gt;</description></item><item><guid>2512.18815v1</guid><title>Controllable Probabilistic Forecasting with Stochastic Decomposition Layers</title><link>http://arxiv.org/abs/2512.18815v1</link><author>John S. Schreck, William E. Chapman, Charlie Becker, David John Gagne, Dhamma Kimpara, Nihanth Cherukuru, Judith Berner, Kirsten J. Mayer, Negin Sobhani</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的随机分解层（SDL），能够将确定性天气预测模型转换为概率集成系统，并在保持高精度和良好校准的同时显著降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的基于扩散的方法在天气预测中计算量大，而连续排名概率分数（CRPS）集成方法虽然准确，但训练策略和噪声注入方式多样，且大多通过全局条件归一化注入噪声，导致训练成本高且难以解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种更高效、可解释的噪声注入机制，使得模型能够生成可重复的概率预测，并在中尺度到大尺度天气预报中实现良好的校准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SDL 采用层级噪声注入，分别在解码器的三个尺度上通过潜在驱动调制、像素级噪声和通道缩放来引入扰动；通过迁移学习将 SDL 应用于 WXFormer，生成的每个集成成员仅需 5 MB 的潜在张量即可重现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 2022 ERA5 重新分析数据上，SDL 集成的扩散与技能比接近 1，排名直方图在中期预报中逐渐趋于均匀，校准水平与 IFS‑ENS 相当；多尺度实验表明粗尺度层调节大尺度天气模式，细尺度层控制中尺度变异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SDL 提供了一种低成本、可解释的概率天气预测框架，适用于操作预报和气候研究，并实现了与现有高性能集成相竞争的校准效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; AI天气预测集成通过潜在噪声注入并使用连续排名概率分数（CRPS）进行优化，已产生既准确又校准良好的预测，并且计算成本远低于基于扩散的方法。然而，当前的CRPS集成方法在训练策略和噪声注入机制上各不相同，大多数通过条件归一化在网络中全局注入噪声。这种结构增加了训练费用，并限制了随机扰动的物理可解释性。我们提出了随机分解层（SDL），用于将确定性机器学习天气模型转换为概率集成系统。SDL 采用来自 StyleGAN 的层级噪声注入，分别在解码器的三个尺度上通过潜在驱动调制、像素级噪声和通道缩放来应用学习到的扰动。当通过迁移学习将 SDL 应用于 WXFormer 时，SDL 的计算成本不到基线模型训练成本的 2%。每个集成成员由一个紧凑的潜在张量（5 MB）生成，实现了完美可重复性，并通过潜在重缩放在推理后进行扩散调整。对 2022 ERA5 重新分析的评估显示，集成的扩散-技能比接近 1，排名直方图在中期预报中逐渐趋向均匀，校准水平与运营 IFS‑ENS 竞争。多尺度实验揭示了层级不确定性：粗尺度层调节大尺度天气模式，细尺度层控制中尺度变异。显式的潜在参数化为运营预报和气候应用提供了可解释的不确定性量化。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN&amp;#x27;s hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.&lt;/p&gt;</description></item><item><guid>2512.18947v1</guid><title>Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm</title><link>http://arxiv.org/abs/2512.18947v1</link><author>Li Yan, Bolun Liu, Chao Li, Jing Liang, Kunjie Yu, Caitong Yue, Xuzhao Chai, Boyang Qu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究动态多模态多目标优化，提出新的基准测试集和基于聚类自编码器的动态响应算法，并结合自适应巢化策略，实验表明在决策空间保持多样性更好，目标空间收敛更优。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 动态多模态多目标优化面临同时跟踪多个等价帕累托最优集和保持种群多样性的双重挑战。现有动态多目标进化算法往往忽视解的多模态性，静态多模态多目标算法缺乏对动态变化的适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决动态多模态多目标优化中对多模态性和动态适应性的双重需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 构建融合动态与多模态特性的基准测试集；2) 提出基于聚类自编码器预测的动态响应机制，利用自编码器处理匹配的聚类生成多样化初始种群；3) 在静态优化器中加入自适应巢化策略以平衡收敛与多样性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在12个动态多模态多目标测试实例上，与多种先进算法相比，本文算法在决策空间更好地保持种群多样性，在目标空间实现更优的收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提出的算法在动态多模态多目标优化中兼顾多样性与收敛性，表现优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 动态多模态多目标优化面临同时跟踪多个等价帕累托最优集和在时变环境中保持种群多样性的双重挑战。然而，现有的动态多目标进化算法往往忽视解的多模态性，而静态多模态多目标进化算法缺乏对动态变化的适应性。为了解决上述挑战，本文做出了两个主要贡献。首先，我们引入了一个新的基准套件，将动态和多模态优化的属性融合，构建了一个严格的评估平台。其次，我们提出了一种基于聚类自编码器预测的动态响应机制的新算法，该机制利用自编码器模型处理匹配的聚类，生成高度多样化的初始种群。此外，为了平衡算法的收敛性和多样性，我们将自适应巢化策略集成到静态优化器中。对12个动态多模态多目标测试实例的经验分析表明，与多种最先进的动态多目标进化算法和多模态多目标进化算法相比，我们的算法不仅在决策空间更有效地保持种群多样性，而且在目标空间实现了更优的收敛。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm&amp;#x27;s convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.&lt;/p&gt;</description></item><item><guid>2512.18954v1</guid><title>VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion</title><link>http://arxiv.org/abs/2512.18954v1</link><author>Zaidao Han, Risa Higashita, Jiang Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. SSC 通过单张图像推断完整的三维语义与几何体素。2. 现有方法忽略可见区域感知与遮挡区域推理之间的干扰，导致特征稀释与误差传播。3. 提出离线可见区域标签提取（VRLE）策略，分离可见体素的监督。4. 设计可见-遮挡交互完成网络（VOIC），双解码器框架分别处理可见语义感知与遮挡场景完成。5. 在 SemanticKITTI 与 SSCBench-KITTI360 基准上，VOIC 在几何完成与语义分割上均超越现有单目 SSC 方法，取得最先进性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; SSC 是自动驾驶与机器人场景理解的关键任务，目标是从单张图像生成完整的三维语义与几何表示。单图输入导致可见区域与被遮挡区域推理之间存在干扰，影响性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决单图输入导致的可见区域感知与遮挡区域推理之间的干扰，提高 SSC 的几何与语义完成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 离线可见区域标签提取（VRLE）策略，从稠密 3D 真实标签中提取可见体素的监督。2. 可见-遮挡交互完成网络（VOIC）双解码器框架：   - 基础 3D 体素表示通过融合图像特征与深度占据率构建。   - 可见解码器生成高保真几何与语义先验。   - 遮挡解码器利用先验与跨模态交互进行全局场景推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; VOIC 在 SemanticKITTI 与 SSCBench-KITTI360 基准上，在几何完成与语义分割准确率上均优于现有单目 SSC 方法，取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过显式分离可见与遮挡子任务的监督，并采用双解码器交互式完成框架，VOIC 有效提升了单目 SSC 的整体表现，树立了新的性能基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Camera-based 3D Semantic Scene Completion (SSC) 是自动驾驶和机器人场景理解中的关键任务。它旨在从单张图像推断完整的三维体素化语义与几何表示。现有方法通常聚焦于端到端的 2D 到 3D 特征提升与体素完成，但往往忽视单图输入导致的高置信度可见区域感知与低置信度被遮挡区域推理之间的干扰，进而产生特征稀释与误差传播。为解决这些挑战，我们提出离线可见区域标签提取（VRLE）策略，显式从稠密 3D 真实标签中分离并提取可见区域的体素级监督，净化可见感知与遮挡推理两项互补子任务的监督空间。在此基础上，我们提出可见-遮挡交互完成网络（VOIC），一种新颖的双解码器框架，将 SSC 明确拆分为可见区域语义感知与遮挡区域场景完成。VOIC 首先通过融合图像特征与深度衍生占据率构建基础 3D 体素表示；可见解码器聚焦生成高保真几何与语义先验；遮挡解码器利用这些先验与跨模态交互进行全局场景推理。对 SemanticKITTI 与 SSCBench-KITTI360 基准的广泛实验表明，VOIC 在几何完成与语义分割准确率上均优于现有单目 SSC 方法，达成了最先进的性能。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决单目图像下的三维语义场景完成问题，即从一张RGB图像推断完整的三维几何和语义体素。该问题在自动驾驶、机器人导航和增强现实等领域至关重要，因为它提供了对环境的完整、结构化理解，而不依赖昂贵的LiDAR传感器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有单目SSC方法在可见与被遮挡区域之间的统一解码导致特征稀释和误差传播，因而提出将任务拆分为可见区域感知和被遮挡区域完成两步。设计中借鉴了MonoScene、VoxFormer等前沿工作，并在此基础上引入可见区域标签提取（VRLE）和双解码器架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用VRLE生成可见体素的监督标签，再通过可见解码器（VD）精确重建可见体素的几何和语义，随后用遮挡解码器（OD）利用VD产生的空间语义先验完成整个场景。实现流程为：输入图像 → 2D特征提取 → VEFC将特征投影到3D → VD预测可见体素 → OD结合VD先验完成全体素 → 输出完整的3D语义体素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) VRLE离线可见区域标签提取，提供明确的可见监督；2) 双解码器（可见-遮挡）架构，采用可见优先、遮挡后续的推理顺序；3) VEFC与多级位置编码提升体素几何辨识度；4) 自适应特征交互模块纠正可见先验误差。与以往统一解码或仅使用多帧/多视角的工作不同，VOIC在单帧单目条件下实现了更高精度的完整场景完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VOIC通过可见-遮挡解耦、可见标签提取和双解码器设计，在单目图像下实现了领先的三维语义场景完成。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.   To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.   Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.&lt;/p&gt;</description></item><item><guid>2512.19036v1</guid><title>Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition</title><link>http://arxiv.org/abs/2512.19036v1</link><author>Xiaoyang Li, Mingming Lu, Ruiqi Wang, Hao Li, Zewei Le</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出CLIP-SPM框架，解决少样本动作识别中的时序建模、视觉相似度和模态差距三大核心挑战，包含HSMR、SPM和PADM三大模块，实验表明在多种基准上表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 少样本动作识别面临数据稀缺，需快速学习新动作类别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决时序建模、视觉相似度和模态差距三大核心挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CLIP-SPM框架：HSMR模块对齐深浅运动特征；SPM策略生成文本提示并与视觉特征融合；PADM方法细化支持原型并与全局语义锚对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Kinetics、SSv2-Full、SSv2-Small、UCF101、HMDB51等基准上，CLIP-SPM在1-shot、3-shot、5-shot设置下表现竞争力；各模块贡献得到消融验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CLIP-SPM有效提升少样本动作识别性能，三大模块共同解决核心挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 少样本动作识别旨在使模型能够从有限的标注样本中快速学习新的动作类别，解决现实应用中数据稀缺的挑战。目前的研究主要聚焦于三个核心难点：（1）时序建模，模型易受无关静态背景信息干扰，难以捕捉动态动作特征的本质；（2）视觉相似度，视觉差异细微的类别难以区分；（3）视觉-文本支持原型与仅视觉查询之间的模态差距，导致在共享嵌入空间中的对齐变得复杂。为解决这些挑战，本文提出了 CLIP-SPM 框架，包含三个组件：（1）层次协同运动细化（HSMR）模块，通过对齐深浅层运动特征，减少静态背景干扰，提升时序建模效果；（2）语义原型调制（SPM）策略，生成与查询相关的文本提示，弥合模态差距并与视觉特征融合，增强相似动作之间的判别力；（3）原型-锚点双重调制（PADM）方法，细化支持原型并将查询特征与全局语义锚点对齐，提升支持样本与查询样本之间的一致性。对 Kinetics、SSv2-Full、SSv2-Small、UCF101 和 HMDB51 等标准基准进行的全面实验表明，CLIP-SPM 在 1-shot、3-shot 和 5-shot 设置下取得了竞争性表现。大量消融研究和可视化分析进一步验证了每个组件的有效性及其在解决核心挑战中的贡献。源代码和模型已公开发布在 GitHub。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.&lt;/p&gt;</description></item><item><guid>2512.19037v1</guid><title>Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms</title><link>http://arxiv.org/abs/2512.19037v1</link><author>Md Minhazul Islam Munna, Md Mahbubur Rahman, Jaroslav Frnda, Muhammad Shahid Anwar, Alpamis Kutlimuratov</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对物联网设备普及导致的Wi‑Fi安全漏洞，提出一种多类别机器学习入侵检测框架，利用高级特征选择和集成学习显著提升检测准确率并降低误报。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 物联网设备广泛使用Wi‑Fi网络，KRACK和Kr00k等攻击利用WPA2加密弱点，能够拦截和篡改敏感数据。传统基于分类器的入侵检测系统易出现过拟合、特征提取不完整和误报率高等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种鲁棒的多类别机器学习入侵检测框架，克服传统方法的局限，提高在真实环境中的检测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用先进的特征选择技术识别关键属性，减少冗余；实现两种机器学习架构：基线分类器流水线和堆叠集成模型（包含噪声注入、主成分分析和元学习），以提升泛化能力并降低误报。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在AWID3数据集上，堆叠集成模型实现了98%的准确率、精确率和召回率，误报率仅为2%，优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将预处理策略与集成学习相结合，可有效加强对复杂Wi‑Fi攻击的网络安全防护，为物联网环境提供可扩展、可靠的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 物联网设备的普及及其对Wi‑Fi网络的依赖引入了显著的安全漏洞，尤其是KRACK和Kr00k攻击，它们利用WPA2加密中的弱点拦截并操纵敏感数据。传统使用分类器的入侵检测系统面临模型过拟合、特征提取不完整和高误报率等挑战，限制了其在实际部署中的有效性。为解决这些挑战，本研究提出了一种基于多类别机器学习的强大入侵检测框架。该方法结合先进的特征选择技术，识别关键属性，减少冗余并提升检测准确性。实现了两种不同的机器学习架构：基线分类器流水线和堆叠集成模型，后者结合噪声注入、主成分分析（PCA）和元学习，以提高泛化能力并降低误报。对AWID3数据集进行评估后，所提出的集成架构实现了98%的准确率、精确率和召回率，误报率仅为2%，优于现有最先进方法。该工作展示了将预处理策略与集成学习相结合的有效性，以加强对复杂Wi‑Fi攻击的网络安全，提供了可扩展且可靠的物联网环境解决方案。未来方向包括实时部署和对抗性鲁棒性测试，以进一步提升模型的适应性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model&amp;#x27;s adaptability.&lt;/p&gt;</description></item><item><guid>2512.19083v2</guid><title>CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models</title><link>http://arxiv.org/abs/2512.19083v2</link><author>Pengyu Chen, Tao Ouyang, Ke Luo, Weijie Hong, Xu Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; CoDrone 是一种云-边缘-端协同计算框架，利用基础模型提升资源受限无人机的自主导航性能。通过使用灰度图像、边缘深度估计和一维占据网格导航，结合深度强化学习调度器和视觉语言交互模块，CoDrone 能在不同飞行速度和网络条件下显著提高飞行距离和导航质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 无人机自主导航受限于机载计算资源，导致只能使用浅层深度网络，难以处理复杂环境；将任务卸载到边缘服务器会产生高延迟，系统设计面临权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决机载计算受限和边缘延迟带来的挑战，构建一种协同计算框架，提升无人机在复杂环境中的导航性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CoDrone 采用灰度图像进行导航模型，必要时使用边缘基础模型 Depth Anything V2 进行深度估计，并引入一维占据网格导航方法；通过深度强化学习神经调度器将深度估计与导航决策融合；同时加入 UAV 专用视觉语言交互模块，支持低级飞行指令与云模型的交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，CoDrone 在不同飞行速度和网络条件下比基线方法提升平均飞行距离 40% ，平均导航质量提升 5%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CoDrone 通过云-边缘-端协同和基础模型的集成，显著提升了资源受限无人机的自主导航能力，验证了其在复杂环境中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了在资源受限的无人机上实现可靠自主导航的问题，避免使用过重的深度神经网络，同时减少因将任务卸载到远程服务器而产生的高延迟。该问题重要，因为无人机在物流、监测和灾害响应等应用中需要在复杂环境中安全、无碰撞飞行，而计算资源和能源往往十分有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了轻量化 DNN 与纯边缘卸载方案的不足，然后借鉴了 Depth Anything V2 等深度估计模型和视觉语言模型的优势，构建了一个混合架构。该架构使用灰度图像进行本地推理，必要时向边缘服务器请求深度估计，并在复杂场景下调用云端视觉语言模型；整个过程由 DRL 调度器学习何时卸载，借鉴了 adaDrone、DRL 调度等已有工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CoDrone 的核心思想是让无人机运行极轻量的导航网络，必要时向边缘服务器请求深度估计，再在更复杂或未知场景下调用云端视觉语言模型。实现流程为：捕获灰度图像 → 本地网络输出转向角和碰撞风险；若需要更细致感知，则将图像发送给边缘服务器，得到深度并构建占据格栅，再反馈给导航网络；若情况复杂，调度器触发云端视觉语言模型，模型通过低级飞行动作原语给出语义指导；所有决策实时完成，保持低延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）云‑边‑端协同框架，将基础模型与轻量化本地推理结合；2）使用灰度图像降低计算和带宽开销；3）边缘深度估计与一维占据格栅实现细粒度感知；4）基于 DRL 的神经调度器学习卸载策略；5）无人机专用视觉语言交互模块，提供可调用的低级飞行动作原语。与以往仅使用浅层 DNN 或简单边缘卸载的方案不同，CoDrone 同时利用深度与语义推理，保持实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoDrone 展示了通过协调轻量化本地导航、边缘深度估计和云端视觉语言推理，在资源受限且网络条件变化的环境下显著提升无人机自主导航性能的可行性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.&lt;/p&gt;</description></item><item><guid>2512.19107v1</guid><title>FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning</title><link>http://arxiv.org/abs/2512.19107v1</link><author>Zhe Yang, Xiaoshuang Sheng, Zhengnan Zhang, Jidong Wu, Zexing Wang, Xin He, Shenghua Xu, Guanjing Xiong</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 FC-MIR 框架，通过关键帧采样和自适应拼接降低视觉冗余，并结合多模态大型语言模型实现移动 UI 轨迹的意图识别与摘要，实验显示压缩后性能保持在 50%–60% 之间，支持轻量化设备部署，但在生成建议方面仍需改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 移动 UI 轨迹意图识别对 UI 理解和任务自动化至关重要；多模态大型语言模型在视频理解上表现优异，但实时移动部署受限于高计算成本和冗余帧处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种高效的框架 FC-MIR，降低视觉冗余并实现移动端实时意图识别与摘要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 关键帧采样、自适应拼接、集成闭源或微调多模态大型语言模型进行轨迹摘要与意图预测；扩展任务生成后续操作与搜索建议；引入细粒度评估指标；构建 UI 轨迹数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 压缩方法在 50%–60% 压缩率下保持性能；闭源和微调模型均能强力摘要意图；但在生成有用且“惊喜”的建议方面仍有不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FC-MIR 能实现高效推理并支持轻量化设备部署，但建议生成仍需提升；已在真实环境部署，为未来研究奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 识别移动用户界面操作轨迹中的用户意图对于推进 UI 理解和实现任务自动化代理至关重要。虽然多模态大型语言模型在视频理解任务中表现出色，但其实时移动部署受到高计算成本和冗余帧处理效率低下的限制。为解决这些问题，我们提出了 FC-MIR 框架：通过关键帧采样和自适应拼接，减少视觉冗余以提升推理效率，同时集成最先进的闭源多模态大型语言模型或微调模型（如 Qwen3-VL）进行轨迹摘要和意图预测。我们进一步扩大任务范围，探索生成预测后操作和搜索建议，并引入细粒度指标评估摘要、预测和建议的实际效用。为严格评估，我们构建了覆盖 UI-Agents（Agent-I）和真实用户交互（Person-I）场景的 UI 轨迹数据集。实验结果表明，我们的压缩方法在 50%–60% 压缩率下保持性能；闭源和微调的多模态大型语言模型均表现出强大的意图摘要能力，支持轻量级设备端部署。然而，多模态大型语言模型在提供有用且“惊喜”的建议方面仍存在困难，仍有提升空间。最后，我们在真实环境中部署了该框架，集成 UI 感知和 UI-Agent 代理，为该领域未来进展奠定基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and &amp;quot;surprising&amp;quot; suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.&lt;/p&gt;</description></item><item><guid>2512.19130v1</guid><title>D$^{2}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection</title><link>http://arxiv.org/abs/2512.19130v1</link><author>Junhao Xiao, Shun Feng, Zhiyu Wu, Jianjun Li, Zhiyuan Ma, Yi Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 D^2Stream 的双流框架，用于音视频说话人检测。该框架将跨帧时间建模与帧内说话人区分分离，采用轻量化的时间交互流和说话人交互流，并通过交叉注意力融合特征。加入 Voice Gate 模块降低非语音面部动作误检。实验表明，在 AVA-ActiveSpeaker 数据集上取得 95.6% mAP 的新纪录，同时相比 GNN 模型减少 80% 计算量，参数量比注意力模型少 30%，并在 Columbia ASD 数据集上表现良好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 音视频说话人检测旨在通过音频和视觉信息识别视频中的活跃说话人。传统方法往往在联合建模时间与说话人交互时出现计算效率低或性能不佳的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种高效且性能优异的双流框架，以解决现有方法的计算瓶颈和交互建模不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先使用跨模态注意力对音频和视觉特征进行对齐，然后分别输入到两个轻量化流：时间交互流捕捉长距离时间依赖；说话人交互流建模每帧内的人际关系。两流提取的时间与关系特征通过交叉注意力相互作用以丰富表示。随后加入轻量化 Voice Gate 模块，抑制非语音面部动作导致的误检。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 AVA-ActiveSpeaker 数据集上，D^2Stream 达到 95.6% mAP 的新纪录；与基于 GNN 的模型相比，计算量减少 80%；与基于注意力的替代方案相比，参数量减少 30%；并在 Columbia ASD 数据集上表现出良好的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; D^2Stream 在保持高精度的同时显著降低计算成本和模型复杂度，证明了双流解耦设计在音视频说话人检测中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 音视频说话人检测旨在通过利用互补的音频和视觉线索识别视频中的活跃说话人。现有方法往往因联合建模时间与说话人交互而导致计算效率低下或性能不佳。我们提出 D^2Stream，一种解耦的双流框架，将跨帧时间建模与帧内说话人区分分离。音频和视觉特征首先通过跨模态注意力对齐，然后输入到两个轻量化流：时间交互流捕捉长距离时间依赖，说话人交互流建模每帧内的人际关系。两流提取的时间与关系特征通过交叉注意力相互作用以丰富表示。轻量化 Voice Gate 模块进一步降低非语音面部动作导致的误检。在 AVA-ActiveSpeaker 数据集上，D^2Stream 在 95.6% mAP 上取得新的最优成绩，相比基于 GNN 的模型计算量减少 80%，相较于基于注意力的替代方案参数量减少 30%，并在 Columbia ASD 数据集上也表现出良好的泛化能力。源代码可在 https://anonymous.4open.science/r/D2STREAM 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Audio-visual speaker detection aims to identify the active speaker in videos by leveraging complementary audio and visual cues. Existing methods often suffer from computational inefficiency or suboptimal performance due to joint modeling of temporal and speaker interactions. We propose D$^{2}$Stream, a decoupled dual-stream framework that separates cross-frame temporal modeling from within-frame speaker discrimination. Audio and visual features are first aligned via cross-modal attention, then fed into two lightweight streams: a Temporal Interaction Stream captures long-range temporal dependencies, while a Speaker Interaction Stream models per-frame inter-person relationships. The temporal and relational features extracted by the two streams interact via cross-attention to enrich representations. A lightweight Voice Gate module further mitigates false positives from non-speech facial movements. On AVA-ActiveSpeaker, D$^{2}$Stream achieves a new state-of-the-art at 95.6% mAP, with 80% reduction in computation compared to GNN-based models and 30% fewer parameters than attention-based alternatives, while also generalizing well on Columbia ASD. Source code is available at https://anonymous.4open.science/r/D2STREAM.&lt;/p&gt;</description></item><item><guid>2512.19150v1</guid><title>AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction</title><link>http://arxiv.org/abs/2512.19150v1</link><author>Ruikai Li, Xinrun Li, Mengwei Xie, Hao Shan, Shoumeng Qiu, Xinyuan Chang, Yizhe Fan, Feng Xiong, Han Jiang, Yilong Ren, Haiyang Yu, Mu Xu, Yang Long, Varun Ojha, Zhiyong Cui</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; AMap提出了一种前瞻性在线高清地图构建框架，通过教师-学生蒸馏方式将未来时序信息压缩到仅使用当前帧的轻量模型，从而在不增加推理成本的前提下显著提升前方道路的感知质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在线高清地图构建是自动驾驶的关键技术。现有方法多利用历史时序融合来提升性能，但其本质上是“向后看的”，主要改善已行驶区域的地图重建，对未见的前方道路改进有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补现有方法在前方感知上的安全缺口，使得前方误差不再导致危险驾驶行为，同时保持单帧推理的高效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用“distill-from-future”范式，先用拥有未来时序上下文的教师模型训练，再将其知识蒸馏到仅使用当前帧的学生模型；技术细节包括多层BEV蒸馏、空间遮罩以及非对称查询适配模块，以有效迁移前瞻性表征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和Argoverse 2基准上，AMap在当前帧感知上显著优于现有时序模型，尤其在关键前方区域表现更佳，同时保持单帧推理的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 前瞻性在线高清地图构建能够在不增加推理成本的情况下提升前方感知安全性，为自动驾驶提供更可靠的地图信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在线高清地图构建是自动驾驶的关键技术。虽然最近的方法利用历史时序融合来提升性能，但我们发现这种范式存在一个关键的安全缺陷：它本质上是“向后看的”。这些方法主要提升已行驶区域的地图重建，对未见的前方道路改进有限。我们的下游规划任务分析显示，后向感知误差往往可以容忍，而前向区域的不准确直接导致危险驾驶行为。为弥补这一安全缺口，我们提出了AMap，一种前瞻性在线高清地图构建框架。我们开创了“distill-from-future”范式，使用拥有未来时序上下文的教师模型指导仅受限于当前帧的轻量学生模型。该过程隐式地将未来知识压缩到学生模型中，使其在零推理时成本下具备“前瞻”能力。技术上，我们引入了多层BEV蒸馏策略，配合空间遮罩和非对称查询适配模块，有效地将前瞻性表征迁移到学生模型的静态查询。对nuScenes和Argoverse 2基准的广泛实验表明，AMap显著提升了当前帧感知。最值得注意的是，它在关键前方区域的表现超过了最先进的时序模型，同时保持了单帧推理的效率。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文指出现有在线高清地图构建方法主要利用历史帧进行融合，导致模型在车辆已通过的后方区域表现良好，而对前方未见道路的感知提升有限。前方区域的准确性对路径规划和安全决策至关重要，后方误差对后续任务影响较小，因此需要一种能够提升前方感知的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先通过实验量化了前后区域对下游任务的不同影响，发现前方信息更关键。随后借鉴知识蒸馏、BEV 表示和向量化地图构建等已有技术，提出“distill-from-future”框架：先训练拥有未来时序信息的教师模型，再将其未来感知知识蒸馏给仅使用当前帧的学生模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用未来帧提供的先验知识，通过知识蒸馏将前瞻性表示压缩进仅使用当前帧的学生网络，从而在推理时获得“看前”能力。实现流程包括：①训练教师模型加入未来帧上下文；②使用BEV遮罩、多层BEV特征蒸馏和不对称查询匹配模块，将教师的未来感知信息迁移给学生；③学生在推理时仅输入当前帧即可得到前方感知增强的地图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①首次系统识别并量化了后向偏差，并提出前向感知评估指标 A‑mAP；②提出“distill-from-future”蒸馏范式，将未来时序信息作为教师监督；③设计了BEV遮罩、多层BEV蒸馏和查询匹配转移模块，解决异构上下文蒸馏难题；④在保持单帧推理效率的同时，使前方感知精度超过现有多帧融合模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AMap通过将未来时序信息蒸馏给仅使用当前帧的学生模型，实现零推理成本的前瞻性高清地图构建，显著提升前方区域的感知精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking.&amp;quot; These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future&amp;quot; paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead&amp;quot; capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student&amp;#x27;s static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.&lt;/p&gt;</description></item><item><guid>2512.19221v1</guid><title>From Pixels to Predicates Structuring urban perception with scene graphs</title><link>http://arxiv.org/abs/2512.19221v1</link><author>Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出三阶段基于图结构的街景感知预测方法，显著提升准确率并具备跨城市泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统感知研究多依赖像素特征或对象共现统计，忽视了显式关系对人类感知的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建能够捕捉街景中对象关系的结构化表示，以更准确、可解释地预测六项感知指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 用OpenPSG解析图像得到对象-谓词-对象三元组；2) 用GraphMAE学习场景级嵌入；3) 用神经网络根据嵌入预测感知分数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法比仅基于图像的基线提升平均26%准确率，且在跨城市预测中保持强泛化；结构化表示揭示如墙面涂鸦、停在人行道上的车辆等关系导致低感知分数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于图的结构化表示为城市感知建模提供了表达力强、可泛化且可解释的信号，推动以人为中心的城市分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Perception research increasingly models streetscapes, yet many approaches rely on pixel features or object co-occurrence statistics, overlooking explicit relations that shape human perception. This study proposes a three-stage pipeline that transforms street view imagery into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object–predicate–object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross‑city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26 % over baseline models, and (ii) maintains strong generalization performance in cross‑city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on walls and cars parked on sidewalks. Overall, this study demonstrates that graph‑based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human‑centric and context‑aware urban analytics.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.&lt;/p&gt;</description></item><item><guid>2512.19246v1</guid><title>From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis</title><link>http://arxiv.org/abs/2512.19246v1</link><author>Moncef Garouani, Ayah Barhrhouj</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; MetaSHAP 是一种可扩展的半自动可解释 AI 方法，利用元学习和 Shapley 值分析，为机器学习模型的超参数调优提供可操作且与数据集相关的洞察。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 超参数调优是优化机器学习模型的关键步骤，但计算成本高；了解超参数的重要性和相互作用对于高效开发至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 MetaSHAP，旨在通过可解释的分析帮助研究者和工程师更好地理解和调优超参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MetaSHAP 在超过 900 万条已评估的机器学习管道的基准上进行训练，先用历史配置学习一个代理性能模型，然后使用基于 SHAP 的方法计算超参数之间的交互，最后从最具影响力的超参数中提取可解释的调优范围。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 164 个分类数据集和 14 种分类器的实验中，MetaSHAP 能够生成可靠的超参数重要性排名，并在使用贝叶斯优化时实现与传统方法相当甚至更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MetaSHAP 为超参数调优提供了可解释且实用的工具，能够帮助优先选择重要超参数、理解其方向性和交互，从而提升调优效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 超参数调优是优化机器学习模型的基本步骤，但计算成本高。除了优化之外，了解超参数的相对重要性和相互作用对于高效的模型开发至关重要。本文提出了 MetaSHAP，一种可扩展的半自动可解释人工智能方法，利用元学习和 Shapley 值分析为调优提供可操作且与数据集相关的洞察。MetaSHAP 在超过 900 万条已评估的机器学习管道的基准上运行，能够生成可解释的重要性得分和可操作的调优洞察，揭示每个超参数的重要程度、与其他超参数的交互以及其影响集中在哪些取值范围。对于给定的算法和数据集，MetaSHAP 从历史配置中学习代理性能模型，使用基于 SHAP 的分析计算超参数交互，并从最具影响力的超参数中推导可解释的调优范围。这使得从业者不仅可以优先考虑需要调优的超参数，还能了解其方向性和交互。我们在 164 个分类数据集和 14 个分类器的多样化基准上对 MetaSHAP 进行了实证验证，证明它在提供可靠的重要性排名和在指导贝叶斯优化时实现竞争性性能方面表现良好。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.&lt;/p&gt;</description></item><item><guid>2512.19504v1</guid><title>FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors</title><link>http://arxiv.org/abs/2512.19504v1</link><author>Georgios Voulgaris</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种物理感知的表示学习框架，利用多光谱信息建模长期物理过程的稳定特征，提升跨光谱和真实环境下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统深度学习模型在多模态视觉信号上依赖的归纳偏置与信号形成的物理过程不匹配，导致在跨光谱和实际条件下性能脆弱，尤其是直接热信号的模型难以捕捉由持续热排放引起的间接环境变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入物理感知的特征选择和深度学习架构，改进多光谱学习的稳健性和泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用地质短波红外比值（对土壤属性变化敏感）与热红外数据通过中间融合网络 FusionNet 进行融合；骨干网络嵌入可训练的差分信号处理先验、混合池化和更宽的感受野，以增强跨光谱鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 每个架构组件均对性能提升有贡献；DGCNN 在 SWIR 比值上达到 88.7% 准确率，FusionNet 达到 90.6%，在五种光谱配置下均优于现有基线；ImageNet 预训练对 TIR 性能有负面影响，强调了模态感知训练的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将物理感知特征选择与严谨的深度学习架构相结合，可获得稳健且可泛化的表示，证明基于第一性原理的信号建模能在挑战性条件下提升多光谱学习效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions. This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities. Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning. Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.   This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.   Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.   Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.&lt;/p&gt;</description></item><item><guid>2512.19509v1</guid><title>Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models</title><link>http://arxiv.org/abs/2512.19509v1</link><author>Shangbo Yun, Xiaodong Gu, Jianghong Huang, Beijun Shen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了编程语言之间的深层关系，并探讨如何利用这些关系提升多语言代码大模型的训练与推理效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着多种编程语言的快速发展，构建多语言代码大模型面临机遇与挑战。现有方法多通过聚合多语言数据训练，但很少深入挖掘语言间的内在联系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 1) 探索编程语言之间的深层语言关系；2) 利用这些关系改进多语言代码大模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 定义21个编程语言的核心语言特征，使用大模型生成跨语言对齐的代码样本，嵌入19种语言的语义平行代码，构建相似度矩阵并进行层次聚类，识别语言家族。基于此，提出三种提升策略：跨相关语言的迁移学习、基于语言亲近度的课程学习以及基于中心的中介代码翻译。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 层次聚类揭示了编程语言的清晰层级结构，相关语言形成聚类（如C、C++、Java、Swift），Go语言在多语言相似度中居中。三种策略在四项代码智能任务中显著提升了多语言大模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提供了对编程语言的统一视角，并提出了更有效的多语言代码大模型训练策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 快速扩张的多样化编程语言为开发多语言代码大模型带来了机遇与挑战。现有技术往往通过简单聚合多语言代码数据来训练代码大模型，但很少探讨编程语言之间的深层关系以及如何利用这些关系来优化代码大模型的训练和推理。本研究探讨了两个基本问题：1）编程语言之间的深层语言关系是什么？2）如何利用这些关系来提升多语言代码大模型？我们提出了一个基于嵌入的框架来揭示编程语言的潜在家族。我们的方法首先定义了21个编程语言的主要语言特征，如变量定义、控制结构和方法声明，然后使用大模型生成跨多种语言的特征对齐代码样本。通过嵌入来自19种语言的语义平行代码片段，我们构建了相似度矩阵并进行层次聚类，以揭示内在的语言关系。我们的分析揭示了编程语言之间的清晰层级结构。相关语言形成了明确定义的聚类（例如C、C++、Java和Swift聚在一起），而Go则作为中心语言，具有最高的跨语言相似度。在揭示语言家族的基础上，我们提出了三种策略来提升多语言大模型的训练：跨语言相关性迁移学习、基于语言亲近度的课程学习以及基于中心的中介代码翻译。对四个代码智能任务的实验表明，我们的方法显著提升了多语言大模型的性能。本工作从统一视角审视编程语言，并推动了更有效的多语言代码大模型训练策略。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.&lt;/p&gt;</description></item><item><guid>2512.19540v1</guid><title>Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</title><link>http://arxiv.org/abs/2512.19540v1</link><author>Adrian A. Moazzam, Anindya Ghoshroy, Breeanne Heusdens, Durdu O. Guney, Roohollah Askari</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨将主动卷积照明（ACI）与深度学习方法结合，以减轻大气湍流对光学系统的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大气湍流是光学成像、遥感和自由空间光通信等领域的根本限制因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估将ACI与神经网络模型集成的可行性，并确定学习表示如何支持ACI的相关注入机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出概念框架，使用卷积神经网络和迁移学习来测试ACI与数据驱动模型的协同工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明存在可行的实现路径，为未来ACI-深度学习混合架构奠定早期基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ACI与现代深度学习模型的协同有潜力提升湍流补偿效果，值得进一步研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大气湍流在光学成像、遥感和自由空间光通信等广泛应用中构成根本限制。最近在自适应光学、波前成形和机器学习方面的进展，得益于基础理论、光电硬件和计算算法的协同发展，已显示出显著的减轻湍流诱发失真的潜力。最近提出的主动卷积照明（ACI）是一种多功能且基于物理的技术，可在极具挑战性的湍流环境中以最小失真传输结构化光束。虽然其形式独特，但ACI与其他基于物理的失真校正方法在概念上相似，并有望与数据驱动的深度学习模型互补集成。受将深度学习与传统湍流补偿策略相结合的最新工作启发，本文研究了将ACI与神经网络方法集成的可行性。我们提出了将ACI与数据驱动模型耦合的概念框架，并确定了学习表示能够有意义支持ACI相关注入机制的条件。作为代表性示例，我们使用卷积神经网络和迁移学习方法，探讨学习模型如何与ACI协同工作。该探索性研究展示了可行的实现路径，并为评估未来ACI-深度学习混合架构的潜力奠定了早期基础，迈出了评估ACI与现代深度学习模型更广泛协同作用的步伐。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Atmospheric turbulence imposes a fundamental limitation across a broad range of applications, including optical imaging, remote sensing, and free-space optical communication. Recent advances in adaptive optics, wavefront shaping, and machine learning, driven by synergistic progress in fundamental theories, optoelectronic hardware, and computational algorithms, have demonstrated substantial potential in mitigating turbulence-induced distortions. Recently, active convolved illumination (ACI) was proposed as a versatile and physics-driven technique for transmitting structured light beams with minimal distortion through highly challenging turbulent regimes. While distinct in its formulation, ACI shares conceptual similarities with other physics-driven distortion correction approaches and stands to benefit from complementary integration with data-driven deep learning (DL) models. Inspired by recent work coupling deep learning with traditional turbulence mitigation strategies, the present work investigates the feasibility of integrating ACI with neural network-based methods. We outline a conceptual framework for coupling ACI with data-driven models and identify conditions under which learned representations can meaningfully support ACI&amp;#x27;s correlation-injection mechanism. As a representative example, we employ a convolutional neural network (CNN) together with a transfer-learning approach to examine how a learned model may operate in tandem with ACI. This exploratory study demonstrates feasible implementation pathways and establishes an early foundation for assessing the potential of future ACI-DL hybrid architectures, representing a step toward evaluating broader synergistic interactions between ACI and modern DL models.&lt;/p&gt;</description></item><item><guid>2512.19575v1</guid><title>Variational Autoregressive Networks Applied to $φ^4$ Field Theory Systems</title><link>http://arxiv.org/abs/2512.19575v1</link><author>Moxian Qian, Shiyang Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文将强化学习与变分自回归网络相结合，针对离散伊辛模型和连续四次标量场理论实现无数据训练与采样，并通过一系列技术提升采样质量与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 研究对象为离散伊辛模型和连续四次标量场理论，这两种模型在统计物理和量子场论中具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 实现无数据训练与采样，量化目标分布的复杂度，并通过迁移学习和Metropolis-Hastings修正提高采样精度与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 使用强化学习训练变分自回归网络进行无数据采样；2. 通过KL散度衡量磁化分布与高斯分布的差异；3. 在单一kappa值上预训练模型后进行微调以减少训练时间；4. 在变分网络提议基础上加入单点和块Metropolis-Hastings更新以校正残差偏差；5. 评估有效样本量和与传统Monte Carlo基准的对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1. KL散度越小的配置需要的训练步骤越少；2. 迁移学习显著降低训练时间；3. 单点和块Metropolis-Hastings更新能系统性降低偏差，同时保持高有效样本量；4. 对伊辛模型和四次标量场理论的结果与传统Monte Carlo基准一致；5. 在探测的参数范围内未观察到明显的临界慢化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结合强化学习与变分自回归网络，并辅以迁移学习和Metropolis-Hastings修正，可在离散与连续模型中实现高效、无数据的训练与采样，且保持与传统Monte Carlo方法相当的精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们将强化学习与变分自回归网络相结合，以实现对离散伊辛模型和连续四次标量场理论的无数据训练和采样。我们通过计算磁化分布与参考高斯分布之间的KL散度来量化目标分布的复杂度，并观察到KL散度较小的配置通常需要更少的训练步骤。基于这一观察，我们研究了迁移学习，并展示了在单一kappa值上预训练模型后进行微调可以减少训练时间。除此之外，受单点和簇Monte Carlo更新的启发，我们在变分网络提议的基础上引入了单点和块Metropolis-Hastings更新。我们发现这些Metropolis-Hastings校正在我们研究的参数范围内系统性地降低了纯变分网络采样的残差偏差，同时保持了高效样本量。对于伊辛模型和四次标量场理论，我们的结果与标准Monte Carlo基准在误差范围内一致，并且在探测的参数范围内未观察到明显的临界慢化。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We combine reinforcement learning with variational autoregressive networks (VANs) to perform data-free training and sampling for the discrete Ising model and the continuous $φ^4$ scalar field theory. We quantify the complexity of the target distribution via the KL divergence between the magnetization distribution and a reference Gaussian distribution, and observe that configurations with smaller KL divergence typically require fewer training steps. Motivated by this observation, we investigate transfer learning and show that fine-tuning models pretrained at a single value of $κ$ can reduce training time compared with training from a Gaussian field. In addition, inspired by single-site and cluster Monte Carlo updates, we introduce single-site and block Metropolis--Hastings (MH) updates on top of VAN proposals. These MH corrections systematically reduce the residual bias of pure VAN sampling in the parameter range we study, while maintaining high sampling efficiency in terms of the effective sample size (ESS). For both the Ising model and the $φ^4$ theory, our results agree with standard Monte Carlo benchmarks within errors, and no clear critical slowing down is observed in the explored parameter ranges.&lt;/p&gt;</description></item><item><guid>2512.19609v1</guid><title>MapTrace: Scalable Data Generation for Route Tracing on Maps</title><link>http://arxiv.org/abs/2512.19609v1</link><author>Artemis Panagopoulou, Aveek Purohit, Achin Kulshrestha, Soroosh Yazdani, Mohit Goyal</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对多模态大型语言模型在细粒度空间理解（如地图路线追踪）方面的不足，提出了一套可扩展的合成数据生成管道，并基于此构建了包含23k条路径样本、覆盖4k张地图的微调数据集。通过对开源和专有模型进行微调，显著提升了模型在MapBench上的鲁棒性和成功率，降低了路径追踪误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大型语言模型在视觉与文本推理任务上表现优异，但在需要精确空间约束的任务（如地图路线追踪）中表现有限，主要原因是缺乏大规模像素级路径标注数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用合成地图与像素级解析技术，生成高质量的路径标注数据，从而提升模型的细粒度空间推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建合成地图图像与像素级解析的自动标注管道，生成23k条路径样本覆盖4k张地图；使用该数据集对开源与专有多模态大型语言模型进行微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 微调后模型在MapBench上的成功率提升至多6.4个百分点，路径追踪误差（NDTW）显著下降，显示出显著的鲁棒性提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 细粒度空间推理缺失的预训练模型可以通过合成监督显式学习，从而获得更接近人类的空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大型语言模型在许多视觉和文本推理任务上已达到类似人类的表现，但它们在细粒度空间理解方面的能力有限，例如在地图上进行路线追踪。与人类能够快速学习解析和导航地图不同，当前模型往往无法遵守基本路径约束，这部分原因是收集大规模像素级路径标注的成本高昂且困难。为了解决这一问题，我们提出了一个可扩展的合成数据生成管道，利用合成地图图像和像素级解析自动生成精确标注。通过该管道，我们构建了一个包含23k条路径样本、覆盖4k张地图的微调数据集，使模型获得更类似人类的空间能力。使用该数据集，我们对开源和专有的多模态大型语言模型进行了微调。在MapBench上的结果表明，微调显著提升了鲁棒性，成功率提升至多6.4个百分点，同时降低了路径追踪误差（NDTW）。这些收益表明，预训练模型缺失的细粒度空间推理可以通过合成监督显式教授。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.&lt;/p&gt;</description></item><item><guid>2512.19687v1</guid><title>Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</title><link>http://arxiv.org/abs/2512.19687v1</link><author>Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 Perception Encoder Audiovisual（PE-AV），一种基于扩展对比学习的音视频编码器，能够在音频、视频和文本三种模态之间实现统一嵌入，并在多项标准基准上取得领先成绩。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的多模态模型往往局限于单一模态或缺乏跨模态对齐，导致在音频、视频与文本的联合理解任务中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够同时处理音频、视频和文本的统一编码器，并通过大规模对比学习提升跨模态对齐与零样本性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在原有 PE 模型基础上，使用十种配对对比目标对音频、视频与文本进行联合训练；通过构建约一亿对音视频-字幕数据集提供一致的监督；随后在帧级别进行对比微调，形成 PE-A-Frame，以实现细粒度音频-帧与文本的对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; PE-AV 在音频检索、视频检索等任务上刷新了现有最优结果；十种对比目标的组合显著提升了跨模态对齐和零样本推理；帧级微调进一步提升了声音事件检测等细粒度任务的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PE-AV 为音视频与文本的联合理解提供了强大的统一表示，开启了语音检索等新任务，并为未来更细粒度的跨模态研究奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 Perception Encoder Audiovisual（PE-AV），这是一种新型的音频和视频理解编码器，采用扩展对比学习进行训练。基于 PE，PE-AV 在将表示扩展到音频的同时，天然支持音频-视频、音频-文本和视频-文本三种模态的联合嵌入。PE-AV 的统一跨模态嵌入使得诸如语音检索等新任务成为可能，并在标准音频和视频基准上取得了新的最先进水平。我们通过构建强大的视听数据引擎，合成了约一亿对高质量音视频字幕，从而实现了跨模态一致的大规模监督。我们的音频数据涵盖语音、音乐和一般音效，避免了以往工作中常见的单一领域限制。我们利用十种配对对比目标，证明扩大跨模态和字幕类型的配对能够加强对齐并提升零样本性能。我们进一步通过帧级对比目标微调 PE-AV，形成 PE-A-Frame，从而实现音频-帧与文本的细粒度对齐，适用于声音事件检测等任务。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV&amp;#x27;s unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.&lt;/p&gt;</description></item><item><guid>2512.19744v1</guid><title>DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</title><link>http://arxiv.org/abs/2512.19744v1</link><author>Gustavo Coelho Haase, Paulo Henrique Dourado da Silva</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; DeepBridge 是一款 80K 行 Python 库，整合了多维度验证、自动合规检查、知识蒸馏和合成数据生成。它提供五大验证套件、自动 EEOC/ECOA/GDPR 检查、多格式报告、HPM‑KD 知识蒸馏框架以及可扩展的合成数据生成。通过六个案例研究，展示了显著的时间节省、完整的公平性检测、快速生成审计报告以及在压缩比 2.3–7 倍时的蒸馏优势。用户研究表明其易用性高、成功率 95% 及低认知负荷。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在机器学习模型部署中，验证、合规、报告和数据生成往往需要多种工具，导致工作流程碎片化、耗时长且易出错。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个统一的、开源的 Python 库，简化验证、合规、报告和数据生成流程，提高效率和覆盖率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 实现 5 个验证套件（公平性、鲁棒性、置信度、漂移、超参数敏感性），自动化 EEOC/ECOA/GDPR 检查，支持交互式/静态 HTML、PDF、JSON 报告，开发 HPM‑KD 知识蒸馏框架，并利用 Dask 进行可扩展的合成数据生成。通过六个行业案例和 20 名参与者的可用性研究进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 验证时间比传统工具快 89%，公平性检测覆盖率从 2/10 提升到 10/10；HPM‑KD 在 2.3–7 倍压缩比下比直接训练提升 1.00–2.04 个百分点；可用性研究显示 SUS 分数 87.5，成功率 95%，NASA‑TLX 28/100。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DeepBridge 能显著降低验证与合规成本，提供完整覆盖的公平性检测，并在知识蒸馏和合成数据生成方面表现优异，适合作为工业级机器学习工作流的核心工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 DeepBridge，一款 80K 行 Python 库，统一了多维度验证、自动合规验证、知识蒸馏和合成数据生成。DeepBridge 提供：5 个验证套件（公平性 15 个指标、鲁棒性弱点检测、不确定性通过一致性预测、抗性 5 种漂移类型、超参数敏感性）、自动 EEOC/ECOA/GDPR 验证、多格式报告系统（交互式/静态 HTML、PDF、JSON）、HPM‑KD 框架用于带元学习的知识蒸馏，以及通过 Dask 可扩展的合成数据生成。通过 6 个案例研究（信用评分、招聘、医疗、抵押、保险、欺诈）我们展示 DeepBridge：将验证时间缩短 89%（17 分钟 vs. 150 分钟碎片化工具）、自动检测公平性违规并实现完整覆盖（10/10 特征 vs. 现有工具 2/10）、在几分钟内生成审计就绪报告。HPM‑KD 在压缩比 2.3–7 倍（CIFAR100）下表现出持续优势：+1.00–2.04 个百分点 vs. 直接训练（p&amp;lt;0.05），确认知识蒸馏在更大教师-学生差距下有效。20 名参与者的可用性研究显示 SUS 分数 87.5（前 10%，“优秀”）、95% 成功率和低认知负荷（NASA‑TLX 28/100）。DeepBridge 在 MIT 许可证下开源，地址 https://github.com/deepbridge/deepbridge，完整文档 https://deepbridge.readthedocs.io&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present DeepBridge, an 80K-line Python library that unifies multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. DeepBridge offers: (i) 5 validation suites (fairness with 15 metrics, robustness with weakness detection, uncertainty via conformal prediction, resilience with 5 drift types, hyperparameter sensitivity), (ii) automatic EEOC/ECOA/GDPR verification, (iii) multi-format reporting system (interactive/static HTML, PDF, JSON), (iv) HPM-KD framework for knowledge distillation with meta-learning, and (v) scalable synthetic data generation via Dask. Through 6 case studies (credit scoring, hiring, healthcare, mortgage, insurance, fraud) we demonstrate that DeepBridge: reduces validation time by 89% (17 min vs. 150 min with fragmented tools), automatically detects fairness violations with complete coverage (10/10 features vs. 2/10 from existing tools), generates audit-ready reports in minutes. HPM-KD demonstrates consistent superiority across compression ratios 2.3--7x (CIFAR100): +1.00--2.04pp vs. Direct Training (p&amp;lt;0.05), confirming that Knowledge Distillation is effective at larger teacher-student gaps. Usability study with 20 participants shows SUS score 87.5 (top 10%, ``excellent&amp;#x27;&amp;#x27;), 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open-source under MIT license at https://github.com/deepbridge/deepbridge, with complete documentation at https://deepbridge.readthedocs.io&lt;/p&gt;</description></item><item><guid>2512.19871v1</guid><title>HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction</title><link>http://arxiv.org/abs/2512.19871v1</link><author>Jong Wook Kim, Wonseok Roh, Ha Dam Baek, Pilhyeon Lee, Jonghyun Choi, Sangpil Kim</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 HyGE-Occ 框架，通过融合连续高斯深度表示与离散深度分箱以及边缘先验，提升 3D 全景占据预测的几何一致性与边界感知，实验表明其在 Occ3D-nuScenes 数据集上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在 3D 场景重建中，需要对每个占据区域进行语义类别和实例身份预测，以实现细粒度的 3D 理解。然而现有方法往往难以保持精确几何和捕捉实例的空间范围，导致全景分离效果不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为克服上述局限，提出 HyGE-Occ，旨在通过混合视图转换分支和边缘先验提升几何一致性和边界感知，从而实现更精确的 3D 全景占据预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; HyGE-Occ 采用混合视图转换分支，将连续高斯深度表示与离散深度分箱融合，生成具有更好几何一致性和结构连贯性的 BEV 特征；同时从 BEV 特征中提取边缘图并作为辅助信息学习边缘线索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 Occ3D-nuScenes 数据集上，HyGE-Occ 的性能优于现有方法，显示出更强的 3D 几何推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; HyGE-Occ 通过融合高斯深度与边缘先验，显著提升了 3D 全景占据预测的几何一致性和边界感知，为复杂环境下的精细 3D 理解提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 全景占据预测旨在通过预测 3D 空间中每个占据区域的语义类别和实例身份来重建密集体素场景图。实现这种细粒度的 3D 理解需要精确的几何推理和跨复杂环境的空间一致场景表示。然而，现有方法往往难以保持精确几何并捕捉 3D 实例的精确空间范围，这对于稳健的全景分离至关重要。为克服这些限制，我们提出了 HyGE-Occ，一种利用混合视图转换分支与 3D 高斯和边缘先验的新框架，以增强 3D 全景占据预测中的几何一致性和边界感知。HyGE-Occ 采用混合视图转换分支，将连续基于高斯的深度表示与离散深度分箱形式融合，生成具有改进几何一致性和结构连贯性的 BEV 特征。与此同时，我们从 BEV 特征中提取边缘图并将其用作辅助信息来学习边缘线索。在 Occ3D-nuScenes 数据集上的广泛实验中，HyGE-Occ 超越了现有工作，展示了卓越的 3D 几何推理能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决 3D 视景占用预测（3D Panoptic Occupancy Prediction）问题，即在三维空间中为每个占用区域同时预测语义类别和实例身份。该问题在自动驾驶等场景中至关重要，因为完整的三维场景理解能够支持安全的运动规划和长期预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为离散深度投影（LSS）在空间定位上精确，但会产生量化误差；连续高斯投影能提供几何连续性和不确定性建模。于是他们结合两种方法，并借鉴了 Lift‑Splat‑Shoot、Gaussian Splatting 以及边缘监督技术，设计了混合视图转换分支和边缘预测模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将离散深度和连续高斯深度的 BEV 特征通过 α‑混合融合，得到既精确又连续的空间表示；随后利用从语义标签生成的伪边缘监督提升边界清晰度。实现流程为：多视图图像 → 共享 backbone 提取特征 → 两个投影分支（LSS 与 Gaussian） → 混合 BEV 特征 → 边缘预测模块 → 体素解码器 → 语义与实例预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 混合视图转换分支：融合离散与连续深度，兼顾几何一致性与空间精度；2) 边缘预测模块：使用伪边缘监督显式强化边界信息；3) 两个模块可无缝集成到现有 BEV 基础模型。与以往仅使用离散深度或仅使用连续深度、缺乏边缘监督的工作相比，HyGE‑Occ 在几何精度和分割边界上均有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HyGE‑Occ 通过混合离散与连续深度投影并加入边缘监督，构建了更精确、更连贯的 BEV 表示，显著提升了 3D 视景占用预测的几何与分割性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.&lt;/p&gt;</description></item><item><guid>2512.19879v1</guid><title>Fine-Tuned In-Context Learners for Efficient Adaptation</title><link>http://arxiv.org/abs/2512.19879v1</link><author>Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种将提示工程与微调相结合的统一方法，通过在微调过程中加入上下文示例，既保留了少样本学习的样本效率，又获得了微调的性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 目前适配大型语言模型的常用方法有两种：提示工程（常用少样本学习）和直接在任务数据上微调。提示方法在少样本场景表现优异，但随着数据增多效果趋于平稳；微调在数据充足时表现好，但样本稀缺时可能不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究一种能同时兼顾两种方法优点的统一策略，并评估其在不同下游任务中的预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在任务特定数据上进行微调时，加入与 k-shot 提示结构相似的上下文示例；使用预序评估（prequential evaluation）在低样本环境下进行超参数选择，避免昂贵的交叉验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 统一方法在所有实验中都能与两种基线相匹配，甚至显著超越；预序评估能够有效地为低样本场景提供可靠的验证信号；实验表明不同任务对三种适配范式的偏好各异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将上下文学习融入微调的统一方法在样本效率和性能上兼具优势，预序评估为低样本调参提供了实用方案，未来可进一步探索其在更广泛任务中的适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 当将大型语言模型（LLM）适配到特定下游任务时，常用两种主要方法：(1) 提示工程，通常配合上下文少样本学习，利用模型固有的泛化能力；(2) 在任务特定数据上进行微调，直接优化模型参数。提示方法在少样本场景表现出色，但随着数据量增加其效果往往趋于平稳；相反，微调在数据充足时扩展性好，但在训练样本稀缺时可能表现不佳。我们研究了一种统一方法，通过在微调过程中直接加入上下文学习，桥接这两种范式。具体而言，我们在任务特定数据上进行微调，并用与 k-shot 提示结构相似的上下文示例进行增强。该方法虽然需要针对每个任务进行微调，但结合了上下文学习的样本效率和微调的性能提升，能够始终匹配甚至显著超过两种基线。为在低样本环境下进行超参数选择，我们提出使用预序评估（prequential evaluation），该方法消除了昂贵的交叉验证需求，并利用所有可用数据进行训练，同时提供稳健的验证信号。我们开展了广泛的实证研究，以确定在具体数据下游任务中，微调、上下文学习或我们提出的统一方法哪一种能提供最佳预测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model&amp;#x27;s inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model&amp;#x27;s parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.&lt;/p&gt;</description></item><item><guid>2512.19909v1</guid><title>Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</title><link>http://arxiv.org/abs/2512.19909v1</link><author>Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出一种基于深度学习的条件生成模型 CGM-FAS，用于非平稳地震动预测，能够快速生成多频率、广域的路径效应地图，性能与传统高斯过程模型相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统非平稳地震动模型使用高斯过程，受限于计算量，难以大规模预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 用深度学习替代高斯过程，学习空间模式和频率相关性，提升预测效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建条件变分自编码器，输入地震与站点坐标作为条件变量，直接从数据学习空间与频率相关性，生成 Fourier Amplitude Spectra 的路径效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CGM-FAS 与最新 GP 基模型在旧金山湾区数据上预测一致；不需要预设相关函数；能捕捉频率间相关；在 10,000 个站点、1,000 频率下仅用 10 秒、几 GB 内存生成地图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CGM-FAS 为大规模、多频率非平稳地震动预测提供了高效、可扩展的方案，具有良好前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近期非平稳地震动模型（GMM）通过显式建模源、场站和路径效应的系统空间变化，将标准偏差降低到平稳模型的30-40%，从而实现更精确的场站特定地震危险分析。目前的非平稳 GMM 依赖于具有预设相关函数的高斯过程（GP）方法，因而在大规模预测时存在计算限制。本研究提出一种名为条件生成 Fourier 幅谱（CGM-FAS）的深度学习方法，作为 GP 基方法的替代方案，用于建模 Fourier 幅谱中的非平稳路径效应。CGM-FAS 采用条件变分自编码器架构，直接从数据学习空间模式和频率间相关性，使用地震和站点的地理坐标作为条件变量。利用旧金山湾区地震数据，我们将 CGM-FAS 与该地区最新的 GP 基 GMM 进行比较，并展示了对非平稳路径效应的一致预测。此外，CGM-FAS 相较于 GP 基方法，在学习空间模式时不需要预设相关函数，能够捕捉频率间相关，并实现快速预测：在 10 秒内使用几 GB 内存为 10,000 个站点、1,000 个频率生成地图。CGM-FAS 的超参数可调节，以确保生成的路径效应具有与 GP 基经验 GMM 一致的变异性。本工作展示了在多频率和大空间域内实现高效非平稳地震动预测的有前景方向。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.&lt;/p&gt;</description></item><item><guid>2512.19927v1</guid><title>The Seismic Wavefield Common Task Framework</title><link>http://arxiv.org/abs/2512.19927v1</link><author>Alexey Yermakov, Yue Zhao, Marine Denolle, Yiyu Ni, Philippe M. Wyder, Judah Goldfeder, Stefano Riva, Jan Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个用于地震波场机器学习的通用任务框架（CTF），通过策划不同尺度的数据集和任务特定指标，提供结构化且严格的评估方法，以实现对预测、重建和泛化等任务的公平比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地震学在状态预测与重建以及源位置、机制和地球模型的参数变异管理方面面临巨大挑战，仿真规模庞大且真实数据受限，机器学习方法缺乏适当的表征和严格比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个标准化、可重复的评估框架，替代临时比较，提升科学机器学习在地震学中的严谨性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CTF 包含全球、地壳和局部尺度的策划数据集，设定预测、重建和泛化等任务的指标，并在噪声与有限数据等现实约束下进行评估，展示了对两组数据集的分数和方法性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CTF 的分数揭示了不同方法在特定问题类别中的优势、局限和适用性，展示了从模拟和真实传感器测量中重建地震波场的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过隐藏测试集上的标准化评估，可替代临时比较，显著提升机器学习在地震学研究中的严谨性和可重复性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 地震学在状态预测与重建（如地震预警和地面运动预测）以及源位置、机制和地球模型（如地下结构和地形效应）的参数变异管理方面面临根本挑战。利用仿真来解决这些问题受到其庞大规模的制约——合成数据量和数值复杂性都很大，而基于真实数据的工作又受限于模型无法充分反映地球复杂性以及现场传感器测量稀疏。最近的机器学习（ML）工作提供了希望，但进展被缺乏适当表征、公平报告和严格比较所掩盖。为了解决这一问题，我们提出了一个用于地震波场机器学习的通用任务框架（CTF），起初包含三个不同的波场数据集。CTF 提供了在不同尺度（全球、地壳和局部）上策划的数据集，以及涵盖预测、重建和在噪声与有限数据等现实约束下的泛化的任务特定指标。受自然语言处理等领域 CTF 的启发，该框架为算法的正面对比评估提供了结构化且严格的基础。我们用两个数据集的分数展示了评估程序，展示了多种方法和基础模型在从模拟和真实传感器测量中重建地震波场的性能。CTF 分数揭示了不同方法在特定问题类别中的优势、局限和适用性。我们的愿景是用隐藏测试集上的标准化评估取代临时比较，提高科学 ML 的严谨性和可重复性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth&amp;#x27;s complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.&lt;/p&gt;</description></item><item><guid>2512.19978v1</guid><title>Regression of Functions by Quantum Neural Networks Circuits</title><link>http://arxiv.org/abs/2512.19978v1</link><author>Fernando M. de Paula Neto, Lucas dos Reis Silva, Paulo S. G. de Mattos Neto, Felipe F. Fanchini</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了使用遗传算法自动构建量子回归网络的架构，探讨了电路深度、参数化门配置和数据重上传模式，并将其视为优化问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 量子神经网络的性能高度依赖于电路架构选择，这一过程与经典神经网络拓扑选择同样计算困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种自动化方法，生成紧凑且性能竞争的量子回归模型，并评估其在多种基准任务上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建遗传算法框架，搜索电路深度、参数化门组合和数据重上传模式；使用12个结构描述符对数据集复杂度进行量化，并在五个元学习场景中验证其预测能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 演化得到的量子模型在22个非线性基准函数和4个解析函数上与17个经典回归模型相当，但参数更少；复杂度指标在多种场景中能近乎完美预测最佳量子架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该研究为基于元学习的量子架构设计提供了理论依据，展示了量子回归模型在参数紧凑性和性能方面的优势，并为系统化、理论化的量子回归方法奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The performance of quantum neural network models depends strongly on architectural decisions, including circuit depth, placement of parametrized operations, and data-encoding strategies. Selecting an effective architecture is challenging and closely related to the classical difficulty of choosing suitable neural-network topologies, which is computationally hard. This work investigates automated quantum-circuit construction for regression tasks and introduces a genetic-algorithm framework that discovers Reduced Regressor QNN architectures. The approach explores depth, parametrized gate configurations, and flexible data re-uploading patterns, formulating the construction of quantum regressors as an optimization process. The discovered circuits are evaluated against seventeen classical regression models on twenty-two nonlinear benchmark functions and four analytical functions. Although classical methods often achieve comparable results, they typically require far more parameters, whereas the evolved quantum models remain compact while providing competitive performance. We further analyze dataset complexity using twelve structural descriptors and show, across five increasingly challenging meta-learning scenarios, that these measures can reliably predict which quantum architecture will perform best. The results demonstrate perfect or near-perfect predictive accuracy in several scenarios, indicating that complexity metrics offer powerful and compact representations of dataset structure and can effectively guide automated model selection. Overall, this study provides a principled basis for meta-learning-driven quantum architecture design and advances the understanding of how quantum models behave in regression settings--a topic that has received limited exploration in prior work. These findings pave the way for more systematic and theoretically grounded approaches to quantum regression.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The performance of quantum neural network models depends strongly on architectural decisions, including circuit depth, placement of parametrized operations, and data-encoding strategies. Selecting an effective architecture is challenging and closely related to the classical difficulty of choosing suitable neural-network topologies, which is computationally hard. This work investigates automated quantum-circuit construction for regression tasks and introduces a genetic-algorithm framework that discovers Reduced Regressor QNN architectures. The approach explores depth, parametrized gate configurations, and flexible data re-uploading patterns, formulating the construction of quantum regressors as an optimization process. The discovered circuits are evaluated against seventeen classical regression models on twenty-two nonlinear benchmark functions and four analytical functions. Although classical methods often achieve comparable results, they typically require far more parameters, whereas the evolved quantum models remain compact while providing competitive performance. We further analyze dataset complexity using twelve structural descriptors and show, across five increasingly challenging meta-learning scenarios, that these measures can reliably predict which quantum architecture will perform best. The results demonstrate perfect or near-perfect predictive accuracy in several scenarios, indicating that complexity metrics offer powerful and compact representations of dataset structure and can effectively guide automated model selection. Overall, this study provides a principled basis for meta-learning-driven quantum architecture design and advances the understanding of how quantum models behave in regression settings--a topic that has received limited exploration in prior work. These findings pave the way for more systematic and theoretically grounded approaches to quantum regression.&lt;/p&gt;</description></item><item><guid>2512.20021v1</guid><title>Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models</title><link>http://arxiv.org/abs/2512.20021v1</link><author>Anna R. Flowers, Christopher T. Franck, Robert B. Gramacy, Justin A. Krometis</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种利用训练数据的元数据来指导后续数据采集的方法，以提升机器学习模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 收集符合实际操作的数据成本高，了解模型缺陷有助于更有效的数据采集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过元学习方法，利用元数据和计算实验工具，最大化模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对训练数据按元数据变化评估学习器，拟合高斯过程响应面，用于指导新数据采集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法相较于随机选择元数据的数据集，能提升学习器性能，在经典学习例子和搜寻飞机的航空图像采集任务中得到验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 元学习结合高斯过程可有效改进模型性能，提供更具成本效益的数据采集策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 收集符合实际操作的数据以指导机器学习模型可能成本高昂。在收集新数据之前，了解模型在哪些方面存在不足是有帮助的。例如，在稀有物体图像上训练的目标检测器可能在代表性不足的条件下识别效果不佳。我们提供了一种利用计算实验工具和描述训练数据采集情况的元数据（如季节、时间、地点）来指导后续数据采集、最大化模型性能的方法。我们通过在训练数据按其元数据变化时评估学习器来实现这一点。对该响应面拟合高斯过程代理可以指导新的数据采集。该元学习方法相较于随机选择元数据的数据，能提升学习器性能，我们在经典学习例子和一个涉及搜寻飞机的航空图像采集的动机性应用中进行了说明。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Collecting operationally realistic data to inform machine learning models can be costly. Before collecting new data, it is helpful to understand where a model is deficient. For example, object detectors trained on images of rare objects may not be good at identification in poorly represented conditions. We offer a way of informing subsequent data acquisition to maximize model performance by leveraging the toolkit of computer experiments and metadata describing the circumstances under which the training data was collected (e.g., season, time of day, location). We do this by evaluating the learner as the training data is varied according to its metadata. A Gaussian process (GP) surrogate fit to that response surface can inform new data acquisitions. This meta-learning approach offers improvements to learner performance as compared to data with randomly selected metadata, which we illustrate on both classic learning examples, and on a motivating application involving the collection of aerial images in search of airplanes.&lt;/p&gt;</description></item><item><guid>2512.20092v1</guid><title>Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents</title><link>http://arxiv.org/abs/2512.20092v1</link><author>Yiming Du, Baojun Wang, Yifan Xiang, Zhaowei Wang, Wenyu Huang, Boyang Xue, Bin Liang, Xingshan Zeng, Fei Mi, Haoli Bai, Lifeng Shang, Jeff Z. Pan, Yuxin Jiang, Kam-Fai Wong</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 Memory-T1 框架，利用强化学习学习时间感知的记忆选择策略，解决长对话中时间推理困难的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在多轮对话中，随着对话历史长度增加并出现噪声，现有长上下文模型难以准确识别时间相关信息，导致推理性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建时间感知的记忆选择机制，提高对长多会话对话的时间推理准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Memory-T1 采用粗到细策略，先用时间和相关性过滤器将对话历史裁剪为候选集，再用强化学习代理精确选择证据会话；训练奖励包括答案准确性、证据定位和时间一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 Time-Dialog 基准上，Memory-T1 将 7B 模型的得分提升至 67.0%，超过 14B 基线 10.2%；消融实验表明时间一致性和证据定位奖励共同提升 15.0%；模型在 128k 令牌下保持鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Memory-T1 在长对话时间推理任务中实现了新的开源模型最佳成绩，并证明了其对噪声的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在长时间、多会话对话中进行时间推理是对话代理的关键能力。然而，现有工作和我们的先行研究表明，随着对话历史长度增加并积累噪声，当前的长上下文模型难以准确识别时间相关信息，显著削弱了推理性能。为了解决这一问题，我们提出了 Memory-T1 框架，该框架使用强化学习学习时间感知的记忆选择策略。它采用粗到细的策略，首先使用时间和相关性过滤器将对话历史裁剪为候选集，然后由强化学习代理选择精确的证据会话。强化学习训练由多层奖励函数引导，优化答案准确性、证据定位和时间一致性。特别是，时间一致性奖励通过在会话级（时间接近度）和发言级（时间忠实度）评估与查询时间范围的对齐，提供了密集信号，使代理能够解决细微的时间歧义。在 Time-Dialog 基准上，Memory-T1 将 7B 模型提升到 67.0% 的整体得分，创下开源模型的新纪录，并比 14B 基线高出 10.2%。消融研究表明，时间一致性和证据定位奖励共同带来 15.0% 的性能提升。此外，Memory-T1 在 128k 令牌下保持鲁棒性，而基线模型会崩溃，证明了其在大规模对话历史中对噪声的有效性。代码和数据集已公开可用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/&lt;/p&gt;</description></item><item><guid>2512.20128v1</guid><title>milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion</title><link>http://arxiv.org/abs/2512.20128v1</link><author>Niraj Prakash Kini, Shiau-Rung Tsai, Guan-Hsun Lin, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文提出 milliMamba，一个基于毫米波雷达的二维人体姿态估计框架，利用跨视角融合 Mamba 编码器和时空交叉注意力解码器，能够从稀疏雷达信号中提取时空特征，并通过上下文信息恢复因镜面反射导致的缺失关节点，训练时加入速度损失以提升运动平滑度，在 TransHuPR 和 HuPR 数据集上实现显著性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统 RGB 传感器在隐私和照明条件下存在局限，毫米波雷达提供了隐私保护且对光照不敏感的替代方案，但雷达信号因镜面反射而稀疏，导致特征提取困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决毫米波雷达稀疏信号下人体姿态估计的挑战，构建能够有效利用时空依赖关系的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用跨视角融合 Mamba 编码器以线性复杂度提取长序列的时空特征；使用时空交叉注意力解码器预测多帧关节点坐标；在训练中加入速度损失以强化运动平滑。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 TransHuPR 和 HuPR 数据集上，milliMamba 相比基线提升了 11.0 AP 和 14.6 AP，且保持了合理的模型复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; milliMamba 通过联合建模时空依赖，能够有效恢复因镜面反射导致的缺失关节点，并在毫米波雷达人体姿态估计任务中取得显著性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; milliMamba is a 2D human pose estimation framework based on millimeter-wave radar that uses a Cross-View Fusion Mamba encoder and a Spatio-Temporal Cross Attention decoder to extract spatio-temporal features from sparse radar signals and recover missing joints caused by specular reflections by leveraging contextual cues. A velocity loss is added during training to enforce motion smoothness. Experiments on the TransHuPR and HuPR datasets show significant performance improvements over baselines, exceeding them by 11.0 AP and 14.6 AP respectively, while maintaining reasonable complexity.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba&lt;/p&gt;</description></item><item><guid>2512.20157v1</guid><title>AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model</title><link>http://arxiv.org/abs/2512.20157v1</link><author>Sofian Chaybouti, Sanath Narayan, Yasser Dahou, Phúc H. Lê Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究系统探讨了多教师蒸馏在视觉基础模型中的应用，并提出了AMoE模型和三项关键技术，显著提升了训练效率和样本利用率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多教师蒸馏被视为实现统一视觉表示的有前景路径，但其学习动态和数据效率尚未得到充分研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过系统研究多教师蒸馏，识别降低计算成本的关键因素，并提升模型训练效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 引入AMoE模型，将SigLIP2和DINOv3的知识同时蒸馏到Mixture-of-Experts学生；2) 采用非对称关系知识蒸馏损失；3) 使用token平衡批处理；4) 采用层次聚类和采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1) 非对称关系蒸馏损失保持教师几何属性并有效转移知识；2) token平衡批处理在不同分辨率下稳定表示学习；3) 层次聚类采样显著提升样本效率；4) 结合这些技术构建的OpenLVD200M数据集在多教师蒸馏中表现出更高效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过上述技术，能够以更低的计算成本训练视觉基础模型，并提供了公开的数据集和蒸馏模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉基础模型通过多教师蒸馏训练，为实现统一视觉表示提供了有前景的路径，但此类方法的学习动态和数据效率仍未得到充分探索。本文系统研究了多教师蒸馏在视觉基础模型中的应用，并识别了实现低计算成本训练的关键因素。我们提出了聚合式混合专家视觉基础模型（AMoE），该模型同时从SigLIP2和DINOv3蒸馏知识到混合专家学生。我们证明：（1）我们的非对称关系知识蒸馏损失在保持每个教师几何属性的同时实现有效知识转移；（2）token平衡批处理将不同分辨率图像打包成具有统一token预算的序列，稳定了跨分辨率的表示学习而不降低性能；（3）层次聚类和采样（通常用于自监督学习）显著提升了多教师蒸馏的样本效率。结合这些发现，我们构建了OpenLVD200M，一个包含2亿张图像的数据集，在多教师蒸馏中表现出更高效率。我们在混合专家框架中实现并发布了OpenLVD200M及其蒸馏模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.&lt;/p&gt;</description></item><item><guid>2512.20217v1</guid><title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title><link>http://arxiv.org/abs/2512.20217v1</link><author>Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的多模态3D检测器LiteFusion，利用LiDAR点云作为相机检测的几何补充，去掉了传统3D骨干网络，提升了部署友好性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态3D检测器依赖复杂架构和LiDAR，导致在无LiDAR时性能下降，且难以在NPU、FPGA等硬件上部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 重新评估LiDAR在相机-LiDAR融合中的角色，设计一种不依赖3D骨干、易部署且鲁棒的检测方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; LiteFusion将LiDAR点云的几何信息嵌入四元数空间，与图像特征融合，保持正交约束，形成紧凑的跨模态嵌入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，LiteFusion相较基线视觉检测器提升了约20.4% mAP和19.7% NDS，仅增加1.1%参数，且在无LiDAR输入时仍保持强劲表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LiteFusion通过简化LiDAR处理，兼顾性能与部署，展示了在多种融合与硬件场景下的鲁棒性与有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D目标检测是安全可靠的智能交通系统的基础。当前多模态3D目标检测器往往依赖复杂的架构和训练策略以获得更高的检测精度。然而，这些方法高度依赖激光雷达传感器，导致在缺失激光雷达时性能大幅下降，影响了自动驾驶系统在实际场景中的鲁棒性和安全性。此外，现有多模态检测器由于使用3D稀疏卷积操作，主要针对NVIDIA GPU进行优化，难以在多样化的硬件平台（如NPU和FPGA）上部署。为了解决这些挑战，我们重新审视了激光雷达在相机-激光雷达融合范式中的作用，并提出了一种新型多模态3D检测器LiteFusion。LiteFusion不将激光雷达点云视为独立模态并使用单独的特征提取骨干网络，而是将激光雷达数据作为相机检测的几何信息补充来源。该直观方法完全消除了对3D骨干的依赖，使方法高度易于部署。具体而言，LiteFusion将激光雷达点的互补特征嵌入四元数空间，在网络训练过程中保持正交约束，从而帮助模型学习跨模态的域特定关系，得到紧凑的跨模态嵌入。在nuScenes数据集上的实验表明，LiteFusion在不使用专门的激光雷达编码器的情况下，将基线视觉检测器的mAP提升了20.4%，NDS提升了19.7%，参数增加仅1.1%。值得注意的是，即使在缺失激光雷达输入的情况下，LiteFusion仍保持强劲的结果，凸显了其在多种融合范式和部署场景下的鲁棒性和有效性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在让基于相机的3D目标检测器在加入激光雷达（LiDAR）后，既能显著提升检测精度，又能保持对LiDAR缺失的鲁棒性，并且易于在不同硬件平台上部署。这个问题重要，因为在自动驾驶等实际场景中，LiDAR可能失效或成本高昂，而高精度的3D感知是安全决策的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到传统多模态检测器对LiDAR的过度依赖和对3D稀疏卷积的硬件依赖，决定把LiDAR视为补充的几何信息而非独立模态。借鉴了BEVFormer、深度感知嵌入（Depth‑Aware Embedding）和几何感知嵌入（Geometry‑Aware Embedding）等现有技术，并引入四元数空间来实现跨模态融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过四元数空间将LiDAR的3D几何信息映射到相机的2D特征中，并在网络的每个阶段逐步注入这些几何特征。实现流程包括：①将点云投影为视角图（PV）和鸟瞰图（BEV）；②分别使用DAE和GAE模块生成深度和几何嵌入；③将嵌入逐层融合到相机特征中；④最终通过BEV解码器和检测头得到3D框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 去掉了独立的3D点云编码器，使用轻量级几何整合器；2) 在四元数空间实现跨模态融合，保持正交约束；3) 采用逐层响应框架，逐步增强视觉特征；4) 仅使用标准conv2d，易于在NPU、FPGA等平台部署。与以往需要稀疏卷积、复杂双流结构且对LiDAR高度依赖的模型不同，LiteFusion在参数增量极小的情况下即可获得相当甚至更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiteFusion通过在四元数空间中轻量级地将LiDAR几何信息注入相机特征，实现了从视觉到多模态的无缝转换，既提升了3D检测精度，又保持了对LiDAR缺失的鲁棒性和跨平台的部署友好性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.&lt;/p&gt;</description></item><item><guid>2512.20374v2</guid><title>CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images</title><link>http://arxiv.org/abs/2512.20374v2</link><author>Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji, Linhai Zhuo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文构建了高质量结肠镜图像数据集，并提出一种基于CLIP模型的自动化BBPS评分框架，融合视觉特征与粪便文本先验，显著提升了肠道清洁度评估的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确评估肠道清洁度对结肠镜检查至关重要，传统的Boston Bowel Preparation Scale（BBPS）虽然标准化，但手工评分存在主观性和观察者间差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建包含2240张图像、517名受试者的BBPS标注数据集，并开发一种无需显式分割的自动评分方法，以提高评分的可靠性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用CLIP模型进行适配器式迁移学习，加入专门的粪便特征提取分支，将全局视觉特征与粪便相关文本先验进行融合，从而实现自动BBPS评分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在自建数据集和公开NERTHU数据集上进行的大量实验表明，该方法在准确性上优于现有基线模型，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该自动化BBPS评分框架具有临床部署潜力，可为计算机辅助结肠镜分析提供可靠支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.&lt;/p&gt;</description></item><item><guid>2512.20407v2</guid><title>AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition</title><link>http://arxiv.org/abs/2512.20407v2</link><author>Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于声音的无人机检测框架 AUDRON，利用多种声学特征和深度学习技术实现高精度识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 无人机在物流、农业、监视和国防等领域得到广泛应用，但其滥用可能带来安全和安保问题，需要有效的检测手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种低成本、非侵入式的声学检测方法，以识别无人机的独特声音特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; AUDRON 结合 Mel 频率倒谱系数、短时傅里叶变换谱图、卷积神经网络、循环层和自编码器表示，并在特征层进行融合后进行分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 AUDRON 能在背景噪声中准确区分无人机声学特征，二分类和多分类准确率分别达到 98.51% 和 97.11%，并在不同条件下保持良好泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多特征融合与深度学习的组合显著提升了声学无人机检测的可靠性，适用于视觉或雷达受限的安全与监视场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 无人机（UAV）在物流、农业、监视和防御等多领域的应用日益增多。虽然这些系统带来诸多好处，但其滥用会引发安全和安保问题，因而需要有效的检测机制。声学感知提供了一种低成本、非侵入式的替代方案，因为无人机螺旋桨会产生独特的声音模式。本研究提出了 AUDRON（基于音频的无人机识别网络），一种混合深度学习框架，用于无人机声音检测，采用 Mel 频率倒谱系数（MFCC）、短时傅里叶变换（STFT）谱图与卷积神经网络（CNN）相结合，使用循环层进行时间建模，并采用自编码器表示。特征层融合整合了互补信息后进行分类。实验评估表明 AUDRON 能有效区分无人机声学特征与背景噪声，在二分类和多分类任务中分别实现 98.51% 和 97.11% 的准确率，并在不同条件下保持良好的泛化能力。结果凸显了多特征表示与深度学习相结合的优势，为可靠的声学无人机检测提供了可能，并建议该框架可在视觉或雷达受限的安全与监视应用中部署。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework&amp;#x27;s potential for deployment in security and surveillance applications where visual or radar sensing may be limited.&lt;/p&gt;</description></item><item><guid>2512.20431v1</guid><title>Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks</title><link>http://arxiv.org/abs/2512.20431v1</link><author>Abdullah Al Shafi, Abdul Muntakim, Pintu Chandra Shill, Rowzatul Zannat, Abdullah Al-Amin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于软投票集成的卷积神经网络的早期皮肤癌分类方法。通过对三大公开数据集进行重平衡、增强和过滤，并使用迁移学习的双编码器进行分割，随后将MobileNetV2、VGG19和InceptionV3三种网络进行软投票集成，实现了高精度的病变识别。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 皮肤癌可通过皮肤镜检查和目视检查识别，但早期发现能显著提高生存率。人工智能结合标注图像和卷积神经网络已被证明能提升诊断准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在实际环境中快速部署且准确率高的早期皮肤癌分类方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先对HAM10000、ISIC 2016和ISIC 2019三组数据进行重平衡、图像增强和过滤；随后使用迁移学习的双编码器完成分割，聚焦临床重要特征；最后将MobileNetV2、VGG19和InceptionV3三种网络通过软投票集成进行分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三组数据集上分别取得了96.32%、90.86%和93.92%的识别准确率；分割步骤显著降低背景噪声并提升整体准确性；系统通过标准皮肤病变检测指标评估，表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该软投票集成方法在保持高准确率的同时兼顾速度，适合在临床实际部署，能够有效提升早期皮肤癌的诊断水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 皮肤癌可以通过皮肤镜检查和目视检查识别，但早期检测显著提高生存机会。人工智能（AI）利用标注的皮肤图像和卷积神经网络（CNN）提高诊断准确性。本文提出一种使用CNN软投票集成的早期皮肤癌分类方法。在本研究中，使用了三个基准数据集，即HAM10000、ISIC 2016和ISIC 2019。该过程包括重新平衡、图像增强和过滤技术，随后通过迁移学习的混合双编码器进行分割。准确的分割使分类模型专注于临床重要特征，减少背景伪影并提高准确性。分类通过MobileNetV2、VGG19和InceptionV3的集成完成，在准确性和速度之间取得平衡，适用于实际部署。该方法在三组数据集上实现了96.32%、90.86%和93.92%的病变识别准确率。系统性能使用已建立的皮肤病变检测指标进行评估，结果令人印象深刻。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\%, 90.86\%, and 93.92\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.&lt;/p&gt;</description></item><item><guid>2512.20501v1</guid><title>Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</title><link>http://arxiv.org/abs/2512.20501v1</link><author>Gorjan Radevski</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该论文研究多模态对齐、翻译、融合和迁移技术，以提升机器对复杂输入的理解。通过五个章节，分别解决空间语言、医学文本、知识图谱、动作识别等多模态学习中的关键挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着多模态数据的广泛应用，传统单模态方法难以充分捕捉文本、图像、视频等信息之间的关联，亟需新的方法来实现跨模态的对齐与融合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在开发一系列方法，提升空间关系的可视化、医学文本的三维定位、结构化文本到知识图谱的映射、以及多模态动作识别的鲁棒性和效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) Spatial-Reasoning Bert 将文本空间关系转化为二维剪贴画布局；2) 通过空间共现的损失函数将医学文本映射到三维解剖图谱；3) 建立基准评估自然语言与知识图谱实体、谓词的链接；4) 结合视频帧与目标检测特征实现动作识别融合；5) 利用多模态知识蒸馏让仅使用RGB的模型逼近多模态融合效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 这些方法分别在自动场景生成、医学文本导航、知识图谱丰富、动作识别准确率和计算效率方面取得显著提升，证明了多模态翻译与融合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本研究为空间语言理解、医学文本解析、知识图谱构建和动作识别等领域提供了可行的多模态技术方案，推动了复杂多模态输入处理的进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该论文探讨了多模态对齐、翻译、融合和迁移，以增强机器对复杂输入的理解。工作被组织成五章，每章解决多模态机器学习中的独特挑战。第3章介绍了 Spatial-Reasoning Bert，将基于文本的空间关系翻译为剪贴画之间的二维排列，从而有效解码空间语言为视觉表示，为与人类空间理解一致的自动场景生成铺平道路。第4章提出了一种将医学文本翻译为解剖图谱中特定三维位置的方法。我们引入了利用医学术语空间共现的损失函数来创建可解释的映射，显著提升了医学文本的可导航性。第5章处理将结构化文本翻译为知识图谱中的规范事实。我们开发了一个基准，用于将自然语言链接到实体和谓词，解决文本提取中的歧义，提供更清晰、可操作的洞察。第6章探讨了用于组合动作识别的多模态融合方法。我们提出了一种融合视频帧和目标检测表示的方法，提升了识别的鲁棒性和准确性。第7章研究了用于自我视角动作识别的多模态知识迁移。我们展示了多模态知识蒸馏如何使仅使用RGB的模型模仿多模态融合的能力，降低计算需求同时保持性能。这些贡献推进了空间语言理解、医学文本解释、知识图谱丰富和动作识别的方法，提升了计算系统处理复杂多模态输入的能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.   Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.   Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.   Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.   Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.   Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.   These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems&amp;#x27; ability to process complex, multimodal inputs across diverse applications.&lt;/p&gt;</description></item><item><guid>2512.20557v1</guid><title>Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</title><link>http://arxiv.org/abs/2512.20557v1</link><author>Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出DSR Suite，包含自动化生成动态空间推理问题的数据集、基准测试以及轻量化几何选择模块，显著提升视觉语言模型在动态空间推理任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视觉语言模型在一般理解任务表现优异，但在动态空间推理（即随时间变化的三维几何与关系推理）方面仍弱，主要原因是缺乏可扩展的四维感知训练资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补数据集、基准与模型三方面的不足，构建支持动态空间推理的完整体系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 设计自动化流水线，从真实视频中提取相机姿态、点云、物体掩码、方向和三维轨迹，生成多项选择问答对；构建DSR-Train用于学习，DSR-Bench用于评估；2) 开发Geometry Selection Module，将预训练的四维重建先验压缩为几何令牌，精准提取与问题相关的几何知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 将DSR-Train与GSM集成到Qwen2.5-VL-7B后，模型在动态空间推理任务上显著提升，同时保持了在通用视频理解基准上的准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DSR Suite通过丰富的数据、严格的基准和高效的几何模块，为视觉语言模型在动态空间推理领域提供了实质性进步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉语言模型（VLM）在一般理解任务上表现出色，但在动态空间推理（DSR）方面仍显薄弱，即关于物体几何和关系随时间在三维空间中演变的推理，主要由于缺乏可扩展的四维感知训练资源。为弥补数据集、基准和模型三方面的差距，我们提出了DSR Suite。首先，我们提出了一套自动化流水线，从真实视频中生成多项选择问答对用于DSR。利用现代视觉基础模型，该流水线提取丰富的几何和运动信息，包括相机姿态、局部点云、物体掩码、方向和三维轨迹。这些几何线索使我们能够构建用于学习的DSR-Train以及进一步人工精炼的用于评估的DSR-Bench。与以往工作相比，我们的数据强调：（i）真实视频来源；（ii）物体和场景级别的三维需求；（iii）视角变换；（iv）多物体交互；（v）细粒度、程序化答案。除了数据，我们提出了轻量化的几何选择模块（GSM），无缝将几何先验整合到VLM中，它将问题语义压缩并从预训练的四维重建先验中提取与问题相关的知识，形成紧凑的几何令牌集合。这种针对性提取避免了模型被无关知识淹没。实验表明，将DSR-Train和GSM集成到Qwen2.5-VL-7B后，显著提升了其动态空间推理能力，同时保持了在通用视频理解基准上的准确性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升视觉语言模型在动态空间推理（DSR）方面的能力。DSR要求模型理解物体在三维空间随时间演变的几何关系，这对机器人、自动驾驶、AR/VR 等需要实时空间感知的应用至关重要，但现有模型在这方面表现薄弱。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出缺乏可扩展的 4D 训练资源、评测基准和有效模型的瓶颈，随后构建了自动化数据生成管线，并结合现有的视觉基础模型（如 Grounded SAM2、Orient Anything、π³）提取几何信息。该思路借鉴了静态空间推理的数据与模型设计，但扩展到动态、视角变换和多物体交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过三阶段管线：视频筛选 → 几何线索提取 → QA 生成，构建大规模训练集 DSR‑Train 和人类校准的评测集 DSR‑Bench。随后提出 Geometry Selection Module（GSM），利用双 Q‑Former 先压缩问题语义，再从 4D 先验中挑选与问题相关的几何 token，避免无关噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 自动化生成 4D QA 对，覆盖视角变换、多物体交互和细粒度答案；2) DSR‑Train 与 DSR‑Bench 两套资源，兼顾规模与质量；3) GSM 轻量化、可控的几何知识注入方式，兼顾 DSR 与通用视频理解；4) 在 Qwen2.5‑VL‑7B 上实现了 DSR 领先性能而不牺牲其他基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提供了一个可扩展的 4D 数据管线、评测基准和轻量化几何注入模块，使视觉语言模型在动态空间推理上实现显著提升，同时保持通用视频理解能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.&lt;/p&gt;</description></item><item><guid>2512.20739v1</guid><title>AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication</title><link>http://arxiv.org/abs/2512.20739v1</link><author>Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出一种AI驱动的绿色认知无线电网络框架，利用深度强化学习、迁移学习、能量收集和可重构智能表面等技术，优化感知时序、发射功率、带宽分配和相位选择，显著降低能耗并提升感知准确率和数据可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着6G技术的推进，目标是实现Tb/s级峰值速率、亚毫秒延迟以及大规模物联网和车联网连接，亟需可持续的空中音频访问和节能方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决频谱稀缺和传统感知/分配能耗高、对频谱快速变化敏感的问题，构建一种绿色、可持续的认知无线电网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 结合深度强化学习与迁移学习、能量收集、可重构智能表面以及轻量级遗传细化操作，最优整合感知时序、发射功率、带宽分配和RIS相位选择，并与两种基线进行对比评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在密集负载下，该框架相比传统固定策略的能量感知CRN和启发式协作感知CRN，能耗降低25-30%，感知AUC超过0.90，数据可靠性提升6-13个百分点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该集成框架易于扩展至大规模物联网和车联网应用，为6G认知无线电网络提供了可行且可持续的路线图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 6G无线网络旨在实现Tb/s级峰值数据速率，亚毫秒延迟，大规模物联网/车辆连接，需要持续的空中音频访问和节能功能。认知无线电网络（CCNs）有助于缓解频谱稀缺问题，但传统的感知和分配仍然能耗高，对频谱快速变化敏感。我们的框架以AI驱动的绿色CRN为核心，结合深度强化学习（DRL）与迁移学习、能量收集（EH）、可重构智能表面（RIS）以及轻量级遗传细化操作，最优地结合感知时序、发射功率、带宽分配和RIS相位选择。与两种基线相比，在密集负载下使用MATLAB+NS-3的传统CRN（固定策略的能量感知）和采用启发式资源分配的协作感知混合CRN，所用能量储备减少25-30%，感知AUC超过0.90，PDR提高6-13个百分点。该集成框架易于扩展到大规模物联网和车联网应用，并为6G CRN提供了可行且可持续的路线图。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The 6G wireless aims at the Tb/s peak data rates are expected, a sub-millisecond latency, massive Internet of Things/vehicle connectivity, which requires sustainable access to audio over the air and energy-saving functionality. Cognitive Radio Networks CCNs help in alleviating the problem of spectrum scarcity, but classical sensing and allocation are still energy-consumption intensive, and sensitive to rapid spectrum variations. Our framework which centers on AI driven green CRN aims at integrating deep reinforcement learning (DRL) with transfer learning, energy harvesting (EH), reconfigurable intelligent surfaces (RIS) with other light-weight genetic refinement operations that optimally combine sensing timelines, transmit power, bandwidth distribution and RIS phase selection. Compared to two baselines, the utilization of MATLAB + NS-3 under dense loads, a traditional CRN with energy sensing under fixed policies, and a hybrid CRN with cooperative sensing under heuristic distribution of resource, there are (25-30%) fewer energy reserves used, sensing AUC greater than 0.90 and +6-13 p.p. higher PDR. The integrated framework is easily scalable to large IoT and vehicular applications, and it provides a feasible and sustainable roadmap to 6G CRNs.   Index Terms--Cognitive Radio Networks (CRNs), 6G, Green Communication, Energy Efficiency, Deep Reinforcement Learning (DRL), Spectrum Sensing, RIS, Energy Harvesting, QoS, IoT.&lt;/p&gt;</description></item><item><guid>2512.20770v1</guid><title>OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective</title><link>http://arxiv.org/abs/2512.20770v1</link><author>Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了语义场景完成（SSC）在移动机器人中的重要性，并指出在空中场景中SSC研究不足。为此，作者提出了首个基于相机的空中SSC基准数据集OccuFly，并开发了无激光雷达的数据生成框架，利用传统三维重建和2D掩码迁移显著降低了三维标注工作量。通过在OccuFly上对现有方法进行基准测试，揭示了高空视角下的挑战，为空中三维场景理解提供了全面的视觉基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; SSC在地面自动驾驶等领域已得到广泛研究，但在无人机等空中场景中应用有限。激光雷达是SSC数据生成的主流方式，但受限于飞行法规、重量和能耗，且高空激光点云稀疏，难以满足需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个真实世界、基于相机的空中SSC基准数据集OccuFly，并提出一种无激光雷达的数据生成框架，以降低三维标注成本并促进空中SSC研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用相机采集不同季节、不同高度的图像，利用传统三维重建技术生成点云；通过将部分已标注的二维掩码投影到重建点云，实现自动标签迁移；在OccuFly数据集上对现有SSC方法进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 高空视角下的SSC面临独特挑战；无激光雷达的相机基准数据集可有效支持空中SSC研究；自动标签迁移显著减少了人工三维标注工作量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OccuFly为空中三维场景理解提供了全面的视觉基准，展示了相机驱动的SSC方法的可行性，并为后续研究指明了方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提供首个真实世界的无人机视角下的语义场景完成（SSC）基准数据集，并提出一种无需 LiDAR 的数据生成方法。该问题重要，因为现有 SSC 数据集主要基于地面车辆，缺乏空中视角的数据，且 LiDAR 在无人机上受重量、能耗和稀疏性限制，阻碍了空中三维感知的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有 SSC 数据集（如 SemanticKITTI、nuScenes 等）和空中重建技术，采用多视角结构光与多视角立体（SfM+MVS）生成稠密点云，并通过 2D–3D 对应关系将少量 2D 标注提升到 3D。随后使用实例分离、表面重建和体素化等步骤完成数据集构建，并参考了 Occ3D、OpenOccupancy 等相关工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用相机图像进行稠密 3D 重建，然后将少量 2D 语义标注投射到点云中，融合多视角信息并进行平滑；接着对实例类进行稠密化、对地面类进行表面重建、对其他类直接体素化；最后对每帧相机视角进行视锥裁剪得到固定大小的语义体素网格，并提供对应的深度图。整体流程包括：重建 → 2D 标注 → 3D 标注投射与融合 → 稠密化/表面重建 → 体素化 → 视锥裁剪 → 深度图生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①首个真实空中 SSC 数据集，覆盖 9 场景、20,000+ 样本、22 类；②基于相机的 LiDAR‑free 数据生成框架，显著降低 3D 标注成本；③利用 2D–3D 对应关系实现高效标注迁移；④实例级稠密化与表面重建的分组处理；⑤提供深度图并训练 Depth‑Anything‑V2。与以往 LiDAR 基础的数据集不同，OccuFly 解决了空中视角稀疏性和硬件限制问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OccuFly 提供了首个真实空中语义场景完成基准数据集，并通过相机驱动的稠密重建与 2D 标注迁移，构建了可扩展、低成本的 SSC 数据生成管线。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.&lt;/p&gt;</description></item><item><guid>2512.20876v1</guid><title>Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task</title><link>http://arxiv.org/abs/2512.20876v1</link><author>Kanata Suzuki, Shota Shimizu, Tetsuya Ogata</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了仅基于离线图像和语言数据训练的基础模型是否能理解机器人运动，并通过视频字幕任务评估视觉语言模型在自动任务字幕和子任务分割方面的能力。提出的方法利用图像字幕和轨迹数据生成场景字幕，再汇总为完整任务字幕，并通过文本嵌入相似度实现子任务分割。实验表明，向模型提供机器人关节和末端执行器状态能提升字幕和分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 未来机器人发展需要验证基础模型能否理解机器人运动；视觉语言模型缺乏低层运动信息，导致视频轨迹理解仍是挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估视觉语言模型在包含低层机器人运动信息的视频字幕任务中的两项能力：自动任务字幕生成和子任务分割，并以此衡量模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 通过图像字幕和轨迹数据生成多条场景字幕；2) 将这些字幕摘要为完整任务字幕；3) 通过比较图像字幕的文本嵌入相似度实现子任务分割；4) 在所有步骤中将机器人关节和末端执行器状态作为输入提供给模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在模拟实验中，加入机器人运动数据后，方法在自动字幕生成和子任务分割任务中表现出更好的效果，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 向视觉语言模型提供低层运动信息可显著提升其在机器人任务字幕和分割方面的表现，为机器人模仿学习提供了更高效的语言-运动链接，并为评估基础模型的运动理解能力提供了可行途径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从未来机器人发展的角度来看，验证仅在离线数据（如图像和语言）上训练的基础模型是否能理解机器人运动至关重要。特别是，视觉语言模型（VLMs）在其训练数据集中不包含机器人低层运动信息，视频理解包括轨迹信息仍是一个重大挑战。在本研究中，我们通过包含低层机器人运动信息的视频字幕任务评估了VLM的两项能力：（1）机器人任务的自动字幕生成和（2）一系列任务的分割。两项能力都预计通过将语言与运动链接来提高机器人模仿学习的效率，并作为衡量基础模型性能的手段。所提出的方法使用图像字幕和机器人任务的轨迹数据生成多条“场景”字幕。然后通过总结这些单独的字幕生成完整的任务字幕。此外，该方法通过比较图像字幕的文本嵌入相似度来执行子任务分割。在两项字幕任务中，所提出的方法通过将机器人的运动数据——关节和末端执行器状态——作为输入提供给VLM，旨在提升性能。通过模拟实验验证了所提出方法的有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model&amp;#x27;s performance. The proposed method generates multiple &amp;quot;scene&amp;quot; captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot&amp;#x27;s motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.&lt;/p&gt;</description></item><item><guid>2512.20921v1</guid><title>Self-supervised Multiplex Consensus Mamba for General Image Fusion</title><link>http://arxiv.org/abs/2512.20921v1</link><author>Yingying Wang, Rongjin Zhuang, Hui Zheng, Xuanhua He, Ke Cao, Xiaotong Tu, Xinghao Ding</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种通用图像融合框架SMC-Mamba，利用自监督多路共识机制提升融合质量并增强下游视觉任务性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 图像融合通过整合不同模态信息生成高质量图像，可提升目标检测、语义分割等下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种既能处理多种融合任务又不增加复杂度的通用图像融合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SMC-Mamba包含MAFE模块通过自适应门控保留细节并通过空间通道与频率扫描增强全局表示；MCCM模块实现专家间动态协作与跨模态扫描以达成共识并高效融合信息；BSCL自监督对比损失在保持高频信息的同时提升下游性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明SMC-Mamba在红外可见、医学、多焦点、多曝光等融合任务以及下游视觉任务上均优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自监督多路共识框架能在不增加计算负担的前提下显著提升图像融合质量和下游任务表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图像融合通过整合不同模态的互补信息来生成高质量的融合图像，从而提升目标检测和语义分割等下游任务的性能。与主要关注整合模态间信息的任务特定技术不同，通用图像融合需要在不增加复杂度的前提下，处理多种任务并提升性能。为此，我们提出了SMC-Mamba，一种用于通用图像融合的自监督多路共识Mamba框架。具体而言，模态无关特征增强（MAFE）模块通过自适应门控保留细节，并通过空间-通道和频率旋转扫描增强全局表示。多路共识跨模态Mamba（MCCM）模块实现专家之间的动态协作，达成共识以高效整合多模态的互补信息。MCCM 内的跨模态扫描进一步加强了模态间的特征交互，促进了两源关键信息的无缝融合。此外，我们引入了双层自监督对比学习损失（BSCL），在不增加计算开销的情况下保留高频信息，同时提升下游任务的性能。大量实验表明，我们的方法在红外-可见、医学、多焦点、多曝光融合以及下游视觉任务中均优于最先进的图像融合算法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.&lt;/p&gt;</description></item><item><guid>2512.20950v1</guid><title>MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment</title><link>http://arxiv.org/abs/2512.20950v1</link><author>Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个名为TriAligner的系统，用于多语言和跨语言的事实核查声明检索，通过双编码器和对比学习结合原文与英文翻译，提升检索准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在信息快速传播的时代，误导性信息泛滥，事实核查变得尤为重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种高效的多语言和跨语言声明检索方法，以提高事实核查的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用双编码器架构与对比学习，融合原文和英文翻译，学习不同来源的重要性；通过大语言模型进行数据预处理和增强，并采用硬负样本采样提升表示学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在单语和跨语基准上，TriAligner显著提升了检索准确率和事实核查性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; TriAligner在多语言和跨语言声明检索任务中表现优异，证明了双编码器与对比学习结合翻译信息的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.&lt;/p&gt;</description></item><item><guid>2512.20988v1</guid><title>PUFM++: Point Cloud Upsampling via Enhanced Flow Matching</title><link>http://arxiv.org/abs/2512.20988v1</link><author>Zhi-Song Liu, Chenhang He, Roland Maier, Andreas Rupp</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; PUFM++ 是一种改进的流匹配框架，旨在从稀疏、噪声和部分观测中重建高质量、密集且准确的点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来，生成模型在点云上采样方面表现出强大潜力，但现有方法在几何保真度、鲁棒性和与后续表面任务的一致性方面仍有不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提升流匹配的几何保真度、对不完美输入的鲁棒性，并确保与下游表面任务的一致性，从而实现更高质量的点云重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用两阶段流匹配策略：先学习稀疏输入到密集目标的直线路径流，再用噪声扰动样本进行细化；引入数据驱动的自适应时间调度器提高采样效率；在采样过程中施加在流形上的约束，保证生成点与底层表面对齐；使用递归接口网络加强层次特征交互，提升重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成基准和真实扫描数据上，PUFM++ 在点云上采样任务中实现了新的最先进水平，提供了更好的视觉保真度和定量精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PUFM++ 通过改进流匹配技术，显著提升了点云重建的质量，成为点云上采样领域的领先方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近年来，生成模型在高质量点云上采样方面展现出强大潜力。在本研究中，我们提出了 PUFM++，一种改进的流匹配框架，用于从稀疏、噪声和部分观测中重建密集且准确的点云。PUFM++ 在流匹配方面在三个关键方向上取得了提升：几何保真度、对不完美输入的鲁棒性以及与下游基于表面的任务的一致性。我们引入了两阶段流匹配策略，首先学习从稀疏输入到密集目标的直接直线路径流，然后使用噪声扰动样本进行细化，以更好地逼近终端边缘分布。为加速和稳定推理，我们提出了基于数据的自适应时间调度器，基于插值行为提高采样效率。我们进一步在采样过程中施加流形约束，确保生成的点保持与底层表面对齐。最后，我们加入了递归接口网络（RIN），以加强层次特征交互并提升重建质量。广泛实验表明，PUFM++ 在点云上采样任务中设定了新的最先进水平，在视觉保真度和定量精度方面均优于现有方法。代码和预训练模型已公开可用，地址为 https://github.com/Holmes-Alan/Enhanced_PUFM。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在将稀疏、噪声或不完整的点云转换为高密度、精确的点云。高质量的点云是机器人导航、工业检测、自动驾驶和三维重建等领域的基础，传统方法往往需要昂贵的硬件或手工设计，难以普适。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在先前的 PUFM 和 PU-DM 等流匹配与扩散模型工作基础上，提出了两阶段流匹配、时间自适应采样、流形约束和递归接口网络等改进。通过对现有方法的局限性（如步数多、缺乏全局一致性、对噪声敏感）进行分析，设计了更高效、更鲁棒的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先在补丁空间学习稀疏到稠密的直线流匹配，再用噪声扰动的样本进行细化；在推理时采用自适应时间步长加速采样，并在采样过程中强制点保持在表面流形上。实现流程包括：提取稀疏与稠密补丁对，训练两阶段流匹配网络，推理时使用自适应 ODE 步长生成稠密点，最后通过 kNN 后处理和 RIN 进一步提升质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 两阶段流匹配训练，提升精度与鲁棒性；2) 基于训练轨迹的自适应时间采样，显著减少采样步数；3) 流形感知先验和 kNN 后处理，保证点云与表面一致；4) 递归接口网络（RIN）提供长期全局上下文。与 PUFM 相比，PUFM++ 在效率、表面一致性和细节保留方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PUFM++ 通过两阶段自适应流匹配、流形约束和递归特征交互，显著提升点云上采样的速度、精度和鲁棒性，成为目前最先进的稀疏到稠密点云生成方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.&lt;/p&gt;</description></item><item><guid>2512.21133v1</guid><title>SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation</title><link>http://arxiv.org/abs/2512.21133v1</link><author>Xiaoyu Mo, Jintian Ge, Zifan Wang, Chen Lv, Karl Henrik Johansson</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种稀疏图学习框架 SparScene，用于高效、可扩展地表示交通场景，从而实现多智能体轨迹生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多智能体轨迹生成是自动驾驶和智能交通系统的核心问题，但在复杂场景中高效建模众多道路使用者与基础设施之间的动态交互仍是挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服现有基于距离阈值或全连接图结构方法的冗余边和高参数化网络导致的低效率和可扩展性不足的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SparScene 利用车道图拓扑构建结构感知的稀疏连接，采用轻量级图编码器聚合代理-地图和代理-代理交互，生成紧凑的场景表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 Waymo Open Motion Dataset 上，SparScene 在保持竞争性能的同时，生成 200 多个代理轨迹仅需 5 毫秒；在 5,000 个代理和 17,000 条车道的规模下，仅需 54 毫秒推理时间和 2.9 GB GPU 内存，显示出卓越的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SparScene 通过稀疏图学习实现了高效、可扩展的交通场景表示，显著提升了多智能体轨迹生成的训练和推理效率，适用于大规模复杂交通场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多智能体轨迹生成是自动驾驶和智能交通系统的核心问题。然而，在复杂场景中高效建模众多道路使用者与基础设施之间的动态交互仍是一个开放问题。现有方法通常采用基于距离或全连接稠密图结构来捕捉交互信息，这不仅引入大量冗余边，还需要复杂且参数量大的网络进行编码，从而导致训练和推理效率低下，限制了对大型复杂交通场景的可扩展性。为克服现有方法的局限性，我们提出了 SparScene，一种为高效可扩展交通场景表示而设计的稀疏图学习框架。SparScene 不是依赖距离阈值，而是利用车道图拓扑构建结构感知的稀疏连接，实现了高效且信息丰富的场景图表示。SparScene 采用轻量级图编码器，能够高效聚合代理-地图和代理-代理交互，得到紧凑的场景表示，并显著提升效率和可扩展性。在 Waymo Open Motion Dataset 的运动预测基准上，SparScene 在保持竞争性能的同时表现出卓越效率。它能在 5 毫秒内为场景中的 200 多个代理生成轨迹，并在仅 54 毫秒推理时间和 2.9 GB GPU 内存下扩展到 5,000 多个代理和 17,000 条车道，凸显了其在大规模交通场景中的卓越可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.&lt;/p&gt;</description></item><item><guid>2512.21144v1</guid><title>Encrypted Traffic Detection in Resource Constrained IoT Networks: A Diffusion Model and LLM Integrated Framework</title><link>http://arxiv.org/abs/2512.21144v1</link><author>Hongjuan Li, Hui Kang, Chenbang Liu, Ruolin Wang, Jiahui Li, Geng Sun, Jiacheng Wang, Shuang Liang, Shiwen Mao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 DMLITE 框架，结合扩散模型和大语言模型，针对资源受限的物联网环境中的网络流量检测，克服了动态流量、计算受限和低延迟等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 物联网基础设施的快速扩张和流量加密的普及，使得在动态流量模式、受限计算能力和严格延迟约束的环境下进行流量检测变得困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在资源受限的物联网环境中高效、准确地检测加密网络流量的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DMLITE 采用三阶段架构：首先进行流量可视化预处理；其次使用自监督扩散模型进行多层特征提取，并通过多层特征融合和对比学习以及代表样本选择来捕捉细粒度和抽象模式；最后利用大语言模型动态调整粒子群优化参数，实现双目标函数的特征选择，既降低分类误差又减少数据分布方差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 USTC‑TFC、ISCX‑VPN 和 Edge‑IIoTset 三个基准数据集上，DMLITE 的分类准确率分别达到 98.87%、92.61% 和 99.83%，平均提升 3.7% 的准确率，并将训练时间平均缩短 41.9% 相比代表性深度学习模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DMLITE 在物联网环境中实现了高准确率、低训练时间的网络流量检测，证明了扩散模型与大语言模型结合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The proliferation of Internet‑of‑Things infrastructures and widespread traffic encryption pose significant challenges, especially in environments with dynamic traffic patterns, limited computational resources, and strict latency constraints. This paper proposes DMLITE, a diffusion model and large language model integrated traffic embedding framework for network traffic detection in resource‑limited IoT environments. DMLITE overcomes these challenges through a tri‑phase architecture including traffic visual preprocessing, diffusion‑based multi‑level feature extraction, and LLM‑guided feature optimization. The framework uses self‑supervised diffusion models to capture fine‑grained and abstract patterns in encrypted traffic through multi‑level feature fusion and contrastive learning with representative sample selection, enabling rapid adaptation to new traffic patterns with minimal labeled data. DMLITE also incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87%, 92.61%, and 99.83% on USTC‑TFC, ISCX‑VPN, and Edge‑IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7% and reduces training time by an average of 41.9% compared to the representative deep learning model.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The proliferation of Internet-of-things (IoT) infrastructures and the widespread adoption of traffic encryption present significant challenges, particularly in environments characterized by dynamic traffic patterns, constrained computational capabilities, and strict latency constraints. In this paper, we propose DMLITE, a diffusion model and large language model (LLM) integrated traffic embedding framework for network traffic detection within resource-limited IoT environments. The DMLITE overcomes these challenges through a tri-phase architecture including traffic visual preprocessing, diffusion-based multi-level feature extraction, and LLM-guided feature optimization. Specifically, the framework utilizes self-supervised diffusion models to capture both fine-grained and abstract patterns in encrypted traffic through multi-level feature fusion and contrastive learning with representative sample selection, thus enabling rapid adaptation to new traffic patterns with minimal labeled data. Furthermore, DMLITE incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87\%, 92.61\%, and 99.83\% on USTC-TFC, ISCX-VPN, and Edge-IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7\% and reduces training time by an average of 41.9\% compared to the representative deep learning model.&lt;/p&gt;</description></item><item><guid>2512.21153v1</guid><title>ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update</title><link>http://arxiv.org/abs/2512.21153v1</link><author>Zhe Su, Giacomo Indiveri</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; ElfCore 是一种 28nm 数字尖峰神经网络处理器，首次高效集成了本地在线自监督学习、动态结构稀疏训练以及基于活动的稀疏权重更新机制，能够在手势识别、语音和生物医学信号处理等任务中实现显著的功耗降低、内存需求减少和网络容量提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在事件驱动的感官信号处理领域，缺乏能够同时实现自监督学习、稀疏训练和活动依赖权重更新的高效处理器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够高效集成自监督学习、稀疏训练和活动依赖权重更新的数字尖峰神经网络处理器，以提升功耗、内存和网络容量效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在 28nm 工艺下实现本地在线自监督学习引擎、动态结构稀疏训练引擎以及基于活动的稀疏权重更新机制，并将其集成到 ElfCore 处理器中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在手势识别、语音和生物医学信号处理等任务中，ElfCore 的功耗比最先进方案低 16 倍，片上内存需求减少 3.8 倍，网络容量效率提升 5.9 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ElfCore 提供了一种高效、低功耗且高性能的事件驱动感官信号处理解决方案，显著优于现有技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了 ElfCore，一种 28nm 数字尖峰神经网络处理器，专为事件驱动的感官信号处理而设计。ElfCore 首次高效集成了：1）本地在线自监督学习引擎，支持多层时序学习而无需标记输入；2）动态结构稀疏训练引擎，支持高精度稀疏到稀疏学习；3）基于活动的稀疏权重更新机制，仅根据输入活动和网络动态选择性更新权重。通过手势识别、语音和生物医学信号处理等任务的验证，ElfCore 在功耗、片上内存需求和网络容量效率方面分别比最先进方案低 16 倍、减少 3.8 倍、提高 5.9 倍。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we present ElfCore, a 28nm digital spiking neural network processor tailored for event-driven sensory signal processing. ElfCore is the first to efficiently integrate: (1) a local online self-supervised learning engine that enables multi-layer temporal learning without labeled inputs; (2) a dynamic structured sparse training engine that supports high-accuracy sparse-to-sparse learning; and (3) an activity-dependent sparse weight update mechanism that selectively updates weights based solely on input activity and network dynamics. Demonstrated on tasks including gesture recognition, speech, and biomedical signal processing, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.&lt;/p&gt;</description></item><item><guid>2512.21204v1</guid><title>SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation</title><link>http://arxiv.org/abs/2512.21204v1</link><author>Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SpidR-Adapt 是一种利用少量无标签音频快速适应新语言的技术，显著提高了语音表示学习的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类婴儿仅需数百小时的语言暴露即可掌握新语言的基本单位，而现有自监督语音模型需要大量数据，存在显著的效率差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决低资源语音表示学习中的数据效率问题，提供一种快速、数据友好的适应方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将低资源学习视为元学习问题，设计多任务自适应预训练协议 MAdaPT，采用双层优化框架，并提出一阶双层优化 FOBLO 以降低计算成本；通过交错监督实现稳健初始化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在少于一小时目标语言音频的训练下，SpidR-Adapt 在音素辨别和口语语言建模任务上显著优于传统模型，数据效率提升超过一百倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法为实现生物学启发、数据高效的语音表示提供了可行、与架构无关的路径，并已开源代码和模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类婴儿仅需数百小时的语言暴露即可掌握新语言的基本单位，显示出与数据需求极高的自监督语音模型之间的显著效率差距。为弥补这一差距，本文提出了 SpidR-Adapt，利用极少量无标签数据实现对新语言的快速适应。我们将低资源语音表示学习视为元学习问题，构建了多任务自适应预训练协议 MAdaPT，并将适应过程表述为双层优化框架。为实现可扩展的元训练，提出了一阶双层优化 FOBLO，避免了高昂的计算成本。最后，通过交错监督的稳健初始化，稳定了元训练。实验表明，SpidR-Adapt 在音素辨别和口语语言建模任务上，在训练不到一小时的目标语言音频后，显著优于领域内语言模型，数据效率提升超过一百倍。该研究为实现生物学启发、数据高效的语音表示提供了实用且与架构无关的路径，并将训练代码和模型检查点开源。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.&lt;/p&gt;</description></item><item><guid>2512.21233v3</guid><title>UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer</title><link>http://arxiv.org/abs/2512.21233v3</link><author>Chi Zhang, Penglin Cai, Haoqi Yuan, Chaoyi Xu, Zongqing Lu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种统一表示 UniTacHand，将人类手部触觉数据与机器人手部触觉数据映射到相同的二维表面空间，并通过对比学习实现零样本策略迁移，提升数据效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 触觉感知对机器人手实现人类级灵巧操作至关重要，但收集大规模真实世界触觉数据困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用低成本人类触觉数据训练机器人策略，并解决人类与机器人触觉数据不匹配的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将人类和机器人触觉信号投影到 MANO 手模型的二维表面空间，统一表示后使用对比学习在10分钟配对数据上训练统一潜在空间，实现零样本迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 零样本触觉策略迁移可在未见过的物体上泛化；混合训练人类与机器人数据比仅使用机器人数据更高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; UniTacHand 为触觉基灵巧手的通用、可扩展、数据高效学习提供了路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 触觉感知对于机器人手实现人类级灵巧操作至关重要，尤其在视觉遮挡场景中更为重要。然而，收集大规模真实世界机器人触觉数据的难度常常阻碍其应用。本文提出利用低成本的触觉手套收集人类操作数据，用于基于触觉的机器人策略学习。人类与机器人触觉数据之间的不匹配使得从人类数据迁移策略到机器人变得具有挑战性。为弥合这一差距，我们提出 UniTacHand，一种统一表示，用于将由灵巧手捕获的机器人触觉信息与手套获取的人类手部触觉对齐。首先，我们将人类手和机器人手的触觉信号投影到 MANO 手模型的形态一致的二维表面空间中。该统一化标准化了异构数据结构，并天然地将触觉信号嵌入空间上下文。随后，我们引入对比学习方法，将它们对齐到统一的潜在空间，仅使用我们数据采集系统的10分钟配对数据进行训练。我们的方法实现了从人类到真实机器人零样本触觉策略迁移，并能泛化到预训练数据中未见过的物体。我们还证明，通过 UniTacHand 对混合数据（包括人类和机器人演示）进行联合训练，比仅使用机器人数据能获得更好的性能和数据效率。UniTacHand 为触觉基灵巧手的通用、可扩展和数据高效学习铺平了道路。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.&lt;/p&gt;</description></item><item><guid>2512.21237v1</guid><title>SegMo: Segment-aligned Text to 3D Human Motion Generation</title><link>http://arxiv.org/abs/2512.21237v1</link><author>Bowen Dang, Lin Wu, Xiaohang Yang, Zheng Yuan, Zhixiang Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SegMo是一种基于段落对齐的文本驱动3D人体动作生成框架，利用文本和动作的细粒度语义片段实现更精准的对应关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成3D人体动作的文本描述是游戏、虚拟现实和增强现实等领域的重要研究问题，现有方法多在序列层面对齐文本与动作，忽略了两种模态内部的语义结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将文本描述和动作序列拆分为语义连贯的片段，构建细粒度的文本-动作对齐机制，从而提升动作生成的准确性和可控性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SegMo包含三大模块：1）文本片段提取，将复杂文本拆分为按时间顺序排列的简单动作短语；2）动作片段提取，将完整动作序列划分为对应的动作片段；3）细粒度文本-动作对齐，利用对比学习将文本片段与动作片段映射到共享嵌入空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在两个主流数据集上进行的大量实验表明，SegMo相较于强基线提升显著，在HumanML3D测试集上TOP 1得分提升至0.553；此外，学习到的共享嵌入空间还能用于动作定位和动作到文本检索等检索式任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SegMo通过段落级对齐实现了更细粒度的文本-动作对应，显著提升了动作生成性能，并扩展到检索任务，证明了其广泛的应用潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成3D人体动作的文本描述是游戏、虚拟现实和增强现实等领域的重要研究问题。最近的方法在序列层面将文本描述与人体动作对齐，忽略了两种模态内部的语义结构。然而，动作描述和动作序列都可以自然地拆分为更小、更具语义连贯性的片段，这些片段可以作为原子对齐单元，实现更细粒度的对应。基于此，我们提出了SegMo，一种基于段落对齐的文本驱动人体动作生成框架，以实现细粒度的文本-动作对齐。我们的框架由三个模块组成：（1）文本片段提取，将复杂文本描述拆分为按时间顺序排列的短语，每个短语代表一个简单的原子动作；（2）动作片段提取，将完整动作序列划分为对应的动作片段；（3）细粒度文本-动作对齐，利用对比学习将文本和动作片段对齐。大量实验表明，SegMo在两个广泛使用的数据集上提升了强基线，在HumanML3D测试集上实现了TOP 1得分0.553的提升。此外，由于学习到的文本和动作片段共享嵌入空间，SegMo还可以应用于检索式任务，如动作定位和动作到文本检索。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.&lt;/p&gt;</description></item><item><guid>2512.21284v1</guid><title>Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential</title><link>http://arxiv.org/abs/2512.21284v1</link><author>Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于脉冲神经网络的实时手术场景分割框架SpikeSurgSeg，利用掩码自编码预训练和轻量化分割头，在非GPU平台上实现高效分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 手术系统需要智能场景理解以提升安全性，传统深度学习模型虽精度高但计算量大，难以在资源受限的手术环境中实时部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索脉冲神经网络在手术场景分割中的可行性，并克服标注稀缺和视频稀疏性带来的性能瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出SpikeSurgSeg框架，采用层级管道掩码自编码预训练学习空间时间表示，并在此基础上加入轻量化脉冲驱动分割头，实现低延迟一致预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在EndoVis18和SurgBleed数据集上，SpikeSurgSeg的mIoU与最先进的ANN模型相当，同时推理延迟至少降低8倍，较大多数基础模型加速超过20倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 脉冲神经网络能够在非GPU平台实现高效、实时的手术场景分割，具有显著的计算节省和速度优势，适用于时间敏感的手术应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现代手术系统越来越依赖智能场景理解，以提供及时的情境感知，从而提升术中安全性。在这一流程中，手术场景分割在准确感知手术事件方面起着核心作用。尽管最近的深度学习模型，尤其是大规模基础模型，已实现显著的分割精度，但它们巨大的计算需求和功耗阻碍了在资源受限的手术环境中实现实时部署。为解决这一限制，我们探索了新兴的脉冲神经网络（SNN）作为实现高效手术智能的有前景范式。然而，它们的性能仍受限于标注手术数据的稀缺以及手术视频表示本身的稀疏性。为此，我们提出了SpikeSurgSeg，这是第一个针对手术场景分割的脉冲驱动视频Transformer框架，具有在非GPU平台实现实时潜力。为解决手术注释有限的问题，我们为SNN引入了一种手术场景掩码自编码预训练策略，通过层级管道掩码实现稳健的时空表示学习。在此预训练骨干网络基础上，我们进一步采用轻量化的脉冲驱动分割头，能够产生时间一致的预测，同时保持SNN的低延迟特性。对EndoVis18和我们内部的SurgBleed数据集进行的大量实验表明，SpikeSurgSeg在mIoU上与最先进的ANN模型相当，同时将推理延迟至少降低8倍。值得注意的是，它相对于大多数基础模型基线实现了超过20倍的加速，凸显了其在时间敏感的手术场景分割中的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.&lt;/p&gt;</description></item><item><guid>2512.21331v2</guid><title>TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</title><link>http://arxiv.org/abs/2512.21331v2</link><author>Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofiène Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis, Maria Vakalopoulou, Dimitris Samaras</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; TICON是一种基于Transformer的统一上下文化嵌入生成器，能够为任何基于图块的病理模型提供丰富的上下文信息，并在多项基准上取得新SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的图块编码器在从图块中提取嵌入时忽略了大幅图像的上下文，导致无法捕捉全局信息；不同图块编码器在下游任务上表现不一。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一个统一的模型，能够对来自任何图块级基础模型的嵌入进行上下文化，并统一表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用单一共享编码器，使用掩码建模目标进行预训练，融合并上下文化多种图块级病理基础模型的表示；随后在TICON上预训练聚合器构建全切片级基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; TICON上下文化嵌入在多项图块级和切片级基准上显著提升性能，刷新HEST-Bench、THUNDER、CATCH、Patho-Bench等SOTA；仅用11K WSIs预训练的聚合器模型在切片级任务上优于使用多达350K WSIs预训练的SOTA模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过统一的Transformer上下文化方法，能够有效提升病理图像分析的表现，证明少量数据即可构建强大的切片级基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在大幅全切片图像（WSI）中，对小图块的解释往往需要更大的图像上下文。我们提出了TICON，一种基于Transformer的图块表示上下文化器，能够为任何计算病理学应用生成丰富的上下文嵌入。传统的基于图块编码器的流程会从图块中提取嵌入，却忽略了对局部和全局任务至关重要的丰富切片级信息。此外，不同的图块编码器在不同的下游任务上表现优异。因此，需要一个统一的模型来对来自任何图块级基础模型的嵌入进行上下文化。TICON通过单一共享编码器，使用掩码建模目标进行预训练，能够同时统一并上下文化来自多种图块级病理基础模型的表示。我们的实验表明，TICON上下文化嵌入在多项任务上显著提升性能，在图块级基准（HEST-Bench、THUNDER、CATCH）和切片级基准（Patho-Bench）上取得新的SOTA。最后，我们在TICON上预训练聚合器，构建切片级基础模型，仅使用11K WSI，优于使用多达350K WSI预训练的SOTA切片级基础模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for &amp;#x27;&amp;#x27;any&amp;#x27;&amp;#x27; application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from &amp;#x27;&amp;#x27;any&amp;#x27;&amp;#x27; tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.&lt;/p&gt;</description></item><item><guid>2512.21333v1</guid><title>Fast SAM2 with Text-Driven Token Pruning</title><link>http://arxiv.org/abs/2512.21333v1</link><author>Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于文本引导的视觉令牌裁剪框架，旨在通过在时间传播前减少不相关的视觉令牌来提升视频目标分割的推理效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的 SAM2 视频分割模型在处理时间维度的视觉令牌时会产生二次方的内存注意力开销，导致计算和内存成本高，限制了其在实际部署中的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不改动原有分割架构的前提下，利用文本信息和视觉上下文，选择性地裁剪视觉令牌，以降低冗余计算并保持分割精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在视觉编码后、基于记忆的传播前，使用轻量级路由机制对令牌进行排序。排序依据包括局部视觉上下文、来自对象中心文本描述的语义相关性（可由用户提供或自动生成）以及不确定性提示，以保留对边界和模糊区域重要的令牌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个挑战性视频分割基准上实验表明，后编码器令牌裁剪可使推理速度提升至 42.5% 以上，GPU 内存使用降低 37.4%，同时保持与未裁剪基线 SAM2 相当的 J 和 F 性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 早期令牌选择为基于 Transformer 的视频分割系统在实时和资源受限场景下提供了可行且有效的可扩展性改进路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Segment Anything Model 2 (SAM2)，一种视觉基础模型，在基于提示的视频目标分割方面取得了显著进展，但其实际部署仍受限于在时间上处理稠密视觉令牌所需的高计算和内存成本。SAM2 的管道通常会将图像编码器生成的所有视觉令牌传播到下游的时间推理模块，而不考虑它们与目标对象的相关性，导致由于二次方的内存注意力开销而降低了可扩展性。本文提出了一种基于文本引导的令牌裁剪框架，通过在时间传播前有选择地减少令牌密度来提高推理效率，而不修改底层分割架构。该方法在视觉编码后、基于记忆的传播前进行操作，使用轻量级路由机制对令牌进行排名，结合局部视觉上下文、来自对象中心文本描述（无论是用户提供还是自动生成）的语义相关性以及不确定性提示，以保留模糊或边界关键区域。通过仅保留最具信息量的令牌进行下游处理，所提出的方法在保持分割精度的同时减少了冗余计算。多项挑战性视频分割基准的广泛实验表明，后编码器令牌裁剪为实现高效、提示感知的视频分割提供了实用且有效的路径，与未裁剪的基线 SAM2 相比，推理速度提升高达 42.50% ，GPU 内存使用降低 37.41% ，同时保持竞争性的 J 和 F 性能。这些结果凸显了早期令牌选择在提升基于 Transformer 的视频分割系统在实时和资源受限应用中的可扩展性方面的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.&lt;/p&gt;</description></item><item><guid>2512.21334v1</guid><title>Streaming Video Instruction Tuning</title><link>http://arxiv.org/abs/2512.21334v1</link><author>Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了Streamo，一种实时流视频LLM，能够执行多种流视频任务，并通过大规模指令跟随数据集实现统一训练，表现出强大的时序推理和广泛泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有在线视频模型主要聚焦于问答或字幕，缺乏对多任务的支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个通用的实时流视频交互助手，支持多种任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建Streamo-Instruct-465K数据集，覆盖多时序上下文和多任务监督，使用端到端训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Streamo在时序推理、响应交互和多任务泛化方面表现优异，实验表明它弥合了离线视频感知模型与实时多模态助手之间的差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Streamo为统一、智能的连续视频流理解迈出了一步。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了Streamo，一种实时流视频LLM，作为通用交互助手。与现有仅专注于问答或字幕的在线视频模型不同，Streamo执行广泛的流视频任务，包括实时旁白、动作理解、事件字幕、时间事件定位和时间敏感问答。为实现这种多样性，我们构建了Streamo-Instruct-465K，一个针对流视频理解的大规模指令跟随数据集。该数据集涵盖多样的时间上下文和多任务监督，支持在异构流任务上统一训练。通过在指令跟随数据集上端到端训练并采用简化管道，Streamo展现出强大的时间推理、响应式交互和在多种流基准上的广泛泛化。大量实验表明，Streamo弥合了离线视频感知模型与实时多模态助手之间的差距，迈出了统一、智能连续视频流理解的一步。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.&lt;/p&gt;</description></item><item><guid>2512.21349v1</guid><title>Physics-Informed Neural Solvers for Periodic Quantum Eigenproblems</title><link>http://arxiv.org/abs/2512.21349v1</link><author>Haaris Mian</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本论文提出了一种基于物理信息的机器学习框架，用来求解二维周期势场中粒子的 Floquet‑Bloch 本征值问题，重点研究蜂窝晶格结构。通过神经网络同时学习 Bloch 函数和能量，构建无网格求解器，并在无监督的情况下通过复合损失函数强制满足薛定谔方程、Bloch 周期性和归一化约束。该方法在布里渊区内训练，能够恢复能带结构和 Bloch 模式，并与传统平面波展开方法进行数值验证。进一步利用迁移学习将求解器从近自由电子势场迁移到强变化势场，展示了其捕捉能带拓扑变化的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 蜂窝晶格具有独特的能带拓扑，出现 Dirac 点，且与石墨烯等材料相关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种物理信息机器学习框架，用于求解二维周期势场中粒子的 Floquet‑Bloch 本征值问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用神经网络同时学习复杂的 Bloch 函数和对应的本征值，构建无网格求解器；通过复合损失函数在无监督的情况下强制满足薛定谔方程、Bloch 周期性和归一化约束；在布里渊区内训练以恢复能带结构和 Bloch 模式；使用迁移学习将求解器从近自由电子势场迁移到强变化势场。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型能够成功恢复能带结构和 Bloch 模式，并与传统平面波展开方法的数值结果相符；迁移学习能够捕捉能带拓扑的变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该工作为量子本征问题的物理信息机器学习领域做出了贡献，提供了对对称性、能带结构与神经网络架构相互作用的见解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本论文提出了一种物理信息机器学习框架，用于求解与二维周期势场中粒子相关的 Floquet‑Bloch 本征值问题，重点研究蜂窝晶格几何结构，因为其独特的能带拓扑具有 Dirac 点，并且与石墨烯等材料相关。通过利用神经网络同时学习复杂的 Bloch 函数及其对应的本征值（能量），我们开发了一个无网格求解器，通过复合损失函数在无监督的情况下强制满足薛定谔方程、Bloch 周期性和归一化约束。该模型在布里渊区内训练，以恢复能带结构和 Bloch 模式，并与传统平面波展开方法进行数值验证。我们进一步探索了迁移学习技术，将求解器从近自由电子势场适配到强变化势场，展示了其捕捉能带拓扑变化的能力。本工作为量子本征问题的物理信息机器学习领域做出了贡献，提供了对对称性、能带结构与神经网络架构相互作用的见解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This thesis presents a physics-informed machine learning framework for solving the Floquet-Bloch eigenvalue problem associated with particles in two-dimensional periodic potentials, with a focus on honeycomb lattice geometry, due to its distinctive band topology featuring Dirac points and its relevance to materials such as graphene. By leveraging neural networks to learn complex Bloch functions and their associated eigenvalues (energies) simultaneously, we develop a mesh-free solver enforcing the governing Schrödinger equation, Bloch periodicity, and normalization constraints through a composite loss function without supervision. The model is trained over the Brillouin zone to recover band structures and Bloch modes, with numerical validation against traditional plane-wave expansion methods. We further explore transfer learning techniques to adapt the solver from nearly-free electron potentials to strongly varying potentials, demonstrating its ability to capture changes in band structure topology. This work contributes to the growing field of physics-informed machine learning for quantum eigenproblems, providing insights into the interplay between symmetry, band structure, and neural architectures.&lt;/p&gt;</description></item><item><guid>2512.21380v1</guid><title>SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram</title><link>http://arxiv.org/abs/2512.21380v1</link><author>Mohammad Hammas Saeed, Howie Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了SENTINEL框架，利用社交媒体讨论中的语言和网络信号实现网络攻击的早期检测，并在Telegram公开频道数据上取得高准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 网络攻击对现代社会技术系统构成严重威胁，传统防御多依赖事后检测与缓解，缺乏主动预防手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够提前识别网络攻击的系统，利用社交媒体中的讨论信息进行实时威胁检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 结合大型语言模型进行语言建模与图神经网络提取协调标记，构建多模态信号；使用来自16个Telegram公共频道的365k条消息进行训练与评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SENTINEL能够将社交媒体讨论与现实世界攻击对齐，实验中取得0.89的F1值，证明语言与网络信号在预测在线威胁中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 利用语言和网络信号进行早期检测是预测网络攻击的重要手段，SENTINEL框架展示了该方法的可行性与高效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 网络攻击对现代社会技术系统构成严重威胁，往往导致严重的技术和社会后果。攻击者通常通过恶意软件、勒索软件或其他技术手段攻击系统和基础设施。传统的防御机制大多依赖事后检测和缓解策略，只有在事件发生后才做出响应，而不是主动预防。近期趋势表明，社交媒体讨论可以作为可靠的威胁检测指标。恶意行为者经常利用在线平台分发攻击工具、分享攻击知识并进行协调。专家也会在网络空间预测正在进行的攻击并讨论潜在的漏洞。在本研究中，我们提出了SENTINEL框架，利用社交媒体信号实现网络攻击的早期检测。SENTINEL通过多模态信号将网络安全讨论与现实世界的网络攻击对齐，即结合大型语言模型的语言建模和图神经网络的协调标记。我们使用来自16个与网络安全和开源情报相关的Telegram公共频道的数据，共计365k条消息。我们强调社交媒体讨论涉及对网络威胁的积极对话，并利用SENTINEL将信号与现实威胁对齐，取得了0.89的F1值。我们的工作强调了在预测在线威胁时利用语言和网络信号的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Cyberattacks pose a serious threat to modern sociotechnical systems, often resulting in severe technical and societal consequences. Attackers commonly target systems and infrastructure through methods such as malware, ransomware, or other forms of technical exploitation. Most traditional mechanisms to counter these threats rely on post-hoc detection and mitigation strategies, responding to cyber incidents only after they occur rather than preventing them proactively. Recent trends reveal social media discussions can serve as reliable indicators for detecting such threats. Malicious actors often exploit online platforms to distribute attack tools, share attack knowledge and coordinate. Experts too, often predict ongoing attacks and discuss potential breaches in online spaces. In this work, we present SENTINEL, a framework that leverages social media signals for early detection of cyber attacks. SENTINEL aligns cybersecurity discussions to realworld cyber attacks leveraging multi modal signals, i.e., combining language modeling through large language models and coordination markers through graph neural networks. We use data from 16 public channels on Telegram related to cybersecurity and open source intelligence (OSINT) that span 365k messages. We highlight that social media discussions involve active dialogue around cyber threats and leverage SENTINEL to align the signals to real-world threats with an F1 of 0.89. Our work highlights the importance of leveraging language and network signals in predicting online threats.&lt;/p&gt;</description></item><item><guid>2512.21391v1</guid><title>ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks</title><link>http://arxiv.org/abs/2512.21391v1</link><author>Mohammad Hammas Saeed, Isaiah J. King, Howie Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 ALETHEIA 系统，用于检测和预测社交媒体中的恶意账号（即“土狼”账号）在影响力活动中的行为。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 影响力活动在网络空间日益严重，政策制定者、版主和研究者正在寻找方法来保护普通用户。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个基于图结构的检测管道，利用拓扑和语言特征来识别恶意账号，并预测其未来互动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用图神经网络检测恶意用户，并将 GNN 与循环神经网络结合进行时间链接预测，预测土狼与土狼以及土狼与普通用户之间的未来边。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 相较于仅使用交互特征的传统方法，ALETHEIA 在检测精度上提升了 3.7%，在时间链接预测上平均 AUC 达到 96.6%，并能有效识别受影响的普通用户。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 利用影响力活动的网络结构信息对检测和预测恶意协同行为具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 影响力活动在在线空间中日益成为关注焦点。政策制定者、版主和研究者已采取多种途径来打击这些活动，并使在线系统对普通用户更安全。为此，我们的论文提出了 ALETHEIA，一个系统化检测在此类操作中使用的恶意账号（或土狼账号）并预测其在社交媒体网络中的行为。我们分析了来自不同国家的 Reddit 和 X 上的影响力活动，并强调了基于图表示、结合拓扑和语言特征的检测管道相较于标准交互和用户特征的改进。ALETHEIA 使用最先进的图神经网络（GNN）检测可扩展到大型网络的恶意用户，并在先前工作中基于交互特征的标准分类上实现了 3.7% 的 F1 分数提升。此外，ALETHEIA 采用了为影响力活动构建的首个时间链接预测机制，通过在循环神经网络（RNN）上堆叠 GNN，能够以平均 AUC 96.6% 预测未来土狼与其他土狼及普通用户的互动。ALETHEIA 预测土狼-土狼边（TTE）和土狼-用户边（TUE），有助于识别受恶意影响努力影响的普通用户。总体而言，我们的结果强调了在预测和检测在线空间中恶意协同活动时利用影响力操作的网络化特性（即结构信息）的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Influence campaigns are a growing concern in the online spaces. Policymakers, moderators and researchers have taken various routes to fight these campaigns and make online systems safer for regular users. To this end, our paper presents ALETHEIA, a system that formalizes the detection of malicious accounts (or troll accounts) used in such operations and forecasts their behaviors within social media networks. We analyze influence campaigns on Reddit and X from different countries and highlight that detection pipelines built over a graph-based representation of campaigns using a mix of topological and linguistic features offer improvement over standard interaction and user features. ALETHEIA uses state-of-the-art Graph Neural Networks (GNNs) for detecting malicious users that can scale to large networks and achieve a 3.7% F1-score improvement over standard classification with interaction features in prior work. Furthermore, ALETHEIA employs a first temporal link prediction mechanism built for influence campaigns by stacking a GNN over a Recurrent Neural Network (RNN), which can predict future troll interactions towards other trolls and regular users with an average AUC of 96.6%. ALETHEIA predicts troll-to-troll edges (TTE) and troll-to-user edges (TUE), which can help identify regular users being affected by malicious influence efforts. Overall, our results highlight the importance of utilizing the networked nature of influence operations (i.e., structural information) when predicting and detecting malicious coordinated activity in online spaces.&lt;/p&gt;</description></item><item><guid>2512.21402v1</guid><title>Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation</title><link>http://arxiv.org/abs/2512.21402v1</link><author>Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; • 评估短视频需超越表面质量指标，关注人类对多模态内容的认知。• 现有框架如VideoScore-2仅评估视觉与语义相似度，未能捕捉音视频属性对观众参与的影响。• 本文提出基于Vision‑Language模型的无监督特征提取与聚类方法，构建回归评估器预测短视频的观众参与度。• 通过精心整理的YouTube Shorts数据集，系统分析VLM特征与人类参与行为的关系。• 实验表明预测值与实际参与度高度相关，证明轻量化特征评估器在可解释性和可扩展性上优于传统指标。• 该方法为实现稳健、可解释的视频理解奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 短视频内容的评估传统上依赖于表面质量指标，但这些指标无法反映观众真实的参与感受。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种基于多模态特征、与人类参与度对齐的评估框架，以更准确、可解释地衡量短视频质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Vision‑Language模型提取无监督的音视频特征，对特征进行聚类得到可解释因子，随后训练回归模型预测观众参与度，并在YouTube Shorts数据集上进行验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 预测的参与度与实际观众行为高度相关，说明VLM提取的特征与人类参与度密切相关；轻量化特征评估器在可解释性和可扩展性上优于传统指标如SSIM、FID。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于多模态特征和人类参与信号的评估方法能够提供更稳健、可解释的短视频质量评估，为视频理解研究提供新思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 评估短视频内容需要超越表面质量指标，转向与人类对齐的多模态推理。现有框架如VideoScore-2评估视觉和语义保真度，但未能捕捉特定音视频属性如何驱动真实观众参与。在本研究中，我们提出一种基于数据驱动的评估框架，利用视觉语言模型提取无监督的音视频特征，将其聚类为可解释因子，并训练回归评估器预测短视频教育娱乐内容的参与度。我们精心整理的YouTube Shorts数据集使我们能够系统分析VLM提取的特征与人类参与行为的关系。实验表明预测值与实际参与度高度相关，证明我们的轻量化、基于特征的评估器在可解释性和可扩展性上优于传统指标（如SSIM、FID）。通过将评估基于多模态特征重要性和以人为中心的参与信号，我们的方法朝着稳健且可解释的视频理解迈进。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Evaluating short-form video content requires moving beyond surface-level quality metrics toward human-aligned, multimodal reasoning. While existing frameworks like VideoScore-2 assess visual and semantic fidelity, they do not capture how specific audiovisual attributes drive real audience engagement. In this work, we propose a data-driven evaluation framework that uses Vision-Language Models (VLMs) to extract unsupervised audiovisual features, clusters them into interpretable factors, and trains a regression-based evaluator to predict engagement on short-form edutainment videos. Our curated YouTube Shorts dataset enables systematic analysis of how VLM-derived features relate to human engagement behavior. Experiments show strong correlations between predicted and actual engagement, demonstrating that our lightweight, feature-based evaluator provides interpretable and scalable assessments compared to traditional metrics (e.g., SSIM, FID). By grounding evaluation in both multimodal feature importance and human-centered engagement signals, our approach advances toward robust and explainable video understanding.&lt;/p&gt;</description></item><item><guid>2512.21452v1</guid><title>Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism</title><link>http://arxiv.org/abs/2512.21452v1</link><author>Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一套完整的框架，利用DCGAN数据增强、MCGA-Net网络以及MS COCO迁移学习，显著提升地面穿透雷达图像中道路缺陷检测的准确性与鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地面穿透雷达（GPR）已成为评估地下道路缺陷的关键工具，但传统图像解释高度依赖主观经验，导致效率低下和误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统GPR图像解释的主观性和数据稀缺问题，构建自动化、准确的缺陷检测方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 使用DCGAN生成高质量GPR图像进行数据增强；2) 设计MCGA-Net，结合多模态链特征融合和全局注意机制，实现多尺度缺陷表征与上下文增强；3) 在MS COCO数据集上进行迁移学习微调，加速收敛并提升泛化能力；4) 通过消融实验和对比实验验证框架有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MCGA-Net在精度92.8%、召回率92.5%和mAP@50 95.9%的指标上表现优异，对高斯噪声、弱信号和小目标具有良好鲁棒性，优于其他模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该工作为基于GPR的缺陷检测提供了新的自动化范式，在复杂地下环境中兼顾计算效率与高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework&amp;#x27;s efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework&amp;#x27;s efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.&lt;/p&gt;</description></item><item><guid>2512.21516v1</guid><title>Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data</title><link>http://arxiv.org/abs/2512.21516v1</link><author>Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种统一的基于对比学习的多视图聚类框架，针对数据缺失和噪声导致的少配对和误配对问题，通过全局图引导的对比学习和局部图加权对比学习分别解决这两类问题，并在实验中表现出优于现有方法的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 对比学习在多视图聚类中发挥重要作用，但现实数据往往存在缺失或噪声，导致少配对或误配对样本，严重影响对比学习的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种统一的对比学习框架，以提升在不完整和噪声多视图数据上的聚类效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 全局图引导对比学习：利用所有视图样本构建全局相似度图，生成新的样本对以充分挖掘互补信息；2) 局部图加权对比学习：利用局部邻居生成样本对权重，动态调整对比学习强度；两者结合形成无插补的全局-局部图引导对比学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多视图数据的缺失和噪声设置下，实验表明该方法在聚类性能上优于最新的基准方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架能够有效克服少配对和误配对问题，提升对比学习在多视图聚类中的鲁棒性和效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近，对比学习在探索多视图聚类的互补信息方面发挥着重要作用，并受到越来越多的关注。然而，现实世界的多视图数据存在数据不完整或噪声，导致少配对样本或误配对样本，这极大地挑战了基于对比学习的多视图聚类的有效性。也就是说，少配对问题阻碍了多视图聚类提取足够的互补信息，而误配对问题导致对比学习在错误的方向上优化模型。为了解决这些问题，我们提出了一个统一的基于对比学习的多视图聚类框架，以增强在不完整和噪声多视图数据上的聚类效果。首先，为了克服少配对问题，我们设计了全局图引导的对比学习，其中所有视图样本构建全局视图相似度图，以形成新的样本对，充分探索互补信息。其次，为了缓解误配对问题，我们提出了局部图加权对比学习，利用局部邻居生成样本对权重，以自适应地加强或减弱样本对的对比学习。我们的方法不需要插补，并可以集成到统一的全局-局部图引导对比学习框架中。在不完整和噪声设置的多视图数据上进行的大量实验表明，我们的方法在与最先进的方法相比具有更优的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recently, contrastive learning (CL) plays an important role in exploring complementary information for multi-view clustering (MVC) and has attracted increasing attention. Nevertheless, real-world multi-view data suffer from data incompleteness or noise, resulting in rare-paired samples or mis-paired samples which significantly challenges the effectiveness of CL-based MVC. That is, rare-paired issue prevents MVC from extracting sufficient multi-view complementary information, and mis-paired issue causes contrastive learning to optimize the model in the wrong direction. To address these issues, we propose a unified CL-based MVC framework for enhancing clustering effectiveness on incomplete and noise multi-view data. First, to overcome the rare-paired issue, we design a global-graph guided contrastive learning, where all view samples construct a global-view affinity graph to form new sample pairs for fully exploring complementary information. Second, to mitigate the mis-paired issue, we propose a local-graph weighted contrastive learning, which leverages local neighbors to generate pair-wise weights to adaptively strength or weaken the pair-wise contrastive learning. Our method is imputation-free and can be integrated into a unified global-local graph-guided contrastive learning framework. Extensive experiments on both incomplete and noise settings of multi-view data demonstrate that our method achieves superior performance compared with state-of-the-art approaches.&lt;/p&gt;</description></item><item><guid>2512.21544v1</guid><title>AVP-Fusion: Adaptive Multi-Modal Fusion and Contrastive Learning for Two-Stage Antiviral Peptide Identification</title><link>http://arxiv.org/abs/2512.21544v1</link><author>Xinru Wen, Weizhong Lin, Xuan Xiao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为AVP-Fusion的两阶段深度学习框架，用于精准识别抗病毒肽（AVPs）。该框架通过自适应特征融合和对比学习，克服了传统方法在捕捉序列依赖和处理模糊样本方面的局限。实验表明，AVP-Fusion在基准数据集上取得了高达0.9531的准确率和0.9064的MCC，显著优于现有技术，并能在样本有限的情况下实现对六个病毒家族和八种特定病毒的子类预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 抗病毒肽的准确识别对于新药研发至关重要，但现有计算方法难以捕捉复杂的序列依赖关系，并且对难以分类的样本处理效果不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够有效融合局部与全局特征，并通过对比学习提升判别边界的深度学习模型，以提高AVP识别的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 构建包含10种描述符的全景特征空间；2) 引入自适应门控机制，动态调节CNN提取的局部基序与BiLSTM捕获的全局依赖的权重；3) 采用在线难例挖掘（OHEM）和基于BLOSUM62的增广进行对比学习；4) 在第二阶段利用迁移学习实现对病毒家族和特定病毒的子类预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; AVP-Fusion在Set 1基准数据集上实现了0.9531的准确率和0.9064的MCC，明显优于现有方法；在子类预测任务中，即使样本有限，也能实现精准预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AVP-Fusion是一种鲁棒且可解释的工具，可用于高通量抗病毒药物筛选，并在多病毒家族和特定病毒的子类识别方面表现出色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确识别抗病毒肽（AVPs）对于加速新药开发至关重要。然而，当前的计算方法难以捕捉复杂的序列依赖关系，并且难以有效处理模糊、难以分类的样本。为了解决这些挑战，我们提出了AVP-Fusion，一种新颖的两阶段深度学习框架，集成了自适应特征融合和对比学习。与传统的静态特征拼接不同，我们使用10种不同的描述符构建了全景特征空间，并引入了自适应门控机制。该机制根据序列上下文动态调节CNN提取的局部基序和BiLSTM捕获的全局依赖的权重。此外，为了解决数据分布挑战，我们采用了由在线难例挖掘（OHEM）和基于BLOSUM62的数据增强驱动的对比学习策略，这显著锐化了模型的决策边界。在基准Set 1数据集上的实验结果表明，AVP-Fusion实现了0.9531的准确率和0.9064的MCC，显著优于最先进的方法。在第二阶段，利用迁移学习，该模型能够在样本有限的情况下，对六个病毒家族和八种特定病毒进行精确的子类预测。总之，AVP-Fusion是一个强大且可解释的工具，可用于高通量抗病毒药物筛选。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate identification of antiviral peptides (AVPs) is critical for accelerating novel drug development. However, current computational methods struggle to capture intricate sequence dependencies and effectively handle ambiguous, hard-to-classify samples. To address these challenges, we propose AVP-Fusion, a novel two-stage deep learning framework integrating adaptive feature fusion and contrastive learning. Unlike traditional static feature concatenation, we construct a panoramic feature space using 10 distinct descriptors and introduce an Adaptive Gating Mechanism.This mechanism dynamically regulates the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Furthermore, to address data distribution challenges, we employ a contrastive learning strategy driven by Online Hard Example Mining (OHEM) and BLOSUM62-based data augmentation, which significantly sharpens the model&amp;#x27;s decision boundaries. Experimental results on the benchmark Set 1 dataset demonstrate that AVP-Fusion achieves an accuracy of 0.9531 and an MCC of 0.9064, significantly outperforming state-of-the-art methods. In the second stage, leveraging transfer learning, the model enables precise subclass prediction for six viral families and eight specific viruses, even under limited sample sizes. In summary, AVP-Fusion serves as a robust and interpretable tool for high-throughput antiviral drug screening.&lt;/p&gt;</description></item><item><guid>2512.21563v1</guid><title>Discovering Sparse Recovery Algorithms Using Neural Architecture Search</title><link>http://arxiv.org/abs/2512.21563v1</link><author>Patrick Yubeaton, Sarthak Gupta, M. Salman Asif, Chinmay Hegde</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨利用元学习工具自动发现信号处理逆问题算法的可能性，并以迭代收缩阈值算法及其加速版本为例，展示了在大规模搜索空间中重新发现关键算法要素的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 设计新算法来解决信号处理中的逆问题既困难又耗时，往往依赖经验和启发式方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证通过元学习方法，特别是神经架构搜索，能够自动重现已知算法并推广到其他算法和数据分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建一个元学习框架，在超过五万维的搜索空间中搜索，针对迭代收缩阈值算法及其加速变体进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架能够在给定的搜索空间内重新发现两种算法的若干关键要素，并且可以应用于不同的数据分布和其他算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自动化算法发现技术在信号处理领域具有潜力，可减少人工设计的负担并扩展到更广泛的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文的摘要：设计用于解决信号处理逆问题的新算法是一项极其困难、基于启发式且耗时的任务。在这篇简短的论文中，我们提出了在信号处理背景下通过元学习工具（如神经架构搜索）实现自动算法发现的想法。具体而言，我们将迭代收缩阈值算法（ISTA）及其加速版本快速ISTA（FISTA）作为算法重发现的候选。我们开发了一个元学习框架，能够在超过五万维的搜索空间中重新发现上述两种算法的若干关键要素。随后，我们展示了该框架如何应用于除ISTA/FISTA之外的各种数据分布和算法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The design of novel algorithms for solving inverse problems in signal processing is an incredibly difficult, heuristic-driven, and time-consuming task. In this short paper, we the idea of automated algorithm discovery in the signal processing context through meta-learning tools such as Neural Architecture Search (NAS). Specifically, we examine the Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated Fast ISTA (FISTA) variant as candidates for algorithm rediscovery. We develop a meta-learning framework which is capable of rediscovering (several key elements of) the two aforementioned algorithms when given a search space of over 50,000 variables. We then show how our framework can apply to various data distributions and algorithms besides ISTA/FISTA.&lt;/p&gt;</description></item><item><guid>2512.21573v1</guid><title>World-Coordinate Human Motion Retargeting via SAM 3D Body</title><link>http://arxiv.org/abs/2512.21573v1</link><author>Zhangzheng Tu, Kailun Su, Shaolong Zhu, Yukun Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种轻量级框架，用于从单目视频恢复世界坐标的人体运动并将其重定向到类人机器人。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在具身智能和机器人领域，获取可直接用于机器人的世界坐标运动数据至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 避免使用复杂的 SLAM 流程或重量级时序模型，提供一种工程化、轻量化的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用冻结的 SAM 3D Body 作为感知骨干，采用 Momentum HumanRig 作为机器人友好的中间表示；锁定身份和骨架尺度以保持骨骼长度一致；在低维 MHR 空间内通过滑动窗口优化平滑帧预测；利用可微软足接触模型和接触感知全局优化恢复物理可行的根轨迹；最后通过两阶段逆运动学将运动重定向到 Unitree G1。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实单目视频上实验表明，该方法能够生成稳定的世界轨迹，并实现可靠的机器人重定向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结构化的人体表示与轻量化的物理约束相结合，可从单目输入生成机器人可用的运动。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从单目视频恢复世界坐标的人体运动并进行类人机器人重定向对于具身智能和机器人技术具有重要意义。为避免复杂的 SLAM 管道或重量级时序模型，我们提出了一个轻量级、面向工程的框架，利用冻结的 SAM 3D Body 作为感知骨干，并使用 Momentum HumanRig 作为机器人友好的中间表示。我们的方法（i）锁定每个跟踪主体的身份和骨架尺度参数，以强制骨骼长度在时间上保持一致；（ii）通过在低维 MHR 潜在空间中进行高效的滑动窗口优化平滑每帧预测；（iii）使用可微软足接触模型和接触感知全局优化恢复物理可行的全局根轨迹。最后，我们使用面向运动学的两阶段逆运动学管道将重建的运动重定向到 Unitree G1 类人机器人。对真实单目视频的结果表明，我们的方法具有稳定的世界轨迹和可靠的机器人重定向，表明结构化的人体表示与轻量化的物理约束可以从单目输入生成机器人可用的运动。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics. To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate. Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization. Finally, we retarget the reconstructed motion to the Unitree G1 humanoid using a kinematics-aware two-stage inverse kinematics pipeline. Results on real monocular videos show that our method has stable world trajectories and reliable robot retargeting, indicating that structured human representations with lightweight physical constraints can yield robot-ready motion from monocular input.&lt;/p&gt;</description></item><item><guid>2512.21633v1</guid><title>MAD-NG: Meta-Auto-Decoder Neural Galerkin Method for Solving Parametric Partial Differential Equations</title><link>http://arxiv.org/abs/2512.21633v1</link><author>Qiuqi Li, Yiting Liu, Jin Zhao, Wencan Zhu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种结合Meta-Auto-Decoder的可扩展框架，改进Neural Galerkin Method，以更好地解决参数化偏微分方程的泛化和长时预测问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 参数化偏微分方程在物理和工程系统建模中非常重要，但传统基于神经网络的求解器在泛化和长时预测方面存在挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 改进Neural Galerkin Method，使其在面对未知参数配置时能够快速泛化，并提高长时预测效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过空间-时间解耦实现更稳定高效的时间积分；使用Meta-Auto-Decoder进行元学习驱动的适配；采用随机稀疏更新降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 数值实验表明该方法在准确性、鲁棒性和适应性方面与基准方法相当，且计算开销显著降低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架实现了物理一致、长时预测的参数化演化方程求解，并在计算效率上具有优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper proposes a scalable framework that enhances the Neural Galerkin Method by incorporating the Meta-Auto-Decoder paradigm, addressing generalization and long-time prediction challenges in parametric partial differential equations. The approach uses space-time decoupling for stable and efficient time integration, meta-learning for rapid adaptation to unseen parameters, and randomized sparse updates to reduce computational cost while maintaining accuracy. Numerical experiments demonstrate comparable performance in accuracy, robustness, and adaptability, with significantly lower computational overhead.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Parametric partial differential equations (PDEs) are fundamental for modeling a wide range of physical and engineering systems influenced by uncertain or varying parameters. Traditional neural network-based solvers, such as Physics-Informed Neural Networks (PINNs) and Deep Galerkin Methods, often face challenges in generalization and long-time prediction efficiency due to their dependence on full space-time approximations. To address these issues, we propose a novel and scalable framework that significantly enhances the Neural Galerkin Method (NGM) by incorporating the Meta-Auto-Decoder (MAD) paradigm. Our approach leverages space-time decoupling to enable more stable and efficient time integration, while meta-learning-driven adaptation allows rapid generalization to unseen parameter configurations with minimal retraining. Furthermore, randomized sparse updates effectively reduce computational costs without compromising accuracy. Together, these advancements enable our method to achieve physically consistent, long-horizon predictions for complex parameterized evolution equations with significantly lower computational overhead. Numerical experiments on benchmark problems demonstrate that our methods performs comparatively well in terms of accuracy, robustness, and adaptability.&lt;/p&gt;</description></item><item><guid>2512.21641v1</guid><title>TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</title><link>http://arxiv.org/abs/2512.21641v1</link><author>Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在动态三维驾驶场景中通过多帧观测实现基于语言的目标定位，并提出了TrackTeller框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在交互式自动驾驶系统中，理解自然语言对对象的引用至关重要，尤其是许多表达依赖于最近运动或短期交互，单靠静态外观或几何无法解决。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨利用多帧观测进行时间语言驱动的三维定位，目标是在当前帧中识别被引用的对象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出TrackTeller，一个融合激光雷达与图像、语言条件解码和时间推理的统一架构，构建与文本语义对齐的共享场景表示，生成语言感知的三维候选框，并通过运动历史和短期动态细化定位决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在NuPrompt基准上实验表明，TrackTeller 在平均多目标跟踪精度上相对提升70%，误报频率降低3.15-3.4倍，显著优于强基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; TrackTeller 通过整合多模态信息和时间推理，显著提升了基于语言的三维跟踪性能，验证了时间多模态定位方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解动态三维驾驶场景中自然语言对对象的引用对于交互式自动驾驶系统至关重要。实际上，许多指代表达通过最近的运动或短期交互来描述目标，这些信息无法仅凭静态外观或几何来解决。我们研究基于时间语言的三维定位，目标是利用多帧观测在当前帧中识别被引用的对象。我们提出了TrackTeller，一个整合激光雷达-图像融合、语言条件解码和时间推理的统一多模态定位框架。TrackTeller 构建了与文本语义对齐的共享 UniScene 表示，生成语言感知的三维候选框，并利用运动历史和短期动态细化定位决策。对 NuPrompt 基准的实验表明，TrackTeller 在平均多目标跟踪精度上相对提升 70%，误报频率降低 3.15-3.4 倍，持续优于强基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在动态驾驶场景中，通过自然语言表达的对象引用需要考虑最近运动或短期交互的情况，从单帧静态信息无法满足的挑战。该问题对自动驾驶和交互式机器人至关重要，因为它们需要实时理解并定位被语言描述的目标，以实现安全、高效的决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出多模态融合和时间推理是关键挑战，并参考了 LanguageRefer、NS3D、GroundFlow 等先前研究。基于这些工作，他们设计了统一的 UniScene 表示、语言对齐解码器以及记忆式时间推理模块，以实现跨帧、跨模态的语义对齐与动态行为理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将 LiDAR 点云与多视角图像融合成统一的 UniScene 令牌，然后通过语言引导的注意力将文本语义注入场景表示，生成候选 3D 框；随后利用历史记忆和运动信息对候选框进行时间推理，最终选取与语言描述最匹配的对象。整体流程包括：多模态融合 → 语言对齐 → 3D 解码 → 时间记忆更新 → 运动推理 → 最终定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：1）UniScene 统一表示融合 LiDAR 与图像语义；2）语言对齐解码器实现文本条件的 3D 框生成；3）记忆式时间推理模块结合历史上下文和短期运动；4）在 NuPrompt 基准上实现显著性能提升。与以往仅关注单帧或静态场景的工作不同，TrackTeller 能够处理基于运动和行为的语言引用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrackTeller 提出了一个统一的多模态与时间推理框架，能够在动态驾驶场景中准确定位基于运动和行为的自然语言对象引用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.&lt;/p&gt;</description></item><item><guid>2512.21689v1</guid><title>Cross-Semantic Transfer Learning for High-Dimensional Linear Regression</title><link>http://arxiv.org/abs/2512.21689v1</link><author>Jiancheng Jiang, Xuejun Jiang, Hongxia Jin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种跨语义迁移学习框架，旨在解决高维线性回归中源域与目标域特征不对齐的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统方法假设源域和目标域特征完全对应，限制了在特征不匹配的实际场景中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用源域与目标域中功能相似但语义不同的特征，提升迁移学习的可迁移性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过对目标系数与所有源系数进行加权融合惩罚，权重来自SCAD惩罚的导数，使用ADMM实现高效计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 理论证明在轻微条件下该方法可获得近似最优估计，实验表明在跨语义和部分信号相似场景中优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 跨语义迁移学习框架在高维回归任务中具有更好的泛化性能和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Current transfer learning methods for high-dimensional linear regression assume feature alignment across domains, restricting their applicability to semantically matched features. In many real-world scenarios, however, distinct features in the target and source domains can play similar predictive roles, creating a form of cross-semantic similarity. To leverage this broader transferability, we propose the Cross-Semantic Transfer Learning (CSTL) framework. It captures potential relationships by comparing each target coefficient with all source coefficients through a weighted fusion penalty. The weights are derived from the derivative of the SCAD penalty, effectively approximating an ideal weighting scheme that preserves transferable signals while filtering out source-specific noise. For computational efficiency, we implement CSTL using the Alternating Direction Method of Multipliers (ADMM). Theoretically, we establish that under mild conditions, CSTL achieves the oracle estimator with overwhelming probability. Empirical results from simulations and a real-data application confirm that CSTL outperforms existing methods in both cross-semantic and partial signal similarity settings.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Current transfer learning methods for high-dimensional linear regression assume feature alignment across domains, restricting their applicability to semantically matched features. In many real-world scenarios, however, distinct features in the target and source domains can play similar predictive roles, creating a form of cross-semantic similarity. To leverage this broader transferability, we propose the Cross-Semantic Transfer Learning (CSTL) framework. It captures potential relationships by comparing each target coefficient with all source coefficients through a weighted fusion penalty. The weights are derived from the derivative of the SCAD penalty, effectively approximating an ideal weighting scheme that preserves transferable signals while filtering out source-specific noise. For computational efficiency, we implement CSTL using the Alternating Direction Method of Multipliers (ADMM). Theoretically, we establish that under mild conditions, CSTL achieves the oracle estimator with overwhelming probability. Empirical results from simulations and a real-data application confirm that CSTL outperforms existing methods in both cross-semantic and partial signal similarity settings.&lt;/p&gt;</description></item><item><guid>2512.21702v1</guid><title>Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning</title><link>http://arxiv.org/abs/2512.21702v1</link><author>Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了孟加拉语音频深度伪造检测，使用BanglaFake数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度伪造音频成为安全问题，孟加拉语深度伪造检测尚未深入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估预训练模型在孟加拉语深度伪造检测中的零样本推理能力，并通过微调提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先使用多种预训练模型进行零样本推理，包括Wav2Vec2-XLSR-53、Whisper、PANNsCNN14、WavLM和Audio Spectrogram Transformer；随后对多种架构进行微调，包括Wav2Vec2-Base、LCNN、LCNN-Attention、ResNet18、ViT-B16和CNN-BiLSTM。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 零样本推理效果有限，最佳模型Wav2Vec2-XLSR-53准确率约为53.8%。微调后性能显著提升，ResNet18达到约79.2%的准确率、F1分数和84.4%的AUC，EER下降到24.4%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 微调深度学习模型显著优于零样本推理，为低资源语言的深度伪造检测提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The rapid growth of speech synthesis and voice conversion systems has made deepfake audio a major security concern. Bengali deepfake detection remains largely unexplored. In this work, we study automatic detection of Bengali audio deepfakes using the BanglaFake dataset. We evaluate zeroshot inference with several pretrained models. These include Wav2Vec2-XLSR-53, Whisper, PANNsCNN14, WavLM and Audio Spectrogram Transformer. Zero-shot results show limited detection ability. The best model, Wav2Vec2-XLSR-53, achieves 53.80% accuracy, 56.60% AUC and 46.20% EER. We then f ine-tune multiple architectures for Bengali deepfake detection. These include Wav2Vec2-Base, LCNN, LCNN-Attention, ResNet18, ViT-B16 and CNN-BiLSTM. Fine-tuned models show strong performance gains. ResNet18 achieves the highest accuracy of 79.17%, F1 score of 79.12%, AUC of 84.37% and EER of 24.35%. Experimental results confirm that fine-tuning significantly improves performance over zero-shot inference. This study provides the first systematic benchmark of Bengali deepfake audio detection. It highlights the effectiveness of f ine-tuned deep learning models for this low-resource language.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid growth of speech synthesis and voice conversion systems has made deepfake audio a major security concern. Bengali deepfake detection remains largely unexplored. In this work, we study automatic detection of Bengali audio deepfakes using the BanglaFake dataset. We evaluate zeroshot inference with several pretrained models. These include Wav2Vec2-XLSR-53, Whisper, PANNsCNN14, WavLM and Audio Spectrogram Transformer. Zero-shot results show limited detection ability. The best model, Wav2Vec2-XLSR-53, achieves 53.80% accuracy, 56.60% AUC and 46.20% EER. We then f ine-tune multiple architectures for Bengali deepfake detection. These include Wav2Vec2-Base, LCNN, LCNN-Attention, ResNet18, ViT-B16 and CNN-BiLSTM. Fine-tuned models show strong performance gains. ResNet18 achieves the highest accuracy of 79.17%, F1 score of 79.12%, AUC of 84.37% and EER of 24.35%. Experimental results confirm that fine-tuning significantly improves performance over zero-shot inference. This study provides the first systematic benchmark of Bengali deepfake audio detection. It highlights the effectiveness of f ine-tuned deep learning models for this low-resource language.&lt;/p&gt;</description></item><item><guid>2512.21707v1</guid><title>Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction</title><link>http://arxiv.org/abs/2512.21707v1</link><author>Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 提出一种新的时空无束混合专家模型 ST-MoE，能够灵活捕捉人类运动的复杂时空依赖关系，并显著降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多人人体运动预测需要全面捕捉复杂时空依赖，但现有方法受限于位置编码导致表示不灵活，以及传统注意力机制的二次时间复杂度导致计算成本高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服上述两大限制，实现灵活的时空表示并降低计算开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ST-MoE 采用四种专门捕捉不同空间或时间依赖的专家，并使用双向时空 Mamba 共享不同组合的时间和空间模块，以提高效率和参数经济。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在四个多人人体运动基准数据集上，ST-MoE 在准确率上优于现有最先进方法，参数量减少 41.38%，训练速度提升 3.6 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ST-MoE 能有效捕捉复杂时空依赖并显著提升模型效率，为多人人体运动预测提供了更优方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 全面且灵活地捕捉人类运动的复杂时空依赖关系对于多人人体运动预测至关重要。现有方法面临两个主要限制：一是由于依赖位置编码来捕捉时空信息，导致时空表示不够灵活；二是传统注意力机制的二次时间复杂度导致计算成本高。为克服这些限制，我们提出了时空无束混合专家（ST-MoE），它能够灵活探索人类运动中的复杂时空依赖关系，并显著降低计算成本。为自适应挖掘人类运动中的复杂时空模式，我们的模型包含四种不同类型的时空专家，每种专家专门捕捉不同的空间或时间依赖关系。为在整合多个专家时降低潜在的计算开销，我们引入了双向时空 Mamba 作为专家，每个专家共享不同组合的双向时间和空间 Mamba，以实现模型效率和参数经济。对四个多人人体运动基准数据集进行的大量实验表明，我们的方法不仅在准确率上优于现有最先进方法，还将模型参数减少了 41.38%，并在训练速度上实现了 3.6 倍的加速。代码可在 https://github.com/alanyz106/ST-MoE 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Comprehensively and flexibly capturing the complex spatio-temporal dependencies of human motion is critical for multi-person motion prediction. Existing methods grapple with two primary limitations: i) Inflexible spatiotemporal representation due to reliance on positional encodings for capturing spatiotemporal information. ii) High computational costs stemming from the quadratic time complexity of conventional attention mechanisms. To overcome these limitations, we propose the Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE), which flexibly explores complex spatio-temporal dependencies in human motion and significantly reduces computational cost. To adaptively mine complex spatio-temporal patterns from human motion, our model incorporates four distinct types of spatiotemporal experts, each specializing in capturing different spatial or temporal dependencies. To reduce the potential computational overhead while integrating multiple experts, we introduce bidirectional spatiotemporal Mamba as experts, each sharing bidirectional temporal and spatial Mamba in distinct combinations to achieve model efficiency and parameter economy. Extensive experiments on four multi-person benchmark datasets demonstrate that our approach not only outperforms state-of-art in accuracy but also reduces model parameter by 41.38% and achieves a 3.6x speedup in training. The code is available at https://github.com/alanyz106/ST-MoE.&lt;/p&gt;</description></item><item><guid>2512.21714v1</guid><title>AstraNav-World: World Model for Foresight Control and Consistency</title><link>http://arxiv.org/abs/2512.21714v1</link><author>Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; AstraNav-World 是一种端到端的世界模型，能够在统一的概率框架内同时预测未来视觉状态和行动序列，并通过同步的滚动方式将预测的场景与计划的动作实时更新，从而提升具身导航的轨迹精度和成功率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在开放且动态的环境中，具身导航需要准确预测世界的演变以及行动随时间展开的方式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够联合推理未来视觉状态和行动序列的模型，以减少传统“先想象再规划”流程中的累计误差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 整合扩散式视频生成器与视觉-语言策略，采用同步滚动；训练时同时优化两项互补目标：生成动作条件的多步视觉预测和基于预测视觉的轨迹规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明该方法在多种具身导航基准上提升了轨迹准确性和成功率；消融实验显示视觉-动作紧耦合和统一训练是必要的；在真实世界测试中表现出卓越的零样本适应能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过在单一生成模型中统一视觉预见与控制，AstraNav-World 使具身代理在开放式真实环境中更可靠、可解释且通用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在开放且动态的环境中，具身导航需要准确预测世界的演变以及行动随时间展开的方式。我们提出 AstraNav-World，一个端到端的世界模型，在统一的概率框架内同时推理未来视觉状态和行动序列。我们的框架将扩散式视频生成器与视觉-语言策略相结合，支持同步滚动，使预测的场景和计划的动作能够同时更新。训练时优化两项互补目标：生成动作条件的多步视觉预测以及基于这些预测视觉的轨迹规划。这种双向约束使视觉预测可执行，并使决策保持在物理一致、任务相关的未来之中，从而减轻了传统“先想象再规划”流程中常见的累计误差。实验在多种具身导航基准上显示出更高的轨迹准确性和成功率。消融实验确认了紧密的视觉-动作耦合和统一训练的必要性，移除任一分支都会降低预测质量和策略可靠性。在真实世界测试中，AstraNav-World 展示了卓越的零样本能力，能够在未见过的场景中适应，而无需任何真实世界的微调。这些结果表明 AstraNav-World 捕捉了可迁移的空间理解和与规划相关的导航动力学，而非仅仅对仿真特定的数据分布过拟合。总体而言，通过在单一生成模型中统一预见视觉和控制，我们更接近于在开放式真实环境中可靠、可解释且通用的具身代理。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在开放、动态环境中实现可靠的机器人导航所需的未来世界预测与动作规划的耦合问题。传统方法缺乏对物理规律和时间动态的建模，导致预测误差累积，影响全局规划的有效性。准确的未来感知与可执行规划对于让机器人在真实世界中自主行动至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为“想象未来”和“规划未来”应在同一模型中同步进行，于是将视觉语言模型（VLM）作为高层规划器，结合扩散式视频生成器和动作策略头，形成统一的概率框架。设计过程中借鉴了世界模型、VLM‑VLA、CoT‑VLA 等现有工作，并在此基础上引入双向约束和同步滚动，以减少误差传播。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将未来视觉状态预测与动作序列生成紧密耦合，在同一概率模型中通过双向约束实现一致性。实现流程为：VLM 处理指令和历史视觉，输出嵌入；嵌入通过交叉注意力注入扩散视频生成器和策略头；视频生成器预测多步未来帧；策略头根据相同嵌入和预测帧生成动作；两者在训练时同步滚动并共同优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）统一生成框架，将视觉预测与动作规划放在同一模型中；2）双向约束与同步滚动，强化物理与因果一致性；3）使用扩散模型同时生成未来视频和动作序列；4）实现零样本迁移到真实机器人。与以往“想象‑然后‑规划”分离的管线不同，本文通过联合训练和紧耦合显著降低误差累积，提高导航成功率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AstraNav‑World 将视觉语言推理、扩散式视频预测和动作生成统一到一个概率模型中，通过双向约束和同步滚动实现物理一致的未来感知与可执行规划，显著提升导航性能并实现零样本真实世界迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Embodied navigation in open, dynamic environments demands accurate foresight of how the world will evolve and how actions will unfold over time. We propose AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. Our framework integrates a diffusion-based video generator with a vision-language policy, enabling synchronized rollouts where predicted scenes and planned actions are updated simultaneously. Training optimizes two complementary objectives: generating action-conditioned multi-step visual predictions and deriving trajectories conditioned on those predicted visuals. This bidirectional constraint makes visual predictions executable and keeps decisions grounded in physically consistent, task-relevant futures, mitigating cumulative errors common in decoupled &amp;quot;envision-then-plan&amp;quot; pipelines. Experiments across diverse embodied navigation benchmarks show improved trajectory accuracy and higher success rates. Ablations confirm the necessity of tight vision-action coupling and unified training, with either branch removal degrading both prediction quality and policy reliability. In real-world testing, AstraNav-World demonstrated exceptional zero-shot capabilities, adapting to previously unseen scenarios without any real-world fine-tuning. These results suggest that AstraNav-World captures transferable spatial understanding and planning-relevant navigation dynamics, rather than merely overfitting to simulation-specific data distribution. Overall, by unifying foresight vision and control within a single generative model, we move closer to reliable, interpretable, and general-purpose embodied agents that operate robustly in open-ended real-world settings.&lt;/p&gt;</description></item><item><guid>2512.21734v2</guid><title>Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation</title><link>http://arxiv.org/abs/2512.21734v2</link><author>Steven Xiao, Xindi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 提出 Knot Forcing 框架；解决扩散模型非因果性问题；采用分块生成与滑动窗口注意力；引入时间节点模块平滑块间运动；使用提前运行机制保持长期一致性；实现高保真、时间一致、无限序列实时人像动画；在消费级 GPU 上实现稳定实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 实时人像动画在虚拟助手、实时头像等交互应用中至关重要，需要高视觉质量、时间一致性、极低延迟以及对动态输入的响应控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够在流式部署中实现高质量、时间一致且低延迟的人像动画框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Knot Forcing 框架包含三大设计：1) 分块生成策略，利用缓存的 KV 状态保持全局身份，滑动窗口注意力实现局部时间建模；2) 时间节点模块，重叠相邻块并通过图像到视频的条件传递空间时间线索，平滑块间运动；3) 提前运行机制，在推理过程中动态更新参考帧的时间坐标，使其语义上下文领先当前帧，支持长期一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该框架在无限序列上实现了高保真、时间一致、交互式人像动画，并在消费级 GPU 上实现了实时性能与强视觉稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Knot Forcing 成功克服了扩散模型非因果性和自回归模型误差累积等挑战，为实时人像动画提供了可扩展、稳定且高质量的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 实时人像动画对于虚拟助手和实时头像等交互应用至关重要，需要高视觉保真度、时间一致性、超低延迟以及对动态输入（如参考图像和驱动信号）的响应控制。虽然基于扩散的模型能够实现强大的质量，但其非因果性质阻碍了流式部署。因果自回归视频生成方法能够实现高效的逐帧生成，但会出现误差累积、块边界处运动不连续以及长期一致性下降等问题。在本研究中，我们提出了一种名为 Knot Forcing 的新型流式框架，用于实时人像动画，通过三项关键设计解决了这些挑战：(1) 采用分块生成策略，利用参考图像的缓存 KV 状态实现全局身份保持，并通过滑动窗口注意力实现局部时间建模；(2) 引入时间节点模块，重叠相邻块并通过图像到视频的条件传递空间时间线索，以平滑块间运动过渡；(3) 采用“提前运行”机制，在推理过程中动态更新参考帧的时间坐标，使其语义上下文领先当前展开帧，以支持长期一致性。Knot Forcing 能够在无限序列上实现高保真、时间一致且交互式的人像动画，并在消费级 GPU 上实现实时性能与强视觉稳定性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Real-time portrait animation is essential for interactive applications such as virtual assistants and live avatars, requiring high visual fidelity, temporal coherence, ultra-low latency, and responsive control from dynamic inputs like reference images and driving signals. While diffusion-based models achieve strong quality, their non-causal nature hinders streaming deployment. Causal autoregressive video generation approaches enable efficient frame-by-frame generation but suffer from error accumulation, motion discontinuities at chunk boundaries, and degraded long-term consistency. In this work, we present a novel streaming framework named Knot Forcing for real-time portrait animation that addresses these challenges through three key designs: (1) a chunk-wise generation strategy with global identity preservation via cached KV states of the reference image and local temporal modeling using sliding window attention; (2) a temporal knot module that overlaps adjacent chunks and propagates spatio-temporal cues via image-to-video conditioning to smooth inter-chunk motion transitions; and (3) A &amp;quot;running ahead&amp;quot; mechanism that dynamically updates the reference frame&amp;#x27;s temporal coordinate during inference, keeping its semantic context ahead of the current rollout frame to support long-term coherence. Knot Forcing enables high-fidelity, temporally consistent, and interactive portrait animation over infinite sequences, achieving real-time performance with strong visual stability on consumer-grade GPUs.&lt;/p&gt;</description></item><item><guid>2512.21769v1</guid><title>BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization</title><link>http://arxiv.org/abs/2512.21769v1</link><author>Evgeny Alves Limarenko, Anastasiia Studenikina</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; BertsWin是一种结合BERT式全掩码和Swin Transformer窗口的混合架构，旨在提升3D医学影像自监督预训练中的空间上下文学习。该方法通过完整的3D令牌网格保持空间拓扑，并采用单层局部窗口降低ViT的二次复杂度，同时引入结构优先损失函数。实验在颞颌关节CBCT和3D CT分割任务中验证，BertsWin相较于传统ViT-MAE实现了5.8倍的语义收敛速度，并与GradientConductor优化器配合将训练周期从660缩短至44，达到15倍的加速。该架构在保持理论FLOP与稀疏ViT基线相当的同时，因更快收敛显著降低了总体计算资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自监督学习和Vision Transformer在二维医学影像中表现出色，但在三维体积图像上面临捕捉三维空间关系的困难，尤其是标准MAE在预训练时会丢弃75%的令牌。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出BertsWin架构，以更好地学习三维空间上下文，克服传统MAE在3D预训练中的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BertsWin采用完整的3D令牌网格（包含掩码和可见令牌），使用单层局部Swin窗口降低复杂度，并引入结构优先损失函数。实验在颞颌关节CBCT和3D CT分割任务中进行，并结合GradientConductor优化器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; BertsWin在语义收敛速度上比标准ViT-MAE快5.8倍；与GradientConductor配合可将训练周期从660缩短至44，实现15倍的加速；在理论FLOP上与稀疏ViT基线持平，因更快收敛而整体计算资源显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过完整保持三维空间拓扑，BertsWin在自监督预训练中实现了显著的加速和高质量重建，且不增加额外的计算负担。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在二维医学影像领域，自监督学习（SSL）和Vision Transformer（ViT）方法已显示出有前景的结果，但将这些方法应用于三维体积图像却面临诸多困难。标准的Masked Autoencoders（MAE）是二维领域的先进方案，但在捕捉三维空间关系方面表现不佳，尤其是在预训练期间丢弃75%令牌时。我们提出了BertsWin，一种混合架构，结合了使用Swin Transformer窗口的完整BERT式令牌掩码，以在SSL预训练期间增强三维空间上下文学习。与经典MAE仅处理可见区域不同，BertsWin引入了完整的三维令牌网格（包括掩码和可见令牌），保留了空间拓扑结构。为平滑ViT的二次复杂度，采用单层局部Swin窗口。我们引入了结构优先损失函数，并在颞颌关节的锥束CT上评估结果。随后对3D CT扫描进行TMJ分割评估。我们证明，BertsWin架构通过保持完整的三维空间拓扑，天然地将语义收敛速度提升了5.8倍，相比标准ViT-MAE基线。进一步地，结合我们提出的GradientConductor优化器，完整的BertsWin框架在达到最先进重建保真度所需的训练周期上实现了15倍的缩减（44 vs 660）。分析表明，BertsWin在不产生典型稠密体积处理所带来的计算代价的情况下实现了这一加速。在标准输入分辨率下，该架构在理论FLOP上与稀疏ViT基线保持平行，因更快的收敛而在总计算资源上实现了显著净减少。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The application of self-supervised learning (SSL) and Vision Transformers (ViTs) approaches demonstrates promising results in the field of 2D medical imaging, but the use of these methods on 3D volumetric images is fraught with difficulties. Standard Masked Autoencoders (MAE), which are state-of-the-art solution for 2D, have a hard time capturing three-dimensional spatial relationships, especially when 75% of tokens are discarded during pre-training. We propose BertsWin, a hybrid architecture combining full BERT-style token masking using Swin Transformer windows, to enhance spatial context learning in 3D during SSL pre-training. Unlike the classic MAE, which processes only visible areas, BertsWin introduces a complete 3D grid of tokens (masked and visible), preserving the spatial topology. And to smooth out the quadratic complexity of ViT, single-level local Swin windows are used. We introduce a structural priority loss function and evaluate the results of cone beam computed tomography of the temporomandibular joints. The subsequent assessment includes TMJ segmentation on 3D CT scans. We demonstrate that the BertsWin architecture, by maintaining a complete three-dimensional spatial topology, inherently accelerates semantic convergence by a factor of 5.8x compared to standard ViT-MAE baselines. Furthermore, when coupled with our proposed GradientConductor optimizer, the full BertsWin framework achieves a 15-fold reduction in training epochs (44 vs 660) required to reach state-of-the-art reconstruction fidelity. Analysis reveals that BertsWin achieves this acceleration without the computational penalty typically associated with dense volumetric processing. At canonical input resolutions, the architecture maintains theoretical FLOP parity with sparse ViT baselines, resulting in a significant net reduction in total computational resources due to faster convergence.&lt;/p&gt;</description></item><item><guid>2512.21778v1</guid><title>Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models</title><link>http://arxiv.org/abs/2512.21778v1</link><author>Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出Scene-VLM框架，实现多模态视频场景分割，并取得SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统编码器方法存在视觉偏差、孤立分类、缺乏叙事理解与可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够利用视觉与文本信息、顺序依赖、可解释性的场景分割模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用微调的视觉语言模型，联合处理帧、转录、元数据；采用因果顺序预测和上下文聚焦窗口；从token logits提取置信度；通过少量监督生成自然语言理由。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Scene-VLM在标准基准上实现SOTA，在MovieNet上提升AP +6、F1 +13.7。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多模态、顺序依赖的VLM框架显著提升视频场景分割性能，并提供可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.&lt;/p&gt;</description></item><item><guid>2512.21822v1</guid><title>Toward Generalizable Surrogate Models for Molecular Dynamics via Graph Neural Networks</title><link>http://arxiv.org/abs/2512.21822v1</link><author>Judah Immanuel, Avik Mahata, Aniruddha Maiti</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于图神经网络的代理框架，用于分子动力学模拟，直接预测原子位移并学习原子系统的演化算子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统分子动力学需要反复计算力并进行数值时间积分，而该方法不需要显式力计算。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种无需力评估即可在时间上推进原子构型的高效代理模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将原子环境表示为图，结合消息传递层和注意力机制，捕捉金属体系中的局部配位和多体相互作用；在体相铝的经典分子动力学轨迹上训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在训练范围内实现亚埃级别精度，并在短到中期时间外推中保持稳定；结构和动力学特征与参考径向分布函数和均方位移趋势一致，表明模型保留了关键物理特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于图神经网络的代理积分器在验证范围内是传统分子动力学的计算高效补充，可加速原子尺度模拟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种基于图神经网络的代理框架，用于分子动力学模拟，能够直接预测原子位移并学习原子系统的演化算子。与传统分子动力学依赖反复计算力和数值时间积分不同，所提出的代理模型在不显式计算力的情况下将原子构型向前推进。该方法将原子环境表示为图，并结合消息传递层和注意力机制，以捕捉金属体系中的局部配位和多体相互作用。在体相铝的经典分子动力学轨迹上训练后，该代理在训练范围内实现亚埃级别精度，并在短到中期时间外推中表现出稳定行为。通过与参考径向分布函数和均方位移趋势的一致性验证了其结构和动力学保真度，表明模型在保持关键物理特征方面优于仅关注逐点坐标精度。结果表明，基于图神经网络的代理积分器在验证范围内是传统分子动力学的有前景且计算高效的补充，可加速原子尺度模拟。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a graph neural network (GNN) based surrogate framework for molecular dynamics simulations that directly predicts atomic displacements and learns the underlying evolution operator of an atomistic system. Unlike conventional molecular dynamics, which relies on repeated force evaluations and numerical time integration, the proposed surrogate model propagates atomic configurations forward in time without explicit force computation. The approach represents atomic environments as graphs and combines message-passing layers with attention mechanisms to capture local coordination and many-body interactions in metallic systems. Trained on classical molecular dynamics trajectories of bulk aluminum, the surrogate achieves sub angstrom level accuracy within the training horizon and exhibits stable behavior during short- to mid-horizon temporal extrapolation. Structural and dynamical fidelity are validated through agreement with reference radial distribution functions and mean squared displacement trends, demonstrating that the model preserves key physical signatures beyond pointwise coordinate accuracy. These results establish GNN-based surrogate integrators as a promising and computationally efficient complement to traditional molecular dynamics for accelerated atomistic simulations within a validated regime.&lt;/p&gt;</description></item><item><guid>2512.21831v1</guid><title>End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration</title><link>http://arxiv.org/abs/2512.21831v1</link><author>Zhenwei Yang, Yibo Ai, Weidong Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为XET-V2X的端到端多模态融合跟踪框架，旨在通过统一多视角多模态感知来提升自动驾驶中的三维时空理解，尤其在遮挡、视角受限和V2X通信延迟的情况下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶场景中，多视角协同感知与多模态融合对于可靠的三维时空理解至关重要，尤其在存在遮挡、有限视角和V2X通信延迟时更显重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够在V2X协作中统一多视角多模态感知的共享时空表示框架，以提升检测与跟踪性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; XET-V2X采用双层空间交叉注意力模块，基于多尺度可变形注意力实现不同视角和模态的高效对齐；先聚合多视角图像特征以增强语义一致性，再通过更新后的空间查询引导点云融合，实现跨模态交互并降低计算开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实V2X-Seq-SPD数据集以及模拟V2X-Sim-V2V和V2X-Sim-V2I基准上，XET-V2X在不同通信延迟条件下均表现出检测和跟踪性能的持续提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; XET-V2X在复杂交通场景中实现了稳健且时序稳定的感知效果，验证了其在V2X协作中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多视角协同感知和多模态融合对于自动驾驶中的可靠三维时空理解至关重要，尤其在遮挡、有限视角和V2X通信延迟的情况下。本研究提出了XET-V2X，一种多模态融合端到端跟踪框架，统一多视角多模态感知于共享时空表示。为高效对齐异构视角和模态，XET-V2X引入基于多尺度可变形注意力的双层空间交叉注意力模块。首先聚合多视角图像特征以增强语义一致性，然后在更新后的空间查询指导下进行点云融合，实现有效的跨模态交互并降低计算开销。实验在真实V2X-Seq-SPD数据集以及模拟V2X-Sim-V2V和V2X-Sim-V2I基准上显示，在不同通信延迟下检测和跟踪性能均持续提升。定量结果和定性可视化均表明XET-V2X在复杂交通场景中实现了稳健且时序稳定的感知。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在自动驾驶中因遮挡、视角受限和 V2X 通信延迟导致的 3D 时空感知不完整和不稳定的问题。该问题对车辆安全至关重要，因为缺失或错误的感知会导致碰撞风险增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将多视角协同、跨模态融合和时空建模整合到一个端到端可学习的框架中，借鉴了 V2VNet、Where2Comm、Transformer‑based BEV、RecurrentBEV 等现有技术，并在此基础上提出了双层空间交叉注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双层空间交叉注意力实现视角和模态的高效对齐，先聚合图像特征提升语义一致性，再用更新后的空间查询引导点云融合，随后在共享 BEV 表示上使用 Transformer 进行时空编码，最终输出检测与跟踪结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 端到端统一的多视角、多模态、时空感知框架；② 双层跨模态跨视角注意力模块；③ 共享 BEV 表示实现几何对齐与信息补全；④ 在 V2X-Seq-SPD 与 V2X-Sim 基准上实现显著性能提升。与以往模块化、单模态或仅空间/时间融合的方法不同，XET‑V2X 在一次优化中同时处理所有维度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XET‑V2X 提供了一个统一的端到端框架，将多视角协同、跨模态融合与时空建模结合，实现了在 V2X 通信延迟下的 3D 感知与跟踪的最先进性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.&lt;/p&gt;</description></item><item><guid>2512.21837v1</guid><title>Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco</title><link>http://arxiv.org/abs/2512.21837v1</link><author>Siyu Li, Chenwei Song, Wan Zhou, Xinyi Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种将图结构信息与大型语言模型相结合的方案，用于烟草害虫与疾病控制领域的知识推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 烟草害虫与疾病控制需要准确的知识检索与推理，但传统方法缺乏结构化知识支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建领域知识图谱并结合LLM，提升知识检索与推理的准确性与深度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 基于GraphRAG框架，先利用LLM辅助构建烟草害虫与疾病知识图谱；随后使用Transformer作为核心推理模型，图神经网络学习节点表示；ChatGLM作为基础LLM，并通过LoRA进行参数高效微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果显示，该方法在多项评估指标上持续优于基线方法，显著提升准确率和推理深度，尤其在多跳与比较推理场景中表现突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 整合图结构信息的LLM方案在烟草害虫与疾病控制知识推理中具有显著优势，可有效提升推理质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种基于大型语言模型（LLM）的方案，将图结构信息整合进烟草害虫与疾病控制的知识推理中。该方法基于GraphRAG框架，通过显式引入领域知识图谱中的结构化信息来提升知识检索与推理。首先利用LLM辅助构建烟草害虫与疾病知识图谱，组织疾病、症状、控制方法等关键实体及其关系。基于该图谱检索相关知识并融入推理过程，以支持准确答案生成。核心推理模型采用Transformer架构，图神经网络（GNN）用于学习表达节点表示，捕获知识图谱中的局部与全局关系。ChatGLM为基础LLM，并通过LoRA进行参数高效微调。大量实验表明，该方法在多项评估指标上持续优于基线方法，显著提升了准确率和推理深度，尤其在复杂的多跳与比较推理场景中表现突出。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.&lt;/p&gt;</description></item><item><guid>2512.21890v1</guid><title>CrownGen: Patient-customized Crown Generation via Point Diffusion Model</title><link>http://arxiv.org/abs/2512.21890v1</link><author>Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了CrownGen，一种利用去噪扩散模型和牙齿级点云表示的生成框架，自动化定制牙冠设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 数字牙冠设计仍是修复牙科中耗时的瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种自动化、可定制的牙冠设计方法，以提高几何精度并降低设计时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CrownGen由边界预测模块和扩散生成模块组成，先预测空间先验，再一次性生成多颗牙齿的高保真形态；通过对496份外部扫描和26例临床修复进行定量基准和临床评估验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CrownGen在几何精度上优于现有最先进模型，显著缩短了设计时间；临床评估显示其生成的牙冠质量与人工工艺相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自动化的CrownGen可降低成本、缩短交付周期，提升患者获得高质量牙科护理的机会。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 数字牙冠设计仍是修复牙科中耗时的瓶颈。我们提出了CrownGen，一种利用去噪扩散模型和新颖的牙齿级点云表示的生成框架，自动化定制患者牙冠设计。该系统采用两个核心组件：边界预测模块用于建立空间先验，扩散生成模块用于在单次推理中合成多颗牙齿的高保真形态。我们通过对496份外部扫描的定量基准和26例临床修复案例的研究验证了CrownGen。结果表明，CrownGen在几何精度上优于最先进模型，并显著减少了主动设计时间。受过训练的牙医的临床评估确认，CrownGen辅助的牙冠在质量上与专家技术人员使用手工工作流程生产的牙冠在统计上无差异。通过自动化复杂的义齿建模，CrownGen提供了可扩展的解决方案，以降低成本、缩短周转时间并提升患者获得高质量牙科护理的机会。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Digital crown design remains a labor-intensive bottleneck in restorative dentistry. We present \textbf{CrownGen}, a generative framework that automates patient-customized crown design using a denoising diffusion model on a novel tooth-level point cloud representation. The system employs two core components: a boundary prediction module to establish spatial priors and a diffusion-based generative module to synthesize high-fidelity morphology for multiple teeth in a single inference pass. We validated CrownGen through a quantitative benchmark on 496 external scans and a clinical study of 26 restoration cases. Results demonstrate that CrownGen surpasses state-of-the-art models in geometric fidelity and significantly reduces active design time. Clinical assessments by trained dentists confirmed that CrownGen-assisted crowns are statistically non-inferior in quality to those produced by expert technicians using manual workflows. By automating complex prosthetic modeling, CrownGen offers a scalable solution to lower costs, shorten turnaround times, and enhance patient access to high-quality dental care.&lt;/p&gt;</description></item><item><guid>2512.21890v2</guid><title>CrownGen: Patient-customized Crown Generation via Point Diffusion Model</title><link>http://arxiv.org/abs/2512.21890v2</link><author>Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了 CrownGen，一种利用去噪扩散模型和牙齿级点云表示的生成框架，用于自动化定制牙冠设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 数字牙冠设计是修复牙科中耗时且繁琐的瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种自动化、可定制的牙冠设计系统，以提高设计效率和质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CrownGen 由边界预测模块和扩散生成模块组成，先预测空间先验，再一次性生成多颗牙齿的高保真形态。通过对 496 个外部扫描和 26 个临床案例进行定量基准测试和临床评估验证其性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CrownGen 在几何保真度上优于现有模型，显著缩短了设计时间；临床评估显示其生成的牙冠质量与人工工艺相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自动化的复杂义齿建模可降低成本、缩短交付周期，并提升患者获得高质量牙科护理的机会。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 数字牙冠设计仍然是修复牙科中劳动密集型的瓶颈。我们提出了 CrownGen，一种利用去噪扩散模型和新颖的牙齿级点云表示的生成框架，自动化定制患者牙冠设计。该系统采用两个核心组件：边界预测模块用于建立空间先验，扩散生成模块用于在单次推理中合成多颗牙齿的高保真形态。我们通过对 496 个外部扫描的定量基准测试和 26 个修复案例的临床研究验证了 CrownGen。结果表明，CrownGen 在几何保真度上超过了最先进的模型，并显著减少了主动设计时间。受过训练的牙医的临床评估确认，CrownGen 辅助的牙冠在质量上与使用手工工作流程的专家技术员生产的牙冠在统计上无差异。通过自动化复杂的义齿建模，CrownGen 提供了一个可扩展的解决方案，以降低成本、缩短周转时间，并提升患者获得高质量牙科护理的机会。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Digital crown design remains a labor-intensive bottleneck in restorative dentistry. We present CrownGen, a generative framework that automates patient-customized crown design using a denoising diffusion model on a novel tooth-level point cloud representation. The system employs two core components: a boundary prediction module to establish spatial priors and a diffusion-based generative module to synthesize high-fidelity morphology for multiple teeth in a single inference pass. We validated CrownGen through a quantitative benchmark on 496 external scans and a clinical study of 26 restoration cases. Results demonstrate that CrownGen surpasses state-of-the-art models in geometric fidelity and significantly reduces active design time. Clinical assessments by trained dentists confirmed that CrownGen-assisted crowns are statistically non-inferior in quality to those produced by expert technicians using manual workflows. By automating complex prosthetic modeling, CrownGen offers a scalable solution to lower costs, shorten turnaround times, and enhance patient access to high-quality dental care.&lt;/p&gt;</description></item><item><guid>2512.21897v1</guid><title>MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction</title><link>http://arxiv.org/abs/2512.21897v1</link><author>Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; MMCTOP 是一种多模态临床试验结果预测框架，融合分子结构、试验协议与疾病本体等多种生物医学信号，并通过文本化、特征对齐和专家混合网络实现高效融合，显著提升预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在高维生物医学信息学中，多模态数据融合面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 MMCTOP 以解决多模态融合难题，提升临床试验结果预测的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用模式引导文本化和输入保真校验，使用域特定编码器生成对齐嵌入，利用 transformer 主干与药物-疾病条件稀疏专家混合网络（SMoE）进行融合，并通过 top‑k 路由实现可扩展计算；同时使用温度缩放校准概率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在基准数据集上，MMCTOP 在精度、F1 和 AUC 上均优于单模态和多模态基线；消融实验表明文本化和专家路由对性能和稳定性贡献显著。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MMCTOP 通过受控叙事标准化、上下文条件专家融合和可审计可复现的操作保障，推动了多模态临床试验建模的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 为了解决高维生物医学信息学中多模态数据融合的挑战，我们提出了 MMCTOP，一种多模态临床试验结果预测框架，整合了分子结构表示、试验方案元数据与长篇适应性叙述以及疾病本体等异质生物医学信号。MMCTOP 结合模式引导文本化和输入保真校验与模态感知表示学习，在其中域特定编码器生成对齐嵌入，并由一个 transformer 主干与药物-疾病条件稀疏专家混合网络（SMoE）融合。该设计明确支持在治疗和设计子空间的专业化，同时通过 top‑k 路由保持可扩展计算。MMCTOP 在基准数据集上在精度、F1 和 AUC 上均优于单模态和多模态基线，消融实验显示模式引导文本化和选择性专家路由对性能和稳定性有显著贡献。我们还应用温度缩放以获得校准概率，确保下游决策支持的可靠风险估计。总体而言，MMCTOP 通过结合受控叙事标准化、上下文条件专家融合和面向可审计与可复现的操作保障，推进了多模态试验建模。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Addressing the challenge of multimodal data fusion in high-dimensional biomedical informatics, we propose MMCTOP, a MultiModal Clinical-Trial Outcome Prediction framework that integrates heterogeneous biomedical signals spanning (i) molecular structure representations, (ii) protocol metadata and long-form eligibility narratives, and (iii) disease ontologies. MMCTOP couples schema-guided textualization and input-fidelity validation with modality-aware representation learning, in which domain-specific encoders generate aligned embeddings that are fused by a transformer backbone augmented with a drug-disease-conditioned sparse Mixture-of-Experts (SMoE). This design explicitly supports specialization across therapeutic and design subspaces while maintaining scalable computation through top-k routing. MMCTOP achieves consistent improvements in precision, F1, and AUC over unimodal and multimodal baselines on benchmark datasets, and ablations show that schema-guided textualization and selective expert routing contribute materially to performance and stability. We additionally apply temperature scaling to obtain calibrated probabilities, ensuring reliable risk estimation for downstream decision support. Overall, MMCTOP advances multimodal trial modeling by combining controlled narrative normalization, context-conditioned expert fusion, and operational safeguards aimed at auditability and reproducibility in biomedical informatics.&lt;/p&gt;</description></item><item><guid>2512.21916v1</guid><title>Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition</title><link>http://arxiv.org/abs/2512.21916v1</link><author>Zeyu Liang, Hailun Xia, Naichuan Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了PAN框架，利用人类中心的图表示学习将包含人体关节的RGB补丁嵌入视为时空图，从而抑制RGB帧冗余并与骨架方法对齐，实现更有效的多模态特征融合。为降低对高质量骨架数据的依赖，作者引入了基于注意力的后期校准。随后提出两种变体：PAN‑Ensemble采用双路径图卷积网络并进行后期融合；PAN‑Unified在单一网络内完成统一的图表示学习。两种方法在三个常用多模态动作识别数据集上均取得了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类动作识别已取得显著进展，但融合RGB与骨架模态的多模态方法仍因两者的异质性而难以充分利用其互补潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够有效融合RGB与骨架信息、并降低对高质量骨架数据依赖的人类中心图表示学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 将包含人体关节的RGB补丁的token嵌入表示为时空图；2. 采用人类中心图建模抑制RGB帧冗余；3. 引入基于注意力的后期校准减少对骨架数据的依赖；4. 提出两种变体：双路径图卷积网络+后期融合的PAN‑Ensemble，以及单网络统一图学习的PAN‑Unified。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; PAN‑Ensemble和PAN‑Unified在三个多模态动作识别数据集上均实现了最先进的性能，验证了人类中心图建模和后期校准的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 人类中心图表示学习能够有效融合RGB与骨架模态，且通过后期校准可降低对高质量骨架数据的依赖，PAN框架在多模态动作识别任务中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管人类动作识别已取得显著成就，但融合RGB和骨架模态的多模态方法仍因其固有的异质性而受限，无法充分利用两者之间的互补潜力。本文提出了PAN，这是首个面向多模态动作识别的人类中心图表示学习框架，其中包含人体关节的RGB补丁的token嵌入被表示为时空图。人类中心图建模范式抑制了RGB帧中的冗余，并与基于骨架的方法很好地对齐，从而实现了更有效且语义上连贯的多模态特征融合。由于token嵌入的采样在很大程度上依赖于二维骨架数据，我们进一步提出了基于注意力的后期校准，以在模型性能几乎不受影响的前提下降低对高质量骨架数据的依赖。为探究PAN与骨架方法整合的潜力，我们提出了两种变体：PAN‑Ensemble采用双路径图卷积网络并进行后期融合；PAN‑Unified则在单一网络内执行统一的图表示学习。在三个广泛使用的多模态动作识别数据集上，PAN‑Ensemble和PAN‑Unified分别在各自的多模态融合设置（分离与统一建模）中实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While human action recognition has witnessed notable achievements, multimodal methods fusing RGB and skeleton modalities still suffer from their inherent heterogeneity and fail to fully exploit the complementary potential between them. In this paper, we propose PAN, the first human-centric graph representation learning framework for multimodal action recognition, in which token embeddings of RGB patches containing human joints are represented as spatiotemporal graphs. The human-centric graph modeling paradigm suppresses the redundancy in RGB frames and aligns well with skeleton-based methods, thus enabling a more effective and semantically coherent fusion of multimodal features. Since the sampling of token embeddings heavily relies on 2D skeletal data, we further propose attention-based post calibration to reduce the dependency on high-quality skeletal data at a minimal cost interms of model performance. To explore the potential of PAN in integrating with skeleton-based methods, we present two variants: PAN-Ensemble, which employs dual-path graph convolution networks followed by late fusion, and PAN-Unified, which performs unified graph representation learning within a single network. On three widely used multimodal action recognition datasets, both PAN-Ensemble and PAN-Unified achieve state-of-the-art (SOTA) performance in their respective settings of multimodal fusion: separate and unified modeling, respectively.&lt;/p&gt;</description></item><item><guid>2512.21924v1</guid><title>Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning</title><link>http://arxiv.org/abs/2512.21924v1</link><author>Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的伪健康图像（PHI）重建框架，利用解耦表示模块和边缘到图像恢复模块，显著提升多模态、多中心脑MRI异常检测的泛化能力和检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 脑MRI中多种病变的检测对临床至关重要，但由于病变多样性和成像条件差异，现有无监督学习方法在多模态和多中心数据上的泛化受限，且重建的PHI中残留异常像素影响检测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决无监督方法在多中心、多模态MRI上的泛化不足和异常残留导致的性能受限问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1）解耦表示模块：将脑MRI分离为成像信息和与成像无关的解剖结构信息，利用解剖先验和可微分的独热编码约束解耦稳定性；2）边缘到图像恢复模块：先从解剖图像的高频边缘信息恢复解剖表示，再与解耦的成像信息重新耦合，生成高质量PHI，抑制异常残留并保留结构细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在九个公开数据集（共4443名患者）上，所提方法在AP和DSC指标上分别比17种最先进方法提升18.32百分比和13.64百分比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架通过解耦与边缘恢复有效提升了多中心、多模态脑MRI异常检测的准确性和泛化能力，为临床诊断提供了更可靠的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper proposes a new pseudo‑healthy image reconstruction framework that uses a disentangled representation module and an edge‑to‑image restoration module to significantly improve the generalization and detection performance of unsupervised anomaly detection in multi‑modal, multi‑center brain MRI. The disentangled module separates imaging information from anatomy‑invariant structures using anatomical priors and a differentiable one‑hot encoding, while the edge‑to‑image module reconstructs high‑quality pseudo‑healthy images from edge information and recombines imaging data, reducing abnormal residuals. Evaluated on nine public datasets with 4,443 patients, the method outperforms 17 state‑of‑the‑art approaches, achieving absolute improvements of 18.32 percent in average precision and 13.64 percent in Dice similarity coefficient.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Detection of various lesions in brain MRI is clinically critical, but challenging due to the diversity of lesions and variability in imaging conditions. Current unsupervised learning methods detect anomalies mainly through reconstructing abnormal images into pseudo-healthy images (PHIs) by normal samples learning and then analyzing differences between images. However, these unsupervised models face two significant limitations: restricted generalizability to multi-modality and multi-center MRIs due to their reliance on the specific imaging information in normal training data, and constrained performance due to abnormal residuals propagated from input images to reconstructed PHIs. To address these limitations, two novel modules are proposed, forming a new PHI reconstruction framework. Firstly, the disentangled representation module is proposed to improve generalizability by decoupling brain MRI into imaging information and essential imaging-invariant anatomical images, ensuring that the reconstruction focuses on the anatomy. Specifically, brain anatomical priors and a differentiable one-hot encoding operator are introduced to constrain the disentanglement results and enhance the disentanglement stability. Secondly, the edge-to-image restoration module is designed to reconstruct high-quality PHIs by restoring the anatomical representation from the high-frequency edge information of anatomical images, and then recoupling the disentangled imaging information. This module not only suppresses abnormal residuals in PHI by reducing abnormal pixels input through edge-only input, but also effectively reconstructs normal regions using the preserved structural details in the edges. Evaluated on nine public datasets (4,443 patients&amp;#x27; MRIs from multiple centers), our method outperforms 17 SOTA methods, achieving absolute improvements of +18.32% in AP and +13.64% in DSC.&lt;/p&gt;</description></item><item><guid>2512.21960v1</guid><title>Modeling high dimensional point clouds with the spherical cluster model</title><link>http://arxiv.org/abs/2512.21960v1</link><author>Frédéric Cazals, Antoine Commaret, Louis Goldenberg</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种球形聚类模型，通过将数据点集近似为球体来获得几何洞察，并展示了其优化特性、求解方法及实验结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统聚类方法如 KMeans 使用质心，而球形聚类模型通过球体半径与距离标准差的比例来定义成本函数，提供更丰富的几何描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究球形聚类模型的优化性质，开发精确求解器，并评估其在不同维度数据集上的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 将球形聚类拟合问题建模为严格凸但非光滑的组合优化问题；2) 构造由超球面排列形成的分层细胞复形，利用 Clarke 梯度求解；3) 在多维数据集上进行实验比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1) 精确算法在低至中等维度且半径比例较小时，比 BFGS 启发式快数倍；在高维（&amp;gt;100）时无论半径比例如何均更快；2) 球形聚类中心表现为参数化的高维中位数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 球形聚类模型对高维多变量数据分析具有重要意义，且其中心可视为高维中位数，后续工作将探讨其在混合模型中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种参数化聚类模型，提供对定义聚类的点的几何洞察。球形聚类模型（SC）通过将有限点集近似为球体来实现。取半径 r 为中心 c 与数据点距离标准差的一个分数（介于 0 与 1 之间），SC 模型的代价是所有位于球体外的点相对于球体的幂距离之和。SC 模型的中心 c 是使该代价最小化的点。值得注意的是，当分数为 0 时，得到的是 KMeans 聚类中使用的质心。我们做出了三项贡献：首先，证明拟合球形聚类产生一个严格凸但不光滑的组合优化问题；其次，提出一种使用 Clarke 梯度的精确求解器，该求解器基于由超球面排列构成的分层细胞复形；最后，在维度从 9 到 10,000 的多种数据集上进行实验，得到两个主要观察结果。第一，精确算法在小/中等维度且分数较小的情况下，比基于 BFGS 的启发式快数倍；在高维（&amp;gt;100）数据集上，无论分数值如何，精确算法均更快。第二，SC 模型的中心表现为参数化的高维中位数。SC 模型直接适用于高维多变量数据分析，关于其在混合 SC 设计中的应用将在后续论文中报告。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;A parametric cluster model is a statistical model providing geometric insights onto the points defining a cluster. The {\em spherical cluster model} (SC) approximates a finite point set $P\subset \mathbb{R}^d$ by a sphere $S(c,r)$ as follows. Taking $r$ as a fraction $η\in(0,1)$ (hyper-parameter) of the std deviation of distances between the center $c$ and the data points, the cost of the SC model is the sum over all data points lying outside the sphere $S$ of their power distance with respect to $S$. The center $c$ of the SC model is the point minimizing this cost. Note that $η=0$ yields the celebrated center of mass used in KMeans clustering. We make three contributions.   First, we show fitting a spherical cluster yields a strictly convex but not smooth combinatorial optimization problem. Second, we present an exact solver using the Clarke gradient on a suitable stratified cell complex defined from an arrangement of hyper-spheres. Finally, we present experiments on a variety of datasets ranging in dimension from $d=9$ to $d=10,000$, with two main observations. First, the exact algorithm is orders of magnitude faster than BFGS based heuristics for datasets of small/intermediate dimension and small values of $η$, and for high dimensional datasets (say $d&amp;gt;100$) whatever the value of $η$. Second, the center of the SC model behave as a parameterized high-dimensional median.   The SC model is of direct interest for high dimensional multivariate data analysis, and the application to the design of mixtures of SC will be reported in a companion paper.&lt;/p&gt;</description></item><item><guid>2512.22014v1</guid><title>HWL-HIN: A Hypergraph-Level Hypergraph Isomorphism Network as Powerful as the Hypergraph Weisfeiler-Lehman Test with Application to Higher-Order Network Robustness</title><link>http://arxiv.org/abs/2512.22014v1</link><author>Chengyu Tian, Wenbin Pei</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文讨论了复杂系统鲁棒性的重要性，并提出了一种新的超图同构网络框架，用于快速预测超图鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的攻击后评估鲁棒性计算量大；CNN和GNN被用于快速预测，但忽略了高阶关联；HGNN的表达能力尚未达到理论上限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提升超图学习的拓扑表达能力，使其等价于超图Weisfeiler-Lehman测试，从而更准确地预测鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 基于图同构网络的思想，构建了超图级别的同构网络框架，并证明其表达能力与超图WL测试相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该方法在训练和预测效率上优于现有图模型，并且在强调拓扑结构的任务中明显优于传统HGNN。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提出的超图同构网络框架在保持高效的同时，显著提升了超图鲁棒性预测的准确性和表达能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 复杂系统的鲁棒性对工程和经济具有重要意义。然而，传统基于攻击的事后鲁棒性评估会产生巨大的计算开销。近年来，卷积神经网络和图神经网络等深度学习方法被广泛用作快速鲁棒性预测的代理模型。然而，这些方法忽略了现实世界系统中普遍存在的复杂高阶相关性，而这些相关性自然可以用超图来建模。虽然超图神经网络已被广泛用于超图学习，但其拓扑表达能力尚未达到理论上限。为了解决这一限制，本文受图同构网络的启发，提出了一种超图级别的超图同构网络框架。理论上，该方法被证明具有严格等价于超图Weisfeiler-Lehman测试的表达能力，并被用于预测超图鲁棒性。实验结果表明，在保持训练和预测效率的同时，所提出的方法不仅优于现有的基于图的模型，而且在强调拓扑结构表示的任务中显著超过传统的超图神经网络。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Robustness in complex systems is of significant engineering and economic importance. However, conventional attack-based a posteriori robustness assessments incur prohibitive computational overhead. Recently, deep learning methods, such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), have been widely employed as surrogates for rapid robustness prediction. Nevertheless, these methods neglect the complex higher-order correlations prevalent in real-world systems, which are naturally modeled as hypergraphs. Although Hypergraph Neural Networks (HGNNs) have been widely adopted for hypergraph learning, their topological expressive power has not yet reached the theoretical upper bound. To address this limitation, inspired by Graph Isomorphism Networks, this paper proposes a hypergraph-level Hypergraph Isomorphism Network framework. Theoretically, this approach is proven to possess an expressive power strictly equivalent to the Hypergraph Weisfeiler-Lehman test and is applied to predict hypergraph robustness. Experimental results demonstrate that while maintaining superior efficiency in training and prediction, the proposed method not only outperforms existing graph-based models but also significantly surpasses conventional HGNNs in tasks that prioritize topological structure representation.&lt;/p&gt;</description></item><item><guid>2512.22022v1</guid><title>Meta-Learning-Based Handover Management in NextG O-RAN</title><link>http://arxiv.org/abs/2512.22022v1</link><author>Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究针对传统手工切换（THO）在高密度和高频段部署中出现的失败与延迟问题，提出了一种新的框架CONTRA，能够在O-RAN架构下联合优化THO和条件手工切换（CHO）。通过两种方案：预先分配切换类型和动态决策切换类型，结合元学习算法，CONTRA实现了接近理想预测的性能，并在实际数据集上显著提升了用户吞吐量，降低了切换成本，优于现有3GPP规范和强化学习基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统手工切换在移动网络中是核心技术，但在密集部署和高频段环境下易出现失败和延迟。3GPP引入条件手工切换以实现前瞻性小区预留和用户驱动执行，但两种切换方式在信令、资源使用和可靠性方面存在复杂权衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用全国范围内的移动性管理数据，提出一种能够自适应、鲁棒地控制手工切换的框架，以满足下一代网络的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CONTRA框架在O-RAN架构下联合优化THO和CHO，提供两种变体：一是根据服务或用户需求预先分配切换类型；二是让控制器根据系统状态实时决定切换类型。采用实用的元学习算法，能够在运行时自适应并保证与完美未来信息的理想性能相当。该框架设计为近实时部署的O-RAN xApp，符合6G灵活智能控制目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在基于众包数据集的广泛评估中，CONTRA显著提升了用户吞吐量，降低了THO和CHO的切换成本，并在动态和真实场景中优于3GPP合规方案和强化学习基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CONTRA提供了一种高效、可适应的手工切换控制方案，能够在O-RAN环境下实现灵活智能的网络管理，为6G及未来网络的发展奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 传统手工切换（THO）一直是移动连接的支柱，但在密集部署和高频段中，它们越来越容易出现失败和延迟。为了解决这些局限，3GPP 引入了条件手工切换（CHO），它们支持前瞻性小区预留和用户驱动执行。然而，两种切换类型在信令、资源使用和可靠性方面都存在复杂的权衡。本文提供了来自顶级移动网络运营商（MNO）的独特全国范围移动性管理数据集，为这些问题提供了新的见解，并呼吁在下一代网络中采用自适应和稳健的切换控制。基于这些发现，我们提出了 CONTRA，一个框架，首次在 O-RAN 架构中联合优化 THO 和 CHO。我们研究了 CONTRA 的两种变体：一种是预先将用户分配到一种切换类型，反映不同的服务或用户特定需求；另一种更动态的形式，控制器根据系统条件和需求实时决定切换类型。为此，它依赖于一种实用的元学习算法，能够适应运行时观察并保证与具有完美未来信息的“全知”相当的性能（通用无后悔）。CONTRA 专门为近实时部署为 O-RAN xApp 设计，并与 6G 的灵活和智能控制目标保持一致。利用众包数据集的广泛评估表明，CONTRA 在动态和真实场景中提高了用户吞吐量并降低了 THO 和 CHO 的切换成本，优于 3GPP 合规和强化学习基线。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While traditional handovers (THOs) have served as a backbone for mobile connectivity, they increasingly suffer from failures and delays, especially in dense deployments and high-frequency bands. To address these limitations, 3GPP introduced Conditional Handovers (CHOs) that enable proactive cell reservations and user-driven execution. However, both handover (HO) types present intricate trade-offs in signaling, resource usage, and reliability. This paper presents unique, countrywide mobility management datasets from a top-tier mobile network operator (MNO) that offer fresh insights into these issues and call for adaptive and robust HO control in next-generation networks. Motivated by these findings, we propose CONTRA, a framework that, for the first time, jointly optimizes THOs and CHOs within the O-RAN architecture. We study two variants of CONTRA: one where users are a priori assigned to one of the HO types, reflecting distinct service or user-specific requirements, as well as a more dynamic formulation where the controller decides on-the-fly the HO type, based on system conditions and needs. To this end, it relies on a practical meta-learning algorithm that adapts to runtime observations and guarantees performance comparable to an oracle with perfect future information (universal no-regret). CONTRA is specifically designed for near-real-time deployment as an O-RAN xApp and aligns with the 6G goals of flexible and intelligent control. Extensive evaluations leveraging crowdsourced datasets show that CONTRA improves user throughput and reduces both THO and CHO switching costs, outperforming 3GPP-compliant and Reinforcement Learning (RL) baselines in dynamic and real-world scenarios.&lt;/p&gt;</description></item><item><guid>2512.22027v1</guid><title>Patch-Discontinuity Mining for Generalized Deepfake Detection</title><link>http://arxiv.org/abs/2512.22027v1</link><author>Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为GenDF的轻量级深度伪造检测框架，利用大型视觉模型迁移并结合专门的表示学习、特征空间重分配以及分类不变特征增强策略，在保持极低可训练参数量的同时实现了跨域和跨操作的最佳泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成式人工智能的快速发展使得能够生成高度逼真的假面部图像，严重威胁个人隐私和网络信息的完整性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在现有深度伪造检测方法对未知伪造模式表现不佳的情况下，提出一种简单有效的框架，以提升跨域和跨操作的检测泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将强大的大规模视觉模型迁移到深度伪造检测任务，采用深度伪造特定的表示学习捕捉真伪面部图像的判别模式，进行特征空间重分配以缓解分布不匹配，并通过分类不变特征增强策略提升泛化性能，同时不增加额外可训练参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; GenDF在跨域和跨操作设置下实现了最先进的泛化性能，仅需0.28M可训练参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的框架在有效性和效率方面均表现出色，验证了其在深度伪造检测中的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 快速发展的生成式人工智能使得能够创建高度逼真的假面部图像，严重威胁个人隐私和在线信息的完整性。现有的深度伪造检测方法往往依赖手工制作的取证线索和复杂的架构，在同域设置中表现强劲，但在面对未知伪造模式时性能显著下降。本文提出了GenDF，一种简单而有效的框架，将强大的大规模视觉模型迁移到深度伪造检测任务，并采用紧凑整洁的网络设计。GenDF结合了深度伪造特定的表示学习，以捕捉真实与伪造面部图像之间的判别模式，特征空间重分配以缓解分布不匹配，以及分类不变特征增强策略，以在不引入额外可训练参数的情况下提升泛化能力。大量实验表明，GenDF在跨域和跨操作设置下实现了最先进的泛化性能，仅需0.28M可训练参数，验证了所提出框架的有效性和效率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid advancement of generative artificial intelligence has enabled the creation of highly realistic fake facial images, posing serious threats to personal privacy and the integrity of online information. Existing deepfake detection methods often rely on handcrafted forensic cues and complex architectures, achieving strong performance in intra-domain settings but suffering significant degradation when confronted with unseen forgery patterns. In this paper, we propose GenDF, a simple yet effective framework that transfers a powerful large-scale vision model to the deepfake detection task with a compact and neat network design. GenDF incorporates deepfake-specific representation learning to capture discriminative patterns between real and fake facial images, feature space redistribution to mitigate distribution mismatch, and a classification-invariant feature augmentation strategy to enhance generalization without introducing additional trainable parameters. Extensive experiments demonstrate that GenDF achieves state-of-the-art generalization performance in cross-domain and cross-manipulation settings while requiring only 0.28M trainable parameters, validating the effectiveness and efficiency of the proposed framework.&lt;/p&gt;</description></item><item><guid>2512.22187v1</guid><title>Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications</title><link>http://arxiv.org/abs/2512.22187v1</link><author>Ndagijimana Cyprien, Mehdi Sookhak, Hosein Zarini, Chandra N Sekharan, Mohammed Atiquzzaman</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种联合无人机和无人地面车辆的部署与轨迹规划框架，以在灾害地区实现高质量通信服务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在灾害地区，使用无人机和无人地面车辆联合部署已被证明能有效建立通信网络，但如何在使用尽可能少的无人机的同时保证良好的服务质量仍是挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够在保证地面用户服务质量的前提下，最优定位和规划无人机与无人地面车辆轨迹的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过构建道路图来描述无人地面车辆的移动约束，将总吞吐量优化问题转化为马尔可夫决策过程，并采用带元学习的异步优势演员-评论家算法（Meta-A3C）实现快速适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 数值实验表明，Meta-A3C 在吞吐量上比传统 A3C 高 13.1%，在执行速度上快 49%，且仍满足服务质量要求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Meta-A3C 在联合 UAV-UGV 部署中能够显著提升通信性能，并在动态环境下保持高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在灾害地区，联合使用无人机（UAV）和无人地面车辆（UGV）已被证明是一种有效的通信建立方法。然而，在尽量少使用无人机的前提下确保良好的服务质量（QoS）也需要对无人机和无人地面车辆进行最优定位和轨迹规划。本文提出了一种联合 UAV-UGV 的定位与轨迹规划框架，能够为地面用户提供最优 QoS。为建模 UGV 的移动性，我们引入了道路图，指导其沿合法道路段移动，并遵守道路网络约束。为求解总吞吐量优化问题，我们将其重新表述为马尔可夫决策过程（MDP），并提出了一种结合元学习的异步优势演员-评论家（A3C）算法，以快速适应新环境和动态条件。数值结果表明，我们提出的 Meta-A3C 方法优于 A3C 和 DDPG，吞吐量提升 13.1%，执行速度加快 49%，同时满足 QoS 要求。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Joint deployment of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) has been shown to be an effective method to establish communications in areas affected by disasters. However, ensuring good Quality of Services (QoS) while using as few UAVs as possible also requires optimal positioning and trajectory planning for UAVs and UGVs. This paper proposes a joint UAV-UGV-based positioning and trajectory planning framework for UAVs and UGVs deployment that guarantees optimal QoS for ground users. To model the UGVs&amp;#x27; mobility, we introduce a road graph, which directs their movement along valid road segments and adheres to the road network constraints. To solve the sum rate optimization problem, we reformulate the problem as a Markov Decision Process (MDP) and propose a novel asynchronous Advantage Actor Critic (A3C) incorporated with meta-learning for rapid adaptation to new environments and dynamic conditions. Numerical results demonstrate that our proposed Meta-A3C approach outperforms A3C and DDPG, delivering 13.1\% higher throughput and 49\% faster execution while meeting the QoS requirements.&lt;/p&gt;</description></item><item><guid>2512.22197v1</guid><title>Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy</title><link>http://arxiv.org/abs/2512.22197v1</link><author>Shivum Telang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种多模态可解释性模型，利用视觉语言模型和少样本学习，模拟眼科医生对视网膜四分区内病变分布的推理过程，并通过配对的 Grad-CAM 热图展示 OCT 与眼底图像中各神经元权重，帮助解释糖尿病视网膜病变（DR）严重程度的分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 糖尿病视网膜病变是全球视力丧失的主要原因，早期检测至关重要，但医生资源有限导致许多病例未被诊断。传统 AI 模型通过病变分割实现可解释性，但人工标注不切实际，且现有模型仅依赖单一影像模态，解释性和效果有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够以自然语言识别单个 DR 病变、并解释分类依据的多模态 AI 系统，以满足临床医生对推理过程的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用视觉语言模型结合少样本学习，对眼底图像中的四分区病变分布进行分析；生成配对的 Grad-CAM 热图，展示 OCT 与眼底图像中各神经元权重；在包含 3,000 张眼底图像和 1,000 张 OCT 图像的数据集上进行训练与评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法克服了单模态、缺乏可解释性等现有诊断模型的局限，提供了实用且全面的工具，可在筛查、治疗和研究场景中提升患者预后。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多模态可解释性模型为糖尿病视网膜病变的诊断提供了更精准、可解释的方案，具有显著的临床应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist&amp;#x27;s reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist&amp;#x27;s reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.&lt;/p&gt;</description></item><item><guid>2512.22207v1</guid><title>GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks</title><link>http://arxiv.org/abs/2512.22207v1</link><author>Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了GamiBench，一个基于折纸灵感的折叠任务，用来评估多模态大语言模型在空间推理和二维到三维规划方面的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大语言模型在感知和指令跟随方面表现出色，但在跨视角和时间的空间推理上仍存在困难。现有基准多聚焦于静态图像或最终输出，未能体现空间推理的序列性和视角依赖性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个全面评估空间推理过程的基准，衡量模型在多视角一致性、物理可行性和中间折叠步骤解释等方面的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GamiBench包含186个可行和186个不可行的二维折痕图案，配以对应的三维折叠形状，并从六个不同视角生成。设计了三类视觉问答任务：预测三维折叠配置、区分有效视角、检测不可行图案，并引入视角一致性（VC）和不可行折叠选择率（IFSR）两项诊断指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，即使是领先模型如GPT-5和Gemini-2.5-Pro在单步空间理解上也表现不佳；GamiBench能够完整评估模型的推理过程并揭示其在不同复杂度折叠任务中的弱点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GamiBench为多模态大语言模型的几何理解和空间推理提供了标准化的评估框架，凸显了当前模型的不足并为后续改进提供了参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大型语言模型（MLLMs）在感知和指令跟随方面表现出色，但它们在空间推理方面仍然存在困难：在多视角和时间上心理跟踪和操作物体的能力。空间推理是人类智能的关键组成部分，但大多数现有基准侧重于静态图像或最终输出，未能考虑到这一技能的序列性和视角依赖性。为弥补这一差距，我们引入了GamiBench，这是一个通过折纸灵感的折叠任务来评估MLLMs空间推理和二维到三维规划的基准。GamiBench包括186个常规和186个不可能的二维折痕图案及其对应的三维折叠形状，这些形状来自六个不同视角，跨越三种视觉问答（VQA）任务：预测三维折叠配置、区分有效视角和检测不可能的图案。与仅评估最终预测的先前基准不同，GamiBench全面评估整个推理过程——测量跨视角一致性、通过不可能折叠检测评估物理可行性以及对中间折叠步骤的解释。它还引入了新的诊断指标——视角一致性（VC）和不可能折叠选择率（IFSR），以衡量模型在不同复杂度折叠上的处理能力。我们的实验表明，即使是领先模型如GPT-5和Gemini-2.5-Pro在单步空间理解上也表现不佳。这些贡献为评估MLLMs的几何理解和空间推理建立了标准化框架。数据集和代码：https://github.com/stvngo/GamiBench.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决多模态大型语言模型在空间推理方面的不足，尤其是从二维折纸图到三维结构的映射以及多视角一致性。空间推理是人类认知和许多实际任务（如家具组装、机器人操作）的核心能力，现有模型在这方面表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出现有基准多聚焦于静态图像或最终结果，缺乏多视角和序列推理。随后借鉴了原有空间推理基准（如GSR‑Bench、LEGO‑Puzzles、3DSRBench）和折纸工具（Oriedita、Origami Simulator、Flat‑Folder），构建了包含可折叠与不可折叠图案的折纸数据集，并引入多视角验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过折纸任务将二维折痕图映射到三维结构，并在六个视角下评估模型的空间一致性和物理可行性。实现流程包括：①收集186个可折叠和186个不可折叠的折痕图；②使用折纸仿真器生成对应的三维模型；③设计三类VQA多选题（单步空间理解、多步推理、不可折叠检测）；④让模型根据提示和图像输出答案；⑤用视角一致性（VC）和不可折叠选择率（IFSR）等指标评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①多视角、序列化的空间推理基准；②引入视角一致性和不可折叠选择率两项诊断指标；③将文本指令与视觉状态结合的折纸任务；④对折叠复杂度进行分级控制。与以往基准相比，GamiBench不再仅关注最终结果，而是全面评估推理过程、跨视角一致性和物理可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GamiBench提供了一个多视角、序列化的折纸基准，揭示并量化了多模态大型语言模型在二维到三维空间推理、视角一致性和物理可行性方面的显著局限。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.&lt;/p&gt;</description></item><item><guid>2512.22226v1</guid><title>VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs</title><link>http://arxiv.org/abs/2512.22226v1</link><author>Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 VideoScaffold，一种针对连续视频理解的动态表示框架，能够根据视频时长自适应调整事件粒度，同时保持细粒度视觉语义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 长视频的理解对多模态大型语言模型提出挑战，主要因为帧间冗余和需要时间连贯的表示。现有的静态策略如稀疏采样、帧压缩和聚类，适用于离线场景，但在连续视频流中往往产生碎片化或过度压缩的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在实时视频流中实现高质量理解的动态框架，克服静态方法的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; VideoScaffold 由 Elastic-Scale Event Segmentation（EES）和 Hierarchical Event Consolidation（HEC）两大组件组成。EES 通过预测引导的分割动态细化事件边界；HEC 则逐层聚合语义相关的片段，形成多级抽象。两者协同工作，使模型从细粒度帧理解平滑过渡到抽象事件推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多项离线和流式视频理解基准上，VideoScaffold 达到了最先进的性能，证明其在连续视频场景中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架模块化且可即插即用，能够无缝扩展现有基于图像的多模态大型语言模型到连续视频理解，并已公开代码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解长视频的多模态大型语言模型（MLLMs）仍然具有挑战性，因为帧之间存在大量冗余，并且需要时间连贯的表示。现有的静态策略，如稀疏采样、帧压缩和聚类，针对离线设置进行了优化，并且在应用于连续视频流时往往产生碎片化或过度压缩的输出。我们提出了 VideoScaffold，一种为流式视频理解设计的动态表示框架。它根据视频时长自适应调整事件粒度，同时保留细粒度视觉语义。VideoScaffold 引入了两个关键组件：Elastic-Scale Event Segmentation（EES），它执行预测引导的分割以动态细化事件边界；以及 Hierarchical Event Consolidation（HEC），它逐步将语义相关的片段聚合为多级抽象。EES 和 HEC 协同工作，使 VideoScaffold 能够在视频流展开时顺利从细粒度帧理解过渡到抽象事件推理。通过在离线和流式视频理解基准上的广泛实验，证明 VideoScaffold 在两种设置下都实现了最先进的性能。该框架模块化且即插即用，能够无缝扩展现有基于图像的 MLLMs 到连续视频理解。代码可在 https://github.com/zheng980629/VideoScaffold 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.&lt;/p&gt;</description></item><item><guid>2512.22241v1</guid><title>Enhanced geometry prediction in laser directed energy deposition using meta-learning</title><link>http://arxiv.org/abs/2512.22241v1</link><author>Abdul Malik Al Mardhouf Al Saadi, Amrita Basak</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于元学习的跨数据集知识迁移模型，用于在激光定向能量沉积过程中快速预测焊丝轨迹几何形状。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在激光定向能量沉积中，实验数据稀缺且来源多样，导致精确的焊丝几何预测困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决数据不足和异质性问题，开发能够在有限样本下快速适应新工况的预测模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用两种梯度型元学习算法——模型无关元学习和Reptile，对来自文献和实验室的多组数据进行训练，并在粉末供料、线材供料和混合供料三种工艺上进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 两种算法在仅用三到九个训练样本的情况下，能够在未见过的目标任务上准确预测焊丝高度，且性能优于传统前馈神经网络；在多种工况下，模型的决定系数接近0.9，平均绝对误差在0.03到0.08毫米之间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 元学习方法实现了跨异质激光定向能量沉积设置的有效知识迁移，能够在数据有限的情况下提供高精度的焊丝几何预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在激光定向能量沉积（L-DED）中，准确预测焊丝几何形状往往受到不同材料、机器配置和工艺参数下收集的实验数据稀缺和异质性的阻碍。为了解决这一挑战，本文提出了一种基于元学习的跨数据集知识迁移模型，用于预测L-DED中的沉积轨迹几何形状。具体而言，研究了两种基于梯度的元学习算法，即模型无关元学习（MAML）和Reptile，以实现对新沉积条件的快速适应，并且仅需有限的数据。该框架使用来自同行评审文献和内部实验的多组实验数据进行训练，并在粉末供料、线材供料和混合线材-粉末L-DED工艺中进行评估。结果表明，MAML和Reptile在仅使用三到九个训练样本的情况下，能够在未见过的目标任务上准确预测焊丝高度，并且在相同数据约束下持续优于传统前馈神经网络。在代表不同打印条件的多种目标任务中，元学习模型实现了强大的泛化性能，决定系数最高可达约0.9，平均绝对误差在0.03-0.08毫米之间，证明了在异质L-DED设置中有效的知识迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate bead geometry prediction in laser-directed energy deposition (L-DED) is often hindered by the scarcity and heterogeneity of experimental datasets collected under different materials, machine configurations, and process parameters. To address this challenge, a cross-dataset knowledge transfer model based on meta-learning for predicting deposited track geometry in L-DED is proposed. Specifically, two gradient-based meta-learning algorithms, i.e., Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated to enable rapid adaptation to new deposition conditions with limited data. The proposed framework is performed using multiple experimental datasets compiled from peer-reviewed literature and in-house experiments and evaluated across powder-fed, wire-fed, and hybrid wire-powder L-DED processes. Results show that both MAML and Reptile achieve accurate bead height predictions on unseen target tasks using as few as three to nine training examples, consistently outperforming conventional feedforward neural networks trained under comparable data constraints. Across multiple target tasks representing different printing conditions, the meta-learning models achieve strong generalization performance, with R-squared values reaching up to approximately 0.9 and mean absolute errors between 0.03-0.08 mm, demonstrating effective knowledge transfer across heterogeneous L-DED settings.&lt;/p&gt;</description></item><item><guid>2512.22243v1</guid><title>Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning</title><link>http://arxiv.org/abs/2512.22243v1</link><author>Alan Inglis, Fiona Doohan, Subramani Natarajan, Breige McNulty, Chris Elliott, Anne Nugent, Julie Meneely, Brett Greer, Stephen Kildea, Diana Bucur, Martin Danaher, Melissa Di Rocco, Lisa Black, Adam Gauley, Naoise McKenna, Andrew Parnell</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究评估了多种神经网络和迁移学习模型在预测爱尔兰燕麦中多种毒素水平的表现，发现TabPFN模型表现最佳，天气历史和种子含水量是最重要的预测因子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 霉菌毒素污染对谷物质量、食品安全和农业生产力构成重大风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 准确预测霉菌毒素水平，以支持早期干预并降低经济损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用包含环境、农艺和地理预测因子的燕麦样本数据，评估五种模型：基础多层感知机、预训练多层感知机、TabPFN、TabNet和FT-Transformer，使用回归和分类指标评估性能，并进行置换变量重要性分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; TabPFN迁移学习模型在所有指标上表现最佳，其次是基础多层感知机；天气历史模式（90天预收获期）和种子含水量是最重要的预测因子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 迁移学习模型尤其是TabPFN可有效预测爱尔兰燕麦中的霉菌毒素水平，天气和种子含水量是关键预测变量，可为早期干预提供依据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Mycotoxin contamination poses a significant risk to cereal crop quality, food safety, and agricultural productivity. Accurate prediction of mycotoxin levels can support early intervention strategies and reduce economic losses. This study investigates the use of neural networks and transfer learning models to predict mycotoxin contamination in Irish oat crops as a multi-response prediction task. Our dataset comprises oat samples collected in Ireland, containing a mix of environmental, agronomic, and geographical predictors. Five modelling approaches were evaluated: a baseline multilayer perceptron (MLP), an MLP with pre-training, and three transfer learning models; TabPFN, TabNet, and FT-Transformer. Model performance was evaluated using regression and classification metrics, with results reported per toxin and on average. Additionally, permutation-based variable importance analysis was conducted to identify the most influential predictors across both prediction tasks. The transfer learning approach TabPFN provided the overall best performance, followed by the baseline MLP. Our variable importance analysis revealed that weather history patterns in the 90-day pre-harvest period were the most important predictors, alongside seed moisture content.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Mycotoxin contamination poses a significant risk to cereal crop quality, food safety, and agricultural productivity. Accurate prediction of mycotoxin levels can support early intervention strategies and reduce economic losses. This study investigates the use of neural networks and transfer learning models to predict mycotoxin contamination in Irish oat crops as a multi-response prediction task. Our dataset comprises oat samples collected in Ireland, containing a mix of environmental, agronomic, and geographical predictors. Five modelling approaches were evaluated: a baseline multilayer perceptron (MLP), an MLP with pre-training, and three transfer learning models; TabPFN, TabNet, and FT-Transformer. Model performance was evaluated using regression (RMSE, $R^2$) and classification (AUC, F1) metrics, with results reported per toxin and on average. Additionally, permutation-based variable importance analysis was conducted to identify the most influential predictors across both prediction tasks. The transfer learning approach TabPFN provided the overall best performance, followed by the baseline MLP. Our variable importance analysis revealed that weather history patterns in the 90-day pre-harvest period were the most important predictors, alongside seed moisture content.&lt;/p&gt;</description></item><item><guid>2512.22252v1</guid><title>Graph Attention-based Adaptive Transfer Learning for Link Prediction</title><link>http://arxiv.org/abs/2512.22252v1</link><author>Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新的图注意力自适应迁移网络（GAATNet），旨在解决传统图神经网络在大规模稀疏图和跨数据集迁移学习中的局限性。通过预训练与微调相结合，并引入两项关键策略，GAATNet在七个公开数据集上实现了领先的链接预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 图神经网络在链接预测任务中取得了显著进展，但在处理大规模稀疏图以及不同数据集之间的高对齐需求时仍面临挑战。自监督方法虽取得成功，但对跨图数据集迁移学习的潜力研究不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够在不同规模数据集之间高效迁移知识、提升链接预测效果的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 采用预训练与微调相结合的框架，捕获跨数据集的全局节点嵌入信息。2. 在自注意力模块中加入远程邻居嵌入作为偏置，以获取全局特征。3. 在微调阶段引入轻量级自适配模块，提升训练效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在七个公开数据集的实验中，GAATNet在链接预测任务上达到了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GAATNet为链接预测任务提供了一种通用且可扩展的解决方案，能够有效整合图神经网络与迁移学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图神经网络（GNNs）为链接预测（LP）领域带来了革命性的进展，提供了强大的工具来挖掘图中的潜在关系。然而，现有方法在处理大规模稀疏图和需要在不同数据集之间实现高度对齐的迁移学习时面临挑战。此外，尽管自监督方法在许多图任务中取得了显著成功，但先前的研究忽视了迁移学习在不同图数据集之间泛化的潜力。为了解决这些局限性，我们提出了一种新颖的图注意力自适应迁移网络（GAATNet）。它结合了预训练和微调的优势，捕获跨不同规模数据集的全局节点嵌入信息，确保高效的知识迁移和改进的LP性能。为增强模型的泛化能力并加速训练，我们设计了两项关键策略：1）在自注意力模块中将远程邻居嵌入作为偏置，以捕获全局特征。2）在微调期间引入轻量级自适配模块，以提高训练效率。对七个公开数据集的全面实验表明，GAATNet在LP任务中实现了最先进的性能。本研究为LP任务提供了一种通用且可扩展的解决方案，以有效整合GNNs与迁移学习。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of alignment between different datasets in transfer learning. Besides, although self-supervised methods have achieved remarkable success in many graph tasks, prior research has overlooked the potential of transfer learning to generalize across different graph datasets. To address these limitations, we propose a novel Graph Attention Adaptive Transfer Network (GAATNet). It combines the advantages of pre-training and fine-tuning to capture global node embedding information across datasets of different scales, ensuring efficient knowledge transfer and improved LP performance. To enhance the model&amp;#x27;s generalization ability and accelerate training, we design two key strategies: 1) Incorporate distant neighbor embeddings as biases in the self-attention module to capture global features. 2) Introduce a lightweight self-adapter module during fine-tuning to improve training efficiency. Comprehensive experiments on seven public datasets demonstrate that GAATNet achieves state-of-the-art performance in LP tasks. This study provides a general and scalable solution for LP tasks to effectively integrate GNNs with transfer learning. The source code and datasets are publicly available at https://github.com/DSI-Lab1/GAATNet&lt;/p&gt;</description></item><item><guid>2512.22262v1</guid><title>INSIGHT: Spatially resolved survival modelling from routine histology crosslinked with molecular profiling reveals prognostic epithelial-immune axes in stage II/III colorectal cancer</title><link>http://arxiv.org/abs/2512.22262v1</link><author>Piotr Keller, Mark Eastwood, Zedong Hu, Aimée Selten, Ruqayya Awan, Gertjan Rasschaert, Sara Verbandt, Vlad Popovici, Hubert Piessevaux, Hayley T Morris, Petros Tsantoulis, Thomas Alexander McKee, André D'Hoore, Cédric Schraepen, Xavier Sagaert, Gert De Hertogh, Sabine Tejpar, Fayyaz Minhas</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了一种名为INSIGHT的图神经网络，可直接从常规组织学图像预测结直肠癌患者的生存风险，并通过多组学数据揭示了与预后相关的细胞与分子机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 常规组织学在II/III期结直肠癌中蕴含丰富的预后信息，但这些信息隐藏在复杂的空间组织结构中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够从常规组织学图像中直接预测患者生存风险的模型，并验证其预后性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用TCGA和SURGEN两组数据训练并交叉验证INSIGHT模型，随后在独立数据集上进行验证，并将空间风险与空间转录组、空间蛋白质组、批量RNA测序和单细胞参考数据整合，以探索风险相关的分子特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; INSIGHT在独立验证中显示出比传统pTNM分期更优的预后性能；其空间风险图与传统组织病理学一致，并发现细胞核的坚固度和圆形度与风险相关；整合多组学数据揭示了上皮-免疫风险空间，涉及上皮去分化、胎儿程序、髓系驱动的基质状态（包括SPP1+巨噬细胞和LAMP3+树突细胞）以及适应性免疫功能障碍；分析还发现患者特异性的上皮异质性、MSI-High肿瘤内部分层，以及CDX2/HNF4A缺失和CEACAM5/6相关增殖程序的高风险路径，提示潜在的治疗靶点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; INSIGHT能够提供精准的空间风险评分，显著提升结直肠癌II/III期患者的预后评估，并通过多组学整合揭示了关键的细胞与分子机制，为个体化治疗提供了新的视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 常规组织学在II/III期结直肠癌中包含丰富的预后信息，这些信息大多嵌入在复杂的空间组织结构中。我们提出了INSIGHT，一种图神经网络，可直接从常规组织学图像预测生存。该模型在TCGA（342例）和SURGEN（336例）上训练并交叉验证，能够生成患者级别的空间分辨风险评分。大规模独立验证显示，与pTNM分期相比，INSIGHT的预后性能更优（C指数0.68-0.69对比0.44-0.58）。INSIGHT的空间风险图重现了传统的预后组织病理学，并识别出细胞核的坚固度和圆形度为定量风险相关因素。将空间风险与基于数据驱动的空间转录组签名、空间蛋白质组、批量RNA测序和单细胞参考整合，揭示了一个上皮-免疫风险流形，捕捉了上皮去分化和胎儿程序、髓系驱动的基质状态（包括SPP1+巨噬细胞和LAMP3+树突细胞）以及适应性免疫功能障碍。该分析揭示了患者特异性的上皮异质性、MSI-High肿瘤内部的分层，以及CDX2/HNF4A缺失和CEACAM5/6相关增殖程序的高风险路径，突出了协调的治疗脆弱性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Routine histology contains rich prognostic information in stage II/III colorectal cancer, much of which is embedded in complex spatial tissue organisation. We present INSIGHT, a graph neural network that predicts survival directly from routine histology images. Trained and cross-validated on TCGA (n=342) and SURGEN (n=336), INSIGHT produces patient-level spatially resolved risk scores. Large independent validation showed superior prognostic performance compared with pTNM staging (C-index 0.68-0.69 vs 0.44-0.58). INSIGHT spatial risk maps recapitulated canonical prognostic histopathology and identified nuclear solidity and circularity as quantitative risk correlates. Integrating spatial risk with data-driven spatial transcriptomic signatures, spatial proteomics, bulk RNA-seq, and single-cell references revealed an epithelium-immune risk manifold capturing epithelial dedifferentiation and fetal programs, myeloid-driven stromal states including $\mathrm{SPP1}^{+}$ macrophages and $\mathrm{LAMP3}^{+}$ dendritic cells, and adaptive immune dysfunction. This analysis exposed patient-specific epithelial heterogeneity, stratification within MSI-High tumours, and high-risk routes of CDX2/HNF4A loss and CEACAM5/6-associated proliferative programs, highlighting coordinated therapeutic vulnerabilities.&lt;/p&gt;</description></item><item><guid>2512.22291v1</guid><title>Multi-Head Spectral-Adaptive Graph Anomaly Detection</title><link>http://arxiv.org/abs/2512.22291v1</link><author>Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种多头谱自适应图神经网络，利用轻量级超网络根据谱指纹动态生成滤波器参数，以适应不同图实例的异常检测需求，并通过双重正则化防止多头模式崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 图异常检测在金融欺诈和风险控制中应用广泛，但现有方法难以处理复杂多变的异常模式，且固定全局滤波器易导致过度平滑，抹去高频异常信号。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决固定滤波器导致的过度平滑问题，提升对高频异常信号的保留和检测效果，并实现对不同图实例的自适应处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计轻量级超网络，输入谱指纹（结构统计和Rayleigh商特征），动态生成Chebyshev滤波器参数；采用多头机制并加入教师-学生对比学习和Barlow Twins多样性损失以保持头间正交，防止模式崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在四个真实数据集上实验表明，该方法能有效保留高频异常信号，显著优于现有最先进方法，尤其在高度异质的数据集上表现出卓越鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多头谱自适应图神经网络通过动态滤波器生成和双重正则化，显著提升了图异常检测的性能和鲁棒性，为金融欺诈检测提供了更可靠的技术方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This &amp;#x27;one-size-fits-all&amp;#x27; approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a &amp;#x27;spectral fingerprint&amp;#x27; containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This &amp;#x27;one-size-fits-all&amp;#x27; approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a &amp;#x27;spectral fingerprint&amp;#x27; containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.&lt;/p&gt;</description></item><item><guid>2512.22304v1</guid><title>PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation</title><link>http://arxiv.org/abs/2512.22304v1</link><author>Darrin Bright, Rakshith Raj, Kanchan Keisham</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 PortionNet，一种跨模态知识蒸馏框架，利用点云学习几何特征，仅需 RGB 图像即可实现食物营养估计，显著提升体积和能量估计精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确的食物营养估计需要三维信息，但大多数手机缺乏深度传感器，导致基于深度的方法不可行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服单张 RGB 图像缺乏三维信息的局限，开发一种无需深度传感器即可实现高精度营养估计的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用双模训练策略，训练时使用点云学习几何特征，推理时使用轻量级适配器网络模拟点云表示，实现伪三维推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; PortionNet 在 MetaFood3D 数据集上实现了最先进的体积和能量估计性能，并在 SimpleFood45 上表现出良好的能量估计泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PortionNet 通过跨模态知识蒸馏实现了仅用 RGB 图像即可高精度食物营养估计，展示了在无深度传感器设备上的可行性和优越性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Accurate food nutrition estimation from single images is challenging due to the loss of 3D information. While depth-based methods provide reliable geometry, they remain inaccessible on most smartphones because of depth-sensor requirements. To overcome this challenge, we propose PortionNet, a novel cross-modal knowledge distillation framework that learns geometric features from point clouds during training while requiring only RGB images at inference. Our approach employs a dual-mode training strategy where a lightweight adapter network mimics point cloud representations, enabling pseudo-3D reasoning without any specialized hardware requirements. PortionNet achieves state-of-the-art performance on MetaFood3D, outperforming all previous methods in both volume and energy estimation. Cross-dataset evaluation on SimpleFood45 further demonstrates strong generalization in energy estimation.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate food nutrition estimation from single images is challenging due to the loss of 3D information. While depth-based methods provide reliable geometry, they remain inaccessible on most smartphones because of depth-sensor requirements. To overcome this challenge, we propose PortionNet, a novel cross-modal knowledge distillation framework that learns geometric features from point clouds during training while requiring only RGB images at inference. Our approach employs a dual-mode training strategy where a lightweight adapter network mimics point cloud representations, enabling pseudo-3D reasoning without any specialized hardware requirements. PortionNet achieves state-of-the-art performance on MetaFood3D, outperforming all previous methods in both volume and energy estimation. Cross-dataset evaluation on SimpleFood45 further demonstrates strong generalization in energy estimation.&lt;/p&gt;</description></item><item><guid>2512.22315v1</guid><title>VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning</title><link>http://arxiv.org/abs/2512.22315v1</link><author>Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 VideoZoomer，一种新型的代理框架，帮助多模态大型语言模型在长视频理解中动态控制视觉关注点，从而克服传统方法在帧采样和预选方面的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大型语言模型在视觉-语言任务上取得显著进展，但由于上下文窗口有限，难以处理长视频。现有方法多采用均匀帧采样或静态预选，容易忽略关键信息且无法在推理过程中纠正初始选择错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入可动态调整的时间缩放工具，使模型能够在推理过程中自主选择关键时刻的高帧率片段，逐步收集细粒度证据，从而提升长视频理解与推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; VideoZoomer 先以低帧率粗略概览视频，然后调用时间缩放工具在自主选择的时刻获取高帧率片段，形成多轮交互式证据收集。训练分两阶段：先在精心挑选的示例与反思轨迹数据集上进行冷启动监督微调，再通过强化学习进一步优化代理策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，7B 版本的 VideoZoomer 在多种长视频理解与推理基准上表现优异，能够产生多样且复杂的推理模式，持续超越现有开源模型，甚至在挑战性任务上与专有系统相当，同时在减少帧预算的情况下保持更高的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; VideoZoomer 通过动态视觉聚焦和交互式证据收集，显著提升了多模态大型语言模型在长视频任务中的性能，展示了代理式框架在视频理解领域的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大型语言模型（MLLMs）在视觉-语言任务上取得了显著进展，但在长视频理解方面仍受限于有限的上下文窗口。因此，现有方法往往依赖均匀帧采样或静态预选，可能忽略关键证据，并且无法在推理过程中纠正初始选择错误。为克服这些限制，我们提出了 VideoZoomer，一种新型代理框架，使 MLLMs 能在推理过程中动态控制其视觉关注点。从粗略的低帧率概览开始，VideoZoomer 调用时间缩放工具，在自主选择的时刻获取高帧率片段，从而以多轮交互方式逐步收集细粒度证据。我们采用两阶段训练策略：在精心策划的示例和反思轨迹数据集上进行冷启动监督微调，然后通过强化学习进一步完善代理策略。大量实验表明，我们的 7B 模型在多种长视频理解和推理基准上表现出多样且复杂的推理模式，取得强劲表现，持续超越现有开源模型，甚至在挑战性任务上与专有系统相当，同时在减少帧预算的情况下实现更高效率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.&lt;/p&gt;</description></item><item><guid>2512.22331v1</guid><title>The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma</title><link>http://arxiv.org/abs/2512.22331v1</link><author>Mariya Miteva, Maria Nisheva-Pavlova</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了利用医学影像非侵入性推断胶质母细胞瘤分子特征，特别是MGMT启动子甲基化的情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在放射基因组学中，利用影像信息预测肿瘤分子特征是重要目标。传统单模态或早期融合方法存在特征冗余和对模态特定信息建模不足的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种基于变分自编码器的多视角潜在表示学习框架，以融合T1Gd和FLAIR MRI的放射组学特征，用于MGMT启动子甲基化分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 分别使用独立的概率编码器对T1Gd和FLAIR影像进行编码，在紧凑的潜在空间进行融合，得到潜在嵌入后用于分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在保持模态特定结构的同时，实现了有效的多模态整合，并能用于MGMT启动子甲基化的分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多视角潜在表示学习框架能够克服传统方法的局限，为非侵入性预测肿瘤分子特征提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从医学影像中非侵入性推断分子肿瘤特征是放射基因组学的核心目标，尤其在胶质母细胞瘤中，MGMT启动子甲基化具有重要的预后和治疗意义。虽然基于放射组学的机器学习方法已显示出潜力，但传统的单模态和早期融合方法往往受限于高特征冗余和对模态特定信息建模不完整。在本研究中，我们提出了一个基于变分自编码器的多视角潜在表示学习框架，用于整合来自对比增强T1加权（T1Gd）和液体衰减反转恢复（FLAIR）磁共振成像的互补放射组学特征。通过使用独立的概率编码器对每个模态进行编码，并在紧凑的潜在空间中进行融合，该方法保留了模态特定结构，同时实现了有效的多模态整合。随后得到的潜在嵌入被用于MGMT启动子甲基化分类。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.&lt;/p&gt;</description></item><item><guid>2512.22349v1</guid><title>Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data</title><link>http://arxiv.org/abs/2512.22349v1</link><author>Alaa Alahmadi, Mohamed Hasan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了利用感知驱动的伪彩色技术提升深度神经网络在心电图（ECG）分析中的可解释性和少样本学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 机器视觉模型，尤其是深度神经网络，已广泛用于生理信号解释，但通常需要大量训练数据且缺乏对预测因果特征的解释，限制了其临床可靠性和与人类推理的一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证感知驱动的伪彩色方法能否在复杂生理数据分析中提升模型的可解释性和少样本学习性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将QT间期等临床重要时间特征编码为结构化颜色表示，使用原型网络和ResNet-18架构，在单个心跳周期和完整10秒节律的ECG图像上进行一次样本和少样本学习，并通过聚合多次心跳来进一步提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 伪彩色引导模型关注临床意义明显的ECG特征，抑制无关信号；在仅有一到五个训练样本的情况下即可学习到可解释且判别性强的特征；聚合多次心跳可进一步提升准确率，类似人类对心跳的平均感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 人类感知式的编码方式能够在数据稀缺的情况下实现高效学习、可解释性和因果推理，为医学机器智能提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data. We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data.   We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.&lt;/p&gt;</description></item><item><guid>2512.22351v1</guid><title>VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement</title><link>http://arxiv.org/abs/2512.22351v1</link><author>Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文通过解决三大关键挑战，弥补了多模态大型语言模型在复杂三维场景操作中的不足。首先，针对模型视觉定位弱的问题，提出了基于MCP的API，将交互从脆弱的原始代码改为更稳健的函数级更新；其次，利用一套专门的视觉工具增强模型对三维场景的理解，收集空间信息并验证操作结果，形成感知反馈循环；再次，构建协作多智能体框架，分别负责规划、执行和验证，提升对多步指令的鲁棒性并纠正中间错误。实验在25个复杂物体排列任务上验证，显著优于现有基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大型语言模型在二维视觉-语言任务上取得显著进展，但在复杂三维场景操作方面仍未得到充分探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补多模态大型语言模型在三维物体排列任务中的应用空白，提升其在三维场景中的操作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 引入MCP‑based API，实现函数级更新以增强视觉定位；2. 结合专用视觉工具进行场景状态分析、空间信息收集和动作验证，形成感知反馈循环；3. 设计协作多智能体框架，分工规划、执行与验证，处理迭代更新中的错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在25个复杂物体排列任务中，所提方法显著优于现有基线，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过函数级API、视觉反馈工具和多智能体协作，本文成功提升了多模态大型语言模型在三维场景操作中的鲁棒性和精确度，为未来相关研究奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管多模态大型语言模型在二维视觉-语言任务上取得了显著进展，但它们在复杂三维场景操作中的应用仍未得到充分探索。本文通过解决三大关键挑战，弥补了这一空白。首先，为了解决多模态大型语言模型在视觉定位方面的弱点，我们引入了基于MCP的API，将交互从脆弱的原始代码改为更稳健的函数级更新。其次，我们通过一套专门的视觉工具增强模型对三维场景的理解，分析场景状态、收集空间信息并验证动作结果，形成感知反馈循环，弥合了基于语言的更新与精确三维感知操作之间的差距。再次，为了管理迭代且易出错的更新，我们提出了一个协作多智能体框架，设定规划、执行和验证的专门角色。该分解使系统能够稳健地处理多步指令并从中间错误中恢复。我们在25个复杂物体排列任务上验证了该方法的有效性，显著优于现有基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 解决复杂3D物体排列任务，让AI能根据文字指令和图像逐步移动、旋转或放置物体，保持物理可行性。重要因为人类日常会重新布置环境，机器人和AI需要具备空间推理和多步规划能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到现有MLLM只能做一次性编辑，缺乏多步推理。借鉴了MLLM工具调用、Model Context Protocol、FirePlace、ScanEdit等工作，设计了Planner-Executor-Evaluator三代理框架，并引入MCP工具和约束求解器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是让MLLM在计划-执行-评估循环中逐步完成任务。流程：Planner根据指令和历史渲染生成动作和目标坐标；Executor用视觉工具选取对象、构造几何约束并求解碰撞自由姿态；Evaluator检查物理和语义质量；若不合格则回溯，循环直至完成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①工具增强的MLLM与约束求解器；②协作多代理分工；③自适应回溯搜索；④新建25个多步排列基准。与以往单步、无工具、无回溯的3D排列方法不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VULCAN通过多代理、工具调用和自适应回溯，使MLLM能够从文字和图像中迭代完成物理可行的3D物体排列，显著优于以往单步方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM&amp;#x27;s 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io&lt;/p&gt;</description></item><item><guid>2512.22388v1</guid><title>BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks</title><link>http://arxiv.org/abs/2512.22388v1</link><author>Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; BLISS是一种基于多臂赌博机的动态重要性采样策略，能够在图神经网络中高效选择最具信息量的节点，从而降低大图训练的计算和内存开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 图神经网络在处理图结构数据时需要遍历每个节点的所有邻居，导致在大规模图上出现显著的计算和内存瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出BLISS以动态选择最重要的节点，解决传统静态采样方法在大图上的局限性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BLISS利用多臂赌博机在每一层动态平衡探索与利用，选择最具信息量的节点；并根据图卷积网络和图注意力网络的聚合机制调整其采样策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，BLISS在保持或超过全批训练准确率的同时，显著降低了计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BLISS为大规模图神经网络提供了一种可扩展、适应性强的采样方法，提升了模型性能并降低了资源消耗。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图神经网络（GNNs）是从图结构数据中学习的强大工具，但它们在大图上的应用受到计算成本的阻碍。每个节点需要处理所有邻居的需求导致内存和计算瓶颈。为了解决这个问题，我们引入了BLISS，一种Bandit Layer Importance Sampling Strategy。它使用多臂赌博机动态选择每一层最具信息量的节点，平衡探索和利用，确保全面覆盖图。与现有的静态采样方法不同，BLISS适应节点重要性的变化，导致更有信息的节点选择和更好的性能。它通过与图卷积网络（GCNs）和图注意力网络（GATs）集成，适应它们特定的聚合机制，展示了其多功能性。实验表明，BLISS保持或超过全批训练的准确率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.&lt;/p&gt;</description></item><item><guid>2512.22428v1</guid><title>Causality-Inspired Safe Residual Correction for Multivariate Time Series</title><link>http://arxiv.org/abs/2512.22428v1</link><author>Jianxiang Xie, Yuncheng Hua</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 现代多变量预测器表现强劲但存在系统性误差；2. 现有残差校正方法可能导致可靠预测被过度校正；3. CRC框架通过因果编码器和混合校正器实现安全校正；4. 四重安全机制确保不退化；5. 实验验证CRC提升准确率并保持高不退化率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现代多变量预测器在基准测试中表现强劲，但在特定变量或预测时段存在系统性误差，并缺乏部署时的性能不退化保证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出CRC框架，确保残差校正后模型性能不下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用因果启发的编码器分离自变量和跨变量动力学，使用混合校正器建模残差误差，并通过四重安全机制防止有害更新。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CRC在多数据集和预测骨干上持续提升准确率，且核心安全机制实现了极高的不退化率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CRC是一种安全可靠的残差校正框架，适合部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然现代多变量预测器如Transformer和GNN在基准测试中表现强劲，但它们往往在特定变量或预测时段出现系统性误差，并且在部署时缺乏性能不下降的保证。现有的事后残差校正方法试图修正这些误差，但本质上是贪婪的：它们可能提升平均准确率，却也可能通过过度校正可靠预测导致在未见场景中的局部失败。为解决这一关键的“安全缺口”，我们提出了CRC（因果启发的安全残差校正），一种专门设计以确保不退化的即插即用框架。CRC遵循分而治之的理念：使用因果启发的编码器通过分离自变量和跨变量动力学来揭示方向感知结构，并使用混合校正器来建模残差误差。关键的是，校正过程受到严格的四重安全机制的约束，防止有害更新。我们在多个数据集和预测骨干上进行实验，结果表明CRC始终提升准确率，而深入的消融研究证实其核心安全机制确保了极高的不退化率（NDR），使CRC成为安全可靠部署的校正框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also &amp;quot;help in the wrong way&amp;quot; by overcorrecting reliable predictions and causing local failures in unseen scenarios.   To address this critical &amp;quot;safety gap,&amp;quot; we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.   Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.&lt;/p&gt;</description></item><item><guid>2512.22428v2</guid><title>Causality-Inspired Safe Residual Correction for Multivariate Time Series</title><link>http://arxiv.org/abs/2512.22428v2</link><author>Jianxiang Xie, Yuncheng Hua, Mingyue Cheng, Flora Salim, Hao Xue</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; • 现代多变量预测模型（如Transformer和GNN）表现优异，但在特定变量或预测时段会出现系统性误差，且缺乏部署时性能不退化的保证。  • 现有的后处理残差校正方法虽能提升平均精度，却可能因过度校正可靠预测而在未见场景中产生局部失效。  • 为填补这一安全空白，本文提出CRC（因果启发式安全残差校正）框架，旨在确保校正后模型性能不下降。  • CRC采用因果启发式编码器分离自变量与交叉变量动力学，并使用混合校正器建模残差误差。  • 校正过程受四重安全机制约束，防止有害更新。  • 在多数据集和多预测骨干上实验表明，CRC持续提升准确率，且消融实验验证其安全机制能实现极高的非退化率。  • 综上，CRC是一种安全可靠的校正框架，适合在实际部署中使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现代多变量预测模型（如Transformer和GNN）在基准测试中表现强劲，但常在特定变量或预测时段出现系统性误差，并且缺乏在部署时保证性能不退化的机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种名为CRC的因果启发式安全残差校正框架，以确保模型校正后性能不下降，填补现有方法的安全缺口。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CRC框架包括：1）因果启发式编码器，用于揭示方向感知结构并分离自变量与交叉变量动力学；2）混合校正器，用于建模残差误差；3）四重安全机制，严格控制校正过程，防止有害更新。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果显示，CRC在多数据集和多预测骨干上持续提升准确率；消融研究证明其核心安全机制能实现极高的非退化率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CRC是一种安全可靠的残差校正框架，适合在实际部署中使用，能够在提升准确率的同时保证性能不退化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现代多变量预测器（如Transformer和GNN）在基准测试中表现强劲，但常在特定变量或预测时段出现系统性误差，并且缺乏在部署时保证性能不退化的机制。现有的后处理残差校正方法虽能提升平均精度，却可能因过度校正可靠预测而在未见场景中产生局部失效。为填补这一安全空白，本文提出CRC（因果启发式安全残差校正）框架，旨在确保校正后模型性能不下降。CRC采用因果启发式编码器分离自变量与交叉变量动力学，并使用混合校正器建模残差误差。校正过程受四重安全机制约束，防止有害更新。在多数据集和多预测骨干上实验表明，CRC持续提升准确率，且消融实验验证其安全机制能实现极高的非退化率。综上，CRC是一种安全可靠的校正框架，适合在实际部署中使用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also &amp;quot;help in the wrong way&amp;quot; by overcorrecting reliable predictions and causing local failures in unseen scenarios.   To address this critical &amp;quot;safety gap,&amp;quot; we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.   Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.&lt;/p&gt;</description></item><item><guid>2512.22439v2</guid><title>SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems</title><link>http://arxiv.org/abs/2512.22439v2</link><author>Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LiDAR感知受限于固定的垂直束分辨率和环境遮挡导致的束掉落。本文提出SuperiorGAT，一种基于图注意力的框架，利用束感知图和门控残差融合与前馈细化，能够在不加深网络的情况下准确重建稀疏点云中的缺失高程信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶系统中，LiDAR感知受限于固定的垂直束分辨率，并因环境遮挡导致束掉落，进一步削弱了点云的稠密度和精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不增加网络深度的前提下，重建稀疏LiDAR点云中的缺失高程信息，以提升感知质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将LiDAR扫描建模为束感知图，采用图注意力网络；通过门控残差融合和前馈细化实现高效重建；在KITTI数据集上通过每四束移除一次的方式模拟结构化束掉落，进行实验评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SuperiorGAT在多种KITTI环境（人、道路、校园、城市）中，重建误差更低，几何一致性更好，优于基于PointNet的模型和更深的GAT基线；X-Z投影显示其能保持结构完整，垂直失真最小。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过架构优化，可在不增加硬件的情况下，以计算效率高的方式提升LiDAR分辨率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model&amp;#x27;s ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; The paper tackles the problem of missing elevation data in sparse LiDAR point clouds caused by beam dropout. This issue is critical because LiDAR is a core sensor for autonomous vehicles, and missing height information can degrade object detection, mapping, and path planning. Low‑resolution LiDAR sensors are common in cost‑effective vehicles, so an efficient reconstruction method is needed for real‑time operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; The authors built on their earlier conference work that used multi‑layer GATs for z‑reconstruction. They realized that increasing depth made the model heavy and unstable, so they redesigned the architecture to be lightweight while still powerful. They incorporated ideas from GCNs, GATs, and residual connections, and used realistic beam‑dropout simulation to train and test the model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; The core idea is to treat a LiDAR scan as a graph where each point is a node and edges reflect the sensor’s beam pattern. A single‑layer graph attention network with multi‑head attention learns how to weight neighboring points, and a gated residual fusion layer blends the raw and refined features. A feed‑forward block further refines the output, producing accurate z‑values for the missing points.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; Key innovations include a realistic beam‑dropout simulation framework, a beam‑aware graph construction that uses beam indices, a lightweight single‑layer GAT with gated residual fusion and feed‑forward refinement, and multi‑head attention. Compared to previous work, the method achieves higher accuracy with far fewer parameters, avoiding the need for deep stacks of attention layers and dense CNNs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SuperiorGAT delivers a real‑time, beam‑aware graph attention network that accurately reconstructs missing LiDAR elevation data, outperforming deeper GATs and PointNet while keeping the model lightweight.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model&amp;#x27;s ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.&lt;/p&gt;</description></item><item><guid>2512.22463v1</guid><title>MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression</title><link>http://arxiv.org/abs/2512.22463v1</link><author>Kai-Hsiang Hsieh, Monyneath Yim, Wen-Hsiao Peng, Jui-Chiu Chiang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为MEGA-PCC的端到端学习框架，用于点云几何与属性的联合压缩，利用共享编码器、双解码器和基于Mamba的熵模型，实现了更高效的压缩性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 点云几何与属性的联合压缩对高效3D数据表示至关重要，但现有方法往往需要后处理重色和手工调节比特率，阻碍了端到端优化并增加了系统复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服上述局限，提出MEGA-PCC，实现无重色、数据驱动的比特率分配，并简化压缩流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MEGA-PCC包含主压缩模型：共享编码器将几何与属性编码为统一潜在表示，随后两个解码器按顺序重建几何和属性；以及Mamba基熵模型，利用空间和通道相关性提升概率估计。两模型均基于Mamba架构，能捕捉长程依赖与丰富上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，MEGA-PCC在率失真性能和运行时效率上均优于传统和基于学习的基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MEGA-PCC为AI驱动的点云压缩提供了强有力的解决方案，具备更高效、简化的端到端流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 点云几何与属性的联合压缩对于高效的3D数据表示至关重要。现有方法往往依赖后期重色处理和手动调节几何与属性比特流之间的比特率，这阻碍了端到端优化并增加了系统复杂度。为克服这些限制，我们提出了MEGA-PCC，一种完全端到端、基于学习的框架，包含两个专门的联合压缩模型。主压缩模型使用共享编码器将几何和属性信息编码为统一的潜在表示，然后通过双解码器依次重建几何和属性。补充地，基于Mamba的熵模型（MEM）通过捕捉空间和通道相关性来提升熵编码的概率估计。两种模型均基于Mamba架构，以有效建模长程依赖和丰富的上下文特征。通过消除重色和启发式比特率调节的需求，MEGA-PCC在训练期间实现了数据驱动的比特率分配，并简化了整体流程。大量实验表明，MEGA-PCC在率失真性能和运行时效率方面优于传统和基于学习的基线，为AI驱动的点云压缩提供了强大的解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云几何和属性的联合压缩问题。传统方法需要在几何和属性之间进行 recoloring，并手动分配比特率，导致压缩效率低且难以端到端优化。高效的点云压缩对沉浸式技术、自动驾驶和远程感知等领域至关重要，因为它们需要在有限带宽下传输大量三维数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了 recoloring 步骤和手工比特率分配的瓶颈，然后提出使用共享编码器和双解码器的统一架构，以消除中间步骤。该设计借鉴了 Mamba 结构的状态空间模型（SSM）以及之前的 Mamba-PCGC、SparsePCGC 等学习式点云压缩工作，并将其扩展到同时处理几何和属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个共享的稀疏卷积+Mamba 编码器将几何和属性编码为统一潜在表示，然后先解码几何，再利用解码后的几何引导属性解码。整个流程包括：体素化点云 → 共享编码器（多方向 SSM）→ 潜在表示 → 先解码几何 → 以几何为条件解码属性 → 采用 Mamba‑based Entropy Model 进行熵编码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 单编码器双解码器的统一架构，消除 recoloring；2) 在编码器、解码器和熵模型中使用 Mamba 以捕获长程依赖；3) 端到端训练实现自动比特率分配，避免手工匹配。与以往方法相比，MEGA‑PCC 不需要手工调节比特率，也不需要耗时的模型匹配，且在压缩质量和运行速度上均优于传统和学习式基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MEGA‑PCC 提供了一种基于 Mamba 的端到端点云几何与属性联合压缩框架，自动学习比特率分配，消除 recoloring 步骤，并在压缩效率和速度上显著优于现有方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Joint compression of point cloud geometry and attributes is essential for efficient 3D data representation. Existing methods often rely on post-hoc recoloring procedures and manually tuned bitrate allocation between geometry and attribute bitstreams in inference, which hinders end-to-end optimization and increases system complexity. To overcome these limitations, we propose MEGA-PCC, a fully end-to-end, learning-based framework featuring two specialized models for joint compression. The main compression model employs a shared encoder that encodes both geometry and attribute information into a unified latent representation, followed by dual decoders that sequentially reconstruct geometry and then attributes. Complementing this, the Mamba-based Entropy Model (MEM) enhances entropy coding by capturing spatial and channel-wise correlations to improve probability estimation. Both models are built on the Mamba architecture to effectively model long-range dependencies and rich contextual features. By eliminating the need for recoloring and heuristic bitrate tuning, MEGA-PCC enables data-driven bitrate allocation during training and simplifies the overall pipeline. Extensive experiments demonstrate that MEGA-PCC achieves superior rate-distortion performance and runtime efficiency compared to both traditional and learning-based baselines, offering a powerful solution for AI-driven point cloud compression.&lt;/p&gt;</description></item><item><guid>2512.22481v1</guid><title>SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding</title><link>http://arxiv.org/abs/2512.22481v1</link><author>Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SPECTRE 是一种针对表面肌电信号的自监督学习框架，利用生理学相关的预训练任务和圆柱形位置编码，显著提升了运动解码性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 从非侵入性表面肌电信号中解码细粒度运动面临信号非平稳和低信噪比的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种专门针对肌电信号的自监督学习框架，以克服通用框架的局限性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 通过对聚类后的短时傅里叶变换表示进行掩码预测伪标签的预训练，学习稳健的频率模式。2. 采用圆柱旋转位置嵌入，将时间和空间维度分离，显式建模前臂电极阵列的拓扑结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SPECTRE 在多个数据集（包括截肢者数据）上实现了新的最先进水平，明显优于监督基线和通用自监督方法；消融实验验证了谱预训练和圆柱位置编码的关键作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SPECTRE 为实用的肌电接口提供了稳健的基础，能够处理真实世界中的肌电复杂性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从非侵入性表面肌电信号中解码细粒度运动是义肢控制的挑战，因为信号非平稳且信噪比低。通用自监督学习框架往往在肌电信号上表现不佳，因为它们试图重建噪声原始信号，且缺乏对电极阵列圆柱拓扑的先验偏置。为克服这些限制，我们提出了 SPECTRE，一种面向领域的自监督学习框架。SPECTRE 主要贡献包括：基于生理学的预训练任务和一种新颖的位置编码。预训练任务通过对聚类后的短时傅里叶变换表示进行掩码预测伪标签，迫使模型学习稳健且与生理学相关的频率模式。此外，我们的圆柱旋转位置嵌入（CyRoPE）将嵌入分解为线性时间维度和环形空间维度，显式建模前臂传感器拓扑，以捕捉肌肉协同。对多个数据集（包括截肢者的挑战性数据）的评估表明，SPECTRE 在运动解码方面设定了新的最先进水平，显著优于监督基线和通用自监督方法。消融研究验证了谱预训练和 CyRoPE 的关键作用。SPECTRE 为能够处理真实世界肌电复杂性的实用肌电接口提供了稳健基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Decoding fine-grained movement from non-invasive surface Electromyography (sEMG) is a challenge for prosthetic control due to signal non-stationarity and low signal-to-noise ratios. Generic self-supervised learning (SSL) frameworks often yield suboptimal results on sEMG as they attempt to reconstruct noisy raw signals and lack the inductive bias to model the cylindrical topology of electrode arrays. To overcome these limitations, we introduce SPECTRE, a domain-specific SSL framework. SPECTRE features two primary contributions: a physiologically-grounded pre-training task and a novel positional encoding. The pre-training involves masked prediction of discrete pseudo-labels from clustered Short-Time Fourier Transform (STFT) representations, compelling the model to learn robust, physiologically relevant frequency patterns. Additionally, our Cylindrical Rotary Position Embedding (CyRoPE) factorizes embeddings along linear temporal and annular spatial dimensions, explicitly modeling the forearm sensor topology to capture muscle synergies. Evaluations on multiple datasets, including challenging data from individuals with amputation, demonstrate that SPECTRE establishes a new state-of-the-art for movement decoding, significantly outperforming both supervised baselines and generic SSL approaches. Ablation studies validate the critical roles of both spectral pre-training and CyRoPE. SPECTRE provides a robust foundation for practical myoelectric interfaces capable of handling real-world sEMG complexities.&lt;/p&gt;</description></item><item><guid>2512.22488v1</guid><title>Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment</title><link>http://arxiv.org/abs/2512.22488v1</link><author>Hassan Wasswa, Timothy Lynar</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种可扩展的自适应物联网威胁检测框架，消除了持续分类器再训练的需求，并通过潜在空间表示、对齐模型和图神经网络实现对概念漂移的鲁棒检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; AI模型在物联网威胁检测中已取得高准确率，但其在企业环境中的部署受限于使用静态数据集，无法反映真实流量的动态变化，且现有方案依赖周期性再训练，导致计算开销大且易遗忘。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种可扩展的框架，能够在不连续再训练分类器的情况下实现自适应物联网威胁检测，并有效应对概念漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先在历史流量的潜在空间表示上训练一次分类器；使用对齐模型将实时流量映射到该潜在空间；将低维潜在表示转换为图结构，并利用图神经网络进行分类，从而保持对已观察攻击的知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在真实异构物联网流量数据集上的实验表明，该框架在概念漂移条件下保持了稳健的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架具有在动态且大规模物联网环境中实际部署的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种可扩展的自适应物联网威胁检测框架，消除了持续分类器再训练的需求。该方法首先在历史流量的潜在空间表示上训练分类器，然后使用对齐模型将实时流量映射到已学习的潜在空间，再通过将低维潜在表示转换为图结构并使用图神经网络进行分类，从而保持对已观察攻击的知识。实验结果表明，该框架在概念漂移下保持了稳健的检测性能，显示出在动态大规模物联网环境中的实用部署潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although AI-based models have achieved high accuracy in IoT threat detection, their deployment in enterprise environments is constrained by reliance on stationary datasets that fail to reflect the dynamic nature of real-world IoT NetFlow traffic, which is frequently affected by concept drift. Existing solutions typically rely on periodic classifier retraining, resulting in high computational overhead and the risk of catastrophic forgetting. To address these challenges, this paper proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining. The proposed approach trains a classifier once on latent-space representations of historical traffic, while an alignment model maps incoming traffic to the learned historical latent space prior to classification, thereby preserving knowledge of previously observed attacks. To capture inter-instance relationships among attack samples, the low-dimensional latent representations are further transformed into a graph-structured format and classified using a graph neural network. Experimental evaluations on real-world heterogeneous IoT traffic datasets demonstrate that the proposed framework maintains robust detection performance under concept drift. These results highlight the framework&amp;#x27;s potential for practical deployment in dynamic and large-scale IoT environments.&lt;/p&gt;</description></item><item><guid>2512.22489v2</guid><title>Tracking by Predicting 3-D Gaussians Over Time</title><link>http://arxiv.org/abs/2512.22489v2</link><author>Tanish Baranwal, Himanshu Gaurav Singh, Jathushan Rajasegaran, Jitendra Malik</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 提出了Video-GMAE，一种自监督方法，将视频序列编码为随时间移动的高斯斑点集合，利用高斯表示的先验偏置捕捉动态三维场景的投影特征。通过预训练网络，模型自然学习到跟踪能力，映射高斯轨迹到图像平面即可实现零样本跟踪，性能与最先进方法相当。对Kinetics和Kubric数据集进行小规模微调后，模型分别提升了34.6%和13.1%，超过了现有自监督视频方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在视频表示学习中，自监督方法需要有效的先验偏置来捕捉空间与时间的关联。传统方法往往缺乏对三维动态场景的自然建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将视频表示为移动的高斯斑点，提供一种合理的先验偏置，使模型能够学习到动态三维结构并实现跟踪与表示学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计Video-GMAE架构，使用自监督预训练将视频序列编码为一组随时间移动的高斯斑点；随后将高斯轨迹投影到图像平面，用于零样本跟踪；在Kinetics和Kubric上进行小规模微调以提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 预训练后模型自然出现跟踪能力；零样本跟踪性能与最先进方法相当；微调后在Kinetics和Kubric上分别提升34.6%和13.1%，优于现有自监督视频方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Video-GMAE通过高斯斑点表示为视频提供了有效的先验偏置，既能实现自然跟踪，又能显著提升自监督视频表示学习的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了视频高斯掩码自编码器（Video-GMAE），这是一种自监督的表示学习方法，它将一系列图像编码为随时间移动的一组高斯斑点。将视频表示为一组高斯斑点强加了一个合理的归纳偏置：二维视频往往是动态三维场景的合理投影。我们发现，当使用这种架构对网络进行预训练时，跟踪能力会自然出现。将学习到的高斯轨迹映射到图像平面即可实现零样本跟踪，其性能与最先进方法相当。通过小规模微调，我们的模型在Kinetics数据集上提升了34.6%，在Kubric数据集上提升了13.1%，超过了现有的自监督视频方法。项目页面和代码公开可用，网址为 https://videogmae.org/ 和 https://github.com/tekotan/video-gmae。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在通过自监督学习从无标签视频中学习像素级对应关系，从而实现点跟踪。点跟踪是理解场景结构、运动和三维重建的基础，对计算机视觉和机器人等领域具有重要意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现传统的自监督视频预训练（如VideoMAE、MAE-ST）在点跟踪上效果不佳，认为缺乏对时间一致性的强约束。于是借鉴了可微分渲染中的三维高斯分布（Gaussian Splatting）和自监督掩码自编码器的思想，设计了在视频中预测高斯原语及其时间增量的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用可微分的三维高斯原语来表示视频帧，并通过预测第一帧的原语以及后续帧的增量来强制时间对应。实现流程包括：用ViT编码器对掩码视频帧进行特征提取；解码器生成第一帧的高斯原语和后续帧的增量；将增量递归累加得到每帧原语；用可微分高斯渲染重建视频并计算重建损失；在此基础上可直接从高斯轨迹生成流场，实现零样本点跟踪。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 将三维高斯分布与自监督掩码自编码器结合，首次在视频预训练中显式强制时间对应；2) 通过预测增量而非完整原语，保持原语身份并实现长时序对应；3) 提供零样本点跟踪算法，并在Kinetics、Kubric等数据集上显著提升性能。与以往仅关注图像重建或对比学习的自监督方法不同，本文通过可微分渲染直接学习三维运动表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Video-GMAE通过自监督学习三维高斯原语及其时间增量，构建了可微分的时间对应表示，实现零样本点跟踪并在多项视频任务中取得领先表现。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.&lt;/p&gt;</description></item><item><guid>2512.22495v1</guid><title>The Quest for Winning Tickets in Low-Rank Adapters</title><link>http://arxiv.org/abs/2512.22495v1</link><author>Hamed Damirchi, Cristian Rodriguez-Opazo, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了彩票假设（LTH）是否适用于参数高效微调（PEFT），尤其是低秩适配（LoRA）方法，并提出了 Partial-LoRA 方法，通过识别稀疏子网络并训练与任务相关子空间对齐的低秩适配器，在多任务和单任务设置下显著减少可训练参数，同时保持或提升准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大型预训练模型的广泛使用，研究者关注如何在保持性能的同时降低微调成本。彩票假设指出，过参数化网络中存在稀疏子网络能够匹配完整模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证彩票假设是否同样适用于低秩适配方法，并基于此提出更高效的微调策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先在 LoRA 适配器中寻找稀疏子网络，观察其性能；随后提出 Partial-LoRA，系统识别并训练与任务相关子空间对齐的稀疏低秩适配器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 彩票假设在 LoRA 中成立；稀疏子网络的效果主要取决于每层的稀疏程度，而非具体权重；Partial-LoRA 能将可训练参数减少多达 87%，同时保持或提升准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文深化了对迁移学习及预训练与微调关系的理论理解，并为开发更高效的适配策略提供了新思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 彩票假设（LTH）认为，过参数化的神经网络中包含稀疏子网络（“中奖票”），这些子网络在从零开始训练时能够匹配完整模型的性能。随着对大型预训练模型微调的依赖日益增加，我们探讨了 LTH 是否适用于参数高效微调（PEFT），特别关注低秩适配（LoRA）方法。我们的主要发现是，LTH 在 LoRA 中成立，揭示了能够匹配稠密适配器性能的稀疏子网络。具体而言，我们发现稀疏子网络的有效性更取决于每层应用的稀疏程度，而非子网络中包含的具体权重。基于这一洞察，我们提出了 Partial-LoRA，一种系统识别这些子网络并训练与预训练模型任务相关子空间对齐的稀疏低秩适配器的方法。在单任务和多任务设置下，对 8 个视觉任务和 12 个语言任务的实验表明，Partial-LoRA 将可训练参数数量减少多达 87%，同时保持或提升准确率。我们的结果不仅深化了对迁移学习以及预训练与微调之间相互作用的理论理解，也为开发更高效的适配策略开辟了新途径。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks (&amp;quot;winning tickets&amp;quot;) capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.&lt;/p&gt;</description></item><item><guid>2512.22503v1</guid><title>SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration</title><link>http://arxiv.org/abs/2512.22503v1</link><author>Xin Chen, Kang Luo, Yangyi Xiao, Hesheng Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了SCAFusion，一种针对月球探测任务的多模态3D目标检测模型，显著提升了对小型不规则目标的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在月球表面探测中，精确检测小型不规则物体（如陨石碎片和岩石）对自主导航至关重要，但现有地面自动驾驶的多模态3D感知方法在外星环境中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种专门针对月球机器人任务的多模态3D检测模型，以克服特征对齐不足、模态协同有限和小目标检测弱的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在BEVFusion框架基础上，SCAFusion集成了认知适配器、对比对齐模块、相机辅助训练分支以及专为小目标设计的分区感知坐标注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes验证集上，SCAFusion实现了69.7%的mAP和72.1%的NDS，分别比基线提升5.0%和2.7%；在Isaac Sim构建的模拟月球环境中，mAP达到90.93%，比基线提升11.5%，在检测小型陨石类障碍物方面表现尤为突出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SCAFusion在保持参数和计算量几乎不变的前提下，显著提升了月球环境中小型不规则目标的检测效果，为月球机器人自主导航提供了可靠的感知支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可靠且精确地检测小型不规则物体（如陨石碎片和岩石）对于月球表面探测中的自主导航和操作至关重要。现有为陆地自动驾驶设计的多模态3D感知方法在外星环境中往往表现不佳，原因包括特征对齐不佳、模态协同有限以及小目标检测弱。本文提出了SCAFusion，一种专为月球机器人任务定制的多模态3D目标检测模型。基于BEVFusion框架，SCAFusion集成了认知适配器以高效调优相机骨干网络、对比对齐模块以增强相机与激光雷达特征的一致性、相机辅助训练分支以强化视觉表征，并最重要的是，设计了分区感知坐标注意力机制，专门提升小型不规则目标的检测性能。通过几乎不增加参数和计算量，模型在nuScenes验证集上实现了69.7%的mAP和72.1%的NDS，分别比基线提升5.0%和2.7%。在Isaac Sim构建的模拟月球环境中，SCAFusion实现了90.93%的mAP，超过基线11.5%，在检测小型陨石类障碍物方面取得显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提高月球表面探测任务中对小型、形状不规则物体（如陨石碎片、岩石）的三维检测精度。准确识别这些障碍物对机器人路径规划、避障和任务安全至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了地面自动驾驶中 BEVFusion 的局限性，发现其在小目标检测、模态对齐和视觉信息利用方面不足。随后在 BEVFusion 基础上引入 Cognitive Adapter、Contrastive Alignment Module、Camera Auxiliary Training Branch 和 Section‑aware Coordinate Attention，借鉴并改进了现有多模态融合与注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在 BEV 空间中实现高效的多模态特征对齐与增强，专门提升小目标的检测能力。实现流程为：提取相机与 LiDAR 特征 → 在相机骨干中使用 Cognitive Adapter 进行参数高效调优 → 通过 Contrastive Alignment Module 对 RGB 与深度特征进行对齐 → 将相机 BEV 与 LiDAR BEV 融合 → 通过 Section‑aware Coordinate Attention 强化小目标特征 → 在训练阶段使用 Camera Auxiliary Branch 进一步提升视觉表示 → 最终通过检测头输出 3D 检测框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) Section‑aware Coordinate Attention 机制专为小目标设计；2) Cognitive Adapter 使相机骨干可在不大幅增加参数的情况下高效适配；3) Contrastive Alignment Module 在 Lift‑Splat‑Shoot 过程中对 RGB 与深度特征进行对齐；4) Camera Auxiliary Training Branch 在训练时充分利用视觉信息。与 BEVFusion 等前置工作相比，SCAFusion 在模态对齐、视觉利用和小目标检测上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCAFusion 通过引入高效相机适配、对齐增强和面向小目标的注意力机制，在保持参数与计算成本不变的前提下，显著提升了月球表面多模态 3D 检测的精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.&lt;/p&gt;</description></item><item><guid>2512.22608v1</guid><title>LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation</title><link>http://arxiv.org/abs/2512.22608v1</link><author>Zhongyang Liu, Haoyu Pei, Xiangyi Xiao, Xiaocong Du, Yihui Li, Suting Hong, Kunpeng Zhang, Haipeng Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出SimVC-CAS系统，通过多智能体交互模拟风险投资决策，提升创业融资成功预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 创业公司价值高、失败率高，预测成功是跨学科挑战；现有方法只考虑单一决策者，忽视投资者群体的集体动态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将创业融资预测转化为群体决策任务，捕捉企业基本面与投资者网络行为，提升预测准确性并提供可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计角色扮演的投资者代理和基于图神经网络的监督交互模块，构建图结构共投网络，实现异质评估与信息交流。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在PitchBook真实数据上，SimVC-CAS相较于传统方法平均精度@10提升约25%，并提供多视角可解释推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SimVC-CAS有效模拟VC群体决策，显著提升预测性能，可推广至其他复杂群体决策场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 由于创业公司的高价值和高失败率，预测其成功已成为跨学科研究的关键挑战。现有方法通常从单一决策者的角度建模成功预测，忽视了主导现实世界风险投资决策的投资者群体的集体动态。本文提出SimVC-CAS，一种新颖的集体代理系统，将风险投资决策模拟为多智能体交互过程。通过设计角色扮演代理和基于图神经网络的监督交互模块，我们将创业融资预测重新定义为群体决策任务，捕捉企业基本面和潜在投资者网络的行为动态。每个代理代表具有独特特征和偏好的投资者，能够通过图结构的共投网络实现异质评估和真实信息交流。使用PitchBook的真实数据并严格控制数据泄漏，我们证明SimVC-CAS显著提升预测准确性，并提供可解释的多视角推理，例如相对于平均精度@10提升约25%。SimVC-CAS还为其他复杂群体决策场景提供了洞见。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.&lt;/p&gt;</description></item><item><guid>2512.22630v1</guid><title>On the Role of Discreteness in Diffusion LLMs</title><link>http://arxiv.org/abs/2512.22630v1</link><author>Ziqi Jin, Bin Wang, Xiang Lin, Lidong Bing, Aixin Sun</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨扩散模型在文本生成中的应用，指出现有方法在连续嵌入空间和离散标记层面各自满足扩散模型的部分关键属性，但存在结构性权衡。通过分析近期大型扩散语言模型，作者发现两大核心问题：一是均匀腐蚀不符合信息在不同位置的分布规律；二是逐标记的边缘训练无法在并行解码时捕捉多标记依赖。基于此，作者呼吁设计更贴合文本结构的扩散过程，以实现更连贯的语言模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 扩散模型因其并行解码和迭代细化的优势而受到关注，但文本的离散和高度结构化特性使得直接应用扩散原理面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 重新审视扩散语言建模，从扩散过程和语言建模的视角，阐明扩散机制与语言特定需求之间的差异，并提出改进方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将现有方法分为连续嵌入空间扩散和离散标记层面扩散两类，评估其满足五个关键属性的程度，并通过对大型扩散语言模型的分析识别核心问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1) 均匀腐蚀不符合信息在不同位置的分布规律；2) 逐标记的边缘训练无法在并行解码时捕捉多标记依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 需要设计更符合文本结构的扩散过程，以提升扩散语言模型的连贯性和效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 扩散模型为语言生成提供了吸引人的特性，例如并行解码和迭代细化，但文本的离散和高度结构化特性挑战了扩散原理的直接应用。本文从扩散过程和语言建模的视角重新审视扩散语言建模，并概述了将扩散机制与语言特定需求分离的五个属性。我们首先将现有方法分为嵌入空间中的连续扩散和基于标记的离散扩散。随后展示每种方法仅满足这五个关键属性中的一部分，从而体现了结构性权衡。通过对近期大型扩散语言模型的分析，我们识别出两个核心问题：（i）均匀腐蚀不尊重信息在不同位置的分布；（ii）逐标记的边缘训练无法在并行解码时捕捉多标记依赖。这些观察促使我们设计更贴合文本结构的扩散过程，并鼓励未来工作朝着更连贯的扩散语言模型方向发展。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.&lt;/p&gt;</description></item><item><guid>2512.22664v1</guid><title>Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains</title><link>http://arxiv.org/abs/2512.22664v1</link><author>Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出CLAdapter，利用注意力机制和聚类中心对大规模预训练模型的特征进行个性化增强，适配数据有限的科学领域下游任务，在10个多领域数据集上实现了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在大数据时代，计算机视觉领域受益于大型数据集和预训练模型，但在专业且数据有限的科学领域下游任务仍面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新的Cluster Attention Adapter（CLAdapter），以改进和适配从大规模数据中学习到的丰富表示，满足数据有限的下游任务需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CLAdapter通过引入注意力机制和聚类中心，对特征进行分布相关性和变换矩阵的个性化增强，使模型在微调时学习到针对不同特征集的独特表示，并提供统一接口，支持CNN、Transformer等多种架构在2D/3D场景下无缝集成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在涵盖通用、多媒体、生物、医学、工业、农业、环境、地理、材料科学、OOD和3D分析等10个数据集的实验中，CLAdapter在数据有限的科学领域实现了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CLAdapter有效释放基础视觉模型的潜力，通过自适应迁移实现了在多种数据有限科学领域的卓越表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在大数据时代，计算机视觉领域受益于大型数据集，如 LAION-2B、LAION-400M、ImageNet-21K、Kinetics 等，流行的 ViT 和 ConvNeXt 系列模型在这些数据集上进行了预训练，获得了丰富的知识。然而，在专业且数据有限的科学领域，许多下游任务仍然面临重大挑战。本文提出了一种新型的 Cluster Attention Adapter（CLAdapter），它通过注意力机制和聚类中心，对从大规模数据中学习到的丰富表示进行细化和适配，以满足各种数据有限的下游任务。CLAdapter 通过分布相关性和变换矩阵个性化增强转换后的特征，使微调后的模型能够学习针对不同特征集的独特表示，从而有效地将丰富的预训练特征迁移到各种下游场景。除此之外，CLAdapter 的统一接口设计支持在 2D 和 3D 环境中无缝集成多种模型架构，包括 CNN 和 Transformer。通过在涵盖通用、多媒体、生物、医学、工业、农业、环境、地理、材料科学、OOD 和 3D 分析等 10 个数据集上的广泛实验，CLAdapter 在多种数据有限的科学领域实现了最先进的性能，证明了其在通过自适应迁移释放基础视觉模型潜力方面的有效性。代码可在 https://github.com/qklee-lz/CLAdapter 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models&amp;#x27; adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter&amp;#x27;s unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.&lt;/p&gt;</description></item><item><guid>2512.22674v1</guid><title>Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction</title><link>http://arxiv.org/abs/2512.22674v1</link><author>Jiashu Dong, Jiabing Xiang, Lisheng Geng, Suqing Tian, Wei Zhao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出一种新的语义特征对比学习损失函数，并结合三阶段 U‑Net 架构，用于稀疏视角 X 光 CT 重建，显著提升图像质量并加快处理速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; X 光 CT 在医学成像中广泛应用，稀疏视角重建可降低辐射剂量，但易产生严重条纹伪影，深度学习方法虽有进步但仍面临挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决稀疏视角 CT 重建中的伪影问题，提升重建质量并保持低计算复杂度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计语义特征对比学习损失函数，在高层潜在空间评估语义相似度，在浅层潜在空间评估解剖相似度；采用三阶段 U‑Net：粗略重建、细节细化、语义相似度测量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在胸部正交投影数据集上，该方法比其他算法获得更好的重建质量和更快的处理速度，图像质量显著提升，计算复杂度低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法为正交 CT 重建提供了实用且高效的解决方案，兼顾图像质量与计算效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; X射线计算机断层扫描（CT）在医学成像中被广泛使用，稀疏视角重建提供了一种有效降低辐射剂量的方法。然而，病态条件往往导致严重的条纹伪影。最近基于深度学习的方法已改善重建质量，但挑战仍然存在。为解决这些挑战，我们提出了一种新颖的语义特征对比学习损失函数，该函数在高层潜在空间评估语义相似度，在浅层潜在空间评估解剖相似度。我们的方法采用三阶段基于 U‑Net 的架构：一个用于粗略重建，一个用于细节细化，一个用于语义相似度测量。在正交投影的胸部数据集上测试表明，我们的方法在保持低计算复杂度的同时，实现了优于其他算法的重建质量和更快的处理速度。结果显示图像质量显著提升，且保持低计算复杂度，使其成为正交 CT 重建的实用解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;X-ray computed tomography (CT) is widely used in medical imaging, with sparse-view reconstruction offering an effective way to reduce radiation dose. However, ill-posed conditions often result in severe streak artifacts. Recent advances in deep learning-based methods have improved reconstruction quality, but challenges still remain. To address these challenges, we propose a novel semantic feature contrastive learning loss function that evaluates semantic similarity in high-level latent spaces and anatomical similarity in shallow latent spaces. Our approach utilizes a three-stage U-Net-based architecture: one for coarse reconstruction, one for detail refinement, and one for semantic similarity measurement. Tests on a chest dataset with orthogonal projections demonstrate that our method achieves superior reconstruction quality and faster processing compared to other algorithms. The results show significant improvements in image quality while maintaining low computational complexity, making it a practical solution for orthogonal CT reconstruction.&lt;/p&gt;</description></item><item><guid>2512.22675v1</guid><title>Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning</title><link>http://arxiv.org/abs/2512.22675v1</link><author>Donghwa Kang, Shana Moothedath</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在数据稀缺环境下的去中心化多任务表示学习，提出了一种交替投影梯度与最小化算法，并证明其在时间、通信和样本复杂度上的优越性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 表示学习已被广泛用于从相关任务中提取公共特征，中心化方法已被深入研究，而去中心化方法仍缺乏系统探讨。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在多任务、有限样本、线性模型的去中心化设置中，恢复低秩特征矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出交替投影梯度与最小化算法，并给出准确性保证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 通信复杂度与目标精度无关，显著降低通信成本；数值实验验证理论，并显示在某些维度和网络拓扑下去中心化学习优于中心化联邦学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该算法在时间、通信和样本方面具有可证明的效率，适用于多任务去中心化学习场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 表示学习是一种广泛采用的框架，用于在数据稀缺环境中学习，旨在从相关任务中提取公共特征。虽然中心化方法已被广泛研究，但去中心化方法仍然很少被探索。本文研究了去中心化多任务表示学习，其中特征共享低秩结构。我们考虑多个任务，每个任务具有有限数量的数据样本，观测遵循具有任务特定参数的线性模型。在去中心化设置中，任务数据分布在多个节点上，节点之间的信息交换受到通信网络的限制。目标是恢复低秩特征矩阵，其秩远小于参数维度和任务数。我们提出了一种新的交替投影梯度与最小化算法，并给出了可证明的准确性保证。我们对时间、通信和样本复杂度进行了全面表征。重要的是，通信复杂度与目标精度无关，这显著降低了与先前方法相比的通信成本。数值模拟验证了理论分析，并展示了在不同维度和网络拓扑下去中心化学习优于中心化联邦方法的情形。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task representation learning in which the features share a low-rank structure. We consider multiple tasks, each with a finite number of data samples, where the observations follow a linear model with task-specific parameters. In the decentralized setting, task data are distributed across multiple nodes, and information exchange between nodes is constrained by a communication network. The goal is to recover the underlying feature matrix whose rank is much smaller than both the parameter dimension and the number of tasks. We propose a new alternating projected gradient and minimization algorithm with provable accuracy guarantees. We provide comprehensive characterizations of the time, communication, and sample complexities. Importantly, the communication complexity is independent of the target accuracy, which significantly reduces communication cost compared to prior methods. Numerical simulations validate the theoretical analysis across different dimensions and network topologies, and demonstrate regimes in which decentralized learning outperforms centralized federated approaches.&lt;/p&gt;</description></item><item><guid>2512.22688v1</guid><title>Autoregressive Flow Matching for Motion Prediction</title><link>http://arxiv.org/abs/2512.22688v1</link><author>Johnathan Xie, Stefan Stojanov, Cristobal Eyzaguirre, Daniel L. K. Yamins, Jiajun Wu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自回归流匹配（ARFM）方法，用于对连续序列数据进行概率建模，并在多样化视频数据集上训练，能够在较长时间范围内生成未来点轨迹。通过构建人类和机器人运动预测的基准，验证了该模型在预测复杂运动方面的有效性，并证明将预测的未来轨迹作为条件可以显著提升下游任务的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的运动预测模型多在狭窄分布上训练，难以推广到人类运动和机器人任务；而大规模视频预测虽然实现了逼真的视觉效果，却在建模复杂运动时仍存在困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 借鉴大规模视频生成的经验，开发一种能够在多样化视频数据上进行概率建模的自回归流匹配方法，并评估其在运动预测中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用自回归流匹配（ARFM）对连续序列数据进行建模，训练时采用多样化视频数据集，生成未来点轨迹；同时设计人类和机器人运动预测的基准来评估模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ARFM能够准确预测复杂运动；将机器人动作预测和人类运动预测与预测的未来轨迹相结合，可显著提升下游任务的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自回归流匹配方法在长时间范围内的运动预测中表现优异，且其预测轨迹可作为有效的条件信息，提升相关下游任务的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 运动预测已在不同背景下得到研究，模型在狭窄分布上训练并应用于人类运动预测和机器人下游任务。与此同时，最近在扩展视频预测方面的努力展示了令人印象深刻的视觉真实性，但它们在准确建模复杂运动方面仍然存在困难，尽管规模庞大。受视频生成扩展的启发，我们开发了自回归流匹配（ARFM），这是一种用于连续序列数据概率建模的新方法，并在多样化视频数据集上训练，以在较长时间范围内生成未来点轨迹。为评估我们的模型，我们开发了用于评估运动预测模型预测人类和机器人运动能力的基准。我们的模型能够预测复杂运动，并且我们证明将机器人动作预测和人类运动预测与预测的未来轨迹相结合可以显著提高下游任务性能。代码和模型公开可在 https://github.com/Johnathan-Xie/arfm-motion-prediction 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.&lt;/p&gt;</description></item><item><guid>2512.22692v1</guid><title>Learning with the $p$-adics</title><link>http://arxiv.org/abs/2512.22692v1</link><author>André F. T. Martins</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨将p-进制数域作为机器学习的基础，提出分类、回归和表示学习的理论框架，并展示其在语义网络中的应用，指出其相较于实数域的优势与未来研究方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统机器学习基于实数域，利用欧氏几何和微积分进行学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究p-进制数域作为替代实数域的可行性，构建相应的学习模型和算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过探索性理论工作，建立p-进制数域下的分类、回归和表示学习的构建块，提供学习模型和算法，并用p-进制线性网络表示Quillian语义网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; p-进制数域的层级结构和无限字符串表示使其适合代码理论和层级表示学习；能够用p-进制线性网络实现实数域无法实现的紧凑语义网络表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; p-进制数域为机器学习提供了新的框架，具有潜在优势，未来研究可进一步探索其应用与开放问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的机器学习框架在实数域上运作，并在实数（欧氏或希尔伯特）向量空间中学习表示。它们的几何属性与线性可分性、最小包围球和子空间投影等直观概念相符，基本微积分为基于梯度的优化提供工具。但这是否是唯一的选择？本文研究将一种截然不同的数域——p-进制数的超度量和非阿基米德空间——作为实数域的替代方案。p-进制数的层级结构及其作为无限字符串的解释，使其成为代码理论和层级表示学习的有吸引力工具。我们的探索性理论工作为使用p-进制数进行分类、回归和表示学习奠定了基础，提供了学习模型和算法。我们展示了如何将简单的Quillian语义网络表示为紧凑的p-进制线性网络，这在实数域上是不可行的。最后，我们讨论了由此新框架启发的开放问题和未来研究机会。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing machine learning frameworks operate over the field of real numbers ($\mathbb{R}$) and learn representations in real (Euclidean or Hilbert) vector spaces (e.g., $\mathbb{R}^d$). Their underlying geometric properties align well with intuitive concepts such as linear separability, minimum enclosing balls, and subspace projection; and basic calculus provides a toolbox for learning through gradient-based optimization.   But is this the only possible choice? In this paper, we study the suitability of a radically different field as an alternative to $\mathbb{R}$ -- the ultrametric and non-archimedean space of $p$-adic numbers, $\mathbb{Q}_p$. The hierarchical structure of the $p$-adics and their interpretation as infinite strings make them an appealing tool for code theory and hierarchical representation learning. Our exploratory theoretical work establishes the building blocks for classification, regression, and representation learning with the $p$-adics, providing learning models and algorithms. We illustrate how simple Quillian semantic networks can be represented as a compact $p$-adic linear network, a construction which is not possible with the field of reals. We finish by discussing open problems and opportunities for future research enabled by this new framework.&lt;/p&gt;</description></item><item><guid>2512.22706v1</guid><title>SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis</title><link>http://arxiv.org/abs/2512.22706v1</link><author>Paul Dobre, Jackson Cooper, Xin Wang, Hongzhou Yang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个统一的仿真框架SCPainter，结合3D高斯散点汽车资产表示和3D场景点云，并利用扩散模型生成高质量图像，从而实现真实的3D资产插入和新视角合成，评估显示能生成多样化、真实的驾驶数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自动驾驶训练需要多样化数据，现有3D资产重建缺乏光照阴影真实感，NVS方法通常与资产插入分离，难以实现场景交互与多样化场景生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个统一框架，融合3D资产插入与新视角合成，以生成更丰富、真实的驾驶训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用3D高斯散点车资产表示和3D场景点云，投影到新视角后作为条件输入给扩散模型，生成高质量图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Waymo Open数据集上，SCPainter能够实现真实的3D资产插入和新视角合成，生成多样化且真实的驾驶数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 统一的3D资产插入与新视角合成框架可有效提升训练数据多样性和真实性，进而增强自动驾驶模型的鲁棒性与安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D 资产插入和新视角合成（NVS）是自动驾驶仿真的关键组成部分，可增强训练数据的多样性。通过更丰富、覆盖更广范围情景（包括长尾驾驶场景）的训练数据，自动驾驶模型可以变得更稳健、更安全。这激发了一个统一的仿真框架，能够同时处理插入的 3D 资产的真实集成和 NVS。最近的 3D 资产重建方法能够从视频中重建动态演员，并支持将其重新插入到仿真驾驶场景中。虽然整体结构和外观可以准确，但在通过光照或阴影捕捉 3D 资产的真实感方面仍然存在困难，尤其是在将其插入场景时。与此同时，最近的 NVS 方法在合成原始记录轨迹之外的视角方面显示出有前景的结果。然而，现有方法大多将资产插入和 NVS 能力视为孤立的。为了与场景其余部分交互并实现更丰富的新场景创建，真实的 3D 资产插入应与 NVS 结合。为此，我们提出了 SCPainter（Street Car Painter），一个统一框架，将 3D 高斯散点（GS）汽车资产表示和 3D 场景点云与基于扩散的生成相结合，联合实现真实的 3D 资产插入和 NVS。3D GS 资产和 3D 场景点云一起投影到新视角，并将这些投影用于调节扩散模型以生成高质量图像。在 Waymo Open 数据集上的评估表明，我们的框架能够实现 3D 资产插入和 NVS，促进了多样化且真实的驾驶数据的创建。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在自动驾驶仿真中实现逼真的3D资产插入和新视角合成的问题。该问题重要，因为高质量、多样化的训练数据能提升自动驾驶模型的鲁棒性和安全性，而现有方法往往分别处理资产插入或新视角合成，缺乏统一且真实的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将3D高斯分裂（Gaussian Splatting）资产表示与场景点云结合，并利用扩散模型进行图像生成，借鉴了Amodal3R、VGGT、Stable Video Diffusion、R3D2和GEN3C等已有技术，形成了一个统一的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用Amodal3R重建车辆的完整3D高斯模型，再将其插入由VGGT生成的彩色点云场景中；随后将资产与场景一起投影到用户指定的新相机轨迹，得到渲染图像和掩码；最后将这些渲染结果作为条件输入到Stable Video Diffusion模型，生成具有真实光照、阴影和时间一致性的最终视频。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①统一框架同时处理资产插入和新视角合成；②使用3D高斯分裂资产与点云的联合投影作为扩散模型的条件；③通过资产专属掩码实现光照和阴影的真实渲染；④利用视频扩散模型保证时间一致性。与之前的工作相比，R3D2仅处理单帧插入，GEN3C仅做新视角合成，SCPainter将两者结合并保持连续性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCPainter通过将3D高斯分裂资产与场景点云的联合渲染作为条件，利用视频扩散模型实现了逼真且时间一致的3D资产插入与新视角合成，为自动驾驶仿真提供了统一且多样化的数据生成方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.&lt;/p&gt;</description></item><item><guid>2512.22712v1</guid><title>Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages</title><link>http://arxiv.org/abs/2512.22712v1</link><author>Anaelia Ovalle, Candace Ross, Sebastian Ruder, Adina Williams, Karen Ullrich, Mark Ibrahim, Levent Sagun</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究大型语言模型在不同语言中的推理质量，发现模型在准确率高的同时，其推理过程往往无法逻辑支持结论，尤其在非拉丁文字中误差更大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型通过链式思维提示表现出强推理能力，但跨语言推理质量是否一致尚未充分探讨。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建人工验证框架，评估模型生成的推理轨迹在多语言环境下是否逻辑支持其结论。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 收集65k条来自GlobalMMLU的推理轨迹，涵盖6种语言和6个前沿模型；通过人工标注建立错误分类体系，分析推理与结论的不一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型准确率高，但推理轨迹与结论不一致的比例显著；非拉丁文字的误差至少是拉丁文字的两倍；错误主要来自证据错误（无依据声明、事实模糊）和逻辑推理步骤错误。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 现有多语言评估方法无法完整反映模型推理能力，需要引入推理感知的评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型通过链式思维提示展现出强大的推理能力，但这种推理质量是否能跨语言迁移仍未得到充分探讨。我们提出了一个人工验证框架，用来评估模型生成的推理轨迹在不同语言中是否能逻辑支持其结论。对来自GlobalMMLU的65,000条推理轨迹进行分析，涵盖6种语言和6个前沿模型，我们发现了一个关键盲点：虽然模型在任务准确率上表现优异，但其推理往往无法支持其结论。在非拉丁文字中的推理轨迹与结论之间的不一致程度至少是拉丁文字的两倍。通过人工标注，我们构建了错误分类体系来描述这些失败，发现它们主要源于证据错误（无依据的主张、模糊事实），其次是逻辑推理步骤错误。我们的研究表明，当前的多语言评估方法无法完整反映模型的推理能力，并强调需要引入推理感知的评估框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.&lt;/p&gt;</description></item><item><guid>2512.22730v1</guid><title>Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning</title><link>http://arxiv.org/abs/2512.22730v1</link><author>Youssef Megahed, Robin Ducharme, Inok Lee, Inbal Willner, Olivier X. Miguel, Kevin Dick, Adrian D. C. Chan, Mark Walker, Steven Hawken</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究利用超声自监督预训练模型检测胎儿第一孕期囊性水肿，评估其在小样本数据集上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 囊性水肿是产前超声高危发现，关联染色体异常、结构畸形和不良妊娠结局。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨超声特定自监督预训练是否能提升深度学习检测囊性水肿的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对预训练于37万无标签超声图像的USF-MAE模型进行微调，进行二分类；使用与DenseNet-169基线相同的数据集、预处理和四折交叉验证，评估准确率、敏感度、特异度和ROC-AUC；通过Score-CAM可视化分析模型可解释性；使用Wilcoxon符号秩检验检验差异显著性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; USF-MAE在所有指标上优于DenseNet-169，平均准确率0.96，敏感度0.94，特异度0.98，ROC-AUC0.98；Score-CAM显示模型关注胎儿颈部相关区域；差异显著（p=0.0057）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 超声自监督预训练模型在囊性水肿检测中表现出更高的准确性和可解释性，可为早期筛查提供可靠工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 囊性水肿是产前超声高危发现，预示染色体异常、结构畸形和不良妊娠结果的高发率。自动检测可提高可重复性并支持可扩展的早期筛查项目，但监督式深度学习方法受限于小标签数据集。本研究评估了超声特定自监督预训练是否能促进在第一孕期超声图像中准确、稳健地检测囊性水肿。我们对预训练于37万无标签超声图像的USF-MAE进行微调，用于本研究中使用的正常对照和囊性水肿病例的二分类。性能在与DenseNet-169基线相同的精心策划超声数据集、预处理管道和四折交叉验证协议下评估，使用准确率、敏感度、特异度和受试者工作特征曲线下面积（ROC-AUC）等指标。模型可解释性通过Score-CAM可视化定性分析。USF-MAE在所有评估指标上优于DenseNet-169基线。所提模型的平均准确率为0.96，敏感度为0.94，特异度为0.98，ROC-AUC为0.98，而DenseNet-169基线为0.93、0.92、0.94和0.94。对模型预测的定性Score-CAM可视化显示通过突出胎儿颈部预期区域，既在阳性也在阴性病例中体现临床相关性。配对Wilcoxon符号秩检验确认USF-MAE所实现的性能提升具有统计学显著性（p=0.0057）。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).&lt;/p&gt;</description></item><item><guid>2512.22771v1</guid><title>Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2512.22771v1</link><author>Yiqian Li, Wen Jiang, Kostas Daniilidis</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这项研究提出了一种基于主动学习的视角选择方法，利用Fisher信息评估帧的有用性，以提升语义和动态场景建模的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 对具身代理而言，理解语义和动态信息至关重要，但相关任务的数据冗余远高于静态场景理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过主动学习框架，优先选择能为模型训练提供最大信息增益的帧，从而改进渲染质量和语义分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了一个结合语义高斯参数和变形网络的Fisher信息主动学习算法，能够同时处理语义推理和动态场景建模，并在多摄像头设置中挑选信息丰富的帧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在大规模静态图像和动态视频数据集上实验表明，该方法持续提升渲染质量和语义分割准确率，优于随机选择和基于不确定性的启发式方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提出的Fisher信息主动学习视角选择策略为语义与动态建模提供了更系统、更有效的解决方案，显著优于传统启发式方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 这项研究通过将视角选择问题视为主动学习问题，提出了一种基于Fisher信息的算法，用以量化候选视角在语义高斯参数和变形网络方面的有用性。该方法能够同时处理语义推理和动态场景建模，并在多摄像头设置中挑选信息丰富的帧。实验结果显示，该方法在大规模静态图像和动态视频数据集上持续提升渲染质量和语义分割性能，优于随机选择和基于不确定性的启发式方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在动态语义场景中如何高效选择最具信息量的视角，以减少训练成本并提升几何重建与语义分割质量。该问题在机器人、AR/VR 和大规模数字内容创建等应用中至关重要，因为这些场景往往拥有大量冗余或无信息的视角，随时间变化的几何与语义不一致会导致重建不完整或语义漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将视角选择视为主动学习问题，借鉴了 FisherRF 的 Fisher 信息量度量，并结合 4D-GS 的动态 Gaussian 以及 Feature 3DGS 的语义扩展。通过对 Gaussian 参数和变形网络分别计算 Fisher 信息，并采用对角线近似与梯度外积迹估计，形成统一的视角评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用 Fisher 信息量化候选视角对模型参数（几何、颜色、语义特征和时间变形）的信息增益。实现流程包括：① 训练当前视角集合并累计 Fisher 信息；② 对每个候选视角计算其 Fisher 信息（对角线近似）并与累计信息相乘得到期望信息增益；③ 选取信息增益最高的视角作为下一训练视角；④ 重复该过程直至满足训练目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ① 首次将 Fisher 信息驱动的 NBV 引入动态语义 3D Gaussian splatting；② 将对角线近似扩展到语义特征和变形网络；③ 提出利用梯度外积迹估计变形 MLP 的 Fisher 信息；④ 将 NBV 直接集成到 3DGS 后端，实现统一的几何、语义与动态建模。与以往随机、基于不确定性或黑盒模型的 NBV 方法不同，该方法在动态语义场景中显著提升重建与分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种基于 Fisher 信息的下一最佳视角策略，能够同时优化 3D Gaussian splatting 的几何、语义和时间变形，从而在动态场景中实现高效、精确的重建与分割。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.&lt;/p&gt;</description></item><item><guid>2512.22772v1</guid><title>GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks</title><link>http://arxiv.org/abs/2512.22772v1</link><author>Xuyan Li, Jie Wang, Zheng Yan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种通用、高效、用户友好的动态图神经网络可解释方法GRExplainer，解决了现有方法在通用性、计算成本和用户体验方面的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 动态图用于表示现实世界网络的演化，TGNN在处理此类图时表现出色，但缺乏可解释性限制了其应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够适用于不同类型TGNN、降低计算成本并提升用户体验的解释方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GRExplainer通过提取节点序列作为统一特征表示，利用广度优先搜索和时间信息构建输入序列，并采用基于RNN的生成模型实现自动连续解释生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在六个真实数据集和三种TGNN上实验表明，GRExplainer在通用性、效率和用户友好性方面优于现有基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GRExplainer为TGNN提供了首个兼具通用性、效率和易用性的解释框架，推动了动态图神经网络的实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.&lt;/p&gt;</description></item><item><guid>2512.22777v1</guid><title>Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning</title><link>http://arxiv.org/abs/2512.22777v1</link><author>Kasra Jalaldoust, Elias Bareinboim</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于因果可传输理论的零样本组合泛化算法，并展示了其在少样本学习中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 跨域泛化需要对未见目标域施加结构约束；因果可传输理论为此提供了理论基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种利用因果图和差异性oracle实现零样本组合泛化的算法，并在缺乏显式因果结构时实现监督式域适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 算法Circuit-TR从源域数据学习一组局部预测模块，然后根据因果结构将其传输并组合成目标域的预测电路；同时提出一种利用有限目标数据的监督域适应方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 理论结果表明，少样本可学习任务可通过图形电路可传输准则来表征，并将少样本泛化与电路规模复杂度联系起来；实验模拟验证了理论结论。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 电路可传输为零样本和少样本跨域泛化提供了可行路径，算法在理论和实验上均表现良好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 跨域泛化在没有对未见目标域施加结构约束的情况下是不可行的。基于因果可传输理论，我们设计了一种零样本组合泛化算法，该算法依赖于以因果图形式的定性域知识来描述域内结构，以及差异性oracle来描述域间机制共享。Circuit-TR从源域数据学习一组模块（即局部预测器），并在因果结构许可的情况下将其传输/组合以获得目标域的预测电路。此外，电路可传输使我们能够设计一种不需要显式因果结构、而是利用有限目标数据的监督域适应方案。我们的理论结果以图形电路可传输准则为基础，表征了可少样本学习任务的类别，并将少样本泛化与已建立的电路规模复杂度概念联系起来；受控模拟验证了我们的理论结果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.&lt;/p&gt;</description></item><item><guid>2512.22819v1</guid><title>Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild</title><link>http://arxiv.org/abs/2512.22819v1</link><author>Hualie Jiang, Ziyang Song, Zhiqiang Lou, Rui Xu, Minglang Tan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; DA360通过学习偏移参数和圆形填充，提升全景深度估计的零样本泛化性能，在室内外基准上显著降低误差，成为新SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 全景深度估计在室内已成熟，但在开放世界的零样本泛化差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥合室内与开放世界全景深度估计的性能差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在Depth Anything V2基础上学习偏移参数，将输出转为尺度不变估计，并在DPT解码器中加入圆形填充，消除缝隙伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DA360在室内基准降低50%误差，户外基准降低10%误差，且相对PanDA提升约30%，成为零样本全景深度估计的SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过偏移参数学习和圆形填充，DA360显著提升全景深度估计的零样本泛化能力，验证了视角域迁移的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 全景深度估计提供了一个全面的解决方案，用于捕捉完整的360度环境结构信息，为机器人和AR/VR应用带来显著好处。然而，虽然在室内环境中已被广泛研究，但其在开放世界领域的零样本泛化远不及视角图像，这些视角图像受益于丰富的训练数据。这种差距使得将视角域的能力迁移成为一个有吸引力的解决方案。为弥合这一差距，我们提出了DA360，即Depth Anything在360度全景中的适配版本。我们的关键创新是从ViT骨干学习一个偏移参数，将模型的尺度和偏移不变输出转换为尺度不变估计，直接生成良好的3D点云。此方法还将圆形填充集成到DPT解码器中，以消除缝隙伪影，确保空间连贯的深度图并保持球面连续性。在标准室内基准和我们新收集的户外数据集Metropolis上评估，DA360相较于其基础模型取得显著提升，在室内和户外基准上分别降低了50%和10%的相对深度误差。此外，DA360显著优于稳健的全景深度估计方法，在所有三个测试数据集上相对误差提升约30%，并确立了零样本全景深度估计的新状态。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现 360 度全景图像的零样本深度估计，并使输出的深度直接可用于生成无缝 3D 点云。全景深度在机器人导航、AR/VR 等应用中至关重要，但现有方法大多受限于室内数据、只能输出仿射不变深度，难以在野外环境中泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先评估了现有零样本视角深度模型，发现 DAV2 在全景任务中具有最小的域差距。随后他们在 DAV2 的 DPT 结构上加入了从 ViT 类 token 学习的偏移量模块和圆形填充，以实现尺度不变深度和无缝边界。该设计借鉴了 Depth Anything、PanDA 等先行工作，并在此基础上做了关键改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习一个全局偏移量，将 DAV2 输出的仿射不变视差转换为尺度不变视差，并使用圆形填充消除全景图像的边界伪影。实现流程为：①用预训练的 DAV2 作为起点；②在 ViT 的 class token 上接一个 MLP 预测偏移量；③在 DPT 解码器中将标准零填充改为圆形填充；④将真实深度反转为视差，使用尺度不变损失对网络进行微调；⑤在合成全景数据上训练后得到可直接生成 3D 点云的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①学习偏移量实现尺度不变深度；②在解码器中使用圆形填充消除左右边界伪影；③构建了新的户外全景基准 Metropolis；④在零样本设置下直接生成可用的 3D 点云。与之前的 PanDA 等方法不同，前者仅输出仿射不变深度并需后处理，且未解决边界问题；本工作在保持零样本优势的同时，显著提升了尺度精度和空间连贯性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DA360 通过在预训练的视角深度模型中学习全局偏移量并采用圆形填充，将零样本视角深度迁移到全景图像，实现可直接生成无缝 3D 点云的尺度不变深度估计，并在室内外基准上刷新了零样本全景深度的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model&amp;#x27;s scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.&lt;/p&gt;</description></item><item><guid>2512.22872v1</guid><title>Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs</title><link>http://arxiv.org/abs/2512.22872v1</link><author>Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael B. Gotway, Jianming Liang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了Lamps模型，利用胸部X光图像的多视角一致性、连贯性和层次性进行自监督学习，显著提升了医学影像基础模型的鲁棒性、可迁移性和临床潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自然语言处理和计算机视觉领域的基础模型因能捕捉语言结构而成功，但医学影像的关键基础是人体解剖结构，现有自监督学习方法往往忽视这一点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服现有自监督学习方法在学习解剖特征方面的局限，构建能够从多视角学习人体解剖结构的基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在大规模胸部X光图像上预训练Lamps模型，利用人体解剖的一致性、连贯性和层次性作为监督信号，采用多视角自监督学习策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在10个数据集上进行微调和新兴属性分析的实验表明，Lamps在鲁棒性、可迁移性和临床潜力方面优于10个基线模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过多视角学习，Lamps为基础模型提供了与人体解剖结构对齐的有意义、稳健的表征，具有重要的临床应用前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基础模型在自然语言处理和计算机视觉领域取得成功，因为它们能够捕捉自然语言的底层结构（基础）。然而，在医学影像中，关键的基础在于人体解剖，因为这些图像直接表示身体内部结构，反映了人体解剖的连贯性、连贯性和层次性。然而，现有的自监督学习（SSL）方法往往忽视这些视角，限制了它们有效学习解剖特征的能力。为克服这一限制，我们构建了Lamps（通过自监督从多视角学习解剖），在大规模胸部X光图像上预训练，和谐地利用人体解剖的一致性、连贯性和层次性作为监督信号。通过在10个数据集上进行微调和新兴属性分析的广泛实验评估，Lamps在鲁棒性、可迁移性和临床潜力方面优于10个基线模型。通过从多视角学习，Lamps为基础模型提供了与人体解剖结构对齐的有意义、稳健的表征的独特机会。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps&amp;#x27; superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.&lt;/p&gt;</description></item><item><guid>2512.22872v2</guid><title>Lamps: Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs</title><link>http://arxiv.org/abs/2512.22872v2</link><author>Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael B. Gotway, Jianming Liang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了Lamps模型，通过利用人体解剖学的连续性、连贯性和层级性作为自监督信号，在大规模胸部X光图像上预训练，显著提升了在10个数据集上的鲁棒性、迁移性和临床潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自然语言处理和计算机视觉中，基础模型因能捕捉自然语言的底层结构而取得成功；但在医学影像领域，关键结构是人体解剖学，现有自监督学习方法往往忽视这一点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服现有自监督学习方法忽视解剖学结构的局限，构建能够从多角度学习解剖特征的基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建Lamps模型，在大规模胸部X光图像上进行自监督预训练，利用人体解剖学的连续性、连贯性和层级性作为监督信号，并通过多角度学习实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在10个数据集上进行微调和新兴属性分析实验，Lamps模型在鲁棒性、迁移性和临床潜力方面优于10个基线模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过多角度学习，Lamps为基础模型提供了与人体解剖结构对齐的有意义、稳健的表示，具有重要的临床应用前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基础模型在自然语言处理和计算机视觉领域取得成功，因为它们能够捕捉自然语言的底层结构。然而，在医学影像中，关键的基础结构在于人体解剖学，因为这些图像直接代表身体内部结构，体现了人体解剖学的连续性、连贯性和层级性。然而，现有的自监督学习方法往往忽视这些视角，限制了它们有效学习解剖特征的能力。为克服这一限制，我们构建了 Lamps（通过自监督从多角度学习解剖学），在大规模胸部X光图像上预训练，和谐地利用人体解剖学的连续性、连贯性和层级性作为监督信号。通过在10个数据集上进行微调和新兴属性分析的广泛实验评估，Lamps在鲁棒性、迁移性和临床潜力方面优于10个基线模型。通过从多角度学习，Lamps为基础模型提供了与人体解剖结构对齐的有意义、稳健的表示的独特机会。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps&amp;#x27; superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.&lt;/p&gt;</description></item><item><guid>2512.22903v1</guid><title>Debugging Tabular Log as Dynamic Graphs</title><link>http://arxiv.org/abs/2512.22903v1</link><author>Chumeng Liang, Zhanyang Jin, Zahaib Akhtar, Mona Pereira, Haofei Yu, Jiaxuan You</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 这篇论文提出了基于动态图的表格日志调试框架 GraphLogDebugger，利用图神经网络在不依赖大型语言模型的情况下实现高效调试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的表格日志调试方法依赖大型语言模型，导致灵活性和可扩展性受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建异构节点和动态图，提供一种更灵活、可扩展的表格日志调试方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构造对象和事件的异构节点，连接节点间的边，形成系统的演化动态图；使用简单的动态图神经网络进行调试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 简单的动态 GNN 在调试表格日志方面能超过大型语言模型，实验验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GraphLogDebugger 在真实系统日志和学术论文日志数据集上表现优异，证明了动态图模型的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 表格日志抽象了现实世界系统中的对象和事件，并报告它们的更新以反映系统的变化，通过调试相应的日志条目可以高效地检测现实世界的不一致性。然而，最近在处理文本丰富的表格日志数据方面的进展过度依赖大型语言模型（LLM）和其他高负载模型，导致灵活性和可扩展性受限。本文提出了一个新的框架 GraphLogDebugger，基于动态图来调试表格日志。通过为对象和事件构造异构节点并连接节点间的边，该框架将表格日志背后的系统恢复为一个演化的动态图。在动态图建模的帮助下，一个简单的动态图神经网络（GNN）足以在调试表格日志方面超过 LLM，这一结果已在计算机系统和学术论文的真实日志数据集上得到验证。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.&lt;/p&gt;</description></item><item><guid>2512.22904v1</guid><title>MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning</title><link>http://arxiv.org/abs/2512.22904v1</link><author>Jin Wu, Chanjin Zheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于持续学习的元学习框架 MetaCD，用于认知诊断，旨在解决长尾分布和数据动态变化导致的性能下降问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 认知诊断是智能教育的重要研究方向，现有研究多采用深度学习模型探究学生、题目与技能之间的复杂交互，但受限于长尾分布和数据动态变化，性能往往受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种结合元学习与持续学习的框架，以提升模型在新任务上的准确性并适应数据的动态变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MetaCD 通过元学习学习最佳初始化状态来缓解长尾问题，并采用参数保护机制实现持续学习，使模型能够在面对新技能或新任务时保持稳定并快速适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在五个真实数据集上的实验表明，MetaCD 在准确率和泛化能力上均优于其他基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MetaCD 在单一任务上提升了模型的可塑性，同时在连续任务上保证了模型的稳定性和泛化性，证明了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 认知诊断是智能教育中的一个重要研究课题，旨在评估学生对不同技能的掌握水平。迄今为止，许多研究工作已使用深度学习模型来探索学生、问题和技能之间的复杂交互。然而，现有方法的性能往往受到长尾分布和数据动态变化的限制。为了解决这些挑战，我们提出了一种基于持续学习的元学习框架（MetaCD）用于认知诊断。该框架通过利用元学习学习最佳初始化状态来缓解长尾问题，使模型能够仅用少量数据在新任务上获得良好准确率。此外，我们采用一种名为参数保护机制的持续学习方法，使 MetaCD 能够适应新技能或新任务，从而适应数据的动态变化。MetaCD 不仅能提升模型在单一任务上的可塑性，还能确保模型在连续任务上的稳定性和泛化能力。对五个真实世界数据集的综合实验表明，MetaCD 在准确率和泛化能力方面均优于其他基线方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.&lt;/p&gt;</description></item><item><guid>2512.22939v2</guid><title>ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving</title><link>http://arxiv.org/abs/2512.22939v2</link><author>Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 ColaVLA，一种统一的视觉-语言-动作框架，用于自动驾驶轨迹生成，解决了现有 VLM 规划器的三大挑战，并在 nuScenes 基准上实现了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自动驾驶需要从复杂的多模态输入生成安全可靠的轨迹，传统管线分离感知、预测和规划，最近的端到端系统将其联合学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服 VLM 规划器在离散文本推理与连续控制不匹配、高延迟以及非因果规划器导致的实时部署受限等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过将文本推理迁移到统一潜在空间，使用认知潜在推理器压缩场景理解为元动作嵌入，并采用层次化并行轨迹解码器一次前向传播生成多尺度因果一致轨迹。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ColaVLA 在 nuScenes 基准的开放环和闭环实验中均实现了最先进的性能，且具有更高的效率和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在保留 VLM 的泛化性和可解释性的同时，实现了高效、准确和安全的轨迹生成，适合实时部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.&lt;/p&gt;</description></item><item><guid>2512.22966v1</guid><title>Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks</title><link>http://arxiv.org/abs/2512.22966v1</link><author>Mengdi Chai, Ali R. Zomorrodi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究评估了三种先进大型语言模型在完整临床决策流程中的表现，并探讨了提示工程对其性能的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型在医学知识评估中表现出潜力，但其在真实临床决策中的实际应用尚未得到充分研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估ChatGPT-4o、Gemini 1.5 Pro和Llama 3.3 70B在典型患者就诊全过程中的决策支持能力，并检验提示工程是否能提升其表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用36个病例，先在默认和零温度两种设置下，分别测评模型在差异诊断、必要即时步骤、相关诊断测试、最终诊断和治疗建议五个任务上的原始表现；随后采用MedPrompt框架的针对性和随机动态少样本提示，评估提示工程对各任务的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在不同任务间表现差异大，最终诊断几乎完美，相关诊断测试表现最差，其余任务中等；ChatGPT在零温度下更好，Llama在默认温度下更好；提示工程显著提升了相关诊断测试的准确率，但对其他任务反而不利；针对性少样本提示并未始终优于随机选择，说明匹配示例的优势可能被缺乏多样性所抵消。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提示工程的效果高度依赖模型和任务，需要针对性、上下文感知的策略才能有效将大型语言模型融入医疗决策。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型（LLMs）在医学知识评估中已显示出前景，但它们在真实临床决策中的实际效用仍未得到充分探索。在本研究中，我们评估了三种最先进的LLM——ChatGPT-4o、Gemini 1.5 Pro和Llama 3.3 70B——在典型患者就诊全过程中的临床决策支持表现。使用36个病例，我们首先在默认和零温度两种设置下，评估LLM在五个关键顺序临床决策任务中的原始表现：鉴别诊断、必要的即时步骤、相关诊断测试、最终诊断和治疗建议。所有模型在任务间表现高度波动，在最终诊断上几乎达到完美准确率，在相关诊断测试上表现较差，其余任务表现中等。ChatGPT在零温度下表现更好，而Llama在默认温度下表现更好。随后，我们通过应用MedPrompt框架的变体，结合针对性和随机动态少样本学习，评估提示工程是否能提升LLM表现。结果表明，提示工程并非一刀切的解决方案；它显著提升了基线准确率最低的任务（相关诊断测试），但对其他任务却适得其反。另一个关键发现是，针对性动态少样本提示并未始终优于随机选择，表明紧密匹配示例的预期优势可能被缺乏更广泛上下文多样性所抵消。这些发现表明，提示工程的影响高度依赖于模型和任务，强调了为将LLM整合到医疗保健中需要定制化、上下文感知的策略。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM&amp;#x27;s out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.&lt;/p&gt;</description></item><item><guid>2512.22972v1</guid><title>Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection</title><link>http://arxiv.org/abs/2512.22972v1</link><author>Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daizong Liu, Tao Huang, Guoqiang Mao, Hui Xiong</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 4D毫米波雷达在自动驾驶和机器人感知中被广泛使用，但其稀疏性和语义不足限制了感知能力。通过将相机数据与4D雷达融合，可利用两种模态的互补优势。本文提出WRCFormer框架，利用原始雷达立方体与相机输入的多视角表示进行融合。核心是小波注意力模块和基于小波的特征金字塔网络，用于增强稀疏雷达信号和图像的表示。进一步引入两阶段查询式、模态无关的几何引导渐进融合机制，以高效整合多视角特征。实验表明，在K-Radar基准上，WRCFormer在所有场景中比最佳模型提升约2.4%，在雨雾场景提升1.6%，显示出在恶劣天气下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 4D毫米波雷达因低成本和全天候性能被广泛采用，但其稀疏性和语义丰富度不足限制了感知能力。相机与雷达的融合被视为一种成本效益高的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决雷达点云信息损失和原始雷达数据计算成本高的问题，提出一种能够高效融合原始雷达立方体和相机数据的3D目标检测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计Wavelet Attention Module作为小波特征金字塔网络的基本模块，增强稀疏雷达信号和图像的表示；引入两阶段查询式、模态无关的Geometry-guided Progressive Fusion机制，利用几何引导高效融合多视角特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; WRCFormer在K-Radar基准上实现了最先进的性能，比现有最佳模型提升约2.4%（所有场景）和1.6%（雨雾场景），证明其在恶劣天气下的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过小波注意力和几何引导渐进融合，WRCFormer显著提升了4D雷达与相机融合的3D目标检测性能，尤其在低语义和稀疏条件下表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 4D毫米波雷达因其低成本和全天候鲁棒性，已被广泛应用于自动驾驶和机器人感知。然而，其固有的稀疏性和有限的语义丰富度显著限制了感知能力。最近，通过将相机数据与4D雷达融合，利用两种模态的互补优势，已成为一种有前景的成本效益解决方案。然而，基于点云的雷达常常受到多阶段信号处理引入的信息损失，而直接使用原始4D雷达数据则会产生巨大的计算成本。为解决这些挑战，我们提出了WRCFormer，一种新颖的3D目标检测框架，通过对解耦雷达立方体的多视角表示，将原始雷达立方体与相机输入进行融合。具体而言，我们设计了Wavelet Attention Module，作为基于小波的特征金字塔网络（FPN）的基本模块，以增强稀疏雷达信号和图像数据的表示。我们进一步引入了一种两阶段查询式、模态无关的融合机制，称为Geometry-guided Progressive Fusion，以高效整合来自两种模态的多视角特征。大量实验表明，WRCFormer在K-Radar基准上实现了最先进的性能，在所有场景中超过最佳模型约2.4%，在雨雾场景中提升1.6%，凸显了其在恶劣天气条件下的鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在提升自动驾驶和机器人在各种天气条件下的三维目标检测准确性，解决毫米波雷达稀疏且语义信息不足的问题。雷达成本低、全天候可靠，但传统点云表示会丢失大量环境信息；相机提供丰富语义，但受光照影响。将两者融合，可在保持低成本的同时获得更鲁棒的感知。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先观察到点云雷达信息损失严重、原始雷达张量计算量大，随后将雷达四维张量拆分为范围-方位和仰角-方位两视图，以保留完整属性并降低维度。借鉴了波形变换与注意力机制的研究，提出波形注意力混合专家（WA‑MoE）模块来提取雷达特征，并设计几何引导的渐进融合（GPF）来实现跨模态对齐，参考了EchoFusion、DPFT、ASF等融合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用原始雷达张量的多视图表示与相机图像进行多尺度特征融合，并通过波形注意力混合专家提升雷达特征表达。实现流程包括：①将雷达张量拆分为RA和EA两张图；②分别用ResNet编码并通过WA‑MoE FPN提取多尺度特征；③使用GPF先用交叉注意力将图像语义与EA雷达对齐，再用基于RA雷达的可变形注意力细化；④将融合后的特征送入检测头，迭代优化得到三维框。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提出WRCFormer框架，首次在同一BEV空间内联合编码雷达张量和相机特征；②设计WA‑MoE模块，将可学习的波形分解与稀疏专家门控结合，显著提升雷达特征质量；③引入几何引导的渐进融合，利用几何先验实现高效、细粒度的跨模态对齐；④在K‑Radar基准上实现显著性能提升，尤其在雨雪等恶劣天气下。与以往点云或原始张量融合方法相比，本文在保持信息完整性的同时大幅降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; WRCFormer通过波形注意力混合专家和几何引导的渐进融合，将原始4D雷达张量与相机图像高效融合，显著提升了在恶劣天气下的三维目标检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.&lt;/p&gt;</description></item><item><guid>2512.22976v2</guid><title>A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging</title><link>http://arxiv.org/abs/2512.22976v2</link><author>Amirali Vakili, Salar Jahanshiri, Armin Salimi-Badr</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 自动睡眠分期是医疗领域的重要任务，尤其在全球睡眠障碍普遍的背景下。2. 本研究聚焦单通道脑电（EEG），该信号易获取且实用。3. 现有方法存在类别不平衡、感受野有限、可解释性差等问题。4. 提出一种上下文感知且可解释的框架，重点提升N1阶段的检测。5. 通过紧凑多尺度特征提取与时间建模，捕获局部与长程依赖。6. 对类别不平衡采用加权损失和数据增强；将EEG切分为子时段，最终预测通过平均softmax概率得到。7. 在SleepEDF数据集上实现整体准确率89.72%，宏平均F1 85.46%，N1阶段F1 61.7%，显著优于以往方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 睡眠障碍在全球范围内普遍存在，自动睡眠分期能有效辅助诊断。单通道EEG因其易获取性成为自动分期的主流信号来源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提升单通道EEG自动睡眠分期的准确性，尤其是难以检测的N1阶段，并增强模型的可解释性与临床适用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 采用紧凑多尺度特征提取模块与时间建模层；2) 通过类别加权损失和数据增强解决类别不平衡；3) 将EEG信号切分为子时段，最终预测通过平均softmax概率实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 整体准确率89.72%，宏平均F1 85.46%，N1阶段F1 61.7%，在SleepEDF数据集上显著优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的框架在保持可解释性的同时显著提升了睡眠分期性能，适合在真实临床环境中应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自动睡眠分期是医疗领域的关键任务，因全球睡眠障碍普遍存在。本研究聚焦单通道脑电（EEG），该信号易获取且广泛可用。现有方法面临类别不平衡、感受野有限和可解释性不足等挑战。本文提出一种上下文感知且可解释的单通道EEG睡眠分期框架，特别强调提升N1阶段的检测。许多先前模型作为黑盒运作，缺乏明确且可解释的特征提取角色。所提模型结合紧凑多尺度特征提取与时间建模，捕获局部与长程依赖。为解决数据不平衡，尤其是N1阶段，采用类别加权损失函数和数据增强。EEG信号被分割为子时段块，最终预测通过平均softmax概率获得，增强了上下文表示和鲁棒性。该框架在SleepEDF数据集上实现整体准确率89.72%，宏平均F1得分85.46%。值得注意的是，它在具有挑战性的N1阶段取得61.7%的F1得分，显著优于以往方法。这些结果表明所提出的方法有效提升睡眠分期性能，同时保持可解释性和适用于真实临床应用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.&lt;/p&gt;</description></item><item><guid>2512.22983v1</guid><title>Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives</title><link>http://arxiv.org/abs/2512.22983v1</link><author>Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, Cheng Chi, Chang Xu, Xiaolong Zheng, Donglin Wang, Haoang Li, Shanghang Zhang, Badong Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文综述了机器人操作的学习方法，提出了高层规划与低层控制的统一框架，并指出未来挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 近年来视觉、语言和多模态学习的进展推动了机器人基础模型的发展，机器人操作仍是核心挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 从算法视角梳理机器人操作的学习方法，构建统一抽象框架，识别研究空白。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 高层面扩展任务规划，加入语言、代码、运动、可供性、3D 表示；低层面提出训练范式分类，按输入建模、潜在表示学习、策略学习组织方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 高层规划与低层控制的结合为结构化、长周期决策提供支持；低层分类揭示现有方法的共性与差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过对高层规划与低层控制的系统化分析，阐明了现代机器人基础模型的设计空间，并提出可扩展性、数据效率、多模态交互与安全等未来研究方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近年来，视觉、语言和多模态学习的进展显著加速了机器人基础模型的发展，而机器人操作仍然是一个核心且具有挑战性的问题。本综述从算法角度审视机器人操作，并在统一的高层规划与低层控制抽象框架下组织了近期基于学习的方法。在高层面，我们扩展了经典的任务规划概念，包含对语言、代码、运动、可供性和三维表示的推理，强调它们在结构化和长期决策中的作用。在低层面，我们提出了以训练范式为导向的分类法，按输入建模、潜在表示学习和策略学习组织现有方法。最后，我们识别了与可扩展性、数据效率、多模态物理交互和安全相关的开放挑战和前景研究方向。上述分析旨在阐明现代机器人操作基础模型的设计空间。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.&lt;/p&gt;</description></item><item><guid>2512.23042v1</guid><title>3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds</title><link>http://arxiv.org/abs/2512.23042v1</link><author>Ryousuke Yamada, Kohsuke Ide, Yoshihiro Fukuhara, Hirokatsu Kataoka, Gilles Puy, Andrei Bursuc, Yuki M. Asano</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督学习框架LAM3C，利用从无标签视频中生成的点云来学习3D表示，并通过构建RoomTours数据集和噪声正则化损失实现了在室内语义与实例分割任务上的显著性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 收集大规模3D场景扫描仍然昂贵且劳动密集，限制了3D自监督学习的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究是否可以仅凭无标签视频而不使用真实3D传感器来学习3D表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 构建RoomTours数据集：从网络收集房间漫游视频并使用现成的前馈重建模型生成49,219个点云场景；2) 设计LAM3C框架：采用拉普拉斯感知多层3D聚类与Sinkhorn-Knopp算法；3) 引入噪声正则化损失：通过局部几何平滑和特征稳定性提升学习效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在室内语义与实例分割任务中，LAM3C在没有任何真实3D扫描数据的情况下，性能超过了之前的自监督方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 无标签视频是3D自监督学习的丰富数据来源，可有效替代昂贵的3D扫描。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 尽管最近在3D自监督学习方面取得了进展，但收集大规模3D场景扫描仍然昂贵且劳动密集。在这项工作中，我们研究了是否可以从未标记的视频中学习3D表示，而不使用任何真实的3D传感器。我们提出了Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp（LAM3C），一种自监督框架，能够从未标记视频生成的点云中学习。我们首先介绍了RoomTours，这是一个通过收集网络上的房间漫游视频（例如房地产导览）并使用现成的前馈重建模型生成49,219个场景的点云数据集。我们还提出了一种噪声正则化损失，通过强制局部几何平滑并确保在噪声点云下特征的稳定性来稳定表示学习。值得注意的是，在没有使用任何真实3D扫描的情况下，LAM3C在室内语义和实例分割任务上取得了比以前的自监督方法更高的性能。这些结果表明，未标记的视频是3D自监督学习的丰富数据来源。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文想解决如何在没有昂贵的真实 3D 扫描数据的情况下，利用无标签视频学习 3D 表示。因为 3D 扫描难以大规模获取，限制了 3D 自监督学习的发展，解决这一问题能让室内场景理解更易推广。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为视频中蕴含丰富的几何信息，现代前馈重建模型可以直接从视频生成点云。于是他们收集室内视频，使用 π³ 等模型生成点云，构建 RoomTours 数据集，并在此基础上设计 LAM3C，借鉴了 Sonata 的教师-学生框架、Sinkhorn‑Knopp 聚类、拉普拉斯平滑和噪声一致性等已有技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用视频生成的点云做大规模自监督预训练，并通过噪声正则化的聚类学习稳定的 3D 表示。实现流程包括：①收集并分割室内视频；②用前馈重建模型生成点云，构建 RoomTours；③训练 LAM3C，使用教师‑学生、聚类损失、拉普拉斯平滑和噪声一致性；④在下游任务上微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①创建了 49k 场景的 RoomTours 视频生成点云数据集；②提出 LAM3C，加入拉普拉斯平滑和噪声一致性损失，使模型能在噪声点云上稳定学习；③在不使用任何真实 3D 扫描的情况下，取得比现有自监督方法更好的室内分割性能。与之前的工作不同，它完全摆脱了对昂贵 3D 扫描的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LAM3C 证明了可以利用视频生成的点云进行大规模自监督 3D 表示学习，从而在无需真实 3D 扫描的情况下实现室内场景理解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.&lt;/p&gt;</description></item><item><guid>2512.23054v2</guid><title>Differentiable Physics-Driven Human Representation for Millimeter-Wave Based Pose Estimation</title><link>http://arxiv.org/abs/2512.23054v2</link><author>Shuntian Zheng, Guangming Wang, Jiaqi Li, Minzhe Ni, Yu Guan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; mmWave技术在人体姿态估计中具有非侵入式感知优势，但现有基于mmWave的方法主要使用热图和点云两种输入范式，分别面临多径传播和硬件调制噪声以及特征稀疏的问题。本文提出一种可微分物理驱动的人体表示（DIPR），将人体建模为一组高斯分布，结合运动学先验和mmWave传播物理，采用两种策略：一是基于热图初始化DIPR并设定多目标优化，保证生物力学有效性并提升运动特征；二是模拟完整的mmWave处理流程，从DIPR重新渲染热图并与原始热图对比，避免因运动学约束过拟合产生噪声。实验表明，DIPR可轻松集成到现有mmWave人体姿态估计方法中，并在三个数据集上实现了更优的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; mmWave传感器因其非侵入式特性在人体姿态估计中被广泛研究，常用的输入范式包括热图和点云。热图提供了密集的多维特征，但易受多径传播和硬件噪声影响；点云通过阈值算法从热图中提取，能抑制噪声但导致特征稀疏。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决热图和点云在mmWave人体姿态估计中的局限性，提出一种更可靠、更具物理意义的输入表示方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 将人体建模为一组高斯分布的可微分物理驱动表示（DIPR），并利用运动学先验对其进行初始化；2. 设定多目标优化目标，确保生物力学有效性并提升运动特征；3. 通过完整的mmWave处理链模拟，从DIPR重新渲染热图，并与原始热图进行对比，避免噪声过拟合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DIPR能够无缝集成到现有的mmWave人体姿态估计方法中，并在三个数据集上通过四种方法验证，显著提升了估计性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可微分物理驱动的人体表示（DIPR）为mmWave人体姿态估计提供了一种更稳健、更高效的输入范式，具有良好的可扩展性和实用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; mmWave技术在人体姿态估计中具有非侵入式感知优势，但现有基于mmWave的方法主要使用热图和点云两种输入范式，分别面临多径传播和硬件调制噪声以及特征稀疏的问题。本文提出一种可微分物理驱动的人体表示（DIPR），将人体建模为一组高斯分布，结合运动学先验和mmWave传播物理，采用两种策略：一是基于热图初始化DIPR并设定多目标优化，保证生物力学有效性并提升运动特征；二是模拟完整的mmWave处理流程，从DIPR重新渲染热图并与原始热图对比，避免因运动学约束过拟合产生噪声。实验表明，DIPR可轻松集成到现有mmWave人体姿态估计方法中，并在三个数据集上实现了更优的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While millimeter-wave (mmWave) presents advantages for Human Pose Estimation (HPE) through its non-intrusive sensing capabilities, current mmWave-based HPE methods face limitations in two predominant input paradigms: Heatmap and Point Cloud (PC). Heatmap represents dense multi-dimensional features derived from mmWave, but is significantly affected by multipath propagation and hardware modulation noise. PC, a set of 3D points, is obtained by applying the Constant False Alarm Rate algorithm to the Heatmap, which suppresses noise but results in sparse human-related features. To address these limitations, we study the feasibility of providing an alternative input paradigm: Differentiable Physics-driven Human Representation (DIPR), which represents humans as an ensemble of Gaussian distributions with kinematic and electromagnetic parameters. Inspired by Gaussian Splatting, DIPR leverages human kinematic priors and mmWave propagation physics to enhance human features while mitigating non-human noise through two strategies: 1) We incorporate prior kinematic knowledge to initialize DIPR based on the Heatmap and establish multi-faceted optimization objectives, ensuring biomechanical validity and enhancing motion features. 2) We simulate complete mmWave processing pipelines, re-render a new Heatmap from DIPR, and compare it with the original Heatmap, avoiding spurious noise generation due to kinematic constraints overfitting. Experimental results on three datasets with four methods demonstrate that existing mmWave-based HPE methods can easily integrate DIPR and achieve superior performance.&lt;/p&gt;</description></item><item><guid>2512.23076v1</guid><title>Multimodal Functional Maximum Correlation for Emotion Recognition</title><link>http://arxiv.org/abs/2512.23076v1</link><author>Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的自监督学习框架 MFMC，用于多模态情感计算，能够捕捉多模态间的高阶依赖关系，并在多个公开数据集上取得优异表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 情绪状态通过中枢和自主系统的多模态生理信号表现为协调但异质的反应，导致多模态表征学习面临挑战；同时情感标注稀缺且主观。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有自监督方法仅使用两两对齐目标，无法充分捕捉多模态间的高阶交互的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 MFMC 框架，使用 Dual Total Correlation 目标最大化高阶多模态依赖，并通过 Functional Maximum Correlation Analysis 的迹式近似实现优化，无需两两对比损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 CEAP-360VR、MAHNOB-HCI 等数据集上，MFMC 在受试者相关和无关评估中均达到或接近最优，尤其在单一 EDA 信号下，受试者相关准确率从 78.9% 提升至 86.8%，受试者无关从 27.5% 提升至 33.1%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MFMC 通过直接捕捉多模态交互，显著提升情感计算的鲁棒性和性能，证明了高阶依赖学习的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Multimodal Functional Maximum Correlation (MFMC) is a self-supervised learning framework that maximizes higher-order multimodal dependence through a Dual Total Correlation objective, directly capturing joint multimodal interactions without relying on pairwise contrastive losses, and achieves state-of-the-art performance on multiple affective computing benchmarks.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.   To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.   Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.&lt;/p&gt;</description></item><item><guid>2512.23080v1</guid><title>QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants</title><link>http://arxiv.org/abs/2512.23080v1</link><author>Tim C. Pearce, Ahmed Ibrahim</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种结合变分自编码器与定量结构-活性关系模型的框架，用于在有限香味分子数据集上生成结构合法且多样化的新香味分子，并通过外部数据验证其有效性与创新性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 香味分子发现是香料与食品行业的关键，但在庞大的化学空间中高效筛选具有理想嗅觉特性的结构仍是挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在训练样本有限的情况下，利用生成式人工智能技术设计新的香味分子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用变分自编码器学习ChemBL数据库中的SMILES语法，并通过加入来自外部QSAR模型的损失项来引导潜在空间按香味概率结构化；采用拒绝采样保证生成分子语法合法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型在外部未见数据集上实现100%语法有效率，94.8%唯一性；潜在空间中香味概率结构化明显，生成分子与已知香味分子的Fréchet ChemNet距离约为6.96，远优于ChemBL基线的21.6；结构分析显示74.4%的候选分子拥有与训练数据不同的新核心框架，表明模型能广泛探索化学空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架能够在有限训练集下生成结构合法、多样且具有创新核心框架的香味分子，且潜在空间有效区分香味概率，展示了在香味分子设计中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 香味分子新颖发现是香料与食品行业的关键，但在庞大的化学空间中高效筛选具有理想嗅觉特性的结构仍是挑战。生成式人工智能提供了一种有前景的方法来进行全新分子设计，但通常需要大量分子数据来学习。为了解决这个问题，我们提出了一个将变分自编码器（VAE）与定量结构-活性关系（QSAR）模型相结合的框架，以从有限的香味分子训练集生成新香味分子。VAE的自监督学习能力使其能够从ChemBL数据库学习SMILES语法，而其训练目标通过来自外部QSAR模型的损失项进行增强，以根据香味概率来构造潜在表示。VAE在学习QSAR监督信号方面表现出高度内部一致性，并通过对外部未见的真实数据集（Unique Good Scents）的验证确认模型生成的分子在语法上是有效的（通过拒绝采样实现100%有效率）且94.8%为唯一结构。潜在空间有效地按香味可能性进行结构化，生成分子与已知香味分子之间的Fréchet ChemNet距离约为6.96，而ChemBL基线约为21.6。通过Bemis-Murcko骨架的结构分析表明，74.4%的候选分子具有与训练数据不同的新核心框架，表明模型在广泛探索化学空间方面表现出色。生成的候选分子显示出物理化学性质……&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The discovery of novel odorant molecules is key for the fragrance and flavor industries, yet efficiently navigating the vast chemical space to identify structures with desirable olfactory properties remains a significant challenge. Generative artificial intelligence offers a promising approach for \textit{de novo} molecular design but typically requires large sets of molecules to learn from. To address this problem, we present a framework combining a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to generate novel odorants from limited training sets of odor molecules. The self-supervised learning capabilities of the VAE allow it to learn SMILES grammar from ChemBL database, while its training objective is augmented with a loss term derived from an external QSAR model to structure the latent representation according to odor probability. While the VAE demonstrated high internal consistency in learning the QSAR supervision signal, validation against an external, unseen ground truth dataset (Unique Good Scents) confirms the model generates syntactically valid structures (100\% validity achieved via rejection sampling) and 94.8\% unique structures. The latent space is effectively structured by odor likelihood, evidenced by a Fréchet ChemNet Distance (FCD) of $\approx$ 6.96 between generated molecules and known odorants, compared to $\approx$ 21.6 for the ChemBL baseline. Structural analysis via Bemis-Murcko scaffolds reveals that 74.4\% of candidates possess novel core frameworks distinct from the training data, indicating the model performs extensive chemical space exploration beyond simple derivatization of known odorants. Generated candidates display physicochemical properties ....&lt;/p&gt;</description></item><item><guid>2512.23137v1</guid><title>Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use</title><link>http://arxiv.org/abs/2512.23137v1</link><author>Runzhi Zhou, Xi Luo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种时间感知图神经网络与变压器融合模型（GNN‑TF），能够将非欧几里得脑影像数据与欧几里得表格数据结合，利用时间序列信息进行预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在医学影像分析中，将脑影像数据与临床、人口统计信息整合，尤其是对未来结果的预测，面临巨大挑战。传统机器学习和深度学习已在横断面分类任务中取得成功，但在纵向影像研究中预测结果仍然困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够同时处理表格数据和动态脑连接数据，并利用时间顺序进行整合的模型，以提高纵向影像研究中对未来结果的预测准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建时间感知图神经网络与变压器融合模型（GNN‑TF），在该模型中将表格数据和动态脑连接数据在统一框架内结合，并利用时间信息。使用来自国家酒精与青少年神经发育研究联盟的纵向静息态fMRI数据进行实验，并与多种传统机器学习和深度学习模型进行对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，GNN‑TF 在预测未来吸烟行为方面的准确率高于现有的多种机器学习和深度学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GNN‑TF 的端到端、时间感知的变压器融合结构能够有效整合多模态数据并利用时间动态，是功能性脑影像研究中预测临床结果的有价值工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 将非欧几里得脑影像数据与欧几里得表格数据（如临床和人口统计信息）整合，尤其是对未来结果的预测，在医学影像分析中面临重大挑战。虽然机器学习和深度学习技术已成功应用于横断面分类和预测任务，但在纵向影像研究中有效预测结果仍然困难。为解决这一挑战，我们提出了一种时间感知图神经网络与变压器融合模型（GNN‑TF）。该模型灵活地整合了表格数据和动态脑连接数据，利用这些变量的时间顺序在统一框架内进行处理。通过将来自国家酒精与青少年神经发育研究联盟（NCANDA）的纵向静息态fMRI数据中的非欧几里得和欧几里得信息纳入，GNN‑TF 实现了对纵向影像数据关键方面的全面分析。与多种已建立的机器学习和深度学习模型进行比较分析表明，GNN‑TF 在预测未来吸烟行为方面优于这些最先进的方法，提供了更高的预测准确性。所提出的 GNN‑TF 的端到端、时间感知变压器融合结构成功整合了多种数据模态，并利用时间动态，使其成为关注临床结果预测的功能性脑影像研究的有价值分析工具。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.&lt;/p&gt;</description></item><item><guid>2512.23141v1</guid><title>Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset</title><link>http://arxiv.org/abs/2512.23141v1</link><author>Wuhao Xie, Kanji Tanaka</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在大规模城市环境中，杆状结构作为长期机器人定位的几何锚点的识别可靠性下降的问题，并通过建立专门的评估框架和数据集，系统比较了对比学习和监督学习在杆标志物特征鲁棒性上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 杆状结构被广泛认为是长期机器人定位的稳定几何锚点，但在典型的大规模城市环境中，杆距观测导致其识别可靠性显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将研究焦点从描述符设计转向系统性探讨描述符鲁棒性，并为此建立评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建了小杆标志物数据集（SPL），通过自动跟踪关联管道获取多视角、多距离的同一物理标志物观测；利用该框架对对比学习和监督学习两种范式进行比较分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 对比学习在稀疏几何特征空间中产生更鲁棒的特征，尤其在5到10米范围内的检索性能优于监督学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提供了实证基础和可扩展的方法，用于在挑战性真实场景中评估标志物的区分度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然杆状结构被广泛认为是长期机器人定位的稳定几何锚点，但在大规模城市环境中典型的杆距观测下，它们的识别可靠性显著下降。本文将焦点从描述符设计转向对描述符鲁棒性的系统性研究。我们的主要贡献是建立了一个以小杆标志物（SPL）数据集为中心的专门评估框架。该数据集通过自动跟踪关联管道构建，捕获同一物理标志物的多视角、多距离观测，无需人工标注。利用该框架，我们对对比学习（CL）和监督学习（SL）范式进行了比较分析。研究结果表明，对比学习在稀疏几何特征空间中产生更鲁棒的特征，尤其在5到10米范围内的检索性能更优。本文为在具有挑战性的真实场景中评估标志物区分度提供了实证基础和可扩展的方法。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在评估在稀疏、长距离观测（Pole-at-Distance）下，基于杆状地标的描述子在机器人定位中的鲁棒性。杆状结构是长期稳定的定位锚点，但在大尺度城市环境中易出现辨识不清，导致定位误差。评估其鲁棒性对于实现可靠的长周期定位和地图维护至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为关键在于构建一个无人工标注、可扩展的评估数据集，并比较不同学习范式的表现。借鉴了已有的杆检测器 Polex、Pole-Image 描述子以及对比学习（Contrastive Learning）和监督学习的研究，构建了自动跟踪关联管道来生成多视角、不同距离的匹配对。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自动跟踪生成稀疏观测与密集观测的匹配对，使用 Pole-Image 将三维点云投影为二维图像，再用轻量级 ResNet 编码为嵌入向量。流程包括：①从 NCLT 数据中检测并跟踪杆；②构建 SPL 数据集（仅保留点数低于阈值的观测）；③分别用监督学习和对比学习训练相同网络；④在不同距离区间评估检索性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) SPL 数据集的自动化构建，消除人工标注成本；2) 在稀疏、长距离条件下系统性比较对比学习与监督学习的鲁棒性；3) 发现对比学习在 5–10 m 范围内显著优于监督学习。与以往仅在密集点云下评估的工作不同，本文聚焦于真实环境中稀疏观测的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提供了一个可扩展的自动化评估框架，证明对比学习在稀疏、长距离杆状地标定位中比监督学习更具鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While pole-like structures are widely recognized as stable geometric anchors for long-term robot localization, their identification reliability degrades significantly under Pole-at-Distance (Pad) observations typical of large-scale urban environments. This paper shifts the focus from descriptor design to a systematic investigation of descriptor robustness. Our primary contribution is the establishment of a specialized evaluation framework centered on the Small Pole Landmark (SPL) dataset. This dataset is constructed via an automated tracking-based association pipeline that captures multi-view, multi-distance observations of the same physical landmarks without manual annotation. Using this framework, we present a comparative analysis of Contrastive Learning (CL) and Supervised Learning (SL) paradigms. Our findings reveal that CL induces a more robust feature space for sparse geometry, achieving superior retrieval performance particularly in the 5--10m range. This work provides an empirical foundation and a scalable methodology for evaluating landmark distinctiveness in challenging real-world scenarios.&lt;/p&gt;</description></item><item><guid>2512.23147v1</guid><title>GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection</title><link>http://arxiv.org/abs/2512.23147v1</link><author>Jingyu Li, Xiaolong Zhao, Zhe Liu, Wenxiao Wu, Li Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 GeoTeacher，利用教师模型的几何知识和体素级数据增强，提升半监督 3D 目标检测中学生模型对物体几何关系的感知能力，并在 ONCE 与 Waymo 数据集上取得新一代最优结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 半监督 3D 目标检测通过利用未标注数据提升检测性能，已有方法通过异构教师模型或特征一致性实现改进，但忽视了模型对有限标注数据下物体几何的低敏感性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 增强学生模型在有限训练数据，尤其是未标注数据下，捕捉物体几何关系的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计基于关键点的几何关系监督模块，将教师模型的几何知识迁移给学生；引入体素级数据增强并加入距离衰减机制以保持远距离物体完整；可与多种半监督 3D 检测方法结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 ONCE 与 Waymo 数据集上实验表明 GeoTeacher 在提升学生模型几何感知方面显著有效，且与其他方法结合后进一步提升性能，达成新最优。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GeoTeacher 通过几何监督与增强策略显著提升半监督 3D 检测性能，具有良好的通用性和可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 半监督 3D 目标检测旨在利用未标注数据提升 3D 目标检测器的性能，近年来已成为活跃的研究领域。一些先前的方法通过使用异构教师模型提供高质量伪标签，或在教师与学生网络之间强制特征视角一致性，已取得显著改进。然而，这些方法忽视了模型在有限标注数据下通常对物体几何的敏感性较低，难以捕捉几何信息，而几何信息对于提升学生模型的物体感知和定位能力至关重要。本文提出 GeoTeacher，旨在增强学生模型在有限训练数据，尤其是未标注数据下，捕捉物体几何关系的能力。我们设计了基于关键点的几何关系监督模块，将教师模型对物体几何的知识迁移给学生，从而提升学生对几何关系的理解能力。此外，我们引入了体素级数据增强策略，增加物体几何多样性，进一步提升学生模型对几何结构的理解能力。为在增强过程中保持远距离物体的完整性，我们在该策略中加入了距离衰减机制。GeoTeacher 还可以与不同的半监督 3D 检测方法结合，以进一步提升其性能。对 ONCE 和 Waymo 数据集的广泛实验表明，我们的方法具有有效性和泛化性，并取得了新的最优结果。代码将发布在 https://github.com/SII-Whaleice/GeoTeacher&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决半监督3D目标检测中标注数据稀缺的问题。通过利用大量未标注点云数据提升检测性能，减少昂贵的人工标注成本。该问题在自动驾驶、机器人等需要高精度3D感知的场景中尤为关键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有半监督方法主要关注伪标签质量或特征一致性，忽视了物体内部几何关系。于是提出基于关键点的几何关系监督和体素级数据增强，并借鉴了教师-学生框架、伪标签加权、距离衰减等已有技术。该方法在设计上兼顾了对几何信息的利用和数据多样性的提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让学生模型通过教师模型学习物体的几何关系，并通过体素级增强增加几何多样性。实现流程分两阶段：第一阶段训练高性能GeoTeacher；第二阶段使用GeoTeacher对学生进行监督，加入几何关系监督损失和距离衰减的体素增强，并在训练中使用伪标签的置信度加权。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 关键点（中心、边缘中点、角点）构成的几何关系监督模块；2) 距离衰减的体素级数据增强策略；3) 通过置信度加权减轻伪标签噪声；4) 可与现有半监督方法无缝结合。与以往仅关注伪标签或特征一致性的工作不同，本文显式利用几何信息和数据多样性来提升学生模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoTeacher通过关键点几何关系监督和距离感知的体素增强，使半监督3D目标检测在利用未标注数据时显著提升检测精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model&amp;#x27;s ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model&amp;#x27;s ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model&amp;#x27;s knowledge of object geometry to the student, thereby improving the student&amp;#x27;s capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model&amp;#x27;s ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher&lt;/p&gt;</description></item><item><guid>2512.23161v1</guid><title>Diffusion-based Decentralized Federated Multi-Task Representation Learning</title><link>http://arxiv.org/abs/2512.23161v1</link><author>Donghwa Kang, Shana Moothedath</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种去中心化的投影梯度下降算法，用于多任务表示学习，特别针对共享低维线性表示的多任务线性回归问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在数据稀缺环境下，表示学习已成为广泛采用的框架，用于从不同但相关的任务中提取特征。然而，去中心化方法尚未得到充分研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种去中心化的投影梯度下降算法，以实现多任务表示学习，并提供可证明的样本和迭代复杂度界限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用交替投影梯度下降与最小化的方式，在基于扩散的去中心化和联邦框架下恢复低秩特征矩阵，并对时间和通信复杂度进行分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验验证显示该算法在样本和迭代效率上优于基准方法，且通信开销低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该算法在多任务线性回归中实现了快速、通信高效的表示学习，并提供了理论保证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 表示学习是一个广泛采用的框架，用于在数据稀缺环境中学习，从各种不同但相关的任务中获得特征提取器或表示。尽管对表示学习进行了广泛研究，但去中心化方法仍相对未被充分探索。本文开发了一种基于去中心化投影梯度下降的多任务表示学习算法。我们关注多任务线性回归问题，其中多个线性回归模型共享一个共同的低维线性表示。我们提出了一种交替投影梯度下降和最小化算法，用于在基于扩散的去中心化和联邦方式下恢复低秩特征矩阵。我们获得了可构造、可证明的保证，提供了所需样本复杂度的下界和所提出算法的迭代复杂度上界。我们分析了算法的时间和通信复杂度，并表明它既快又通信高效。我们进行了数值模拟以验证算法的性能，并将其与基准算法进行了比较。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.&lt;/p&gt;</description></item><item><guid>2512.23176v1</guid><title>GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</title><link>http://arxiv.org/abs/2512.23176v1</link><author>Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于图像的3D目标检测框架GVSynergy-Det，通过协同学习高斯-体素表示来提升检测精度，避免使用深度传感器或密集3D监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的基于点云的3D检测需要昂贵的深度传感器，而仅使用RGB图像的检测方法往往需要密集的3D监督才能达到高精度，缺乏监督时难以准确提取几何信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种无需深度或密集3D监督即可实现高精度3D目标检测的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用双重表示架构：一方面利用可泛化的高斯光散射提取细粒面细节特征；另一方面通过交叉表示增强机制将高斯场的几何细节注入体素特征；两种表示通过可学习的融合实现特征协同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ScanNetV2和ARKitScenes室内基准上，GVSynergy-Det在不使用任何深度或密集3D几何监督的情况下，显著超过现有方法，取得最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 协同高斯-体素表示能够有效弥补单一表示的不足，实现高精度的图像基3D检测，并且不依赖昂贵的深度传感器或密集3D监督。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于图像的3D目标检测旨在仅使用RGB图像识别并定位三维空间中的目标，从而消除了点云方法所需的昂贵深度传感器。现有的基于图像的方法面临两个关键挑战：实现高精度的方法通常需要密集的三维监督，而不使用此类监督的方法则难以仅凭图像提取准确的几何信息。在本文中，我们提出了GVSynergy-Det，一种通过协同高斯-体素表示学习提升3D检测的新框架。我们的核心见解是，连续高斯和离散体素表示能够捕获互补的几何信息：高斯擅长建模细粒表面细节，而体素提供结构化的空间上下文。我们引入了双重表示架构：1）适配可泛化的高斯光散射以提取用于检测任务的互补几何特征；2）开发交叉表示增强机制，用高斯场的几何细节丰富体素特征。与以前依赖耗时的逐场景优化或仅将高斯表示用于深度正则化的方法不同，我们的协同策略通过可学习的集成直接利用两种表示的特征，从而实现更准确的目标定位。大量实验表明，GVSynergy-Det在具有挑战性的室内基准上实现了最先进的结果，在ScanNetV2和ARKitScenes数据集上显著优于现有方法，且不需要任何深度或密集三维几何监督（例如点云或TSDF）。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现仅使用 RGB 图像的 3D 目标检测，避免依赖昂贵的深度传感器或稠密 3D 注释。该问题重要，因为它能降低成本、扩大部署场景，并解决点云稀疏或缺失导致的几何不确定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到连续高斯表示能捕捉细腻表面细节，而离散体素表示提供结构化空间上下文。基于此，他们借鉴了 3D Gaussian Splatting、体素检测和 MVSDet 等现有技术，提出了双表示架构和跨表示增强机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将连续高斯和离散体素两种几何表示在特征层面融合，以获得更完整的空间信息。实现流程包括：提取 2D 图像特征 → 通过几何反投影构建体素体积 → 用可泛化的高斯 Splatting 预测高斯原语 → 将高斯特征体素化并与体素特征进行自适应融合 → 在融合后的特征上执行检测头。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次在检测任务中实现高斯与体素的深度交叉学习；2) 设计自适应跨表示增强模块，动态加权两种特征；3) 无需逐场景优化或深度监督即可获得高精度检测。与之前仅将高斯用于深度正则化或需要昂贵 3D 注释的工作不同，GVSynergy-Det 直接利用两种表示的互补优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GVSynergy-Det 通过联合学习和融合连续高斯与离散体素表示，在仅使用 RGB 图像且不依赖深度或稠密 3D 注释的条件下，实现了室内场景中最先进的 3D 目标检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).&lt;/p&gt;</description></item><item><guid>2512.23180v1</guid><title>GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation</title><link>http://arxiv.org/abs/2512.23180v1</link><author>Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo Zhang, Wuxiong Huang, Hesheng Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于3D高斯场景表示的统一驾驶世界模型框架，能够实现3D场景理解与多模态生成，并通过文本与3D场景的早期对齐提升生成质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有驾驶世界模型缺乏3D场景理解能力，且仅能基于输入数据生成内容，无法解释或推理驾驶环境；当前方法使用点云或BEV特征，无法准确将文本信息与3D场景对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够同时实现3D场景理解和多模态生成的统一框架，并通过上下文丰富提升理解与生成任务的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将丰富的语言特征嵌入每个高斯原语，实现早期模态对齐；设计任务感知的语言引导采样策略，去除冗余高斯并注入精确紧凑的3D令牌；构建双条件多模态生成模型，利用视觉语言模型捕获的信息作为高层语言条件与低层图像条件共同引导生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes和NuInteract数据集上进行的综合实验表明，该框架在多模态生成任务上达到了最先进的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提出的基于3D高斯表示的驾驶世界模型框架在理解与生成方面均表现出色，且将公开代码以促进后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本文旨在解决驾驶场景中缺乏统一的 3D 场景理解与多模态生成的难题。现有驾驶世界模型只能生成基于输入的内容，无法解释或推理环境信息，限制了其在风险预测、路径规划和训练数据生成等关键应用中的实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 3D Gaussian 场景表示（3DGS）、LangSplat、CLIP、SAM 等技术，并参考了 BEV‑based HERMES 等工作，提出将语言特征嵌入 3D 高斯体素并通过投影对齐到文本空间。随后设计了任务感知的语言引导采样和双条件多模态生成框架，以实现高效的理解与生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将图像信息编码为带语言嵌入的 3D 高斯体素，投影为统一的 token，按查询进行稀疏采样后输入 LLM 进行理解，得到文本答案和高层语言特征；随后将高层语言特征与低层图像特征作为条件，驱动扩散式生成器产生 RGB、深度等多模态输出。整体流程包括：世界分词器 → 投影器 → 任务感知采样 → LLM 理解 → 双条件生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首个基于 3D Gaussian 的统一世界模型，兼顾场景理解与生成；2) 语言嵌入与 3D 高斯的直接对齐，提升跨模态精度；3) 任务感知语言引导采样，解决 token 过长与冗余问题；4) 双条件多模态生成，结合高层语言与低层图像特征；与 HERMES 等 BEV 方法相比，提供更精确的空间对齐和更高效的 token 表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GaussianDWM 通过将 3D 高斯场景表示与视觉‑语言推理相结合，实现了驾驶场景的精准理解与连贯多模态生成，弥合了感知与合成之间的鸿沟。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.&lt;/p&gt;</description></item><item><guid>2512.23210v1</guid><title>Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks</title><link>http://arxiv.org/abs/2512.23210v1</link><author>Changgyoon Oh, Jongoh Jeong, Jegyeong Cho, Kuk-Jin Yoon</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了去噪扩散概率模型在稠密预测任务中的时间步特征选择问题，并提出了自适应时间步选择与特征整合两种模块，显著提升了少样本稠密预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 去噪扩散模型在生成任务中取得了突破，但现有方法在单任务预测中仍依赖经验性时间步特征选择，导致性能偏向特定任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究多样化时间步特征的重要性，开发自适应时间步选择机制，以提升在未见任务上的少样本稠密预测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出Task-aware Timestep Selection (TTS) 模块根据时间步损失和相似度选择最佳时间步；提出Timestep Feature Consolidation (TFC) 模块整合所选时间步特征；配合参数高效微调适配器，在Taskonomy数据集上进行验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 自适应时间步选择与特征整合显著提高了稠密预测的准确性，尤其在仅有少量支持查询的少样本场景中优于传统方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可学习的时间步整合方法在大规模稠密预测任务中表现出色，为通用和少样本学习提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 去噪扩散概率模型在生成任务中取得了巨大进展，迄今为止已达到最先进的性能。基于扩散模型的当前应用通过在多步前向-后向马尔可夫过程中学习视觉表示，并附加任务特定解码器，来实现单任务预测。然而，扩散时间步特征的启发式选择仍然严重依赖经验直觉，往往导致偏向某些任务的次优性能。为缓解这一限制，我们研究了多样化扩散时间步特征的重要性，并通过自适应选择最适合少样本稠密预测任务的时间步，在任意未见任务上进行评估。为此，我们提出了两个模块：Task-aware Timestep Selection (TTS)，根据时间步损失和相似度分数选择理想的扩散时间步；以及 Timestep Feature Consolidation (TFC)，整合所选时间步特征，以提升少样本设置下的稠密预测性能。配合我们的参数高效微调适配器，我们的框架在仅使用少量支持查询的情况下，能够有效提升稠密预测性能。我们在大规模、具有挑战性的 Taskonomy 数据集上对可学习的时间步整合方法进行了经验验证，特别针对实用的通用和少样本学习场景。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.&lt;/p&gt;</description></item><item><guid>2512.23210v2</guid><title>Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks</title><link>http://arxiv.org/abs/2512.23210v2</link><author>Changgyoon Oh, Jongoh Jeong, Jegyeong Cho, Kuk-Jin Yoon</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种针对少样本稠密预测任务的扩散模型时间步选择与特征整合方法，通过自适应选择最佳时间步并整合其特征，显著提升了在少量支持查询下的预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 扩散概率模型在生成任务中取得了突破性进展，现有方法利用多步前向后向过程学习的视觉表示，并通过任务特定解码器完成单任务预测，但时间步特征的选择仍依赖经验直觉，容易导致性能偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决时间步特征选择的经验性限制，探索适用于少样本稠密预测的最佳时间步，并提升模型在未知任务上的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出任务感知时间步选择模块（TTS），根据每个时间步的损失和相似度选择理想时间步；提出时间步特征整合模块（TFC），将选定时间步的特征融合以增强稠密预测；配合参数高效微调适配器，在少样本设置下实现性能提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，学习式时间步整合方法在大型 Taskonomy 数据集上显著提升了稠密预测性能，尤其在通用和少样本学习场景中优于传统方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应时间步选择与特征整合能够有效克服传统扩散模型在少样本稠密预测中的局限，为通用稠密预测任务提供了可行且高效的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 去噪扩散概率模型在生成任务中取得了巨大进展，迄今为止已达到最先进的性能。目前基于扩散模型的应用通过在多步前向-后向马尔可夫过程中学习视觉表示，并附加任务特定解码器，来实现单任务预测。然而，扩散时间步特征的启发式选择仍严重依赖经验直觉，往往导致偏向某些任务的次优性能。为缓解这一限制，我们研究了多功能扩散时间步特征的重要性，通过自适应选择最适合少样本稠密预测任务的时间步，在任意未见任务上进行评估。为此，我们提出了两个模块：任务感知时间步选择（TTS），根据时间步损失和相似度分数选择理想时间步；时间步特征整合（TFC），整合所选时间步特征以提升少样本设置下的稠密预测性能。配合我们的参数高效微调适配器，我们的框架在仅使用少量支持查询的情况下，能够有效提升稠密预测性能。我们在大型挑战性 Taskonomy 数据集上对可学习的时间步整合方法进行了经验验证，特别是在实用通用和少样本学习场景中。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.&lt;/p&gt;</description></item><item><guid>2512.23215v1</guid><title>AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding</title><link>http://arxiv.org/abs/2512.23215v1</link><author>Jongoh Jeong, Taek-Jin Song, Jong-Hwan Kim, Kuk-Jin Yoon</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个新的模拟环境下的道路障碍检测数据集AVOID，并在该数据集上评估了实时网络和多任务网络的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 道路场景理解对自动驾驶至关重要，尤其是在不同恶劣天气和光照条件下实时检测小型意外障碍物。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有数据集中缺乏同一视觉域下道路障碍物的问题，并提供支持多视觉感知任务的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建AVOID数据集，包含不同天气和时间条件下的道路障碍物图像、语义图、深度图、LiDAR数据和航路点；使用高性能实时网络进行障碍物检测基准测试，并开展多任务网络的消融实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; AVOID数据集丰富了道路障碍物样本，实验表明实时网络在该数据集上表现良好，多任务网络在语义分割、深度预测和航路点预测任务上具有可观的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AVOID为在恶劣视觉条件下的实时障碍物检测提供了有效的数据资源，并证明了多任务网络在相关任务中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解道路场景以实现视觉感知仍然是智能自动驾驶汽车的关键。特别是，在各种恶劣条件（例如天气和昼夜）下实时可靠地检测意外的小型道路障碍物是非常重要的。然而，现有的道路驾驶数据集仅提供在正常或恶劣场景下获取的大规模图像，并且通常不包含与其他类别在同一视觉域中捕获的道路障碍物。为了解决这个问题，我们引入了一个名为AVOID（Adverse Visual Conditions Dataset）的新数据集，用于在模拟环境中进行实时障碍物检测。AVOID包含在各种天气和时间条件下沿每条路径捕获的大量意外道路障碍物。每张图像都配有相应的语义图和深度图、原始和语义LiDAR数据以及航路点，从而支持大多数视觉感知任务。我们在高性能实时网络上对障碍物检测任务进行了基准测试，并使用全面的多任务网络对语义分割、深度和航路点预测任务进行了消融研究。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决在恶劣天气和光照条件下，实时检测道路上突发小型障碍物的问题。此类障碍物往往无预警出现，导致交通事故和人员伤亡，尤其在自动驾驶系统中对安全性构成严重威胁。现有数据集缺乏在同一视觉域下包含障碍物的标注，限制了模型在真实环境中的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有道路场景数据集的不足，发现它们要么只包含正常或恶劣条件，要么缺少障碍物标注。随后借鉴CARLA模拟器和之前的TransFuser、SHIFT等基于模拟的道路数据集，设计了在模拟环境中放置多种障碍物并同步采集多模态数据的流程。通过手工设置障碍物位置和避障路径，确保数据在不同天气和时间条件下的真实性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个大规模、标注完整的AVOID数据集，包含在42种恶劣环境组合下的道路场景，并提供语义、深度、LiDAR和航路点等多模态标注。实现流程包括：①在CARLA中导入45种障碍物模型并手动调整位置；②使用A*规划和PID控制器生成避障路径；③同步采集立体RGB、深度相机、语义LiDAR等数据；④自动生成像素级语义标签和障碍物类别。随后在此数据集上对单任务和多任务网络进行基准测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①提供了首个包含障碍物类别的恶劣条件道路数据集，规模达21,403帧；②在同一视觉域下同步采集立体RGB、深度、语义LiDAR和航路点，支持多任务学习；③引入了多天气-时间组合（42种），并保持环境不随帧变化，提升真实性；④在此数据集上实现了实时轻量级障碍物检测网络，取得51.81% IoU、18.87 FPS的性能。与之前的数据集相比，AVOID在障碍物标注、模态多样性和环境多样性方面均有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AVOID提供了一个大规模、标注完整的恶劣视觉条件下的道路障碍物数据集，并通过多模态同步采集和实时检测网络验证了其在自动驾驶感知任务中的价值。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding road scenes for visual perception remains crucial for intelligent self-driving cars. In particular, it is desirable to detect unexpected small road hazards reliably in real-time, especially under varying adverse conditions (e.g., weather and daylight). However, existing road driving datasets provide large-scale images acquired in either normal or adverse scenarios only, and often do not contain the road obstacles captured in the same visual domain as for the other classes. To address this, we introduce a new dataset called AVOID, the Adverse Visual Conditions Dataset, for real-time obstacle detection collected in a simulated environment. AVOID consists of a large set of unexpected road obstacles located along each path captured under various weather and time conditions. Each image is coupled with the corresponding semantic and depth maps, raw and semantic LiDAR data, and waypoints, thereby supporting most visual perception tasks. We benchmark the results on high-performing real-time networks for the obstacle detection task, and also propose and conduct ablation studies using a comprehensive multi-task network for semantic segmentation, depth and waypoint prediction tasks.&lt;/p&gt;</description></item><item><guid>2512.23318v1</guid><title>PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering</title><link>http://arxiv.org/abs/2512.23318v1</link><author>Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了PCR-ORB框架，通过深度学习点云细化技术提升动态环境下的视觉SLAM性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在动态环境中，移动物体会破坏跟踪精度和地图一致性，给视觉SLAM带来挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 降低动态物体干扰，提高vSLAM在复杂场景中的定位与地图构建可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在ORB-SLAM3基础上加入YOLOv8语义分割与CUDA加速，实现实时处理；采用多阶段过滤策略，包括地面平面估计、天空区域剔除、边缘过滤和时间一致性验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在KITTI数据集上，PCR-ORB在部分序列（如序列04）显著提升ATE RMSE和中位数；但不同序列表现不一，效果受场景影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 动态物体过滤仍具挑战，但PCR-ORB展示了在复杂环境中实现鲁棒导航的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 视觉同步定位与地图构建（vSLAM）系统在动态环境中面临重大挑战，移动物体会破坏跟踪精度和地图一致性。本文提出了PCR-ORB（点云细化ORB），一种增强的ORB-SLAM3框架，集成了基于深度学习的点云细化技术，以减轻动态物体干扰。我们的方法使用YOLOv8进行语义分割，并结合CUDA加速处理，实现实时性能。系统实现了多阶段过滤策略，包括地面平面估计、天空区域移除、边缘过滤和时间一致性验证。对KITTI数据集（序列00-09）的全面评估展示了不同环境条件和场景类型下的性能特征。在特定序列中观察到显著改进，序列04在ATE RMSE上提升了25.9%，ATE中位数提升了30.4%。然而，结果在不同序列中表现混合，表明效果取决于场景。实现为动态物体过滤挑战和在复杂环境中实现鲁棒导航的机遇提供了见解。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决视觉SLAM在动态环境中因移动物体导致的跟踪误差和地图不一致问题。动态物体会破坏静态世界假设，导致定位漂移、地图错误和回环检测失败。随着自动驾驶、机器人和无人机等系统在真实世界中部署，能够在动态场景下保持高精度定位变得尤为重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在ORB‑SLAM3的基础上加入了一个点云精炼模块，利用YOLOv8进行语义分割并结合多阶段几何与时序过滤。设计过程中参考了ORB‑SLAM3的三线程架构、YOLO系列的实时检测能力以及CUDA加速技术，并借鉴了先前使用YOLOv5进行动态物体过滤的工作。通过将深度学习与传统SLAM的并行线程相结合，保持了实时性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度学习得到的语义掩码来识别并剔除动态物体、天空和地面等不可靠点云，再将精炼后的点云输入SLAM流程。实现流程包括：①预处理帧并送入YOLOv8得到分割掩码；②对掩码进行后处理，执行地面估计、天空剔除、边缘过滤和时序一致性校验；③将过滤后的点云交给跟踪线程和地图线程，完成定位与地图构建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①无缝集成YOLOv8语义分割到ORB‑SLAM3；②多阶段过滤策略结合语义、几何和时序信息；③CUDA加速的并行线程实现实时性能；④专门的点云过滤线程。与以往仅使用YOLOv5或简单统计滤波的动态SLAM方法不同，PCR‑ORB提供更精细的动态物体识别和更完整的过滤流程，显著提升了在复杂动态场景下的定位精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PCR‑ORB通过CUDA加速的YOLOv8语义分割和多阶段点云精炼，显著提升了ORB‑SLAM3在动态环境中的定位精度与地图一致性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.&lt;/p&gt;</description></item><item><guid>2512.23335v2</guid><title>Visual Language Hypothesis</title><link>http://arxiv.org/abs/2512.23335v2</link><author>Xiu Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究从结构和拓扑视角探讨视觉表征学习，提出视觉理解依赖于一种语义语言，许多感知观测对应少量离散语义状态，从而将视觉观测空间组织成类似纤维束的结构，其中无关变异填充纤维，语义对应基空间。基于此结构推导出两条理论结论：一是语义不变性不能仅靠平滑变形实现，需要通过标签、跨实例识别或多模态对齐等显式语义等价目标来实现；二是逼近语义商对模型架构提出结构性要求，需具备能够展开与收缩的表征机制，以形成离散语义区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视觉表征学习常假设可迁移性和抽象性，且视觉观测可被组织成结构化空间，其中无关变异形成纤维，语义形成基空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 从拓扑角度分析视觉表征学习，探讨纤维束结构对语义不变性和模型设计的理论影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 以语义语言假设为起点，运用拓扑推理将观测空间建模为纤维束，随后推导出关于语义不变性和模型结构的理论后果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 1) 语义不变性无法仅通过平滑变形实现，需要显式的语义目标；2) 逼近语义商要求模型具备展开与收缩的机制，以形成离散语义区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该拓扑框架提供了一种解释性视角，与大规模判别式和多模态模型的经验规律以及统计学习理论的经典原则相契合，但并非具备指导性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们从结构和拓扑的角度研究视觉表征学习。我们从一个单一假设出发：视觉理解预设了一个视觉语义语言，在该语言中，许多感知观测对应少量离散的语义状态。结合对表征学习中可迁移性和抽象性的广泛假设，这一假设意味着视觉观测空间必须组织成类似纤维束的结构，其中无关变异填充纤维，语义对应基空间。基于此结构，我们推导出两个理论后果。首先，语义商不是观测空间的子流形，不能仅通过平滑变形获得，语义不变性需要非同构的、可区分的目标，例如通过标签、跨实例识别或多模态对齐提供显式语义等价。其次，我们展示了逼近语义商也对模型架构提出结构性要求。语义抽象不仅需要外部语义目标，还需要能够支持拓扑变化的表征机制：先将流形几何展开以分离结构，再收缩形成离散语义区域的展开与收缩过程。我们强调这些结果是解释性的，而非规定性的：该框架提供了一种拓扑视角，与大规模判别式和多模态模型观察到的经验规律以及统计学习理论的经典原则相一致。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient X/G is not a submanifold of X and cannot be obtained through smooth deformation alone, semantic invariance requires a non homeomorphic, discriminative target for example, supervision via labels, cross-instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand and snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.&lt;/p&gt;</description></item><item><guid>2512.23365v1</guid><title>SpatialMosaic: A Multiview VLM Dataset for Partial Visibility</title><link>http://arxiv.org/abs/2512.23365v1</link><author>Kanghee Lee, Injae Lee, Minseok Kwak, Kwonyoung Ryu, Jungi Hong, Jaesik Park</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个可扩展的多视角数据生成与标注流程，构建了包含两百万问答对的 SpatialMosaic 数据集，并推出了 SpatialMosaic-Bench 评测基准和 SpatialMosaicVLM 框架，以提升视觉语言模型在真实环境中对三维空间的推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大型语言模型在三维场景理解和空间推理方面取得快速进展，但现有方法多依赖预构建的三维表示或现成的重建管线，限制了可扩展性和实际应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决在真实环境中部分可见、遮挡和低重叠等条件下，基于多视角图像进行空间推理的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计了可扩展的多视角数据生成与标注管线，生成真实的空间推理问答对；构建了包含两百万问答对的 SpatialMosaic 数据集；推出了包含一百万问答对、六个任务的 SpatialMosaic-Bench 评测基准；并提出了将三维重建模型作为几何编码器集成到视觉语言模型中的 SpatialMosaicVLM 框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，所提出的数据集和问答任务能显著提升模型在具有挑战性的多视角条件下的空间推理性能，验证了数据生成管线的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过构建大规模、多样化的空间推理问答数据以及相应的评测基准和混合框架，本文为提升视觉语言模型在真实三维场景中的空间推理能力提供了有效方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 快速发展的多模态大型语言模型解锁了增强三维场景理解和空间推理的潜力。然而，现有方法往往依赖预构建的三维表示或现成的重建管线，限制了可扩展性和现实应用。最近的一系列工作探索了直接从多视角图像学习空间推理，使视觉语言模型能够在没有显式三维重建的情况下理解三维场景。然而，在现实环境中经常出现的部分可见、遮挡和低重叠等关键挑战仍未得到充分探讨。为解决这些限制，我们提出了一个可扩展的多视角数据生成与标注管线，构建了包含两百万问答对的 SpatialMosaic 数据集。我们进一步推出了 SpatialMosaic-Bench，一个在现实且具有挑战性场景下评估多视角空间推理的基准，包含六个任务的一百万问答对。此外，我们提出了 SpatialMosaicVLM，一个将三维重建模型作为几何编码器集成到视觉语言模型中的混合框架，以实现稳健的空间推理。大量实验表明，我们提出的数据集和问答任务能有效提升在具有挑战性的多视角条件下的空间推理能力，验证了数据生成管线在构建真实多样化问答对方面的有效性。代码和数据集将很快发布。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在让多模态大型语言模型在面对部分可见、遮挡和低重叠的多视角图像时，仍能进行可靠的三维空间推理。当前模型往往依赖预构建的三维地图或重建管线，成本高且难以扩展到动态环境；而在现实场景中，摄像机视角稀疏、物体遮挡常见，准确的空间推理对机器人导航、场景理解等任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有多模态模型在稀疏视角下的不足，随后设计了一套基于ScanNet++稠密三维注释的可扩展标注管线，用于计算遮挡比例、视角重叠并筛选符合部分可见条件的图像与实例。随后他们自动生成符合六类空间推理任务的问答对，并提出SpatialMosaicVLM，将最新的三维重建模型作为几何编码器与视觉语言模型融合。该方案借鉴了Flamingo、BLIP‑2、MiniGPT‑4等多模态模型，以及DUSt3R、MASt3R、VGGT等三维重建网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自动化的数据生成和几何编码，将稀疏、多视角的视觉信息与三维几何知识融合，以实现对部分可见、遮挡和低重叠场景的空间推理。实现流程包括：① 计算每个实例的遮挡比例和图像对的重叠比例；② 过滤满足部分可见条件的图像与实例；③ 依据空间关系填充模板生成问答对；④ 训练SpatialMosaicVLM，将重建模型产生的几何标记与每视角视觉特征融合后输入语言模型进行推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文提出了可扩展的标注框架、规模达200万条问答的训练集和100万条的评测集，专门针对部分可见、遮挡和低重叠场景；引入SpatialMosaicVLM，将三维重建模型的几何编码器与视觉语言模型结合，实现跨视角一致性和鲁棒推理；与以往仅使用2–3视角或视频帧、依赖预构建三维地图的Benchmark不同，SpatialMosaic提供2–5视角的多样化、稀疏视角设置，并在规模与任务多样性上实现突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpatialMosaic通过构建大规模、真实多视角问答数据集与基于几何编码的混合模型，首次实现了在部分可见、遮挡和低重叠条件下的鲁棒三维空间推理。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.&lt;/p&gt;</description></item><item><guid>2512.23371v1</guid><title>Domain matters: Towards domain-informed evaluation for link prediction</title><link>http://arxiv.org/abs/2512.23371v1</link><author>Yilin Bi, Junhao Bian, Shuyan Wan, Shuaijia Wang, Tao Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文系统评估了12种主流链路预测算法在740个真实网络（涵盖七个领域）上的表现，发现算法在不同领域的排名差异显著，提出了基于赢家得分的领域特定最佳算法选择方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 链路预测是复杂网络分析的基础任务，广泛应用于社交推荐、药物靶点发现和知识图谱补全等关键场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过大规模实验验证算法在各领域的真实表现，揭示跨领域一致性不足，并寻找领域特定的最佳算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对12种算法在740个网络上进行评估，使用主成分分析聚类排名向量，提出赢家得分指标来识别每个领域的顶尖算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 跨领域算法排名一致性低，而同一领域内部一致性高；主成分分析显示排名向量按领域聚类；赢家得分确定社交网络最佳为非负矩阵分解，经济领域为邻域重叠感知图神经网络，化学领域为图卷积网络，生物领域为L3资源分配；但这些领域最佳算法在其他领域表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 算法性能高度依赖网络结构，选择时需考虑领域特性，不能盲目使用所谓的“通用最优”算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 链路预测是复杂网络分析的基础任务，在社交推荐、药物靶点发现和知识图谱补全等关键场景中有广泛应用。然而，现有的算法评估往往仅在有限数量的网络上进行实验，并假设算法在不同领域的性能排名保持一致。尽管不同领域的生成机制和语义背景存在显著差异，之前的研究往往仅基于跨领域网络的简单平均值，错误地强调“通用最优”算法。本文系统评估了12种主流链路预测算法，在涵盖七个领域的740个真实网络上进行实验。我们提供了大量实证证据，阐明了算法在特定领域的表现。研究发现，跨领域的算法排名一致性显著低于同一领域内部的一致性。主成分分析显示，12种算法的排名向量在低维空间中按领域明显聚类，确认领域属性是影响算法性能的关键因素。我们提出了一种名为赢家得分的指标，可识别每个领域的最佳算法：社交网络中为非负矩阵分解，经济领域中为邻域重叠感知图神经网络，化学领域中为图卷积网络，生物领域中为基于L3的资源分配。然而，这些领域特定的顶尖算法在其他领域往往表现不佳。该发现强调了将算法机制与网络结构对齐的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Link prediction, a foundational task in complex network analysis, has extensive applications in critical scenarios such as social recommendation, drug target discovery, and knowledge graph completion. However, existing evaluations of algorithmic often rely on experiments conducted on a limited number of networks, assuming consistent performance rankings across domains. Despite the significant disparities in generative mechanisms and semantic contexts, previous studies often improperly highlight ``universally optimal&amp;quot; algorithms based solely on naive average over networks across domains. This paper systematically evaluates 12 mainstream link prediction algorithms across 740 real-world networks spanning seven domains. We present substantial empirical evidence elucidating the performance of algorithms in specific domains. This findings reveal a notably low degree of consistency in inter-domain algorithm rankings, a phenomenon that stands in stark contrast to the high degree of consistency observed within individual domains. Principal Component Analysis shows that response vectors formed by the rankings of the 12 algorithms cluster distinctly by domain in low-dimensional space, thus confirming domain attributes as a pivotal factor affecting algorithm performance. We propose a metric called Winner Score that could identify the superior algorithm in each domain: Non-Negative Matrix Factorization for social networks, Neighborhood Overlap-aware Graph Neural Networks for economics, Graph Convolutional Networks for chemistry, and L3-based Resource Allocation for biology. However, these domain-specific top-performing algorithms tend to exhibit suboptimal performance in other domains. This finding underscores the importance of aligning an algorithm&amp;#x27;s mechanism with the network structure.&lt;/p&gt;</description></item><item><guid>2512.23406v1</guid><title>Task-driven Heterophilic Graph Structure Learning</title><link>http://arxiv.org/abs/2512.23406v1</link><author>Ayushman Raghuvanshi, Gonzalo Mateos, Sundeep Prabhakar Chepuri</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于频率引导的图结构学习框架 FgGSL，能够同时学习同类和异类图结构，并通过谱编码器提升节点表征。该方法利用可学习的特征掩码函数生成互补图，并通过低通和高通滤波器处理，结合标签驱动的结构损失实现任务导向的图结构学习。实验表明，在六个异类图基准上，FgGSL 在性能上优于现有最先进的 GNN 和图重连方法，验证了频率信息与监督拓扑推断相结合的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统图神经网络在异类图（相连节点标签不相似、特征相似度弱）上难以学习区分性节点表征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种端到端的图推断框架，联合学习同类与异类图结构以及谱编码器，以提升异类图上的节点表征效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; FgGSL 采用可学习的对称特征驱动掩码函数推断互补图，随后使用预设的低通和高通图滤波器组处理；通过基于标签的结构损失显式促进同类与异类边的恢复，实现任务驱动的图结构学习；并对结构损失推导稳定性界限，对滤波器组在图扰动下的鲁棒性给出保证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在六个异类图基准实验中，FgGSL 一致优于最先进的 GNN 和图重连方法，显示将频率信息与监督拓扑推断相结合能显著提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将频率信息与监督拓扑推断相结合的图结构学习方法能有效克服异类图的挑战，显著提升 GNN 的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.&lt;/p&gt;</description></item><item><guid>2512.23413v1</guid><title>Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment</title><link>http://arxiv.org/abs/2512.23413v1</link><author>Henglin Liu, Nisha Huang, Chang Liu, Jiangpeng Yan, Huijuan Huang, Jixuan Ying, Tong-Yee Lee, Pengfei Wan, Xiangyang Ji</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; • 研究聚焦于人工智能生成内容（AIGC）的审美质量评估，强调其对人类对齐评价体系的重要性。• 现有评估面临视觉感知、认知与情感交织的复杂性，导致数据稀缺与不平衡以及模型碎片化问题。• 为解决数据不足，提出了 Refined Aesthetic Description（RAD）数据集，采用迭代生成流程，规模约七万条多维结构化描述，成本低且易扩展。• 为解决模型碎片化，设计了 ArtQuant 框架，将独立审美维度通过联合描述生成耦合，并利用大型语言模型解码器提升长文本语义建模。• 理论分析表明，RAD 的语义充分性与生成范式共同降低预测熵，为框架提供数学依据。• 实验显示，ArtQuant 在多个数据集上实现了最先进性能，仅需传统训练周期的 33%，显著缩小艺术图像与审美判断之间的认知差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 审美质量评估是构建与人类审美一致的 AIGC 评价系统的核心任务，但其涉及视觉感知、认知与情感的多维复杂性，使得数据集往往偏重视觉层面，缺乏深层描述；同时现有视觉网络将审美属性拆分为多分支编码器，跨模态方法在处理长文本描述时效果有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建规模化、多维结构化的审美描述数据集（RAD），以及提出能够联合生成审美维度并有效处理长文本的评估框架（ArtQuant），解决数据稀缺与模型碎片化两大挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. RAD 数据集采用迭代生成管道，减少人工标注成本，生成约七万条多维审美描述。2. ArtQuant 框架通过联合描述生成将独立审美维度耦合，并利用大型语言模型解码器提升长文本语义建模。3. 对框架进行理论分析，证明 RAD 的语义充分性与生成范式共同降低预测熵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ArtQuant 在多个公开数据集上实现了最先进的评估性能，仅需传统训练周期的 33%，显著提升了训练效率并缩小了艺术图像与审美判断之间的认知差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 构建多维审美描述数据集与联合生成评估框架能够有效克服数据稀缺与模型碎片化问题，提升审美质量评估的准确性与效率，为未来 AIGC 研究提供了可复用的数据与方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 审美质量评估任务对于构建与人类审美一致的 AIGC 定量评价体系至关重要。然而，其固有的复杂性涵盖视觉感知、认知与情感，带来了根本性的挑战。虽然审美描述提供了对这种复杂性的可行表示，但仍存在两个关键挑战：（1）数据稀缺与不平衡：现有数据集过度关注视觉感知，因昂贵的人工标注而忽视更深层维度；（2）模型碎片化：当前视觉网络通过多分支编码器孤立审美属性，而以对比学习为代表的跨模态方法难以有效处理长文本描述。为解决挑战（1），我们首先提出了 Refined Aesthetic Description（RAD）数据集，这是一个规模约七万条、结构化的多维数据集，采用迭代管道生成，成本低且易于扩展。为解决挑战（2），我们提出了 ArtQuant，一种针对艺术图像的审美评估框架，它不仅通过联合描述生成耦合孤立的审美维度，还借助大型语言模型解码器更好地建模长文本语义。理论分析进一步确认了这种协同作用：RAD 的语义充分性（数据）与生成范式（模型）共同最小化预测熵，为框架提供了数学依据。我们的方案在多个数据集上实现了最先进的性能，仅需传统训练周期的 33%，缩小了艺术图像与审美判断之间的认知差距。我们将发布代码和数据集，以支持未来研究。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD&amp;#x27;s semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.&lt;/p&gt;</description></item><item><guid>2512.23441v1</guid><title>Stochastic Siamese MAE Pretraining for Longitudinal Medical Images</title><link>http://arxiv.org/abs/2512.23441v1</link><author>Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunović</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; STAMP是一种基于Siamese MAE的时序自监督学习框架，通过在两张扫描图像的时间差上进行条件化，使用随机过程来学习疾病进展的非确定性时序动态。它在OCT和MRI数据集上表现优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的自监督学习方法如MAE在医学影像中表现良好，但缺乏对时间信息的感知，无法捕捉疾病进展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够在自监督学习中引入时间信息、并考虑疾病进展不确定性的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; STAMP采用Siamese MAE结构，利用时间差作为条件，重构损失改写为条件变分推断目标，从而以随机方式学习时序动态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多访视OCT和MRI数据集上，STAMP预训练的ViT模型在晚期年龄相关性黄斑变性和阿尔茨海默病进展预测任务中，优于现有时序MAE方法和基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入随机时序编码，STAMP能够更好地捕捉疾病进展的非确定性时序特征，提升预测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时间感知的图像表示对于捕捉3D体积纵向医学数据中的疾病进展至关重要。然而，最近的自监督学习方法如掩码自编码器（MAE）虽然具备强大的表示学习能力，却缺乏时间感知。在本文中，我们提出了STAMP（带掩码预训练的随机时间自编码器），这是一种Siamese MAE框架，通过在两张输入体积的时间差上进行条件化，利用随机过程来编码时间信息。与仅比较不同时间点扫描的确定性Siamese方法不同，STAMP通过将MAE重构损失重新表述为条件变分推断目标，以随机方式学习时序动态。我们在两个OCT和一个MRI数据集上进行评估，结果显示STAMP预训练的ViT模型在预测晚期年龄相关性黄斑变性和阿尔茨海默病进展方面，优于现有的时序MAE方法和基础模型，证明了其在学习疾病非确定性时序动态方面的优势。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer&amp;#x27;s Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.&lt;/p&gt;</description></item><item><guid>2512.23449v1</guid><title>Universal and Experiment-calibrated Prediction of XANES through Crystal Graph Neural Network and Transfer Learning Strategy</title><link>http://arxiv.org/abs/2512.23449v1</link><author>Zichang Lin, Wenjie Chen, Yitao Lin, Xinxin Zhang, Yuegang Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于晶体图神经网络的XANES预测方法，先用模拟数据预训练，再用少量实验数据进行迁移学习校准，显著提升预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统理论模拟方法在处理大规模数据时既复杂又耗时，且现有AI模型多基于模拟数据训练，导致与实验谱存在显著差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够普适预测48种元素XANES谱的模型，并通过实验数据校准提高其准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用晶体图神经网络在48种元素的模拟XANES数据上预训练，然后利用迁移学习在少量实验XANES数据上进行校准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 预训练模型平均相对平方误差为0.020223；校准后S、Ti、Fe K边缘能量误差下降约55%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法实现了快速、普适且实验校准的XANES预测，为材料研究提供了新工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理论模拟有助于准确解释包含丰富原子和电子结构信息的X射线吸收近边结构（XANES）光谱。然而，当前的模拟方法通常过于复杂，无法在需要大量数据分析时提供所需的准确性和时效性，例如用于电池材料的原位表征。为了解决这些问题，已经开发了用于XANES预测的人工智能（AI）模型。然而，现有模型并未使用实验XANES数据，而是使用模拟数据进行训练，导致预测光谱与实验光谱之间存在显著差异。此外，这些模型在不同元素之间的普适性尚未得到充分研究。在本研究中，我们首先建立了一个晶体图神经网络，在覆盖48种元素的模拟XANES数据上进行预训练，以实现低平均相对平方误差为0.020223的普适XANES预测；随后利用迁移学习使用少量实验XANES数据对模型进行校准。校准后，预测的S、Ti和Fe K边缘XANES的边缘能量误差显著降低约55%。本工作展示的方法为实现快速、普适且实验校准的XANES预测开辟了新途径。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Theoretical simulation is helpful for accurate interpretation of experimental X-ray absorption near-edge structure (XANES) spectra that contain rich atomic and electronic structure information of materials. However, current simulation methods are usually too complex to give the needed accuracy and timeliness when a large amount of data need to be analyzed, such as for in-situ characterization of battery materials. To address these problems, artificial intelligence (AI) models have been developed for XANES prediction. However, instead of using experimental XANES data, the existing models are trained using simulated data, resulting in significant discrepancies between the predicted and experimental spectra. Also, the universality across different elements has not been well studied for such models. In this work, we firstly establish a crystal graph neural network, pre-trained on simulated XANES data covering 48 elements, to achieve universal XANES prediction with a low average relative square error of 0.020223; and then utilize transfer learning to calibrate the model using a small experimental XANES dataset. After calibration, the edge energy misalignment error of the predicted S, Ti and Fe K edge XANES is significantly reduced by about 55%. The method demonstrated in this work opens up a new way to achieve fast, universal, and experiment-calibrated XANES prediction.&lt;/p&gt;</description></item><item><guid>2512.23472v1</guid><title>MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration</title><link>http://arxiv.org/abs/2512.23472v1</link><author>Shuyuan Lin, Wenwu Peng, Junjie Huang, Qiang Qi, Miaohui Wang, Jian Weng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种多域上下文集成网络MCI-Net，用于改进点云配准中的特征学习和配准性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有基于深度学习的方法主要依赖欧氏邻域策略，难以有效捕捉点云中的隐含语义和结构一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统方法在特征提取和配准中的局限性，提升特征表达和配准的鲁棒性与辨别力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MCI-Net包含三大模块：全局图邻域聚合模块构建整体结构关系；渐进式上下文交互模块通过域内特征解耦和域间上下文交互提升辨别力；动态内点选择模块利用多次位姿估计的残差信息优化内点权重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在室内RGB-D和室外LiDAR数据集上，MCI-Net显著优于现有最先进方法，在3DMatch数据集上实现了96.4%的最高配准召回率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多域上下文集成网络通过全局结构捕捉、上下文交互和动态内点选择，有效提升了点云配准的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出一种多域上下文集成网络（MCI-Net），通过聚合来自不同域的上下文线索来改进特征表示和配准性能。具体而言，作者提出了一个图邻域聚合模块，构建全局图以捕捉点云中的整体结构关系；随后提出了渐进式上下文交互模块，通过域内特征解耦和域间上下文交互来增强特征辨别力；最后设计了动态内点选择方法，利用多次位姿估计的残差信息优化内点权重，从而提升配准的准确性和鲁棒性。实验结果表明，MCI-Net在室内RGB-D和室外LiDAR数据集上显著优于现有最先进方法，在3DMatch数据集上实现了最高96.4%的配准召回率。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决点云配准中的鲁棒性和精度问题，尤其是传统方法仅依赖欧氏邻域难以捕捉全局语义和结构一致性。点云配准是三维重建、机器人定位和自动驾驶等领域的核心任务，鲁棒的配准直接影响后续处理的质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了现有基于欧氏邻域的特征提取方法的局限，随后借鉴图神经网络、注意力机制和历史残差信息等技术，提出了全局图聚合、进阶上下文交互和动态内点选择三大模块。该设计在多篇前沿工作（如DCP、CoFiNet、Hyperbolic embedding等）的基础上进一步融合多域信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多域（坐标、特征、全局图）上下文整合来提升特征判别力，并通过迭代内点权重更新提高配准鲁棒性。实现流程为：①使用PAConv提取点/块特征；②将特征映射到坐标、特征和全局图域；③GNAM构建全局图并自适应聚合邻域；④PCIM在各域内进行特征分解并跨域交互；⑤匹配得到对应关系；⑥DISM利用多轮残差动态更新内点权重；⑦最终求解刚性变换。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; ①全局图邻域聚合模块（GNAM）捕获整体结构；②进阶上下文交互模块（PCIM）实现局部-全局分解与跨域注意力；③动态内点选择（DISM）通过历史残差迭代优化对应权重。与以往仅使用局部欧氏邻域或单轮对应的配准方法不同，MCI‑Net在特征提取、上下文融合和内点更新上均实现了多域协同与迭代改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MCI‑Net通过全局图聚合、跨域上下文交互和动态内点迭代，构建了一个多域上下文整合框架，显著提升了点云配准的鲁棒性和精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\% on 3DMatch. Source code is available at http://www.linshuyuan.com.&lt;/p&gt;</description></item><item><guid>2512.23485v1</guid><title>FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence</title><link>http://arxiv.org/abs/2512.23485v1</link><author>Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一种新的参数高效微调方法 FRoD，能够在保持高表达能力的同时显著减少可训练参数量，并在多种任务上与完整模型微调取得相同的准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大型基础模型的普及，传统微调需要大量计算和内存，参数高效微调方法通过只更新少量参数来降低成本，但现有方法如 LoRA 在低秩约束下表现出收敛慢和适应性有限的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决低秩约束导致的收敛慢和表达不足，使参数高效微调能够捕捉更复杂的模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; FRoD 通过层间共享全局基底和在缩放因子中注入稀疏可学习扰动，实现全秩更新的灵活性，同时保持参数量低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 20 个涵盖视觉、推理和语言理解的基准上，FRoD 在相同训练预算下仅使用 1.72% 的可训练参数，却能达到与完整模型微调相同的准确率，并且收敛更快、更稳健。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FRoD 在保持高表达能力的同时实现了参数高效微调的目标，为大模型的实际部署提供了更优的方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 参数高效微调（PEFT）方法已成为将大型基础模型适配到下游任务的实用方案，通过仅更新少量参数来降低计算和内存成本。其中，LoRA 等方法试图在效率与表达力之间取得平衡，但由于其固有的低秩约束，往往导致收敛慢和适应能力有限。这种权衡阻碍了 PEFT 方法捕捉多样任务所需的复杂模式。为解决这些挑战，我们提出了 FRoD，一种结合层级联合分解与旋转自由度的新型微调方法。通过在各层提取全局共享基底并在缩放因子中注入稀疏可学习扰动，实现灵活的全秩更新，FRoD 提升了表达力和效率，带来更快、更稳健的收敛。在涵盖视觉、推理和语言理解的 20 个基准上，FRoD 在相同训练预算下仅使用 1.72% 的可训练参数，却能匹配完整模型微调的准确率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.&lt;/p&gt;</description></item><item><guid>2512.23486v1</guid><title>Multi-label Classification with Panoptic Context Aggregation Networks</title><link>http://arxiv.org/abs/2512.23486v1</link><author>Mingyuan Jiu, Hailong Zhu, Wenchuan Wei, Hichem Sahbi, Rongrong Ji, Mingliang Xu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 研究背景：上下文建模对视觉识别至关重要；2. 现有方法局限于基本几何关系或局部特征，忽略跨尺度交互；3. 研究目的：提出PanCAN网络，层次化整合多阶几何上下文；4. 方法：在每个尺度学习多阶邻域关系，结合随机游走和注意力机制；5. 关键技术：跨尺度模块级联，细尺度显著锚点选择并动态融合邻域特征；6. 主要发现：PanCAN在NUS-WIDE、PASCAL VOC2007、MS-COCO上表现优异，超越现有技术；7. 结论：跨尺度、多阶上下文聚合显著提升多标签分类性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 上下文建模是视觉识别的关键，能够通过整合图像中对象与标签的内在和外在关系，生成高度区分的图像表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新方法——Deep Panoptic Context Aggregation Network（PanCAN），以层次化方式整合多阶几何上下文，并通过跨尺度特征聚合提升复杂场景的理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PanCAN在每个尺度学习多阶邻域关系，结合随机游走和注意力机制；不同尺度的模块级联，细尺度显著锚点被选取并通过注意力动态融合其邻域特征，实现跨尺度建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在NUS-WIDE、PASCAL VOC2007和MS-COCO三大多标签分类基准上，PanCAN在定量和定性评估中均优于现有最先进技术，显著提升多标签分类性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 跨尺度、多阶上下文聚合能够显著增强视觉识别的表现，PanCAN为多标签分类提供了有效的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 上下文建模对视觉识别至关重要，通过整合图像中对象与标签的内在和外在关系，能够生成高度区分的图像表示。目前的方法局限于基本几何关系或局部特征，往往忽视对象之间跨尺度的上下文交互。本文提出了Deep Panoptic Context Aggregation Network（PanCAN），一种新方法，通过跨尺度特征聚合在高维Hilbert空间中层次化整合多阶几何上下文。具体而言，PanCAN在每个尺度学习多阶邻域关系，结合随机游走与注意力机制。不同尺度的模块级联，细尺度的显著锚点被选取，其邻域特征通过注意力动态融合。该方法实现了有效的跨尺度建模，显著提升了复杂场景的理解，结合多阶和跨尺度的上下文感知特征。对NUS-WIDE、PASCAL VOC2007和MS-COCO基准的多标签分类实验表明，PanCAN始终取得竞争性结果，且在定量和定性评估中均优于最先进技术，显著提升了多标签分类性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.&lt;/p&gt;</description></item><item><guid>2512.23489v1</guid><title>The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction</title><link>http://arxiv.org/abs/2512.23489v1</link><author>Haoyu Pei, Zhongyang Liu, Xiangyi Xiao, Xiaocong Du, Haipeng Zhang, Kunpeng Zhang, Suting Hong</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了风险投资投资成功预测的问题，提出了一种基于检索增强生成的多视角框架MIRAGE-VC，利用信息增益驱动的路径检索和多代理门控融合三类证据，显著提升了预测性能，并为离图预测任务提供了新的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 风险投资大多数投资失败，少数产生巨大回报。传统机器学习和图神经网络缺乏显式推理能力，难以整合公司披露、投资者记录和投资网络结构等复杂关系证据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 准确预测创业公司成功需要通过显式推理整合多源关系证据，形成可解释的投资论点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; MIRAGE-VC采用检索增强生成框架，信息增益驱动的路径检索逐步挑选高价值邻居，将投资网络压缩为可管理的链路；多代理架构通过可学习门控机制融合公司属性、投资者记录和网络结构三类证据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在严格防泄漏控制下，MIRAGE-VC相较基线提升了5.0% F1和16.6% PrecisionAt5，并为推荐和风险评估等离图预测任务提供了启示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结合LLM推理与图检索的MIRAGE-VC有效解决路径爆炸和异质证据融合问题，显著提升风险投资成功预测性能，为离图预测任务开辟了新路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大多数风险投资（VC）投资失败，而少数则产生超额回报。准确预测创业公司成功需要综合复杂的关系证据，包括公司披露、投资者记录和投资网络结构，通过显式推理形成连贯、可解释的投资论点。传统机器学习和图神经网络都缺乏这种推理能力。大型语言模型（LLM）提供强大的推理能力，但与图的模态不匹配。最近的图-LLM方法针对图内任务，即答案位于图内，而VC预测是离图任务：目标存在于网络之外。核心挑战是选择最大化外部目标预测性能的图路径，同时实现逐步推理。我们提出MIRAGE-VC，一种多视角检索增强生成框架，解决两个障碍：路径爆炸（成千上万的候选路径压垮LLM上下文）和异质证据融合（不同创业公司需要不同的分析重点）。我们的信息增益驱动路径检索器迭代选择高价值邻居，将投资网络压缩为紧凑链路以进行显式推理。多代理架构通过基于公司属性的可学习门控机制整合三条证据流。在严格的防泄漏控制下，MIRAGE-VC实现了+5.0% F1和+16.6% PrecisionAt5，并为推荐和风险评估等离图预测任务提供了启示。代码：https://anonymous.4open.science/r/MIRAGE-VC-323F。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.&lt;/p&gt;</description></item><item><guid>2512.23545v1</guid><title>PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</title><link>http://arxiv.org/abs/2512.23545v1</link><author>Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 现有病理基础模型在视觉表示学习和多模态交互方面取得显著进展，但大多采用一次性推理，缺乏在诊断不确定时的重新评估和有针对性的证据获取。2. 这与临床诊断流程不同，临床诊断通过多次切片观察和进一步检查来细化假设。3. 本文提出 PathFound，一种具备代理能力的多模态模型，支持证据寻求推理。4. PathFound 结合病理视觉基础模型、视觉-语言模型和强化学习训练的推理模型，按初步诊断、证据寻求和最终决策阶段主动获取信息并细化诊断。5. 在多种大型多模态模型中，采用该策略始终提升诊断准确率，验证了证据寻求工作流程在计算病理学中的有效性。6. PathFound 在多种临床场景下实现了最先进的诊断性能，并能发现细微特征，如核特征和局部浸润。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 病理基础模型在视觉表示学习和多模态交互方面取得显著进展，但大多采用一次性推理，缺乏在诊断不确定时的重新评估和有针对性的证据获取。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 PathFound，一种具备代理能力的多模态模型，支持证据寻求推理，以改进病理诊断流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 整合病理视觉基础模型、视觉-语言模型和强化学习训练的推理模型，按初步诊断、证据寻求和最终决策阶段主动获取信息并细化诊断。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 采用证据寻求策略在多种大型多模态模型中始终提升诊断准确率；PathFound 在多种临床场景下实现了最先进的诊断性能，并能发现细微特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 证据寻求工作流程在计算病理学中有效，PathFound 展示了强大的潜力，可发现细微细节并提升诊断准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近的病理基础模型在视觉表示学习和多模态交互方面取得了显著进展。然而，大多数模型仍依赖于一次性推理范式，即一次性处理全切片图像以生成预测，而不在诊断不确定时重新评估或获取有针对性的证据。这与临床诊断工作流程形成对比，后者通过重复观察切片和进一步检查来细化假设。我们提出了 PathFound，一种具备代理能力的多模态模型，旨在支持病理诊断中的证据寻求推理。PathFound 结合了病理视觉基础模型、视觉-语言模型和通过强化学习训练的推理模型，能够通过初步诊断、证据寻求和最终决策阶段主动获取信息并细化诊断。在多种大型多模态模型中，采用此策略始终提升诊断准确率，表明证据寻求工作流程在计算病理学中的有效性。在这些模型中，PathFound 在多种临床场景下实现了最先进的诊断性能，并展示了强大的潜力，能够发现细微细节，如核特征和局部浸润。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.&lt;/p&gt;</description></item><item><guid>2512.23558v1</guid><title>A NEAT Approach to Evolving Neural-Network-based Optimization of Chiral Photonic Metasurfaces: Application of a Neuro-Evolution Pipeline</title><link>http://arxiv.org/abs/2512.23558v1</link><author>Davide Filippozzi, Arash Rahimi-Iman</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文将神经进化算法NEAT与深度学习优化框架相结合，用于设计具有定制光学特性的手性介质超表面。通过NEAT自动演化网络拓扑和权重，配合强化学习策略，构建了更适合任务的模型。实验使用9,600个GaP超表面模拟数据，比较不同输入维度、特征缩放和数据量下的表现。结果表明标准化特征缩放最稳健，NEAT演化出的紧凑网络在完整优化流程中与传统稠密感知机相比，预测精度和泛化能力相当或更优，并能成功推断出在可见光谱中具有强圆二色性的超表面，实现了模拟与实验数据之间的迁移学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 手性超表面的设计因几何与手性光学响应之间高度非线性关系而面临挑战；机器学习辅助的优化流程已成为加速设计的有效工具，但其性能高度依赖神经网络架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将NEAT算法集成到现有的深度学习优化框架中，以实现自动化、任务特定的网络架构演化，从而提升设计效率和模型性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用NEAT自动演化网络拓扑和权重，并与强化学习策略并行，构建优化管道；利用9,600个GaP超表面模拟数据，评估不同输入维度、特征缩放方法和数据规模下的模型表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 标准化特征缩放在两种输出维度下表现最为一致；NEAT演化出的紧凑网络在完整优化流程中预测精度和泛化能力与传统稠密感知机相当或更优；这些模型能够推断出具有强圆二色性的可见光谱超表面，并实现模拟与实验数据之间的迁移学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; NEAT驱动的资源高效模型为自适应、自动化的光子设计提供了可扩展路径，可作为独立或构建代理式人工智能的基础模块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The design of chiral metasurfaces with tailored optical properties remains a central challenge in nanophotonics due to the highly nonlinear relationship between geometry and chiroptical response. Machine-learning-assisted optimization pipelines have recently emerged as efficient tools to accelerate this process, yet their performance strongly depends on the choice of neural-network (NN) architecture. In this work, we integrate the NeuroEvolution of Augmenting Topologies (NEAT) algorithm into an established deep-learning optimization framework for dielectric chiral metasurfaces. NEAT autonomously evolves both network topology and connection weights, enabling task-specific architectures without manual tuning, whereas the reinforcement-learning strategy in our framework evolves knowledge of the solution space and fine-tunes a model&amp;#x27;s weights in parallel. Using a pipeline-produced dataset of 9,600 simulated GaP metasurface geometries, we evaluate NEAT under varying input dimensionalities, feature-scaling methods, and data sizes. With standardized feature scaling yielding the most consistent performance for both examined output dimensionalities, the relatively compact NEAT-evolved NN models, when integrated into the full optimization pipeline, achieve similar or improved predictive accuracy and generalization compared to initially employed dense few-layer perceptrons. Accordingly, these resource-efficient models successfully perform inference of metasurfaces exhibiting strong circular dichroism in the visible spectrum, allowing for transfer learning between simulated and experimental data. This approach demonstrates a scalable path toward adaptive, self-configuring machine-learning frameworks for automated photonic design both standalone and as building block for agentic artificial intelligence (AI).&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The design of chiral metasurfaces with tailored optical properties remains a central challenge in nanophotonics due to the highly nonlinear relationship between geometry and chiroptical response. Machine-learning-assisted optimization pipelines have recently emerged as efficient tools to accelerate this process, yet their performance strongly depends on the choice of neural-network (NN) architecture. In this work, we integrate the NeuroEvolution of Augmenting Topologies (NEAT) algorithm into an established deep-learning optimization framework for dielectric chiral metasurfaces. NEAT autonomously evolves both network topology and connection weights, enabling task-specific architectures without manual tuning, whereas the reinforcement-learning strategy in our framework evolves knowledge of the solution space and fine-tunes a model&amp;#x27;s weights in parallel. Using a pipeline-produced dataset of 9,600 simulated GaP metasurface geometries, we evaluate NEAT under varying input dimensionalities, feature-scaling methods, and data sizes. With standardized feature scaling yielding the most consistent performance for both examined output dimensionalities, the relatively compact NEAT-evolved NN models, when integrated into the full optimization pipeline, achieve similar or improved predictive accuracy and generalization compared to initially employed dense few-layer perceptrons. Accordingly, these resource-efficient models successfully perform inference of metasurfaces exhibiting strong circular dichroism in the visible spectrum, allowing for transfer learning between simulated and experimental data. This approach demonstrates a scalable path toward adaptive, self-configuring machine-learning frameworks for automated photonic design both standalone and as building block for agentic artificial intelligence (AI).&lt;/p&gt;</description></item><item><guid>2512.23585v1</guid><title>Unsupervised Learning for Detection of Rare Driving Scenarios</title><link>http://arxiv.org/abs/2512.23585v1</link><author>Dat Le, Thomas Manhardt, Moritz Venator, Johannes Betz</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种无监督学习框架，利用自然驾驶数据通过深度孤立森林和t‑SNE方法检测罕见和危险驾驶场景，并通过代理真值和视频帧评估验证其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自动驾驶系统中，识别罕见和危险的驾驶情景是确保安全和可靠性的关键挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索一种无监督学习框架，利用自然驾驶数据检测罕见和极端驾驶场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用深度孤立森林结合神经网络特征表示，预处理感知模块数据为滑动窗口统计特征，采用t‑SNE降维可视化，评估使用代理真值和视频帧检查。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法能够有效识别罕见和危险驾驶场景，提供可扩展的异常检测解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在自动驾驶系统中可实现可扩展的异常检测，但依赖代理真值和手工特征组合，未能覆盖所有真实世界异常及其细微上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 检测罕见和危险驾驶场景是确保自动驾驶系统安全可靠的关键挑战。本研究探讨了一个无监督学习框架，利用自然驾驶数据检测罕见和极端驾驶场景。我们采用最近提出的深度孤立森林（DIF），这是一种将基于神经网络的特征表示与孤立森林相结合的异常检测算法，能够识别非线性和复杂的异常。来自感知模块的数据，捕捉车辆动力学和环境条件，被预处理为从滑动窗口提取的结构化统计特征。该框架结合t-分布随机邻域嵌入（t-SNE）进行降维和可视化，以提高检测到的异常的可解释性。评估使用代理真值，结合定量指标和定性视频帧检查。我们的结果表明，所提出的方法能够有效识别罕见和危险的驾驶场景，为自动驾驶系统的异常检测提供了可扩展的解决方案。鉴于研究方法，无法避免依赖代理真值和手工定义的特征组合，这些并未涵盖真实世界驾驶异常的全部范围或其细微的上下文依赖。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The detection of rare and hazardous driving scenarios is a critical challenge for ensuring the safety and reliability of autonomous systems. This research explores an unsupervised learning framework for detecting rare and extreme driving scenarios using naturalistic driving data (NDD). We leverage the recently proposed Deep Isolation Forest (DIF), an anomaly detection algorithm that combines neural network-based feature representations with Isolation Forests (IFs), to identify non-linear and complex anomalies. Data from perception modules, capturing vehicle dynamics and environmental conditions, is preprocessed into structured statistical features extracted from sliding windows. The framework incorporates t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization, enabling better interpretability of detected anomalies. Evaluation is conducted using a proxy ground truth, combining quantitative metrics with qualitative video frame inspection. Our results demonstrate that the proposed approach effectively identifies rare and hazardous driving scenarios, providing a scalable solution for anomaly detection in autonomous driving systems. Given the study&amp;#x27;s methodology, it was unavoidable to depend on proxy ground truth and manually defined feature combinations, which do not encompass the full range of real-world driving anomalies or their nuanced contextual dependencies.&lt;/p&gt;</description></item><item><guid>2512.23617v1</guid><title>Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</title><link>http://arxiv.org/abs/2512.23617v1</link><author>Deniz Akdemir</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文指出传统无监督领域适应方法在面对信息不均衡的域时会导致负迁移，并提出基于Le Cam理论的决策框架，通过方向性可模拟性替代对称不变性，实现风险可控的迁移学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 分布偏移是现实机器学习中的核心挑战，主流方法通过对称散度最小化实现特征不变性，但在高质量与降质传感器等信息不均衡场景下会破坏信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 证明严格不变性导致信息破坏并产生负迁移，提出一种新的理论框架以避免此问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用Le Cam理论构造Le Cam失真度量，采用方向性可模拟性学习核函数，使源域模拟目标域，从而在不降低源域效用的前提下完成迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在基因组学、视觉分类和强化学习等五个实验中，Le Cam失真度量实现了近乎完美的频率估计、保持CIFAR‑10分类准确率、以及安全的策略迁移，显著优于传统不变性方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架为医疗影像、自动驾驶和精准医学等对负迁移不可接受的领域提供了首个风险可控的迁移学习理论基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 分布偏移是现实机器学习的核心挑战。主流范式——无监督领域适应（UDA）通过对称散度最小化实现特征不变性，将源域和目标域的表示对齐。我们证明了这种方法根本存在缺陷：当域信息不均衡（例如高质量与降质传感器）时，严格的不变性会导致信息破坏，产生“负迁移”，在安全关键应用中可能灾难性。我们提出了一个基于Le Cam统计实验理论的决策框架，利用构造逼近将对称不变性替换为方向性可模拟性。我们引入Le Cam失真度量，通过缺陷距离δ(E₁,E₂)量化可模拟性下的迁移风险上界。该框架通过学习一个核函数，使源域模拟目标域，从而实现无源域降解的迁移。在基因组学、视觉、强化学习等五个实验中，Le Cam失真度量取得了：1）在HLA基因组学中近乎完美的频率估计（相关系数r=0.999，匹配传统方法）；2）在CIFAR‑10图像分类中零源域效用损失（81.2%准确率保持，对比CycleGAN下降至34.7%）；3）在RL控制中的安全策略迁移，传统不变性方法导致灾难性崩溃。Le Cam失真度量为在负迁移不可接受的领域（医疗影像、自动系统、精准医学）提供了首个基于风险控制的迁移学习原则框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing &amp;quot;negative transfer&amp;quot; that can be catastrophic in safety-critical applications [Wang et al., 2019].   We propose a decision-theoretic framework grounded in Le Cam&amp;#x27;s theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.&lt;/p&gt;</description></item><item><guid>2512.23622v1</guid><title>Information is localized in growing network models</title><link>http://arxiv.org/abs/2512.23622v1</link><author>Till Hoffmann, Jukka-Pekka Onnela</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究机制网络模型的参数推断问题，证明在许多增长网络模型中，参数信息局限于网络的小子图内。作者从贝叶斯角度出发，提出使用图神经网络（受限感受野）构建的神经密度估计器来逼近后验分布，并在九种模型上验证了信息局部化与估计器结果的一致性。即使在非局部化模型中，该方法也能以较低成本得到与传统方法相当的高保真后验。研究表明信息局部化是网络增长的基本属性，为在更大、未观测网络中利用局部子图和有限感受野GNN进行无似然推断提供了理论依据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 机制网络模型通过少量可解释机制捕捉经验网络特征，但由于似然往往不可计算，参数推断具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 证明广泛增长网络模型中参数信息局限于小子图；开发基于图神经网络的贝叶斯推断方法；评估其在多种模型上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用贝叶斯推断框架，构建神经密度估计器近似后验；使用受限感受野的图神经网络仅关注小子图；对九种增长网络模型进行局部化特征化，并在模拟数据上比较预测与估计结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 局部化预测与神经密度估计器结果一致；即使在非局部化模型中，该方法也能以较低成本得到与专门推断方法相匹配的高保真后验；信息局部化是网络增长的基本属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 信息局部化为网络增长提供了理论基础，支持在更大、未观测网络中利用局部子图和有限感受野图神经网络进行无似然推断，并实现高效准确的参数估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 机制网络模型能够利用少量领域特定、可解释的机制捕捉经验网络的显著特征。然而，由于似然往往不可计算，推断仍然具有挑战性。我们证明，对于广泛的增长网络模型，模型参数的信息局限于网络中，即似然可以用小子图来表达。我们从贝叶斯角度进行推断，开发神经密度估计器（NDE）来近似模型参数的后验分布，使用具有有限感受野的图神经网络（GNN），即GNN只能“看到”小子图。我们对九种增长网络模型进行局部化特征化，并证明局部化预测与模拟数据上的NDE结果一致。即使对于非局部化模型，NDE也能以较低成本推断出与模型特定推断方法相匹配的高保真后验。我们的发现确立了信息局部化作为网络增长的基本属性，从理论上证明了在更大、未观测网络中分析局部子图以及使用有限感受野GNN进行无似然推断的合理性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Mechanistic network models can capture salient characteristics of empirical networks using a small set of domain-specific, interpretable mechanisms. Yet inference remains challenging because the likelihood is often intractable. We show that, for a broad class of growing network models, information about model parameters is localized in the network, i.e., the likelihood can be expressed in terms of small subgraphs. We take a Bayesian perspective to inference and develop neural density estimators (NDEs) to approximate the posterior distribution of model parameters using graph neural networks (GNNs) with limited receptive size, i.e., the GNN can only &amp;quot;see&amp;quot; small subgraphs. We characterize nine growing network models in terms of their localization and demonstrate that localization predictions agree with NDEs on simulated data. Even for non-localized models, NDEs can infer high-fidelity posteriors matching model-specific inference methods at a fraction of the cost. Our findings establish information localization as a fundamental property of network growth, theoretically justifying the analysis of local subgraphs embedded in larger, unobserved networks and the use of GNNs with limited receptive field for likelihood-free inference.&lt;/p&gt;</description></item><item><guid>2512.23635v1</guid><title>Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</title><link>http://arxiv.org/abs/2512.23635v1</link><author>Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的时空对齐模块HAT，能够让每个目标对象在没有直接监督的情况下，从多个假设中自适应地解码出最佳对齐方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在端到端自动驾驶感知中，时空对齐对于时间建模至关重要，现有方法多依赖注意力机制和统一的简化运动模型，忽视了不同类别和帧间运动状态与特征的差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入多种显式运动模型和多假设解码，提升目标对齐的精度和鲁棒性，从而改善后续的检测与跟踪性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; HAT首先利用多种显式运动模型生成历史实例的空间锚点和运动感知特征提案；随后通过将语义与运动线索嵌入缓存的目标查询中进行多假设解码，最终为目标帧提供最优对齐方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，HAT在多种基线模型中均提升了3D时序检测器和跟踪器的性能，单独与DETR3D配合实现了46.0% AMOTA的最先进跟踪结果；在端到端自动驾驶方法中，HAT提升了感知精度（+1.3% mAP，+3.1% AMOTA），并将碰撞率降低32%；当语义信息被破坏时，HAT的运动建模增强了感知与规划的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 显式运动建模与自适应多假设对齐显著提升了端到端自动驾驶感知与规划的性能，HAT为时空对齐提供了一种有效且可推广的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文关注在自动驾驶中端到端3D感知的时空对齐问题。准确的时空对齐能为检测、跟踪和规划提供结构和纹理先验，提升感知精度并降低碰撞风险。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者发现现有方法往往只使用单一运动假设，且需要手工调参，导致对不同物体类别和动态场景的适应性差。为此，他们借鉴了传统的Kalman滤波、显式运动模型（如常速、常加、转弯等）以及查询传播和注意力机制，提出多假设生成与自适应解码的思路。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是先用多种显式运动模型生成多组锚点和特征假设，再通过嵌入在缓存查询中的运动线索自适应地解码出最佳对齐方案。实现流程包括：1）时空对齐模块生成多假设锚点和特征；2）空间对齐模块利用查询中的运动信息为每个假设分配动态权重，挑选最优对齐；3）融合并细化锚点与特征后送入检测或跟踪头。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）可插拔的HAT模块，能无监督地集成到多种端到端感知框架；2）多假设显式-隐式混合对齐，克服单假设的局限；3）自适应解码器利用查询中的运动线索，无需手工调参。与以往方法相比，HAT不再依赖单一运动模型或人工规则，显著提升检测、跟踪和整体系统性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HAT通过多假设显式运动模型与自适应解码，提供一种可插拔、无监督的时空对齐方案，显著提升自动驾驶端到端3D感知与跟踪的精度与鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.&lt;/p&gt;</description></item><item><guid>2512.23646v1</guid><title>OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</title><link>http://arxiv.org/abs/2512.23646v1</link><author>Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; OmniAgent 是一种完全由音频引导的主动感知代理，能够动态调用专用工具，实现细粒度的音视推理，显著提升多模态理解与对齐能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的全模态大型语言模型在统一音频与视觉模态方面取得进展，但往往缺乏细粒度的跨模态理解和多模态对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决上述局限，提升音频与视觉信息的细粒度推理与对齐效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用动态规划自主调度工具调用，聚焦任务相关线索；引入粗到细的音频引导感知范式，利用音频线索定位时间事件并引导后续推理；从被动响应生成转向主动多模态询问。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个音视频理解基准上，OmniAgent 达到最先进性能，较领先的开源和专有模型提升 10%–20% 的准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; OmniAgent 通过主动感知与音频引导的细粒度推理，证明了从被动到主动的多模态交互范式的有效性，并在性能上实现显著突破。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Omnimodal 大型语言模型在统一音频和视觉模态方面取得了显著进展；然而，它们往往缺乏细粒度的跨模态理解，并且在多模态对齐方面存在困难。为了解决这些局限，我们提出了 OmniAgent，一种完全由音频引导的主动感知代理，能够动态协调专用工具，实现更细粒度的音视推理。与依赖僵化、静态工作流程和密集帧字幕的以往工作不同，本文展示了从被动响应生成向主动多模态询问的范式转变。OmniAgent 采用动态规划自主调度工具调用，战略性地将感知注意力集中在任务相关线索上。我们方法的核心是新颖的粗到细音频引导感知范式，利用音频线索定位时间事件并引导后续推理。对三个音视频理解基准的广泛实证评估表明，OmniAgent 达到最先进性能，超过领先的开源和专有模型 10%–20% 的准确率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.&lt;/p&gt;</description></item><item><guid>2512.23649v2</guid><title>RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</title><link>http://arxiv.org/abs/2512.23649v2</link><author>Zhe Li, Cheng Chi, Boan Zhu, Yangyang Wei, Shuanghao Bai, Yuheng Ji, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, S. -H. Gary Chan, Chang Xu, Shanghang Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; RoboMirror是一种无重定向的视频到运动框架，先通过视觉理解再进行动作生成，显著提升类人机器人运动的可行性与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有类人机器人运动系统主要依赖精心策划的动作捕捉轨迹或稀疏文本命令，导致视觉理解与控制之间存在显著差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出RoboMirror，填补视觉理解与动作控制之间的空白，实现“先理解后模仿”的运动生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用视觉语言模型将原始第一人称或第三人称视频提炼为视觉运动意图，直接为基于扩散的策略提供条件，生成物理可行且语义对齐的运动，无需显式姿态重建或重定向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明RoboMirror通过第一人称视频实现远程存在，第三人称控制延迟降低80%，任务成功率比基线高3.7%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过将类人控制重新框定为视频理解，RoboMirror有效弥合了视觉理解与动作之间的鸿沟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类通过视觉观察学习运动，先解释视觉内容再模仿动作。然而，最先进的类人机器人运动系统依赖于精心策划的动作捕捉轨迹或稀疏文本命令，导致视觉理解与控制之间存在关键差距。文本到运动的方法因语义稀疏和分阶段管道错误而受限，而基于视频的方法仅执行机械姿态模仿，缺乏真正的视觉理解。我们提出RoboMirror，这是第一个无重定向的视频到运动框架，体现“先理解后模仿”。利用视觉语言模型，它将原始第一人称/第三人称视频提炼为视觉运动意图，直接为基于扩散的策略提供条件，生成物理可行、语义对齐的运动，而无需显式姿态重建或重定向。大量实验验证了RoboMirror的有效性，它通过第一人称视频实现远程存在，显著降低第三人称控制延迟80%，并比基线高出3.7%的任务成功率。通过将类人控制重新框定为视频理解，我们弥合了视觉理解与动作之间的鸿沟。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying &amp;quot;understand before you imitate&amp;quot;. Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.&lt;/p&gt;</description></item><item><guid>2512.23675v2</guid><title>End-to-End Test-Time Training for Long Context</title><link>http://arxiv.org/abs/2512.23675v2</link><author>Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文将长上下文语言建模视为持续学习问题，使用标准Transformer滑动窗口注意力，并在测试时通过下一词预测持续学习，将上下文压缩到模型权重中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统方法关注架构设计来处理长上下文，但作者认为可以通过持续学习来解决。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种在测试时持续学习的端到端方法，并通过元学习改进初始化，以提升长上下文建模性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用标准Transformer滑动窗口注意力，在测试时进行下一词预测实现持续学习；训练时通过元学习为测试时学习提供更好初始化；整体实现Test‑Time Training的端到端流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在3B模型、164B训练令牌的实验中，所提方法在上下文长度上与全注意力Transformer相同扩展性；相比Mamba 2和Gated DeltaNet等方法，扩展性更好；且推理延迟保持常数，128K上下文时比全注意力快2.7倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Test‑Time Training的端到端方法在长上下文建模中既保持了良好的扩展性，又实现了常数推理延迟，显示出优于现有方法的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们将长上下文语言建模视为持续学习问题，而非架构设计问题。在此框架下，我们仅使用标准架构——带滑动窗口注意力的Transformer。然而，我们的模型在测试时通过对给定上下文的下一词预测继续学习，将读取的上下文压缩到其权重中。此外，我们通过在训练时的元学习改进模型在测试时学习的初始化。总体而言，我们的方法是一种Test‑Time Training（TTT）的形式，在测试时（通过下一词预测）和训练时（通过元学习）都是端到端（E2E）的，这与以往的形式不同。我们进行了广泛的实验，重点关注扩展性。特别是，对于用164B个令牌训练的3B模型，我们的方法（TTT‑E2E）在上下文长度上与全注意力Transformer的扩展方式相同，而其他方法，如Mamba 2和Gated DeltaNet，则不具备此特性。然而，类似于RNN，TTT‑E2E的推理延迟与上下文长度无关，使其在128K上下文时比全注意力快2.7倍。我们的代码已公开。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model&amp;#x27;s initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.&lt;/p&gt;</description></item><item><guid>2512.23691v1</guid><title>Galaxy Zoo Evo: 1 million human-annotated images of galaxies</title><link>http://arxiv.org/abs/2512.23691v1</link><author>Mike Walmsley, Steven Bamford, Hugh Dickinson, Tobias Géron, Alexander J. Gordon, Annette M. N. Ferguson, Lucy Fortson, Sandor Kruk, Natalie Lines, Chris J. Lintott, Karen L. Masters, Robert G. Mann, James Pearson, Hayley Roberts, Anna M. M. Scaife, Stefan Schuldt, Brooke Simmons, Rebecca Smethurst, Josh Speagle, Kyle Willett</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Galaxy Zoo Evo 是一个包含 104M 公开标签、823k 望远镜图像的星系数据集，提供细粒度标签，支持基础模型预训练和微调，并包含 167k 目标标签用于强引力透镜和欧几里得望远镜星系描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 星系图像的标注工作量大，传统标注方式难以满足大规模基础模型训练需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个大规模、细粒度标注的数据集，作为计算机视觉领域的真实世界基准，推动天文领域基础模型的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过众包方式收集 104M 标签，覆盖 823k 图像，使用四台望远镜；对每张图像进行一系列细粒度问题与答案的标注；另外提供 167k 目标标签用于下游任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该数据集提供了丰富的细粒度标签，可用于预训练/微调基础模型，并可作为域适应、众包不确定性学习等研究的基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Galaxy Zoo Evo 将成为天文图像分析的关键资源，支持未来天文学家更好理解宇宙。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们介绍了 Galaxy Zoo Evo，这是一个用于构建和评估星系图像基础模型的标注数据集。GZ Evo 包含 104M 个众包标签，覆盖 823k 张来自四台望远镜的图像。每张图像都被标注了一系列细粒度的问题和答案（例如“有特征的星系，两个螺旋臂，紧密缠绕，与另一颗星系合并”）。这些详细标签对于预训练或微调非常有用。我们还提供了四个较小的标签集（共 167k 颗星系），用于天文学家感兴趣的下游任务，包括寻找强引力透镜和描述来自新空间望远镜 Euclid 的星系。我们希望 GZ Evo 能成为计算机视觉领域的真实世界基准，例如从陆地到天文或望远镜之间的域适应，或从众包标签中学习不确定性。我们也希望它能支持天文学新一代基础模型；这些模型将对未来天文学家更好理解宇宙至关重要。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce Galaxy Zoo Evo, a labeled dataset for building and evaluating foundation models on images of galaxies. GZ Evo includes 104M crowdsourced labels for 823k images from four telescopes. Each image is labeled with a series of fine-grained questions and answers (e.g. &amp;quot;featured galaxy, two spiral arms, tightly wound, merging with another galaxy&amp;quot;). These detailed labels are useful for pretraining or finetuning. We also include four smaller sets of labels (167k galaxies in total) for downstream tasks of specific interest to astronomers, including finding strong lenses and describing galaxies from the new space telescope Euclid. We hope GZ Evo will serve as a real-world benchmark for computer vision topics such as domain adaption (from terrestrial to astronomical, or between telescopes) or learning under uncertainty from crowdsourced labels. We also hope it will support a new generation of foundation models for astronomy; such models will be critical to future astronomers seeking to better understand our universe.&lt;/p&gt;</description></item><item><guid>2512.23739v1</guid><title>Break Out the Silverware -- Semantic Understanding of Stored Household Items</title><link>http://arxiv.org/abs/2512.23739v1</link><author>Michaela Levi-Richter, Reuth Mirsky, Oren Glickman</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了存储家居物品挑战（Stored Household Item Challenge），通过评估机器人在家庭场景中推断物品隐藏存放位置的能力，提供了两个数据集并设计了NOAM（Non-visible Object Allocation Model）混合式视觉-语言推理管线，实验表明NOAM显著提升预测准确率，接近人类水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 家用服务机器人需要推断日常物品的存放位置，但缺乏常识推理能力，导致无法完成诸如“给我一盘子”之类的简单指令。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个评估服务机器人认知能力的基准任务，提供真实且可扩展的数据集，并探索有效的推理方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 采集100对真实物品-图像样本并人工标注；2) 构建6500对公开厨房图像的存储多边形标注；3) 设计NOAM管线，将视觉输入转化为自然语言描述，再通过大型语言模型推断隐藏存放位置。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; NOAM在预测准确率上显著优于随机、传统视觉-语言管线、主流多模态模型以及人类基准，显示出新颖的常识推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 集成视觉-语言推理的NOAM展示了可扩展的常识推理效果，为在家庭环境中部署认知型机器人提供了最佳实践。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;``Bring me a plate.&amp;#x27;&amp;#x27; For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots&amp;#x27; cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.   Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants&amp;#x27; kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.   To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.   We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.&lt;/p&gt;</description></item><item><guid>2512.23749v1</guid><title>Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents</title><link>http://arxiv.org/abs/2512.23749v1</link><author>Amin Sadri, M Maruf Hossain</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种小型模型Coordinate Matrix Machine，利用文档结构进行分类，实现人类级别的概念学习，只需每类一个样本即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统机器学习需要大量样本，而人类能从单个例子学习新概念。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种绿色、低能耗、可解释的模型，提升文档分类效率并减少对大规模预训练的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过识别文档的结构性重要特征，构建坐标矩阵模型进行一次性学习和分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该模型在仅使用一张样本的情况下，准确率高于传统向量化方法和复杂深度学习模型，且在CPU环境下计算更快、能耗更低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Coordinate Matrix Machine实现了高效、可解释、绿色的文档分类，具备经济可行性和可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类级概念学习认为人类通常能从单个例子学习新概念，而机器学习算法通常需要数百个样本。我们的脑子在潜意识中识别重要特征，从而更有效地学习。本文提出Coordinate Matrix Machine（CM^2），一种专为此目的的小型模型，通过学习文档结构并利用这些信息进行分类，增强人类智能。与依赖大规模预训练和高能耗GPU基础设施的“红色AI”趋势不同，CM^2被设计为绿色AI解决方案。它通过识别人类会考虑的结构性“重要特征”，实现人类级概念学习，能够仅用每类一个样本对非常相似的文档进行分类。该算法在仅使用最少数据的情况下，优于传统向量化方法和需要更大数据集及显著计算资源的复杂深度学习模型。通过关注结构坐标而非全面语义向量，CM^2提供了高准确率、几何与结构智能、绿色AI与环境可持续性、CPU仅环境优化、内在可解释性、快速计算与低延迟、对不平衡类别的鲁棒性、经济可行性以及通用、可扩展和可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Human-level concept learning argues that humans typically learn new concepts from a single example, whereas machine learning algorithms typically require hundreds of samples to learn a single concept. Our brain subconsciously identifies important features and learns more effectively. \vspace*{6pt}   Contribution: In this paper, we present the Coordinate Matrix Machine (CM$^2$). This purpose-built small model augments human intelligence by learning document structures and using this information to classify documents. While modern &amp;quot;Red AI&amp;quot; trends rely on massive pre-training and energy-intensive GPU infrastructure, CM$^2$ is designed as a Green AI solution. It achieves human-level concept learning by identifying only the structural &amp;quot;important features&amp;quot; a human would consider, allowing it to classify very similar documents using only one sample per class.   Advantage: Our algorithm outperforms traditional vectorizers and complex deep learning models that require larger datasets and significant compute. By focusing on structural coordinates rather than exhaustive semantic vectors, CM$^2$ offers: 1. High accuracy with minimal data (one-shot learning) 2. Geometric and structural intelligence 3. Green AI and environmental sustainability 4. Optimized for CPU-only environments 5. Inherent explainability (glass-box model) 6. Faster computation and low latency 7. Robustness against unbalanced classes 8. Economic viability 9. Generic, expandable, and extendable&lt;/p&gt;</description></item><item><guid>2512.23749v2</guid><title>Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents</title><link>http://arxiv.org/abs/2512.23749v1</link><author>Amin Sadri, M Maruf Hossain</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种小型模型Coordinate Matrix Machine（CM2），通过学习文档结构实现人类水平的概念学习，能够仅用一个样本对相似文档进行分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统机器学习需要数百个样本才能学习一个概念，而人类只需一个例子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种绿色 AI 解决方案，利用结构特征实现高效、低能耗的文档分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CM2 通过识别文档的结构坐标而非完整语义向量，构建可解释的玻璃盒模型，进行一次性学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该算法在仅使用一个样本的情况下，准确率高于传统向量化方法和复杂深度学习模型，且在 CPU 环境下计算更快、能耗更低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CM2 提供了高精度、可解释、绿色、经济且可扩展的文档分类方案，适用于资源受限环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类水平的概念学习认为人类通常只需一个例子就能学习新概念，而机器学习算法通常需要数百个样本。我们的脑子在潜意识中识别重要特征并更有效地学习。本文提出了坐标矩阵机（CM2），这是一种专为小型模型设计的工具，通过学习文档结构并利用这些信息来分类文档。与依赖大规模预训练和高能耗 GPU 基础设施的“红色 AI”趋势不同，CM2 设计为绿色 AI 解决方案。它通过识别人类会考虑的结构“重要特征”，实现人类水平的概念学习，能够仅用每类一个样本来分类非常相似的文档。我们的算法在仅使用最小数据（一次性学习）的情况下，优于传统向量化方法和需要更大数据集和显著计算的复杂深度学习模型。通过关注结构坐标而非完整语义向量，CM2 提供了高精度、几何和结构智能、绿色 AI 与环境可持续性、仅 CPU 环境优化、内在可解释性、快速计算与低延迟、对不平衡类别的鲁棒性、经济可行性以及通用、可扩展和可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Human-level concept learning argues that humans typically learn new concepts from a single example, whereas machine learning algorithms typically require hundreds of samples to learn a single concept. Our brain subconsciously identifies important features and learns more effectively. \vspace*{6pt}   Contribution: In this paper, we present the Coordinate Matrix Machine (CM$^2$). This purpose-built small model augments human intelligence by learning document structures and using this information to classify documents. While modern &amp;quot;Red AI&amp;quot; trends rely on massive pre-training and energy-intensive GPU infrastructure, CM$^2$ is designed as a Green AI solution. It achieves human-level concept learning by identifying only the structural &amp;quot;important features&amp;quot; a human would consider, allowing it to classify very similar documents using only one sample per class.   Advantage: Our algorithm outperforms traditional vectorizers and complex deep learning models that require larger datasets and significant compute. By focusing on structural coordinates rather than exhaustive semantic vectors, CM$^2$ offers: 1. High accuracy with minimal data (one-shot learning) 2. Geometric and structural intelligence 3. Green AI and environmental sustainability 4. Optimized for CPU-only environments 5. Inherent explainability (glass-box model) 6. Faster computation and low latency 7. Robustness against unbalanced classes 8. Economic viability 9. Generic, expandable, and extendable&lt;/p&gt;</description></item><item><guid>2512.23755v1</guid><title>HINTS: Extraction of Human Insights from Time-Series Without External Sources</title><link>http://arxiv.org/abs/2512.23755v1</link><author>Sheo Yon Jhin, Noseong Park</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种自监督学习框架 HINTS，利用时间序列残差自动提取人类因素，并通过 Friedkin-Johnsen 模型捕捉社会影响、记忆和偏差，随后将这些因素作为注意力图融入预测模型，实验表明在九个数据集上显著提升预测精度，并通过案例和消融验证其可解释性与实际意义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 金融与经济系统的时间动态受人类决策、情绪和集体心理影响，传统方法通过外部数据捕捉这些因素，但成本高昂。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种无需外部数据、能够从时间序列内部提取人类因素并提升预测准确性的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建自监督学习框架 HINTS，利用 Friedkin-Johnsen 观点动态模型作为结构先验，提取残差中的人类因素，并将其作为注意力图加入基线模型进行预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; HINTS 在九个真实与基准数据集上持续提升预测精度；案例与消融实验显示提取因素与实际事件语义高度一致，证明其可解释性与实用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自监督框架 HINTS 能有效从内部残差中捕捉人类因素，提升预测性能并提供可解释的社会心理洞察，避免了外部数据的高成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类决策、情绪和集体心理是塑造金融和经济系统中观察到的时间动态的复杂因素。许多近期的时间序列预测模型利用外部来源（例如新闻和社交媒体）来捕捉人类因素，但这些方法在财务、计算和实践方面产生高数据依赖成本。在本研究中，我们提出了 HINTS，一种自监督学习框架，能够从时间序列残差中内生提取这些潜在因素，而无需外部数据。HINTS 利用 Friedkin-Johnsen（FJ）观点动态模型作为结构诱导偏差，以建模不断演变的社会影响、记忆和偏差模式。提取的人类因素被整合到最先进的骨干模型中，作为注意力图。使用九个真实世界和基准数据集的实验结果表明，HINTS 一贯提高了预测准确性。此外，多项案例研究和消融研究验证了 HINTS 的可解释性，展示了提取因素与现实事件之间的强语义一致性，证明了 HINTS 的实际效用。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.&lt;/p&gt;</description></item><item><guid>2512.23777v1</guid><title>A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms</title><link>http://arxiv.org/abs/2512.23777v1</link><author>Kanishka Hewageegana, Janani Harischandra, Nipuna Senanayake, Gihan Danansuriya, Kavindu Hapuarachchi, Pooja Illangarathne</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究通过图神经网络探讨网约车平台的欺诈检测，评估多种模型效果，分析常见欺诈行为，比较现有工作，关注类别不平衡和欺诈伪装，概述GNN架构与异常检测方法，指出进展与不足，并呼吁进一步研究实际应用与技术改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 网约车平台面临日益复杂的欺诈行为，需要有效检测手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估图神经网络在欺诈检测中的有效性，比较现有方法，识别方法进展与缺口，提出改进方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用图神经网络模型进行欺诈检测，分析不同模型效果，梳理GNN架构与异常检测方法，关注类别不平衡与欺诈伪装。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 发现GNN在检测欺诈方面具有潜力，但仍存在方法差距和类别不平衡问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 需要进一步探索GNN在真实场景中的应用，并改进技术以提升欺诈检测策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究通过图神经网络（GNN）探讨网约车平台的欺诈检测，重点评估各种模型的有效性。通过分析常见的欺诈行为，研究强调并比较了与欺诈检测相关的现有工作，这些工作在处理在线网约车平台的欺诈事件时具有参考价值。本文还强调了处理类别不平衡和欺诈伪装的问题，并概述了应用于异常检测的GNN架构和方法，识别了显著的方法进展和不足。本文呼吁进一步探索其在真实世界中的适用性和技术改进，以提升在快速发展的网约车行业中的欺诈检测策略。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This study investigates fraud detection in ride hailing platforms through Graph Neural Networks (GNNs),focusing on the effectiveness of various models. By analyzing prevalent fraudulent activities, the research highlights and compares the existing work related to fraud detection which can be useful when addressing fraudulent incidents within the online ride hailing platforms. Also, the paper highlights addressing class imbalance and fraudulent camouflage. It also outlines a structured overview of GNN architectures and methodologies applied to anomaly detection, identifying significant methodological progress and gaps. The paper calls for further exploration into real-world applicability and technical improvements to enhance fraud detection strategies in the rapidly evolving ride-hailing industry.&lt;/p&gt;</description></item><item><guid>2512.23786v1</guid><title>Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments</title><link>http://arxiv.org/abs/2512.23786v1</link><author>Ankan Aich, Yangming Lee</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出利用Depth Anything V2架构的高保真合成先验，并通过Dynamic Vector Low-Rank Adaptation (DV-LORA)高效适配医学领域，解决单目深度估计在镜面、液体环境下的边界崩溃问题，并在SCARED数据集上引入物理分层评估，取得98.1%的准确率，显著优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 单目深度估计在机器人手术中至关重要，但在镜面、液体充满的内镜环境中易失效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过高保真合成先验和低秩适配，提升在高镜面环境下的深度估计鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Depth Anything V2的合成先验，结合Dynamic Vector Low-Rank Adaptation (DV-LORA)进行参数高效迁移，并在SCARED数据集上采用物理分层评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 方法在SCARED数据集上实现98.1%的准确率，平方相对误差比基线降低17%以上，显示出在恶劣手术照明下的优越鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在高镜面手术环境中实现了新的最优性能，证明了合成先验与低秩适配的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 准确的单目深度估计（MDE）对机器人手术至关重要，但在镜面、液体充满的内镜环境中仍然脆弱。现有的自监督方法通常依赖于使用嘈杂真实世界伪标签训练的基础模型，往往在薄型手术工具和透明表面上出现边界崩溃。在本研究中，我们通过利用Depth Anything V2架构的高保真合成先验来解决这一问题，该架构本身能够捕捉薄结构的精确几何细节。我们使用Dynamic Vector Low-Rank Adaptation（DV-LORA）高效地将这些先验适配到医学领域，最小化参数预算，同时弥合合成到真实的差距。此外，我们在SCARED数据集上引入了物理分层评估协议，以严格量化高镜面环境下的性能，这些环境往往被聚合指标掩盖。我们的方案确立了新的最优性能，准确率在1.25以内达到98.1%，并将平方相对误差降低了超过17%，相较于既定基线，展示了在恶劣手术照明下的卓越鲁棒性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 本论文旨在解决在光照强烈、含液体的内窥镜环境中，单目深度估计容易出现边界崩溃和透明表面误差的问题。准确的深度信息对机器人手术的安全性、定位、跟踪和增强现实等关键任务至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先识别出传统自监督方法在真实世界伪标签噪声下对薄工具和透明表面处理不佳的缺陷，随后借鉴了EndoDAC的参数高效微调框架和DV‑LoRA技术，并将其与Depth Anything V2的合成先验相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是冻结在合成数据上预训练的DAv2 Transformer骨干，插入可学习的DV‑LoRA模块以适应手术图像的光照与纹理，并通过卷积颈部恢复高频细节。整体流程包括DepthNet与Pose‑Intrinsics Net的联合训练，使用视图合成的自监督损失进行优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）利用合成先验而非噪声伪标签；2）使用DV‑LoRA实现极低参数量的高效微调；3）引入物理分层评估协议，专门衡量高光反射区域的性能；4）在这些改进下实现了在SCARED数据集上新的最优指标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过将高保真合成深度先验与参数高效的DV‑LoRA相结合，并提出物理分层评估，本研究实现了在光照强烈、含液体的内窥镜环境中最先进的单目深度估计。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (&amp;lt; 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.&lt;/p&gt;</description></item><item><guid>2512.23787v1</guid><title>TabMixNN: A Unified Deep Learning Framework for Structural Mixed Effects Modeling on Tabular Data</title><link>http://arxiv.org/abs/2512.23787v1</link><author>Deniz Akdemir</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; TabMixNN 是一个基于 PyTorch 的深度学习框架，融合了传统混合效应模型与现代神经网络架构，用于表格数据分析。它支持分层数据结构和多种结果类型，并提供模块化的三阶段设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着数据结构日益层级化，研究者需要既能处理分层数据又能支持回归、分类和多任务学习的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一个灵活的框架，结合混合效应模型的理论基础和深度学习的表达能力，以满足表格数据分析的多样需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用三阶段模块化架构：第一阶段是带变分随机效应和可变协方差结构的混合效应编码器；第二阶段是 GSEM 和时空流形网络等主干架构；第三阶段是针对不同结果族的预测头。创新点包括 R 风格公式接口、DAG 约束、SPDE 核函数以及 SHAP 和方差分解等可解释工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 框架在纵向数据分析、基因组预测和时空建模等应用中表现出高度灵活性，验证了其设计的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; TabMixNN 为研究者提供了一个统一接口，使他们能够在保持传统混合效应模型可解释性和理论基础的同时，利用深度学习的强大功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 TabMixNN，这是一个灵活的基于 PyTorch 的深度学习框架，它将经典的混合效应建模与现代神经网络架构相结合，用于表格数据分析。TabMixNN 解决了对能够处理分层数据结构并支持多种结果类型（包括回归、分类和多任务学习）的方法的日益增长的需求。该框架实现了一个模块化的三阶段架构：（1）带变分随机效应和灵活协方差结构的混合效应编码器；（2）包括广义结构方程模型（GSEM）和时空流形网络在内的主干架构；（3）支持多种结果族的特定结果预测头。主要创新包括 R 风格公式接口、用于因果结构学习的有向无环图约束、用于空间建模的随机偏微分方程核函数，以及包括 SHAP 值和方差分解在内的全面可解释工具。我们通过对纵向数据分析、基因组预测和时空建模的应用，展示了该框架的灵活性。TabMixNN 为研究者提供了一个统一接口，使他们能够在利用深度学习的同时，保持经典混合效应模型的可解释性和理论基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present TabMixNN, a flexible PyTorch-based deep learning framework that synthesizes classical mixed-effects modeling with modern neural network architectures for tabular data analysis. TabMixNN addresses the growing need for methods that can handle hierarchical data structures while supporting diverse outcome types including regression, classification, and multitask learning. The framework implements a modular three-stage architecture: (1) a mixed-effects encoder with variational random effects and flexible covariance structures, (2) backbone architectures including Generalized Structural Equation Models (GSEM) and spatial-temporal manifold networks, and (3) outcome-specific prediction heads supporting multiple outcome families. Key innovations include an R-style formula interface for accessibility, support for directed acyclic graph (DAG) constraints for causal structure learning, Stochastic Partial Differential Equation (SPDE) kernels for spatial modeling, and comprehensive interpretability tools including SHAP values and variance decomposition. We demonstrate the framework&amp;#x27;s flexibility through applications to longitudinal data analysis, genomic prediction, and spatial-temporal modeling. TabMixNN provides a unified interface for researchers to leverage deep learning while maintaining the interpretability and theoretical grounding of classical mixed-effects models.&lt;/p&gt;</description></item><item><guid>2512.23808v1</guid><title>MiMo-Audio: Audio Language Models are Few-Shot Learners</title><link>http://arxiv.org/abs/2512.23808v1</link><author>Xiaomi LLM-Core Team, :, Dong Zhang, Gang Wang, Jinlong Xue, Kai Fang, Liang Zhao, Rui Ma, Shuhuai Ren, Shuo Liu, Tao Guo, Weiji Zhuang, Xin Zhang, Xingchen Song, Yihan Yan, Yongzhe He, Cici, Bowen Shen, Chengxuan Zhu, Chong Ma, Chun Chen, Heyu Chen, Jiawei Li, Lei Li, Menghang Zhu, Peidian Li, Qiying Wang, Sirui Deng, Weimin Xiong, Wenshan Huang, Wenyu Yang, Yilin Jiang, Yixin Yang, Yuanyuan Tian, Yue Ma, Yue Yu, Zihan Zhang, Zihao Yue, Bangjun Xiao, Bingquan Xia, Bofei Gao, Bowen Ye, Can Cai, Chang Liu, Chenhong He, Chunan Li, Dawei Zhu, Duo Zhang, Fengyuan Shi, Guoan Wang, Hailin Zhang, Hanglong Lv, Hanyu Li, Hao Tian, Heng Qu, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianguang Zuo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Linghao Zhang, Meng Chen, Nuo Chen, Peng Zhang, Qianli Chen, Qiantong Wang, Rang Li, Shaohui Liu, Shengfan Wang, Shicheng Li, Shihua Yu, Shijie Cao, Shimao Chen, Shuhao Gu, Weikun Wang, Wenhan Ma, Xiangwei Deng, Xing Yong, Xing Zhang, Xu Wang, Yifan Song, Yihao Zhao, Yingbo Zhao, Yizhao Gao, Yu Cheng, Yu Tu, Yudong Wang, Zhaojun Huang, Zhengju Tang, Zhenru Lin, Zhichao Song, Zhipeng Xu, Zhixian Zheng, Zihan Jiang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了MiMo-Audio模型，通过大规模预训练实现了音频任务的少样本学习和通用性，并在多项公开基准上取得领先成绩。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统音频语言模型依赖任务特定微调，难以快速适应新任务；人类则能仅凭少量示例或指令进行泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证将GPT-3的预训练范式应用于音频领域，并通过大规模数据提升模型的通用能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将MiMo-Audio的预训练数据扩展到超过一亿小时，构建系统评估体系，并在后训练阶段使用多样化指令调优语料，引入思考机制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MiMo-Audio-7B-Base在语音智能和音频理解基准上实现SOTA，并能在未见任务如语音转换、风格迁移、语音编辑等上表现良好；其语音续写能力可生成逼真对话、朗诵、直播和辩论；MiMo-Audio-7B-Instruct在多项公开基准上达到或超过闭源模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 大规模预训练与指令调优可使音频语言模型具备强大的少样本学习和通用性，成为开源领域的领先方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的音频语言模型通常依赖于任务特定的微调来完成特定的音频任务。相比之下，人类能够仅凭少量示例或简单指令就能对新音频任务进行泛化。GPT-3已表明，扩展下一个词预测的预训练可以在文本中实现强大的泛化能力，我们认为这一范式同样适用于音频领域。通过将MiMo-Audio的预训练数据扩展到超过一亿小时，我们观察到在多种音频任务中出现了少样本学习能力。我们开发了一个系统的评估方法，发现MiMo-Audio-7B-Base在公开模型中在语音智能和音频理解基准上实现了SOTA表现。除了标准指标外，MiMo-Audio-7B-Base还能在其训练数据中未出现的任务上泛化，例如语音转换、风格迁移和语音编辑。MiMo-Audio-7B-Base还展示了强大的语音续写能力，能够生成高度逼真的脱口秀、朗诵、直播和辩论。训练后阶段，我们策划了多样化的指令调优语料库，并在音频理解和生成中引入了思考机制。MiMo-Audio-7B-Instruct在音频理解基准（MMSU、MMAU、MMAR、MMAU-Pro）、口语对话基准（Big Bench Audio、MultiChallenge Audio）和指令TTS评估中实现了开源SOTA，接近或超过闭源模型。模型检查点和完整评估套件可在 https://github.com/XiaomiMiMo/MiMo-Audio 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio&amp;#x27;s pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio) and instruct-TTS evaluations, approaching or surpassing closed-source models. Model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-Audio.&lt;/p&gt;</description></item><item><guid>2512.23813v1</guid><title>StressRoBERTa: Cross-Condition Transfer Learning from Depression, Anxiety, and PTSD to Stress Detection</title><link>http://arxiv.org/abs/2512.23813v1</link><author>Amal Alqahtani, Efsun Kayi, Mona Diab</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种跨条件迁移学习方法 StressRoBERTa，用于自动检测英文推文中的自报慢性压力。通过在与慢性压力高度共病的临床相关疾病（抑郁、焦虑、创伤后应激障碍）上持续训练模型，并在公开数据集上微调，最终实现了 82% 的 F1 分数，优于现有最佳系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 慢性压力的流行率是公共卫生的重要关注点，社交媒体平台如 Twitter 成为人们分享压力经历的重要场所。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究在慢性压力相关疾病上持续训练是否能提升压力检测效果，比较其与通用语言模型和广义心理健康模型的差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用 RoBERTa 在 Stress‑SMHD 语料库（108M 词，包含自报抑郁、焦虑、PTSD 的用户）上进行持续训练，然后在 SMM4H 2022 Task 8 数据集上进行微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; StressRoBERTa 获得 82% F1 分数，比最佳共享任务系统高 3 个百分点；相较于普通 RoBERTa，跨条件迁移提升了 1% F1，证明针对压力相关疾病的迁移能提供更强的表示；在 Dreaddit 上的 81% F1 进一步验证了从临床心理健康语境向情境压力讨论的迁移效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 针对慢性压力相关疾病的跨条件迁移学习能够显著提升自报慢性压力检测的性能，且该方法在不同平台和语境下均表现出良好的迁移能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 慢性压力的流行率是公共卫生的重要关注点，社交媒体平台如 Twitter 成为人们分享压力经历的重要场所。本文提出了一种跨条件迁移学习方法 StressRoBERTa，用于自动检测英文推文中的自报慢性压力。研究探讨了在与慢性压力高度共病的临床相关疾病（抑郁、焦虑、创伤后应激障碍）上持续训练是否能提升压力检测效果，并与通用语言模型和广义心理健康模型进行比较。RoBERTa 在 Stress‑SMHD 语料库（108M 词，包含自报抑郁、焦虑、PTSD 的用户）上持续训练，并在 SMM4H 2022 Task 8 数据集上微调。StressRoBERTa 获得 82% 的 F1 分数，优于最佳共享任务系统（79% F1）3 个百分点。结果表明，针对压力相关疾病的跨条件迁移（比普通 RoBERTa 提升 1% F1）提供了更强的表示，且在 Dreaddit 上的 81% F1 进一步证明了从临床心理健康语境向情境压力讨论的迁移效果。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The prevalence of chronic stress represents a significant public health concern, with social media platforms like Twitter serving as important venues for individuals to share their experiences. This paper introduces StressRoBERTa, a cross-condition transfer learning approach for automatic detection of self-reported chronic stress in English tweets. The investigation examines whether continual training on clinically related conditions (depression, anxiety, PTSD), disorders with high comorbidity with chronic stress, improves stress detection compared to general language models and broad mental health models. RoBERTa is continually trained on the Stress-SMHD corpus (108M words from users with self-reported diagnoses of depression, anxiety, and PTSD) and fine-tuned on the SMM4H 2022 Task 8 dataset. StressRoBERTa achieves 82% F1-score, outperforming the best shared task system (79% F1) by 3 percentage points. The results demonstrate that focused cross-condition transfer from stress-related disorders (+1% F1 over vanilla RoBERTa) provides stronger representations than general mental health training. Evaluation on Dreaddit (81% F1) further demonstrates transfer from clinical mental health contexts to situational stress discussions.&lt;/p&gt;</description></item><item><guid>2512.23817v1</guid><title>Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware</title><link>http://arxiv.org/abs/2512.23817v1</link><author>Seyed Mohamad Ali Tousi, Adib Bazgir, Yuwen Zhang, G. N. DeSouza</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出一种结合量子与经典计算的混合框架，并加入学习式误差补偿，用于在噪声中等规模量子设备上求解粘性布格方程。通过Cole-Hopf变换将非线性方程转化为扩散方程，随后在均匀网格上离散化并编码为量子态，利用Trotter化的相邻门电路在Qiskit中实现时间演化。量子模拟在噪声Aer后端和IBM超导量子设备上执行，并与使用Krylov求解器得到的高精度经典解进行对比。通过测量的量子幅度重建速度场，评估L2误差、冲击位置和耗散率等物理与数值诊断，并比较使用与不使用零噪声外推的结果。为实现数据驱动的误差补偿，构建了一个大规模参数化数据集，覆盖粘性、时间步长、网格分辨率和边界条件等维度，生成匹配的噪声、零噪声校正、硬件和经典解以及详细的电路元数据。利用该数据集，训练了一个基于注意力的图神经网络，该网络结合电路结构、光锥信息、全局电路参数和噪声量子输出，预测误差补偿后的解。实验表明，学习模型在广泛参数范围内持续降低量子与经典解之间的差距，优于单独使用零噪声外推。研究讨论了将此方法推广到更高维布格系统和更通用的量子偏微分方程求解器的可能性，并强调学习式误差补偿是NISQ设备上物理噪声抑制技术的有前景的补充。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在噪声中等规模量子设备上求解偏微分方程是量子计算的关键挑战之一。布格方程作为典型的非线性流体动力学模型，常被用来评估量子算法的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种结合量子模拟与学习式误差补偿的混合框架，以提高在噪声量子硬件上求解粘性布格方程的精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 采用Cole-Hopf变换将非线性方程转化为扩散方程；2. 在均匀网格上离散化并编码为量子态；3. 使用Trotter化的相邻门电路在Qiskit中实现时间演化；4. 在Aer后端和IBM设备上执行量子模拟；5. 通过测量幅度重建速度场并评估诊断指标；6. 采用零噪声外推进行误差补偿；7. 构建大规模参数化数据集；8. 训练基于注意力的图神经网络，利用电路结构、光锥信息和全局参数预测误差补偿解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 学习式误差补偿模型在广泛参数范围内显著降低量子与经典解之间的差距，优于单独使用零噪声外推。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 学习式误差补偿是NISQ设备上物理噪声抑制技术的有前景的补充，可推广到更高维度的布格系统和更通用的量子偏微分方程求解器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了一种结合量子与经典的混合框架，并加入学习式误差补偿，用于在噪声中等规模量子（NISQ）硬件上求解粘性布格方程。利用Cole-Hopf变换，非线性布格方程被映射为扩散方程，在均匀网格上离散化后编码为量子态，其时间演化通过在Qiskit中实现的Trotter化相邻门电路来近似。量子模拟在噪声Aer后端和IBM超导量子设备上执行，并与使用Krylov求解器得到的高精度经典解进行基准比较。通过测量的量子幅度，我们重建速度场并评估物理与数值诊断指标，包括L2误差、冲击位置和耗散率，分别在使用与不使用零噪声外推（ZNE）的情况下进行。为实现数据驱动的误差补偿，我们构建了一个大型参数化数据集，扫描粘性、时间步长、网格分辨率和边界条件等维度，生成匹配的噪声、ZNE校正、硬件和经典解以及详细的电路元数据。利用该数据集，我们训练了一个基于注意力的图神经网络，该网络结合电路结构、光锥信息、全局电路参数和噪声量子输出，预测误差补偿后的解。在广泛参数范围内，学习模型始终将量子与经典解之间的差距降低到超出单独使用ZNE的水平。我们讨论了将此方法扩展到更高维布格系统和更通用的量子偏微分方程求解器的可能性，并强调学习式误差补偿是NISQ设备上物理噪声抑制技术的有前景的补充。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a hybrid quantum-classical framework augmented with learned error mitigation for solving the viscous Burgers equation on noisy intermediate-scale quantum (NISQ) hardware. Using the Cole-Hopf transformation, the nonlinear Burgers equation is mapped to a diffusion equation, discretized on uniform grids, and encoded into a quantum state whose time evolution is approximated via Trotterized nearest-neighbor circuits implemented in Qiskit. Quantum simulations are executed on noisy Aer backends and IBM superconducting quantum devices and are benchmarked against high-accuracy classical solutions obtained using a Krylov-based solver applied to the corresponding discretized Hamiltonian. From measured quantum amplitudes, we reconstruct the velocity field and evaluate physical and numerical diagnostics, including the L2 error, shock location, and dissipation rate, both with and without zero-noise extrapolation (ZNE). To enable data-driven error mitigation, we construct a large parametric dataset by sweeping viscosity, time step, grid resolution, and boundary conditions, producing matched tuples of noisy, ZNE-corrected, hardware, and classical solutions together with detailed circuit metadata. Leveraging this dataset, we train an attention-based graph neural network that incorporates circuit structure, light-cone information, global circuit parameters, and noisy quantum outputs to predict error-mitigated solutions. Across a wide range of parameters, the learned model consistently reduces the discrepancy between quantum and classical solutions beyond what is achieved by ZNE alone. We discuss extensions of this approach to higher-dimensional Burgers systems and more general quantum partial differential equation solvers, highlighting learned error mitigation as a promising complement to physics-based noise reduction techniques on NISQ devices.&lt;/p&gt;</description></item><item><guid>2512.23832v1</guid><title>Exploiting the Prior of Generative Time Series Imputation</title><link>http://arxiv.org/abs/2512.23832v1</link><author>YuYang Miao, Chang Li, Zehua Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Bridge-TS提出了一种基于数据到数据的生成式时间序列缺失值填补框架，通过设计更具信息量的先验来提升填补精度，取得了新的记录水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 时间序列缺失值填补在电力、金融和气象建模等领域具有广泛应用。以往的方法多采用扩散概率模型或Schrödinger桥模型，从高斯噪声或线性插值结果中条件生成缺失值，但其先验信息不足，导致生成过程负担加重且填补精度受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 改进生成式时间序列填补方法，设计更具信息量的先验，以提升填补准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Bridge-TS采用两种新颖的先验设计：一是专家先验，利用预训练的Transformer模块以确定性估计填补缺失值，并将结果作为目标的先验；二是组合先验，使用多个预训练模型提供不同估计结果，并在数据到数据生成过程中将它们组合，形成组合先验到目标的填补流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在ETT、Exchange和Weather等基准数据集上进行的实验表明，Bridge-TS在均方误差和平均绝对误差方面均达到了新的记录，验证了改进先验对生成式时间序列填补的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入更具信息量的先验，Bridge-TS显著提升了生成式时间序列缺失值填补的准确性，树立了新的性能基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时间序列填补，即填补时间记录中的缺失值，在电力、金融和气象建模等领域有多种应用。以往的方法引入了扩散概率模型和Schrödinger桥模型，从高斯噪声或直接从线性插值结果中有条件地生成缺失值。然而，由于它们的先验对真实目标缺乏信息，生成过程不可避免地增加了负担并限制了填补精度。在本研究中，我们提出了Bridge-TS，构建了一个数据到数据的生成过程，用于生成式时间序列填补，并利用两种新颖的先验设计来改进先验。首先，我们提出了专家先验，利用预训练的基于Transformer的模块作为专家，以确定性估计填补缺失值，然后将结果作为真实目标的先验。其次，我们探索了组合先验，利用多个预训练模型提供不同的估计结果，然后在数据到数据生成过程中将它们组合，以实现组合先验到目标的填补过程。在ETT、Exchange和Weather等几个基准数据集上进行的实验表明，Bridge-TS在均方误差和平均绝对误差方面达到了新的记录，证明了改进先验对生成式时间序列填补的优越性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Time series imputation, i.e., filling the missing values of a time recording, finds various applications in electricity, finance, and weather modelling. Previous methods have introduced generative models such as diffusion probabilistic models and Schrodinger bridge models to conditionally generate the missing values from Gaussian noise or directly from linear interpolation results. However, as their prior is not informative to the ground-truth target, their generation process inevitably suffer increased burden and limited imputation accuracy. In this work, we present Bridge-TS, building a data-to-data generation process for generative time series imputation and exploiting the design of prior with two novel designs. Firstly, we propose expert prior, leveraging a pretrained transformer-based module as an expert to fill the missing values with a deterministic estimation, and then taking the results as the prior of ground truth target. Secondly, we explore compositional priors, utilizing several pretrained models to provide different estimation results, and then combining them in the data-to-data generation process to achieve a compositional priors-to-target imputation process. Experiments conducted on several benchmark datasets such as ETT, Exchange, and Weather show that Bridge-TS reaches a new record of imputation accuracy in terms of mean square error and mean absolute error, demonstrating the superiority of improving prior for generative time series imputation.&lt;/p&gt;</description></item><item><guid>2512.23848v1</guid><title>Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs</title><link>http://arxiv.org/abs/2512.23848v1</link><author>Yukun Zhang, Stefan Elbl Droguett, Samyak Jain</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究针对金融数值推理问答任务中因缺乏专业金融知识导致的错误，提出了多检索增强生成器系统，并结合最新大型语言模型，显著提升了数值推理的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 金融数值问题需要专业知识和多步推理，现有大型语言模型在这类任务上仍表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过检索外部金融知识和内部问题上下文，利用最新LLM改进金融数值推理问答的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建多检索RAG系统，使用SecBERT进行领域特定训练，进行消融实验和错误分析，比较神经符号模型与提示式LLM生成器的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 领域特定训练显著提升模型性能，最佳神经符号模型超过FinQA基准；提示式LLM生成器实现SOTA，提升超过7%，但仍低于人类专家；在小模型中外部知识收益与幻觉损失权衡，较大模型外部事实收益更大；最新LLM在少样本学习中表现更佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 领域特定训练和外部知识检索能显著提升金融数值推理能力，提示式LLM在少样本场景下可达SOTA，但仍需进一步减少幻觉，未来可进一步优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究项目解决了金融数值推理问答任务中因缺乏金融领域知识导致的错误。尽管大型语言模型最近取得了进展，金融数值问题仍然具有挑战性，因为它们需要特定的金融领域知识和复杂的多步数值推理。我们实现了一个多检索增强生成器（RAG）系统，以检索外部领域知识和内部问题上下文，并利用最新的大型语言模型来解决这些任务。通过全面的消融实验和错误分析，我们发现使用SecBERT编码器进行领域特定训练显著提升了我们的最佳神经符号模型，超过了FinQA论文的顶级模型，后者作为我们的基准。这表明领域特定训练具有潜在的更优性能。此外，我们最佳的基于提示的大型语言模型生成器实现了SOTA性能，显著提升（超过7%），但仍低于人类专家的表现。本研究强调了在小型模型和少样本示例中幻觉损失与外部知识收益之间的权衡。对于更大的模型，外部事实的收益通常超过幻觉损失。最后，我们的发现确认了最新大型语言模型在少样本学习中优化后的数值推理能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper&amp;#x27;s top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (&amp;gt;7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.&lt;/p&gt;</description></item><item><guid>2512.23897v1</guid><title>Wireless Multimodal Foundation Model (WMFM): Integrating Vision and Communication Modalities for 6G ISAC Systems</title><link>http://arxiv.org/abs/2512.23897v1</link><author>Mohammad Farzanullah, Han Zhang, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于对比学习的无线多模态基础模型WMFM，利用无线信道系数和视觉图像共同学习特征，并在下游任务中表现出显著优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态基础模型的出现改变了学习范式，能够跨多种数据类型实现联合理解。在下一代无线网络中，融合感知与通信模式为构建通用且数据高效的模型提供了机会。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个可在无线信道和视觉图像上共同预训练的多模态模型，并验证其在用户定位和LoS/非LoS分类等任务中的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用自监督的对比学习对摄像头图像和信道数据进行嵌入对齐，预训练编码器后冻结并作为特征提取器，随后在轻量级任务头上进行微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在DeepVerse6G数据集上，WMFM在LoS/非LoS分类的平衡准确率提升17%，定位误差降低48.5%，训练时间缩短90倍；即使仅使用20%数据，微调头也优于全监督端到端模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; WMFM为集成感知与通信系统提供了可扩展的多模态学习基础，为智能自适应6G网络奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态基础模型的出现通过实现跨多种数据类型的联合理解，彻底改变了学习范式。在下一代无线网络的背景下，整合感知与通信模式为开发通用且数据高效的模型提供了独特机会。在本研究中，我们提出了基于对比学习的无线多模态基础模型（WMFM），这是一个大规模框架，能够同时学习无线信道系数和视觉图像。WMFM使用对比学习进行预训练，这是一种自监督学习技术，能够在不需要显式标签的情况下对齐摄像头和信道数据的嵌入。预训练的编码器随后被冻结并用作特征提取器，配合轻量级的任务特定头部，在下游任务（包括用户定位和LoS/非LoS分类）中进行微调。对DeepVerse6G数据集的广泛实验表明，所提出的WMFM在LoS/非LoS分类的平衡准确率上提升了17%，定位误差降低了48.5%，与端到端（E2E）基准相比，训练时间缩短了多达90倍。即使仅使用20%的数据进行训练，WMFM的头部也优于完全监督的E2E模型，凸显了其鲁棒性和数据高效学习。所提出的方法为集成感知与通信（ISAC）系统中的可扩展多模态学习奠定了基础，为智能和自适应的6G网络铺平了道路。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The emergence of multimodal foundation models has revolutionized learning paradigms by enabling joint understanding across diverse data types. In the context of next-generation wireless networks, integrating sensing and communication modalities presents a unique opportunity to develop generalizable and data-efficient models. In this work, we introduce the contrastive learning based Wireless Multimodal Foundation Model (WMFM), a large-scale framework that jointly learns from wireless channel coefficients and visual imagery. The WMFM is pretrained using contrastive learning, a self-supervised learning technique that aligns embeddings of camera and channel data without requiring explicit labels. The pretrained encoders are then frozen and employed as feature extractors, with lightweight task-specific heads, fine-tuned for downstream tasks, including user localization and LoS/nLoS classification. Extensive experiments on the DeepVerse6G dataset demonstrate that the proposed WMFM achieves a 17% improvement in balanced accuracy for LoS/nLoS classification and a 48.5% reduction in localization error compared to the end-to-end (E2E) benchmark, while reducing training time by up to 90-fold. Even when trained with as little as 20% of the data, the WMFM-based heads outperform the fully supervised E2E model, underscoring their robustness and data-efficient learning. The proposed approach establishes a foundation for scalable, multimodal learning in Integrated Sensing and Communication (ISAC) systems, paving the way for intelligent and adaptive 6G networks.&lt;/p&gt;</description></item><item><guid>2512.23898v1</guid><title>Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City</title><link>http://arxiv.org/abs/2512.23898v1</link><author>Tin Hoang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 本研究对十种深度学习模型在胡志明市1小时前瞻性全球水平辐照度预测进行了全面基准测试。 2. 采用2011-2020年高分辨率NSRDB卫星数据进行训练与评估。 3. 结果显示Transformer模型在预测精度上最高，R平方为0.9696。 4. SHAP分析揭示Transformer更关注近期大气条件，而Mamba利用24小时周期性依赖。 5. 通过知识蒸馏将Transformer压缩23.5%，并将平均绝对误差降至23.78瓦/平方米，为边缘设备部署提供了可行路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 全球水平辐照度的可靠预测对于缓解太阳能在电网中的波动性至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 比较十种深度学习架构在胡志明市短期（1小时）GHI时间序列预测中的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用2011-2020年NSRDB卫星数据，对LSTM、TCN等传统基线模型与Transformer、Informer、iTransformer、TSMixer、Mamba等前沿模型进行训练与评估；利用SHAP分析对模型的时间推理进行对比；采用知识蒸馏压缩Transformer模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Transformer模型在预测精度上最优，R平方为0.9696；Transformer表现出强烈的“最近性偏差”，侧重即时大气条件；Mamba显式利用24小时周期性依赖；知识蒸馏将Transformer压缩23.5%，并将平均绝对误差降至23.78瓦/平方米。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Transformer是最优的GHI预测架构，且通过知识蒸馏可实现高效、低延迟的边缘设备部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可靠的全球水平辐照度（GHI）预测对于缓解太阳能在电网中的波动性至关重要。本研究在胡志明市使用2011-2020年高分辨率NSRDB卫星数据，对十种深度学习架构进行短期（1小时）GHI时间序列预测的全面基准测试，比较了传统基线模型（如LSTM、TCN）与最新技术（Transformer、Informer、iTransformer、TSMixer、Mamba）。实验结果表明，Transformer在预测精度上最高，R平方为0.9696。通过SHAP分析对比模型的时间推理，发现Transformer更关注近期大气条件，表现出强烈的“最近性偏差”，而Mamba则显式利用24小时周期性依赖来指导预测。进一步地，我们演示了知识蒸馏可以将高性能Transformer压缩23.5%，并意外地降低平均绝对误差至23.78瓦/平方米，为在资源受限的边缘设备上部署复杂、低延迟的预测模型提供了可行路径。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids. This study presents a comprehensive benchmark of ten deep learning architectures for short-term (1-hour ahead) GHI time series forecasting in Ho Chi Minh City, leveraging high-resolution NSRDB satellite data (2011-2020) to compare established baselines (e.g. LSTM, TCN) against emerging state-of-the-art architectures, including Transformer, Informer, iTransformer, TSMixer, and Mamba. Experimental results identify the Transformer as the superior architecture, achieving the highest predictive accuracy with an R^2 of 0.9696. The study further utilizes SHAP analysis to contrast the temporal reasoning of these architectures, revealing that Transformers exhibit a strong &amp;quot;recency bias&amp;quot; focused on immediate atmospheric conditions, whereas Mamba explicitly leverages 24-hour periodic dependencies to inform predictions. Furthermore, we demonstrate that Knowledge Distillation can compress the high-performance Transformer by 23.5% while surprisingly reducing error (MAE: 23.78 W/m^2), offering a proven pathway for deploying sophisticated, low-latency forecasting on resource-constrained edge devices.&lt;/p&gt;</description></item><item><guid>2512.23902v1</guid><title>Beamforming for Massive MIMO Aerial Communications: A Robust and Scalable DRL Approach</title><link>http://arxiv.org/abs/2512.23902v1</link><author>Hesam Khoshkbari, Georges Kaddoum, Omid Abbasi, Bassant Selim, Halim Yanikomeroglu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种分布式波束成形框架，适用于大规模多输入多输出非地面网络中的空中平台站，旨在在局部信道状态信息不完美的情况下最大化下行总速率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着非地面网络的发展，空中平台站的波束成形成为提升网络容量的关键技术，但传统方法在大规模部署和信道不确定性下表现有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不完美的局部信道信息条件下，设计一种可扩展、鲁棒的分布式波束成形方案，以提高下行总速率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用基于熵的多智能体深度强化学习，每个基站独立使用傅里叶神经算子计算波束向量，并结合共轭先验的迁移学习和低秩分解技术实现可扩展性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仿真表明该方法在平均总速率、对信道不完美的鲁棒性、用户移动性以及不同网络规模和用户密度下的可扩展性方面优于WMMSE、ZF、MRT、CNN‑DRL和DDPG等基线方案，并在计算效率和通信开销上也具有优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提框架在大规模非地面网络中实现了高效、鲁棒的分布式波束成形，显著提升了网络性能并降低了资源消耗。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种适用于大规模多输入多输出非地面网络中空中平台站星座的分布式波束成形框架，目标是在局部信道状态信息不完美的情况下最大化下行总速率。我们提出了一种新颖的基于熵的多智能体深度强化学习方法，每个非地面基站独立使用傅里叶神经算子来捕捉频域中的长程依赖关系，以计算其波束向量。为确保可扩展性和鲁棒性，所提框架结合了基于共轭先验机制的迁移学习和低秩分解技术，从而有效支持大规模用户部署和空中层。仿真结果表明，与WMMSE、ZF、MRT、基于CNN的DRL以及深度确定性策略梯度方法等基线方案相比，所提方法在平均总速率、对信道不完美的鲁棒性、用户移动性以及不同网络规模和用户密度下的可扩展性方面具有优势。此外，我们还显示所提方法在计算效率上显著优于基于CNN和WMMSE的方法，并在与共享评论者DRL方法相比时降低了通信开销。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents a distributed beamforming framework for a constellation of airborne platform stations (APSs) in a massive Multiple-Input and Multiple-Output (MIMO) non-terrestrial network (NTN) that targets the downlink sum-rate maximization under imperfect local channel state information (CSI). We propose a novel entropy-based multi-agent deep reinforcement learning (DRL) approach where each non-terrestrial base station (NTBS) independently computes its beamforming vector using a Fourier Neural Operator (FNO) to capture long-range dependencies in the frequency domain. To ensure scalability and robustness, the proposed framework integrates transfer learning based on a conjugate prior mechanism and a low-rank decomposition (LRD) technique, thus enabling efficient support for large-scale user deployments and aerial layers. Our simulation results demonstrate the superiority of the proposed method over baseline schemes including WMMSE, ZF, MRT, CNN-based DRL, and the deep deterministic policy gradient (DDPG) method in terms of average sum rate, robustness to CSI imperfection, user mobility, and scalability across varying network sizes and user densities. Furthermore, we show that the proposed method achieves significant computational efficiency compared to CNN-based and WMMSE methods, while reducing communication overhead in comparison with shared-critic DRL approaches.&lt;/p&gt;</description></item><item><guid>2512.23903v1</guid><title>Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale</title><link>http://arxiv.org/abs/2512.23903v1</link><author>Charith Wickrema, Eliza Mace, Hunter Brown, Heidys Cabrera, Nick Krall, Matthew O'Neill, Shivangi Sarkar, Lowell Weissman, Eric Hughes, Guido Zarrella</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 1. 研究人工智能在高分辨率电光（EO）数据集上的扩展行为，目标是训练超越现有规模的基础模型。2. 现代多模态机器学习（如图像字幕、搜索、推理）依赖于强大的、领域专用的非文本编码器。3. 在自然图像领域，已知的扩展定律可优化模型容量、训练算力和数据集大小，但在遥感等高价值领域尚不清楚。4. 通过使用超过一千万亿像素的商业卫星EO数据和MITRE联邦AI沙盒，逐步训练更大规模的视觉变换器（ViT）骨干网络。5. 记录了在拍字节级别训练时的成功与失败模式，并分析了跨遥感模态的领域差距。6. 发现即使在此规模下，性能仍受数据限制而非模型参数限制。7. 这些实用见解旨在指导数据采集策略、算力预算和优化计划，推动前沿规模遥感基础模型的发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在自然图像领域，互联网规模的数据丰富，已建立的扩展定律帮助优化模型容量、训练算力和数据集大小；但在遥感等高价值领域，类似关系尚不充分了解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索人工智能的扩展行为，建立在高分辨率电光数据集上训练基础模型的实用技术，超越当前最先进规模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用超过一千万亿像素的商业卫星EO数据和MITRE联邦AI沙盒，逐步训练更大规模的视觉变换器（ViT）骨干网络，记录在拍字节级别训练时的成功与失败模式，并分析跨遥感模态的领域差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 即使在拍字节级别的规模下，模型性能仍受数据限制而非模型参数限制；记录了成功与失败模式，并分析了跨遥感模态的领域差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提供实用见解，指导数据采集策略、算力预算和优化计划，以推进前沿规模遥感基础模型的未来发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们探讨人工智能的扩展行为，以建立在高分辨率电光（EO）数据集上训练基础模型的实用技术，这些数据集的规模比当前最先进的规模大数个数量级。现代多模态机器学习（ML）应用，如图像字幕、搜索和推理的生成式人工智能（GenAI）系统，依赖于强大、领域专用的非文本模态编码器。在自然图像领域，互联网规模的数据丰富，已建立的扩展定律有助于优化模型容量、训练算力和数据集大小的联合扩展。不幸的是，在遥感（RS）等高价值领域，这些关系的理解要差得多。利用超过一千万亿像素的商业卫星EO数据和MITRE联邦AI沙盒，我们训练了更大规模的视觉变换器（ViT）骨干网络，报告了在拍字节级别训练时观察到的成功和失败模式，并分析了跨遥感模态的领域差距的影响。我们观察到，即使在此规模下，性能仍与数据受限的情况一致，而非模型参数受限。 这些实用见解旨在为数据采集策略、算力预算和优化计划提供信息，以推动前沿规模遥感基础模型的未来发展。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.&lt;/p&gt;</description></item><item><guid>2512.23906v1</guid><title>A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe</title><link>http://arxiv.org/abs/2512.23906v1</link><author>Wendong Yao, Binhua Huang, Soumyabrata Dev</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种多模态补丁Transformer，用于对欧洲地面运动服务（EGMS）时间序列的位移图进行单步、固定间隔的即时预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着城市规划、关键基础设施管理和自然灾害缓解需求的增加，近实时的区域尺度地面变形监测变得尤为重要。传统的干涉合成孔径雷达（InSAR）和EGMS等大尺度服务能够提供过去运动的密集观测，但由于长期趋势、季节周期和突发不连续性等因素的叠加，预测下一次观测仍然具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够同时利用最近位移快照、静态运动指标和日周期编码的多模态Transformer模型，以提高对未来位移的预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 模型以64×64网格的位移快照为输入，并结合从训练窗口中安全计算得到的平均速度、加速度、季节振幅等静态运动指标以及日周期的谐波编码。与STGCN、CNN-LSTM、CNN-LSTM+Attn以及多模态STGCN等模型进行对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在东爱尔兰的E32N34瓦片上，STGCN在仅使用位移数据时表现最佳；而当所有模型接收相同的多模态输入时，多模态Transformer显著优于其他模型，测试集误差均方根为0.90毫米，决定系数为0.97。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 多模态Transformer在近实时地面变形预测方面具有显著优势，可为城市规划、基础设施管理和灾害缓解提供可靠支持。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近实时区域尺度地面变形监测越来越被用于支持城市规划、关键基础设施管理和自然灾害缓解。干涉合成孔径雷达（InSAR）和欧洲地面运动服务（EGMS）等大陆尺度服务提供了过去运动的密集观测，但由于长期趋势、季节周期和偶发的突发不连续性（如共震步进）以及强烈的空间异质性，预测下一次观测仍然具有挑战性。本文提出了一种多模态补丁Transformer，用于对EGMS时间序列（重采样为100公里×100公里瓦片上的64×64网格）进行单步、固定间隔的即时预测。该模型同时输入最近的位移快照、（i）仅从训练窗口安全计算得到的静态运动指标（平均速度、加速度、季节振幅）以及（ii）日周期的谐波编码。在东爱尔兰的E32N34瓦片上，STGCN在仅使用位移数据时表现最佳；而当所有模型接收相同的多模态输入时，多模态Transformer明显优于CNN-LSTM、CNN-LSTM+Attn和多模态STGCN，测试集误差均方根为0.90毫米，决定系数为0.97。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation. While Interferometric Synthetic Aperture Radar (InSAR) and continental-scale services such as the European Ground Motion Service (EGMS) provide dense observations of past motion, predicting the next observation remains challenging due to the superposition of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a multimodal patch-based Transformer for single-step, fixed-interval next-epoch nowcasting of displacement maps from EGMS time series (resampled to a 64x64 grid over 100 km x 100 km tiles). The model ingests recent displacement snapshots together with (i) static kinematic indicators (mean velocity, acceleration, seasonal amplitude) computed in a leakage-safe manner from the training window only, and (ii) harmonic day-of-year encodings. On the eastern Ireland tile (E32N34), the STGCN is strongest in the displacement-only setting, whereas the multimodal Transformer clearly outperforms CNN-LSTM, CNN-LSTM+Attn, and multimodal STGCN when all models receive the same multimodal inputs, achieving RMSE = 0.90 mm and $R^2$ = 0.97 on the test set with the best threshold accuracies.&lt;/p&gt;</description></item><item><guid>2512.23914v1</guid><title>Hardware Acceleration for Neural Networks: A Comprehensive Survey</title><link>http://arxiv.org/abs/2512.23914v1</link><author>Bin Xu, Ayan Banerjee, Sandeep Gupta</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本综述探讨了深度学习硬件加速技术，包括GPU、TPU、FPGA、ASIC等；提出统一分类法；总结关键架构思想；讨论软件堆栈；指出挑战与未来方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度学习模型规模增长导致硬件瓶颈主要是内存移动、通信和不规则算子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 综述深度学习硬件加速技术，梳理技术景观，提出统一分类法，识别关键架构与挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过对GPU、TPU、FPGA、ASIC、LLM加速器等技术进行系统梳理，使用统一的分类法（工作负载、执行环境、优化杠杆）组织空间，并综合关键架构思想与软件堆栈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 发现内存移动和通信是主要瓶颈；关键架构包括Systolic arrays、向量/ SIMD 引擎、专用注意力/softmax 核、量化感知数据通路、高带宽内存；软件堆栈在模型语义与硬件之间起桥梁作用；存在高效长上下文 LLM 推理、动态稀疏工作负载、能耗与安全部署、以及公平基准等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 未来神经网络加速器需关注长上下文推理、动态稀疏支持、能耗与安全、以及公平基准，推动下一代加速技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 神经网络已成为云端和边缘平台的主导计算工作负载，但模型规模的快速增长和部署多样性暴露了硬件瓶颈，主要由内存移动、通信和不规则算子主导，而非峰值算术吞吐量。本文综述了深度学习硬件加速技术的技术景观，涵盖GPU和张量核心架构、专用加速器（如TPU/ NPU）、基于FPGA的设计、ASIC推理引擎以及新兴的LLM服务加速器（如LPU），并讨论了内/近内存计算和神经形态/模拟方法。我们使用统一的分类法组织空间，涵盖（i）工作负载（CNN、RNN、GNN、Transformer/LLM）、（ii）执行环境（训练与推理；数据中心与边缘）和（iii）优化杠杆（低精度、稀疏与剪枝、算子融合、编译与调度、内存系统/互连设计）。我们综合了关键架构思想，包括Systolic数组、向量和SIMD引擎、专用注意力和softmax核、量化感知数据通路以及高带宽内存，并讨论了软件堆栈和编译器如何将模型语义映射到硬件。最后，我们强调了开放挑战——包括高效长上下文LLM推理（KV缓存管理）、对动态和稀疏工作负载的鲁棒支持、能耗与安全感知部署以及公平基准——并指出了下一代神经加速器的有前景方向。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Neural networks have become a dominant computational workload across cloud and edge platforms, but rapid growth in model size and deployment diversity has exposed hardware bottlenecks increasingly dominated by memory movement, communication, and irregular operators rather than peak arithmetic throughput. This survey reviews the technology landscape for hardware acceleration of deep learning, spanning GPUs and tensor-core architectures; domain-specific accelerators (e.g., TPUs/NPUs); FPGA-based designs; ASIC inference engines; and emerging LLM-serving accelerators such as LPUs (language processing units), alongside in-/near-memory computing and neuromorphic/analog approaches. We organize the space using a unified taxonomy across (i) workloads (CNNs, RNNs, GNNs, and Transformers/LLMs), (ii) execution settings (training vs.\ inference; datacenter vs.\ edge), and (iii) optimization levers (reduced precision, sparsity and pruning, operator fusion, compilation and scheduling, and memory-system/interconnect design). We synthesize key architectural ideas including systolic arrays, vector and SIMD engines, specialized attention and softmax kernels, quantization-aware datapaths, and high-bandwidth memory, and we discuss how software stacks and compilers bridge model semantics to hardware. Finally, we highlight open challenges -- including efficient long-context LLM inference (KV-cache management), robust support for dynamic and sparse workloads, energy- and security-aware deployment, and fair benchmarking -- and point to promising directions for the next generation of neural acceleration.&lt;/p&gt;</description></item><item><guid>2512.23941v1</guid><title>Disentangling Learning from Judgment: Representation Learning for Open Response Analytics</title><link>http://arxiv.org/abs/2512.23941v1</link><author>Conrad Borchers, Manit Patel, Seiyon M. Lee, Anthony F. Botelho</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种以分析为先的框架，能够将开放式回答的内容信号与评分者倾向分离，从而使评分过程可见、可审计，并通过该框架提升自动评分的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 开放式回答在学习中非常重要，但传统的自动评分往往把学生写作内容与教师评分方式混为一谈，导致评分结果受教师偏好影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种能够区分内容与评分者倾向的分析框架，使评分判断透明且可检验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用匿名化的ASSISTments数学回答，先将教师历史建模为动态先验，再利用句子嵌入得到文本表示，并通过中心化和残差化消除提示词和教师的混杂效应；随后用时间验证的线性模型量化各信号贡献，并通过投影方法展示模型不一致性供定性检查。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 教师先验对评分预测影响显著；将先验与内容嵌入结合时预测效果最佳；仅用内容的模型虽然优于随机，但明显弱于结合模型；校正评分者效应后，残差内容表示更具信息量，能揭示语义证据支持理解的情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该流程将嵌入从单纯特征转化为学习分析工具，帮助教师和研究者审视评分实践与学生推理与学习证据的一致性或冲突，从而提升评分透明度和教学反思。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 开放式回答对学习至关重要，但自动评分往往将学生写作内容与教师评分方式混为一谈。我们提出一种以分析为先的框架，能够将内容信号与评分者倾向分离，使判断过程可见且可审计。利用匿名化的ASSISTments数学回答，我们将教师历史建模为动态先验，并通过句子嵌入得到文本表示，结合中心化和残差化来减轻提示词和教师的混杂效应。时间验证的线性模型量化每种信号的贡献，投影方法则将模型不一致性呈现出来供定性检查。结果显示，教师先验对评分预测影响很大；当先验与内容嵌入结合时，预测效果最佳（AUC约为0.815），而仅使用内容的模型虽然优于随机，但明显弱于此（AUC约为0.626）。校正评分者效应后，残差内容表示更为锐利，保留了更多信息丰富的嵌入维度，并揭示了语义证据支持理解的情况，而非仅仅表面差异。该贡献提供了一个实用的流程，将嵌入从单纯特征转化为学习分析工具，帮助教师和研究者审视评分实践与学生推理与学习证据的一致性或冲突。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Open-ended responses are central to learning, yet automated scoring often conflates what students wrote with how teachers grade. We present an analytics-first framework that separates content signals from rater tendencies, making judgments visible and auditable via analytics. Using de-identified ASSISTments mathematics responses, we model teacher histories as dynamic priors and derive text representations from sentence embeddings, incorporating centering and residualization to mitigate prompt and teacher confounds. Temporally-validated linear models quantify the contributions of each signal, and a projection surfaces model disagreements for qualitative inspection. Results show that teacher priors heavily influence grade predictions; the strongest results arise when priors are combined with content embeddings (AUC~0.815), while content-only models remain above chance but substantially weaker (AUC~0.626). Adjusting for rater effects sharpens the residual content representation, retaining more informative embedding dimensions and revealing cases where semantic evidence supports understanding as opposed to surface-level differences in how students respond. The contribution presents a practical pipeline that transforms embeddings from mere features into learning analytics for reflection, enabling teachers and researchers to examine where grading practices align (or conflict) with evidence of student reasoning and learning.&lt;/p&gt;</description></item><item><guid>2512.23959v1</guid><title>Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling</title><link>http://arxiv.org/abs/2512.23959v1</link><author>Chulun Zhou, Chunkang Zhang, Guoxin Yu, Fandong Meng, Jie Zhou, Wai Lam, Mo Yu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于超图的记忆机制HGMem，旨在通过捕捉事实之间的高阶关联，提升多步检索增强生成（RAG）在全局理解和推理任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多步RAG在需要全局理解和深入推理的任务中被广泛采用，但现有的记忆模块往往仅作为被动存储孤立事实，缺乏对高阶关系的建模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种动态、表达性强的记忆结构，能够捕捉并利用事实之间的高阶交互，从而增强多步推理和知识演化的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将记忆表示为超图，超边对应不同的记忆单元，支持在记忆中逐步形成高阶交互，连接围绕核心问题的事实和思路，形成集成且情境化的知识结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个针对全局意义建构的挑战性数据集上，HGMem在多步RAG任务中始终表现更好，并显著优于强基线系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于超图的记忆机制有效提升了RAG系统的全局理解和多步推理能力，优于传统的被动存储式记忆设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多步检索增强生成（RAG）已成为提升大型语言模型（LLM）在需要全局理解和深入推理任务中的广泛采用策略。许多RAG系统引入工作记忆模块以整合检索到的信息。然而，现有的记忆设计主要作为被动存储，累积孤立事实，用于压缩冗长输入并通过推理生成新的子查询。这种静态性质忽视了原始事实之间的高阶相关性，而这些组合往往能为后续步骤提供更强的指导。因此，它们的表征强度和对多步推理与知识演化的影响有限，导致在扩展上下文中推理碎片化、全局意义建构能力弱。我们提出HGMem，一种基于超图的记忆机制，将记忆概念从简单存储扩展为动态、表达性结构，以支持复杂推理和全局理解。在我们的方法中，记忆被表示为超图，超边对应不同的记忆单元，支持在记忆中逐步形成高阶交互。该机制将围绕核心问题的事实和思路连接起来，演化为一个集成且情境化的知识结构，为后续步骤的更深层推理提供强有力的命题。我们在几个为全局意义建构设计的挑战性数据集上评估了HGMem。大量实验和深入分析表明，我们的方法在多步RAG中始终表现更好，并在多样化任务中显著优于强基线系统。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.&lt;/p&gt;</description></item><item><guid>2512.23964v1</guid><title>Physics-informed Graph Neural Networks for Operational Flood Modeling</title><link>http://arxiv.org/abs/2512.23964v1</link><author>Carlo Malapad Acosta, Herath Mudiyanselage Viraj Vidura Herath, Jia Yu Lim, Abhishek Saha, Sanka Rasnayaka, Lucy Marshall</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; DUALFloodGNN 是一种新型洪水预测模型，利用图神经网络在全局和局部层面嵌入物理约束，能够同时预测节点水量和边缘流量，并通过多步损失与动态课程学习提升自回归推理性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统基于物理的洪水模型精度高但计算量大，难以满足快速预测需求；图神经网络能够处理无结构空间域，速度快且可与物理信息结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一种既快速又准确、并能解释物理过程的洪水预测模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计 DUALFloodGNN 架构，在共享消息传递框架中加入全局和局部物理约束的显式损失；使用多步损失和动态课程学习进行训练，以提升自回归推理效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 与标准 GNN 及现有最先进的洪水 GNN 模型相比，DUALFloodGNN 在预测多种水文变量时显著提升准确性，同时保持高效的计算性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将物理约束嵌入图神经网络可实现快速、准确且可解释的洪水预测，DUALFloodGNN 在实验中验证了这一点，并已开源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 洪水模型通过模拟洪水的时空水动力学为战略灾害管理提供信息。虽然基于物理的数值洪水模型精确，但其巨大的计算成本限制了其在需要快速预测的操作环境中的使用。使用图神经网络（GNN）设计的模型既能提供速度和精度，又能处理无结构空间域。鉴于其灵活的输入和架构，GNN 可以轻松与物理信息技术结合，显著提高可解释性。本研究提出了一种新型洪水 GNN 架构 DUALFloodGNN，通过显式损失在全局和局部尺度嵌入物理约束。该模型通过共享消息传递框架同时预测节点的水量和边缘的流量。为提升自回归推理性能，模型训练采用多步损失并增强动态课程学习。与标准 GNN 架构和最先进的 GNN 洪水模型相比，DUALFloodGNN 在预测多种水文变量时实现了显著改进，同时保持高计算效率。该模型已在 https://github.com/acostacos/dual_flood_gnn 开源。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Flood models inform strategic disaster management by simulating the spatiotemporal hydrodynamics of flooding. While physics-based numerical flood models are accurate, their substantial computational cost limits their use in operational settings where rapid predictions are essential. Models designed with graph neural networks (GNNs) provide both speed and accuracy while having the ability to process unstructured spatial domains. Given its flexible input and architecture, GNNs can be leveraged alongside physics-informed techniques with ease, significantly improving interpretability. This study introduces a novel flood GNN architecture, DUALFloodGNN, which embeds physical constraints at both global and local scales through explicit loss terms. The model jointly predicts water volume at nodes and flow along edges through a shared message-passing framework. To improve performance for autoregressive inference, model training is conducted with a multi-step loss enhanced with dynamic curriculum learning. Compared with standard GNN architectures and state-of-the-art GNN flood models, DUALFloodGNN achieves substantial improvements in predicting multiple hydrologic variables while maintaining high computational efficiency. The model is open-sourced at https://github.com/acostacos/dual_flood_gnn.&lt;/p&gt;</description></item><item><guid>2512.23966v1</guid><title>Efficient Context Scaling with LongCat ZigZag Attention</title><link>http://arxiv.org/abs/2512.23966v1</link><author>Chen Zhang, Yang Bai, Jiahuan Li, Anchun Gui, Keheng Wang, Feifan Liu, Guanyu Wu, Yuwei Jiang, Defei Bu, Li Wei, Haihang Jing, Hongyin Tang, Xin Chen, Xiangzhou Huang, Fengcun Li, Rongxiang Weng, Yulei Qian, Yifan Lu, Yerui Sun, Jingang Wang, Yuchen Xie, Xunliang Cai</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LoZA 是一种稀疏注意力方案，可将现有全注意力模型转换为计算预算有限的稀疏版本，在长上下文场景中实现显著加速。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 全注意力模型在长上下文任务中计算量大，效率低下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提供一种可直接应用于现有模型的稀疏注意力机制，以降低计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入 LoZA 并在中期训练期间将其应用于 LongCat-Flash，生成 LongCat-Flash-Exp。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LoZA 在预填充密集和解码密集任务中均能显著提升速度；LongCat-Flash-Exp 能快速处理多达 100 万 token，支持高效长期推理和长周期代理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LoZA 能将全注意力模型转化为高效稀疏版本，适用于长上下文应用，显著提升处理速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 LongCat ZigZag Attention（LoZA），这是一种稀疏注意力方案，旨在将任何现有的全注意力模型转换为计算预算有限的稀疏版本。在长上下文场景中，LoZA 能够在预填充密集（例如检索增强生成）和解码密集（例如工具集成推理）情况下实现显著的速度提升。具体而言，通过在中期训练期间将 LoZA 应用于 LongCat-Flash，我们提供了 LongCat-Flash-Exp 作为一种长上下文基础模型，能够快速处理多达 100 万个 token，支持高效的长期推理和长周期代理能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.&lt;/p&gt;</description></item><item><guid>2512.23972v1</guid><title>SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration</title><link>http://arxiv.org/abs/2512.23972v1</link><author>Liangtao Feng, Zhenchang Liu, Feng Zhang, Xuefeng Ren</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 SHIELD，一种基于球面投影的混合前沿集成方法，用于高效的 LiDAR 无人机探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然激光 LiDAR 具有宽视场优势，但在无人机探索中仍面临观测质量低于深度相机、传统前沿方法计算量大、无点云区域难以判定空旷等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为解决 LiDAR 观测质量不一致、计算负担重以及无点云区域判定困难等问题，提出 SHIELD 方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SHIELD 通过维护观测质量占据图并在该图上进行光线投射，采用混合前沿策略减轻计算负担并克服点云质量限制，同时引入向外球面投射光线投射策略，以保证开放区域的飞行安全和探索效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仿真和飞行实验表明 SHIELD 在 LiDAR 无人机探索中具有显著的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SHIELD 能有效提升 LiDAR 探索性能，且将开源以促进研究社区发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了 SHIELD，一种用于高效 LiDAR 无人机探索的球面投影混合前沿集成方法。尽管激光 LiDAR 具有宽视场的优势，但其在无人机探索中的应用仍面临若干挑战。LiDAR 点云的观测质量通常不如深度相机。基于已知和未知区域的传统前沿方法在处理 LiDAR 的宽视场时会产生沉重的计算负担。此外，缺乏点云的区域也难以通过光线投射来分类为空旷空间。为解决这些问题，提出了 SHIELD。它维护一个观测质量占据图，并在该图上执行光线投射，以解决探索过程中点云质量不一致的问题。采用混合前沿方法来同时解决计算负担和点云质量探索的局限性。此外，还提出了一种向外球面投射光线投射策略，以在开放区域中共同确保飞行安全和探索效率。仿真和飞行实验证明了 SHIELD 的有效性。本工作将开源，以贡献于研究社区。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决使用激光雷达进行无人机大规模开放环境探索时，宽视场导致的观测质量不均、无返回点难以判定自由空间以及前沿检测计算量大的问题。该问题重要，因为激光雷达在无人机自主探索中提供了广阔视野，但若无法准确建图和规划，飞行安全与效率将受到严重影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了激光雷达的局限性，借鉴了 EPIC、FLARE、SOAR 等现有前沿与观测质量方法，提出将表面法线信息与观测角度结合形成质量度量，并设计了混合前沿策略和外向球面投影射线投射。通过多线程架构平衡负载，最终形成 SHIELD 系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将观测质量融入占据图、前沿检测与射线投射三大模块，并利用球面投影补全无返回区域。实现流程包括：① 通过点云与法线计算质量并构建质量占据图；② 依据质量进行射线投射；③ 检测质量前沿与未知前沿，聚类并生成视点；④ 采用外向球面投影射线投射标记无返回方向的自由空间；⑤ 规划路径完成探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：① 基于表面法线的观测质量占据图与质量射线投射；② 混合前沿策略同时考虑质量前沿和未知前沿；③ 外向球面投影射线投射与自校准方法，解决无返回点的自由空间判定；④ 通过多线程实现高效实时运行。与以往仅使用统一占据图或仅关注未知前沿的工作不同，SHIELD 在观测质量和射线投射上做了更细粒度的处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SHIELD 通过观测质量映射、混合前沿规划和外向球面投影射线投射，构建了一套高效安全的激光雷达无人机探索框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper introduces SHIELD, a Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone exploration method. Although laser LiDAR offers the advantage of a wide field of view, its application in UAV exploration still faces several challenges. The observation quality of LiDAR point clouds is generally inferior to that of depth cameras. Traditional frontier methods based on known and unknown regions impose a heavy computational burden, especially when handling the wide field of view of LiDAR. In addition, regions without point cloud are also difficult to classify as free space through raycasting. To address these problems, the SHIELD is proposed. It maintains an observation-quality occupancy map and performs ray-casting on this map to address the issue of inconsistent point-cloud quality during exploration. A hybrid frontier method is used to tackle both the computational burden and the limitations of point-cloud quality exploration. In addition, an outward spherical-projection ray-casting strategy is proposed to jointly ensure flight safety and exploration efficiency in open areas. Simulations and flight experiments prove the effectiveness of SHIELD. This work will be open-sourced to contribute to the research community.&lt;/p&gt;</description></item><item><guid>2512.23983v1</guid><title>DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation</title><link>http://arxiv.org/abs/2512.23983v1</link><author>Yuang Jia, Jinlong Wang, Jiayi Zhao, Chunlam Li, Shunzhou Wang, Wei Gao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种仅使用图像和可选相机位姿的视角外推方法，结合点云估计、四维高斯重建和视频扩散模型，能够在自动驾驶场景中生成高质量的新视角图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有方法依赖昂贵的 LiDAR 点云、三维边界框和车道标注等先验信息，限制了实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在不使用昂贵传感器或繁重标注的前提下，实现自动驾驶场景中的高质量视角外推。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先估计全局静态点云和每帧动态点云并融合为统一表示；使用可变形四维高斯框架重建场景；训练四维高斯模型渲染退化和伪图像以训练视频扩散模型；随后用扩散模型迭代细化偏移的高斯渲染，并将增强结果回馈给四维高斯模型，直至完成视角外推。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法在新外推视角下生成的图像质量显著优于基线方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提供了一种无需昂贵传感器或繁重标注的有效视角外推方案，能够在自动驾驶场景中生成高质量图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种在自动驾驶场景中进行视角外推的有效解决方案。最近的方法侧重于使用扩散模型从给定视角生成偏移的新视角图像。然而，这些方法严重依赖于 LiDAR 点云、三维边界框和车道注释等先验信息，这需要昂贵的传感器或劳动密集型标注，限制了其在真实世界部署中的适用性。在本工作中，仅使用图像和可选的相机位姿，我们首先估计全局静态点云和每帧动态点云，并将它们融合成统一的表示。随后，我们采用可变形四维高斯框架重建场景。最初训练的四维高斯模型渲染出退化和伪图像，用于训练视频扩散模型。随后，逐步偏移的高斯渲染被扩散模型迭代细化，增强的结果再作为训练数据回馈给四维高斯模型。该过程持续进行，直至视角外推达到目标视角。与基线方法相比，我们的方法在新外推视角下生成的图像质量更高。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文旨在解决在自动驾驶场景中，仅使用图像（可选相机位姿）生成远距离视角的新颖图像的问题。该问题重要，因为逼真的闭环仿真和感知系统需要在大视角偏移下保持高质量渲染，而现有方法往往依赖昂贵的 LiDAR、3D 边框或车道标注，限制了实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了显式重建方法（NeRF、3D Gaussian Splatting）和扩散模型的优势，首先从图像中直接分离静态与动态点云并融合，随后使用 4D Gaussian 模型进行场景重建。接着训练一个以伪图像和动态掩码为条件的视频扩散模型，并在推理时通过扩散模型生成偏移视角，再将生成结果回馈给 Gaussian 模型进行迭代训练。该设计借鉴了 Drive‑Dreamer4D、ReconDreamer 等工作中的多条件扩散和逐步恢复策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将图像仅能获取的稠密点云估计与 4D Gaussian Splatting 结合，再利用条件视频扩散模型逐步恢复并生成偏移视角。实现流程为：①从图像估计静态与每帧动态点云；②用这些点云初始化 Gaussian 并渲染退化图像；③训练以伪图像和动态掩码为条件的视频扩散模型；④推理时使用扩散模型生成偏移视角图像；⑤将生成图像作为新训练数据重新训练 Gaussian，循环直到达到目标视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①仅使用图像的训练范式，去除了 LiDAR、3D 边框和车道标注等先验；②将静态与动态点云分离并融合，使用 4D Gaussian 进行动态建模；③在视频扩散模型中加入伪图像和动态掩码的多条件输入；④采用逐步恢复循环，将扩散生成的图像反馈给 Gaussian 进行迭代训练。与之前工作相比，它不依赖昂贵传感器或人工标注，并将显式重建与扩散生成有效结合，显著提升了大视角偏移下的图像质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了一种仅基于图像的管线，结合 4D Gaussian Splatting 与条件视频扩散模型，并通过迭代恢复实现高质量的驾驶视角外推，且不需要任何外部先验。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper presents an effective solution for view extrapolation in autonomous driving scenarios. Recent approaches focus on generating shifted novel view images from given viewpoints using diffusion models. However, these methods heavily rely on priors such as LiDAR point clouds, 3D bounding boxes, and lane annotations, which demand expensive sensors or labor-intensive labeling, limiting applicability in real-world deployment. In this work, with only images and optional camera poses, we first estimate a global static point cloud and per-frame dynamic point clouds, fusing them into a unified representation. We then employ a deformable 4D Gaussian framework to reconstruct the scene. The initially trained 4D Gaussian model renders degraded and pseudo-images to train a video diffusion model. Subsequently, progressively shifted Gaussian renderings are iteratively refined by the diffusion model,and the enhanced results are incorporated back as training data for 4DGS. This process continues until extrapolation reaches the target viewpoints. Compared with baselines, our method produces higher-quality images at novel extrapolated viewpoints.&lt;/p&gt;</description></item><item><guid>2512.23986v1</guid><title>Anomaly detection in satellite imagery through temporal inpainting</title><link>http://arxiv.org/abs/2512.23986v1</link><author>Bertrand Rouet-Leduc, Claudia Hulbert</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文展示了利用深度学习和卫星时间序列的时间冗余来检测地表变化的方法。通过训练基于SATLAS的修复模型，预测无变化时的地表外观，并与实际观测对比，能够发现传统方法难以检测的异常。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 快速灾害响应和环境监测需要及时识别地表变化，但大气噪声、季节变化和传感器伪影使得检测变得困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证深度学习模型在卫星影像中以更高灵敏度和特异性检测地表异常的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用全球分布的Sentinel-2时间序列数据训练SATLAS基础模型的修复网络，重建序列最后一帧，并将预测结果与观测结果比较，识别差异作为异常。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在2023年土耳其-叙利亚地震后，模型成功检测到Tepehan地区的断裂特征，检测灵敏度和特异性均优于时间中位数和Reed‑Xiaoli异常检测器，阈值约为基线方法的三倍低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法实现了更低阈值的异常检测，为利用免费多光谱卫星数据实现全球自动化地表变化监测提供了可行路径。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Detecting surface changes from satellite imagery is critical for rapid disaster response and environmental monitoring, yet remains challenging due to the complex interplay between atmospheric noise, seasonal variations, and sensor artifacts. Here we show that deep learning can leverage the temporal redundancy of satellite time series to detect anomalies at unprecedented sensitivity, by learning to predict what the surface should look like in the absence of change. We train an inpainting model built upon the SATLAS foundation model to reconstruct the last frame of a Sentinel-2 time series from preceding acquisitions, using globally distributed training data spanning diverse climate zones and land cover types. When applied to regions affected by sudden surface changes, the discrepancy between prediction and observation reveals anomalies that traditional change detection methods miss. We validate our approach on earthquake-triggered surface ruptures from the 2023 Turkey-Syria earthquake sequence, demonstrating detection of a rift feature in Tepehan with higher sensitivity and specificity than temporal median or Reed-Xiaoli anomaly detectors. Our method reaches detection thresholds approximately three times lower than baseline approaches, providing a path towards automated, global-scale monitoring of surface changes from freely available multi-spectral satellite data.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Detecting surface changes from satellite imagery is critical for rapid disaster response and environmental monitoring, yet remains challenging due to the complex interplay between atmospheric noise, seasonal variations, and sensor artifacts. Here we show that deep learning can leverage the temporal redundancy of satellite time series to detect anomalies at unprecedented sensitivity, by learning to predict what the surface should look like in the absence of change. We train an inpainting model built upon the SATLAS foundation model to reconstruct the last frame of a Sentinel-2 time series from preceding acquisitions, using globally distributed training data spanning diverse climate zones and land cover types. When applied to regions affected by sudden surface changes, the discrepancy between prediction and observation reveals anomalies that traditional change detection methods miss. We validate our approach on earthquake-triggered surface ruptures from the 2023 Turkey-Syria earthquake sequence, demonstrating detection of a rift feature in Tepehan with higher sensitivity and specificity than temporal median or Reed-Xiaoli anomaly detectors. Our method reaches detection thresholds approximately three times lower than baseline approaches, providing a path towards automated, global-scale monitoring of surface changes from freely available multi-spectral satellite data.&lt;/p&gt;</description></item><item><guid>2512.23987v1</guid><title>MeLeMaD: Adaptive Malware Detection via Chunk-wise Feature Selection and Meta-Learning</title><link>http://arxiv.org/abs/2512.23987v1</link><author>Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新型的恶意软件检测框架MeLeMaD，利用模型无关元学习和梯度提升的特征选择方法，显著提升了在大规模高维数据集上的检测效率和准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在网络安全领域，恶意软件检测面临不断演化的威胁和海量数据的挑战，需要既稳健又可适应的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入MeLeMaD框架，提升恶意软件检测的鲁棒性、适应性和处理大规模高维数据的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用模型无关元学习MAML进行快速适应，结合基于梯度提升的分块特征选择CFSGB处理大规模数据；在CIC-AndMal2020、BODMAS和自定义EMBOD数据集上进行实验验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在CIC-AndMal2020上实现98.04%的准确率，在BODMAS上实现99.97%的准确率，EMBOD上达到97.85%；在准确率、精确率、召回率、F1分数、MCC和AUC等指标上均优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MeLeMaD展示了在恶意软件检测中对鲁棒性、适应性和大规模高维数据处理的有效性，为更高效的网络安全解决方案奠定了基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种名为MeLeMaD的新型恶意软件检测框架，利用模型无关元学习（MAML）的适应性和泛化能力，并结合一种基于梯度提升的分块特征选择方法（CFSGB），专门处理大规模高维恶意软件数据集，从而显著提升检测效率。作者在两个基准恶意软件数据集（CIC-AndMal2020和BODMAS）以及一个自定义数据集（EMBOD）上进行了严格验证，取得了卓越的性能，包括准确率、精确率、召回率、F1分数、MCC和AUC等关键评估指标。在CIC-AndMal2020上实现了98.04%的准确率，在BODMAS上实现了99.97%的准确率，EMBOD数据集也达到了97.85%的准确率。结果表明，MeLeMaD在鲁棒性、适应性以及处理大规模高维数据方面具有潜力，为更有效、更高效的网络安全解决方案铺平了道路。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Confronting the substantial challenges of malware detection in cybersecurity necessitates solutions that are both robust and adaptable to the ever-evolving threat environment. The paper introduces Meta Learning Malware Detection (MeLeMaD), a novel framework leveraging the adaptability and generalization capabilities of Model-Agnostic Meta-Learning (MAML) for malware detection. MeLeMaD incorporates a novel feature selection technique, Chunk-wise Feature Selection based on Gradient Boosting (CFSGB), tailored for handling large-scale, high-dimensional malware datasets, significantly enhancing the detection efficiency. Two benchmark malware datasets (CIC-AndMal2020 and BODMAS) and a custom dataset (EMBOD) were used for rigorously validating the MeLeMaD, achieving a remarkable performance in terms of key evaluation measures, including accuracy, precision, recall, F1-score, MCC, and AUC. With accuracies of 98.04\% on CIC-AndMal2020 and 99.97\% on BODMAS, MeLeMaD outperforms the state-of-the-art approaches. The custom dataset, EMBOD, also achieves a commendable accuracy of 97.85\%. The results underscore the MeLeMaD&amp;#x27;s potential to address the challenges of robustness, adaptability, and large-scale, high-dimensional datasets in malware detection, paving the way for more effective and efficient cybersecurity solutions.&lt;/p&gt;</description></item><item><guid>2512.24002v1</guid><title>Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective</title><link>http://arxiv.org/abs/2512.24002v1</link><author>Tan Pan, Yixuan Sun, Chen Jiang, Qiong Gao, Rui Sun, Xingmeng Zhang, Zhenqi Yang, Limei Han, Yixiu Liang, Yuan Cheng, Kaiyu Guo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一个名为 CLEAR-HUG 的两阶段框架，用于改进多导联心电图的自监督学习。该框架通过捕捉心脏传导过程中的细微差异，并遵循心电图诊断指南的层级逻辑，提升了心电图表示学习的质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多导联心电图是心脏诊断的基石，近年来自监督学习在心电图表示学习中展现出前景。然而，现有方法往往只关注导联和心跳之间的一致模式，忽视了心跳在传导过程中的差异，且未能与诊断指南的层级逻辑相匹配。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有自监督学习方法忽视心跳差异和诊断指南层级逻辑的问题，构建能够捕捉细微传导差异并符合临床诊断流程的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先构建 CLEAR 模型，使用稀疏注意力机制对每个心跳进行重建，既保留特定差异又提取共性；随后设计 HUG 头部，按层级统一导联进行疾病诊断，模拟临床工作流程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在六项任务上实验表明，CLEAR-HUG 相比基线提升了 6.84%，验证了其在提升心电图表示和符合诊断指南方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CLEAR-HUG 能够增强心脏传导表示，并将学习模式与专家诊断指南对齐，为心电图自监督学习提供了更具临床意义的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The multi-lead electrocardiogram (ECG) stands as a cornerstone of cardiac diagnosis. Recent strides in electrocardiogram self-supervised learning (eSSL) have brightened prospects for enhancing representation learning without relying on high-quality annotations. Yet earlier eSSL methods suffer a key limitation: they focus on consistent patterns across leads and beats, overlooking the inherent differences in heartbeats rooted in cardiac conduction processes, while subtle but significant variations carry unique physiological signatures. Moreover, representation learning for ECG analysis should align with ECG diagnostic guidelines, which progress from individual heartbeats to single leads and ultimately to lead combinations. This sequential logic, however, is often neglected when applying pre-trained models to downstream tasks. To address these gaps, we propose CLEAR-HUG, a two-stage framework designed to capture subtle variations in cardiac conduction across leads while adhering to ECG diagnostic guidelines. In the first stage, we introduce an eSSL model termed Conduction-LEAd Reconstructor (CLEAR), which captures both specific variations and general commonalities across heartbeats. Treating each heartbeat as a distinct entity, CLEAR employs a simple yet effective sparse attention mechanism to reconstruct signals without interference from other heartbeats. In the second stage, we implement a Hierarchical lead-Unified Group head (HUG) for disease diagnosis, mirroring clinical workflow. Experimental results across six tasks show a 6.84% improvement, validating the effectiveness of CLEAR-HUG. This highlights its ability to enhance representations of cardiac conduction and align patterns with expert diagnostic guidelines.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The multi-lead electrocardiogram (ECG) stands as a cornerstone of cardiac diagnosis. Recent strides in electrocardiogram self-supervised learning (eSSL) have brightened prospects for enhancing representation learning without relying on high-quality annotations. Yet earlier eSSL methods suffer a key limitation: they focus on consistent patterns across leads and beats, overlooking the inherent differences in heartbeats rooted in cardiac conduction processes, while subtle but significant variations carry unique physiological signatures. Moreover, representation learning for ECG analysis should align with ECG diagnostic guidelines, which progress from individual heartbeats to single leads and ultimately to lead combinations. This sequential logic, however, is often neglected when applying pre-trained models to downstream tasks. To address these gaps, we propose CLEAR-HUG, a two-stage framework designed to capture subtle variations in cardiac conduction across leads while adhering to ECG diagnostic guidelines. In the first stage, we introduce an eSSL model termed Conduction-LEAd Reconstructor (CLEAR), which captures both specific variations and general commonalities across heartbeats. Treating each heartbeat as a distinct entity, CLEAR employs a simple yet effective sparse attention mechanism to reconstruct signals without interference from other heartbeats. In the second stage, we implement a Hierarchical lead-Unified Group head (HUG) for disease diagnosis, mirroring clinical workflow. Experimental results across six tasks show a 6.84% improvement, validating the effectiveness of CLEAR-HUG. This highlights its ability to enhance representations of cardiac conduction and align patterns with expert diagnostic guidelines.&lt;/p&gt;</description></item><item><guid>2512.24007v1</guid><title>TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems</title><link>http://arxiv.org/abs/2512.24007v1</link><author>Bulent Soykan, Sean Mondesire, Ghaith Rabadi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; TESO是一种结合短期Tabu列表、长期精英记忆和期望准则的元启发式框架，能够在噪声评估和高成本环境下平衡探索与开发，并在排队优化问题上优于基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 模拟优化常面临噪声评估、高计算成本和复杂多峰搜索空间等挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新的Tabu增强模拟优化框架（TESO），以改进模拟优化的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; TESO整合自适应搜索与基于记忆的策略，使用短期Tabu列表防止循环并鼓励多样化，使用长期精英记忆通过扰动高性能解来引导强化，并通过期望准则在特殊候选解上覆盖Tabu限制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在排队优化问题上，TESO表现出更好的性能，验证了其记忆组件的贡献。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; TESO通过动态平衡探索与开发，在随机环境中提高了模拟优化的有效性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 模拟优化（SO）经常面临噪声评估、高计算成本和复杂多峰搜索景观的挑战。本文提出了Tabu增强模拟优化（TESO），一种将自适应搜索与基于记忆的策略相结合的新型元启发式框架。TESO利用短期Tabu列表防止循环并鼓励多样化，并使用长期精英记忆通过扰动高性能解来引导强化。期望准则允许在特殊候选解上覆盖Tabu限制。这种组合在随机环境中实现了探索与开发的动态平衡。我们通过排队优化问题演示了TESO的有效性和可靠性，显示其相较于基准的改进，并验证了其记忆组件的贡献。源代码和数据可在 https://github.com/bulentsoykan/TESO 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO&amp;#x27;s effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.&lt;/p&gt;</description></item><item><guid>2512.24052v1</guid><title>AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives</title><link>http://arxiv.org/abs/2512.24052v1</link><author>Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究大型音频语言模型的幻觉问题，提出了事件缺失、错误事件身份、时间关系错误和定量时间错误四类幻觉，并通过AHA框架和AHA-Eval基准提升模型的时间推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型音频语言模型在性能上处于领先水平，但常出现生成与音频输入不一致的文本幻觉。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 分析幻觉的根源，构建幻觉分类体系，并开发一种对齐框架以减少幻觉并提升模型的时间推理准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用反事实硬负采样构建高质量偏好数据集，训练模型区分真实声学证据与语言可行的虚构；创建AHA-Eval诊断基准评估细粒度时间推理；将该数据用于对齐Qwen2.5-Omni，得到Qwen-Audio-AHA模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Qwen-Audio-AHA在AHA-Eval上提升13.7%，并在公开基准MMAU-Test提升1.3%，MMAR提升1.6%，均超过最新SOTA。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AHA框架有效降低音频语言模型的幻觉，并显著提升时间推理性能，效果可推广到其他评测集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然大型音频语言模型（LALMs）在性能上达到了最先进水平，但它们经常出现幻觉，例如生成与音频输入不相关的文本。我们分析了这些定位失败，并识别出一个独特的分类体系：事件遗漏、错误事件身份、时间关系错误和定量时间错误。为了解决这个问题，我们提出了AHA（Audio Hallucination Alignment）框架。通过利用反事实硬负采样，我们的流程构建了一个高质量的偏好数据集，迫使模型区分严格的声学证据与语言上可行的虚构。此外，我们建立了AHA-Eval，一个旨在严格测试这些细粒度时间推理能力的诊断基准。我们将这些数据用于对齐Qwen2.5-Omni。得到的模型Qwen-Audio-AHA在AHA-Eval上实现了13.7%的提升。关键的是，这种收益超出了我们的诊断集。我们的模型在公开基准上也表现出显著提升，包括在MMAU-Test上提升1.3%，在MMAR上提升1.6%，并超过了最新的SOTA方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although Large Audio-Language Models (LALMs) deliver state-of-the-art (SOTA) performance, they frequently suffer from hallucinations, e.g. generating text not grounded in the audio input. We analyze these grounding failures and identify a distinct taxonomy: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. To address this, we introduce the AHA (Audio Hallucination Alignment) framework. By leveraging counterfactual hard negative mining, our pipeline constructs a high-quality preference dataset that forces models to distinguish strict acoustic evidence from linguistically plausible fabrications. Additionally, we establish AHA-Eval, a diagnostic benchmark designed to rigorously test these fine-grained temporal reasoning capabilities. We apply this data to align Qwen2.5-Omni. The resulting model, Qwen-Audio-AHA, achieves a 13.7% improvement on AHA-Eval. Crucially, this benefit generalizes beyond our diagnostic set. Our model shows substantial gains on public benchmarks, including 1.3% on MMAU-Test and 1.6% on MMAR, outperforming latest SOTA methods.&lt;/p&gt;</description></item><item><guid>2512.24062v1</guid><title>Hyperspherical Graph Representation Learning via Adaptive Neighbor-Mean Alignment and Uniformity</title><link>http://arxiv.org/abs/2512.24062v1</link><author>Rui Chen, Junjun Guo, Hongbin Wang, Yan Xiang, Yantuan Xian, Zhengtao Yu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 HyperGRL，一种通过自适应邻居均值对齐和无采样均匀化实现超球面图表示学习的统一框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统图表示学习方法多依赖对比目标或互信息最大化，需要复杂架构、负采样和敏感的超参数，容易导致过平滑、过压缩和训练不稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种更稳健、无采样、几何基础的图表示学习方法，减少对复杂设计的依赖并提升表示质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; HyperGRL 在单位超球面上嵌入节点，采用邻居均值对齐目标构造稳定的语义目标，并通过基于距离的超球面正则化实现全局均匀分布；同时引入熵引导的自适应平衡机制动态调节两目标的权重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在节点分类、聚类和链路预测任务上，HyperGRL 相比最强现有方法平均提升 1.49%、0.86% 和 0.74%，证明了无采样几何对比目标的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 几何基础、无采样的对比目标能够显著提升图表示学习的质量和泛化能力，HyperGRL 为此提供了一个统一且稳定的实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图表示学习旨在将图结构数据的结构和语义依赖编码为低维嵌入。然而，现有方法往往依赖代理对比目标或互信息最大化，这通常需要复杂的架构、负采样策略和敏感的超参数调优。这些设计选择可能导致过平滑、过压缩和训练不稳定。在本文中，我们提出 HyperGRL，一种通过自适应邻居均值对齐和无采样均匀化实现超球面图表示学习的统一框架。HyperGRL 通过两个对抗耦合目标将节点嵌入单位超球面：邻居均值对齐和无采样均匀化。对齐目标利用每个节点局部邻域的均值表示构造语义基础、稳定的目标，捕捉共享的结构和特征模式。均匀化目标通过基于距离的超球面正则化来表征分散性，鼓励全局均匀的嵌入分布，同时保留判别信息。为进一步稳定训练，我们引入熵引导的自适应平衡机制，动态调节对齐与均匀化之间的相互作用，无需手动调参。大量实验表明，HyperGRL 在节点分类、节点聚类和链路预测任务上相较最强现有方法平均提升 1.49%、0.86% 和 0.74%，展示了几何基础、无采样对比目标在图表示学习中的有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Graph representation learning (GRL) aims to encode structural and semantic dependencies of graph-structured data into low-dimensional embeddings. However, existing GRL methods often rely on surrogate contrastive objectives or mutual information maximization, which typically demand complex architectures, negative sampling strategies, and sensitive hyperparameter tuning. These design choices may induce over-smoothing, over-squashing, and training instability. In this work, we propose HyperGRL, a unified framework for hyperspherical graph representation learning via adaptive neighbor-mean alignment and sampling-free uniformity. HyperGRL embeds nodes on a unit hypersphere through two adversarially coupled objectives: neighbor-mean alignment and sampling-free uniformity. The alignment objective uses the mean representation of each node&amp;#x27;s local neighborhood to construct semantically grounded, stable targets that capture shared structural and feature patterns. The uniformity objective formulates dispersion via an L2-based hyperspherical regularization, encouraging globally uniform embedding distributions while preserving discriminative information. To further stabilize training, we introduce an entropy-guided adaptive balancing mechanism that dynamically regulates the interplay between alignment and uniformity without requiring manual tuning. Extensive experiments on node classification, node clustering, and link prediction demonstrate that HyperGRL delivers superior representation quality and generalization across diverse graph structures, achieving average improvements of 1.49%, 0.86%, and 0.74% over the strongest existing methods, respectively. These findings highlight the effectiveness of geometrically grounded, sampling-free contrastive objectives for graph representation learning.&lt;/p&gt;</description></item><item><guid>2512.24074v1</guid><title>Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images</title><link>http://arxiv.org/abs/2512.24074v1</link><author>Jingzhou Chen, Dexin Chen, Fengchao Xiong, Yuntao Qian, Liang Xiao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文针对遥感细粒度检测中层级标签结构的挑战，提出一种平衡层级对比损失与解耦学习策略，结合DETR框架，显著提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 细粒度遥感数据常采用层级标签结构，对象在多个层级被标注；但将层级语义嵌入表示学习空间以提升检测效果仍困难。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决层级标签不平衡导致高频类主导学习，以及语义关系学习干扰类别无关定位的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出平衡层级对比损失，引入可学习的类别原型并在每个层级均衡梯度；同时采用解耦策略，将DETR的查询分为分类和定位两组，实现任务特定特征提取与优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三个细粒度层级标注数据集上实验表明，该方法优于现有最先进方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 平衡层级对比损失与解耦学习策略有效提升了细粒度遥感检测性能，可为类似任务提供参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 细粒度遥感数据集通常使用层级标签结构以粗到细的方式区分对象，每个对象在多个层级上进行标注。然而，将这种语义层级嵌入表示学习空间以提升细粒度检测性能仍然具有挑战性。先前的研究在不同层级上应用监督对比学习，将属于同一父类的对象聚集在一起，同时区分兄弟子类别。然而，它们忽视了两个关键问题：(1) 标签层级中的数据分布不平衡导致高频类主导学习过程；(2) 学习类别之间的语义关系会干扰类别无关的定位。为了解决这些问题，我们提出了一种结合解耦学习策略的平衡层级对比损失，并将其应用于检测变压器(DETR)框架。该损失引入可学习的类别原型，并在每个层级上平衡不同类别贡献的梯度，确保每个层级类别在每个小批量中对损失计算的贡献相等。解耦策略将DETR的对象查询分为分类和定位集合，使任务特定的特征提取和优化成为可能。在三个具有层级注释的细粒度数据集上的实验表明，我们的方法优于最先进的方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Fine-grained remote sensing datasets often use hierarchical label structures to differentiate objects in a coarse-to-fine manner, with each object annotated across multiple levels. However, embedding this semantic hierarchy into the representation learning space to improve fine-grained detection performance remains challenging. Previous studies have applied supervised contrastive learning at different hierarchical levels to group objects under the same parent class while distinguishing sibling subcategories. Nevertheless, they overlook two critical issues: (1) imbalanced data distribution across the label hierarchy causes high-frequency classes to dominate the learning process, and (2) learning semantic relationships among categories interferes with class-agnostic localization. To address these issues, we propose a balanced hierarchical contrastive loss combined with a decoupled learning strategy within the detection transformer (DETR) framework. The proposed loss introduces learnable class prototypes and equilibrates gradients contributed by different classes at each hierarchical level, ensuring that each hierarchical class contributes equally to the loss computation in every mini-batch. The decoupled strategy separates DETR&amp;#x27;s object queries into classification and localization sets, enabling task-specific feature extraction and optimization. Experiments on three fine-grained datasets with hierarchical annotations demonstrate that our method outperforms state-of-the-art approaches.&lt;/p&gt;</description></item><item><guid>2512.24097v1</guid><title>Factorized Learning for Temporally Grounded Video-Language Models</title><link>http://arxiv.org/abs/2512.24097v1</link><author>Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 D^2VLM 框架，解耦视频语言模型中的时间定位和文本回答任务，并通过证据标记和分解偏好优化算法提升了事件级时间定位和文本回答的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视频语言模型在视频理解方面表现出潜力，但在事件级时间定位方面仍存在困难，现有工作往往将时间定位和文本回答耦合处理，缺乏清晰的逻辑层次。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 从分解学习角度解决耦合问题，解耦时间定位和文本回答任务，同时保留它们的内在依赖关系，以提升整体性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 D^2VLM 框架，采用“先定位后回答并引用证据”范式，引入证据标记；设计分解偏好优化（FPO）算法，将概率时间定位建模融入优化目标；构建合成数据集支持分解偏好学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，分解学习与证据标记和 FPO 算法相结合，在多项任务上显著优于传统耦合方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 解耦时间定位与文本回答并结合分解偏好优化，可显著提升视频语言模型的时间定位和文本回答质量，验证了所提方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 最近的视频语言模型在视频理解方面表现出巨大潜力，但在事件级感知的准确时间定位方面仍然存在困难。我们观察到视频理解中的两个主要因素（即时间定位和文本回答）形成了逻辑层次结构：准确的时间证据定位是可靠文本回答的基础。然而，现有工作通常以耦合方式处理这两个任务，缺乏清晰的逻辑结构，导致目标次优。我们从分解学习的角度解决此问题。首先提出 D^2VLM 框架，解耦这两个任务的学习，同时强调它们的内在依赖关系。我们采用“先定位后回答并引用证据”的范式，引入证据标记用于证据定位，强调事件级视觉语义捕捉，超越现有工作对时间戳表示的关注。为进一步促进这两个任务的学习，我们提出了一种新颖的分解偏好优化（FPO）算法。与标准偏好优化不同，FPO 在优化目标中显式加入概率时间定位建模，使偏好学习同时适用于时间定位和文本回答。我们还构建了一个合成数据集，以解决缺乏适合分解偏好学习并具有显式时间定位的数据集的问题。对多种任务的实验表明，我们的方法具有明显优势。我们的源代码可在 https://github.com/nusnlp/d2vlm 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a &amp;quot;grounding then answering with evidence referencing&amp;quot; paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.&lt;/p&gt;</description></item><item><guid>2512.24103v1</guid><title>Enhancing LLM Planning Capabilities through Intrinsic Self-Critique</title><link>http://arxiv.org/abs/2512.24103v1</link><author>Bernd Bohnet, Pierre-Alexandre Kamienny, Hanie Sedghi, Dilan Gorur, Pranjal Awasthi, Aaron Parisi, Kevin Swersky, Rosanne Liu, Azade Nova, Noah Fiedel</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出并验证了一种让大型语言模型自我批评其答案的方法，显著提升了规划任务的性能，超过了现有基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 先前研究对LLM自我批评的有效性持怀疑态度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过内部自我批评提升LLM在规划数据集上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用少样本学习逐步扩展到多样本基线，并在此基础上采用迭代纠正与细化的过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在Blocksworld、Logistics和Mini-grid数据集上实现了显著性能提升，创下2024年10月LLM检查点的新最高水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自我批评能显著提升规划性能，且方法对模型版本具有普适性，未来可应用于更复杂搜索和更强模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们展示了一种让大型语言模型（LLM）批判自身答案的方法，旨在提升其性能，并在已建立的规划基准上取得显著改进。尽管早期研究对LLM使用自我批评方法的有效性提出质疑，我们通过内部自我批评在Blocksworld领域的规划数据集上实现了显著的性能提升，而无需外部验证者。我们还在Logistics和Mini-grid数据集上展示了类似的改进，超过了强基线准确率。我们采用少样本学习技术，并逐步扩展到多样本方法作为基础，并证明通过迭代纠正和细化的过程可以在此已有竞争力的方法上获得实质性提升。我们说明了自我批评如何显著提升规划性能。我们的实验结果在所考虑的模型类别（即2024年10月的LLM检查点）上呈现了新的最先进水平。我们的主要关注点在于方法本身，展示了可在不受特定模型版本限制的情况下实现的内在自我改进能力，并且我们相信将此方法应用于更复杂的搜索技术和更强大的模型将带来更好的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We demonstrate an approach for LLMs to critique their \emph{own} answers with the goal of enhancing their performance that leads to significant improvements over established planning benchmarks. Despite the findings of earlier research that has cast doubt on the effectiveness of LLMs leveraging self critique methods, we show significant performance gains on planning datasets in the Blocksworld domain through intrinsic self-critique, without external source such as a verifier. We also demonstrate similar improvements on Logistics and Mini-grid datasets, exceeding strong baseline accuracies. We employ a few-shot learning technique and progressively extend it to a many-shot approach as our base method and demonstrate that it is possible to gain substantial improvement on top of this already competitive approach by employing an iterative process for correction and refinement. We illustrate how self-critique can significantly boost planning performance. Our empirical results present new state-of-the-art on the class of models considered, namely LLM model checkpoints from October 2024. Our primary focus lies on the method itself, demonstrating intrinsic self-improvement capabilities that are applicable regardless of the specific model version, and we believe that applying our method to more complex search techniques and more capable models will lead to even better performance.&lt;/p&gt;</description></item><item><guid>2512.24119v1</guid><title>GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation</title><link>http://arxiv.org/abs/2512.24119v1</link><author>Yuan Feng, Yue Yang, Xiaohan He, Jiatong Zhao, Jianlong Chen, Zijun Chen, Daocheng Fu, Qi Liu, Renqiu Xia, Bo Zhang, Junchi Yan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了 GeoBench，一个用于评估视觉语言模型在几何问题求解方面的分层基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有评估方法存在教材数据污染、过度关注答案而忽视推理过程、诊断粒度不足等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过构建多层次的评估体系，解决上述局限，提供更细致的几何推理评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GeoBench 包含四个推理层级：视觉感知、目标导向规划、严谨定理应用和自我反思回溯，并通过 TrustGeoGen 生成的六个形式化验证任务进行系统评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; OpenAI-o3 等推理模型在一般 MLLM 之上表现更好，但随着任务复杂度提升性能显著下降；子目标分解和无关前提过滤对最终准确率影响大；链式思考提示在某些任务中反而降低了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GeoBench 为几何推理提供了全面、可操作的评估框架，并为系统开发提供了实用指导。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 几何问题求解是数学推理的重要分支，需要对形状和空间关系进行精确分析。目前对视觉语言模型在几何推理方面的评估存在局限，包括教材基准可能导致测试数据污染、过度强调最终答案而忽视推理过程、诊断粒度不足等。为解决这些问题，我们提出了 GeoBench，一个分层基准，包含四个几何问题求解的推理层级：视觉感知、目标导向规划、严谨定理应用和自我反思回溯。通过 TrustGeoGen 生成的六个形式化验证任务，我们系统评估了从属性提取到逻辑错误纠正的能力。实验表明，像 OpenAI-o3 这样的推理模型在一般多模态大型语言模型之上表现更好，但随着任务复杂度增加，性能显著下降。关键发现表明，子目标分解和无关前提过滤对最终问题求解准确率具有关键影响，而链式思考提示在某些任务中意外降低了性能。这些发现确立了 GeoBench 作为一个全面的基准，并为开发几何问题求解系统提供了可操作的指导。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Geometric problem solving constitutes a critical branch of mathematical reasoning, requiring precise analysis of shapes and spatial relationships. Current evaluations of geometric reasoning in vision-language models (VLMs) face limitations, including the risk of test data contamination from textbook-based benchmarks, overemphasis on final answers over reasoning processes, and insufficient diagnostic granularity. To address these issues, we present GeoBench, a hierarchical benchmark featuring four reasoning levels in geometric problem-solving: Visual Perception, Goal-Oriented Planning, Rigorous Theorem Application, and Self-Reflective Backtracking. Through six formally verified tasks generated via TrustGeoGen, we systematically assess capabilities ranging from attribute extraction to logical error correction. Experiments reveal that while reasoning models like OpenAI-o3 outperform general MLLMs, performance declines significantly with increasing task complexity. Key findings demonstrate that sub-goal decomposition and irrelevant premise filtering critically influence final problem-solving accuracy, whereas Chain-of-Thought prompting unexpectedly degrades performance in some tasks. These findings establish GeoBench as a comprehensive benchmark while offering actionable guidelines for developing geometric problem-solving systems.&lt;/p&gt;</description></item><item><guid>2512.24160v1</guid><title>Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset</title><link>http://arxiv.org/abs/2512.24160v1</link><author>TsaiChing Ni, ZhenQi Chen, YuanFu Yang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了IMDD-1M工业多模态缺陷数据集，包含一百万张图像与文本配对，并基于该数据集训练了一个扩散式视觉语言基础模型，展示了在工业检测任务中仅使用少量任务数据即可达到与专业模型相当的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 制造业和质量检测领域缺乏大规模、真实且多模态的缺陷数据，限制了多模态学习技术的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建首个大规模工业多模态缺陷数据集，并利用其训练可迁移的基础模型，以提升工业检测与生成任务的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 收集超过60种材料、400多种缺陷类型的高分辨率图像，并配以专家验证的注释和细粒度文本描述；随后从零开始训练基于扩散模型的视觉语言基础模型，并通过轻量级微调适配特定领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该数据集覆盖广泛，模型在仅使用少于5%任务专用数据的情况下，性能与传统专家模型相当，证明了数据高效的基础模型适配潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 数据高效的基础模型适配为工业检测与生成提供了可扩展、领域自适应且知识驱动的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了IMDD-1M，这是首个大型工业多模态缺陷数据集，包含一百万个对齐的图像-文本对，旨在推动制造业和质量检测领域的多模态学习。IMDD-1M包含超过60种材料类别和400多种缺陷类型的高分辨率真实缺陷，每个缺陷都配有专家验证的注释和细粒度文本描述，详细说明缺陷位置、严重程度和上下文属性。该数据集支持分类、分割、检索、标题生成和生成建模等多种应用。基于IMDD-1M，我们从零开始训练了一个扩散式视觉语言基础模型，专门针对工业场景设计。该模型作为通用基础模型，可通过轻量级微调高效适配到专业领域。仅使用少于5%的任务专用数据，它即可达到与专门专家模型相当的性能，凸显了数据高效基础模型适配在工业检测与生成中的潜力，为可扩展、领域自适应和知识驱动的制造智能铺平道路。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.&lt;/p&gt;</description></item><item><guid>2512.24165v1</guid><title>DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models</title><link>http://arxiv.org/abs/2512.24165v1</link><author>Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, Yu Cheng</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种生成式多模态推理范式，并开发了基于扩散模型的推理框架 DiffThinker。DiffThinker 将多模态推理转化为图像到图像的生成任务，在视觉中心任务中实现了更高的逻辑一致性和空间精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的多模态大型语言模型在推理方面取得进展，但其推理过程主要以文本为中心，导致在复杂的长周期、以视觉为主的任务中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索并验证生成式多模态推理的有效性，提出 DiffThinker 并与现有 MLLM 进行系统比较，揭示其核心特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建 DiffThinker 框架，将推理任务视为图像生成任务；对比实验在四个领域（顺序规划、组合优化、约束满足和空间配置）进行评估；对比闭源模型 GPT-5、Gemini-3-Flash 以及微调的 Qwen3-VL-32B。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DiffThinker 在所有实验领域显著优于 GPT-5（+314.2%）、Gemini-3-Flash（+111.6%）和 Qwen3-VL-32B（+39.0%），并展现出效率、可控性、本地并行性和协作性四大核心属性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 生成式多模态推理是解决视觉中心推理任务的有前景方法，DiffThinker 在性能和特性上均优于现有主流模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2%) and Gemini-3-Flash (+111.6%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\%) and Gemini-3-Flash (+111.6\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.&lt;/p&gt;</description></item><item><guid>2512.24172v1</guid><title>Deep Global Clustering for Hyperspectral Image Segmentation: Concepts, Applications, and Open Challenges</title><link>http://arxiv.org/abs/2512.24172v1</link><author>Yu-Tang Chang, Pin-Wei Chen, Shih-Fang Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种名为 Deep Global Clustering (DGC) 的框架，用于在内存受限的环境下对高光谱图像进行分割。该方法通过在小块图像上学习全局聚类结构，避免了预训练和大规模内存占用，能够在消费级硬件上30分钟内完成训练。实验表明，DGC 在叶片病害数据集上实现了背景与组织的高精度分离，并能进行无监督的病害检测，但在多目标损失平衡上存在优化不稳定的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高光谱成像数据量巨大，常超出可用内存，导致计算瓶颈。虽然在大规模遥感数据集上预训练的基础模型表现良好，但其学习到的表示往往无法迁移到如近距离农业监测等领域，因光谱特征、空间尺度和语义目标差异显著。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种无需预训练、内存占用恒定、可在短时间内完成训练的高光谱图像分割方法，以解决传统方法在大数据量下的计算瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DGC 在小块图像上进行训练，块与块之间有重叠区域以保证一致性；通过学习全局聚类结构，避免了对大规模预训练模型的依赖；训练过程在消费级硬件上可在30分钟内完成，内存使用保持恒定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在叶片病害数据集上，DGC 实现了背景与组织的分离，平均交并比达到 0.925；同时展示了通过可导航的语义粒度实现无监督病害检测的能力。然而，框架在多目标损失平衡上出现优化不稳定，导致特征空间中聚类过度合并，影响表示质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DGC 的设计理念具有一定价值，但要实现稳定的实际应用，需要采用更为原则化的动态损失平衡方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 高光谱成像（HSI）分析因数据量巨大而超出可用内存，导致计算瓶颈。虽然在大型遥感数据集上预训练的基础模型显示出前景，但其学习到的表示往往无法迁移到如近距离农业监测等领域，因为光谱特征、空间尺度和语义目标根本不同。本报告提出了 Deep Global Clustering（DGC），一种内存高效的 HSI 分割概念框架，能够从局部块观测中学习全局聚类结构，而不需要预训练。DGC 在小块上操作，并使用重叠区域来强制一致性，使得在消费级硬件上训练时间不足 30 分钟，同时保持恒定的内存使用。在叶片病害数据集上，DGC 实现了背景-组织分离（平均交并比 0.925），并通过可导航的语义粒度展示了无监督病害检测。然而，该框架在多目标损失平衡上存在优化不稳定性：有意义的表示快速出现但随后因特征空间中聚类过度合并而退化。我们将此工作定位为知识架构——设计理念有价值，但稳定实现需要原则化的动态损失平衡方法。代码和数据可在 https://github.com/b05611038/HSI_global_clustering 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Hyperspectral imaging (HSI) analysis faces computational bottlenecks due to massive data volumes that exceed available memory. While foundation models pre-trained on large remote sensing datasets show promise, their learned representations often fail to transfer to domain-specific applications like close-range agricultural monitoring where spectral signatures, spatial scales, and semantic targets differ fundamentally. This report presents Deep Global Clustering (DGC), a conceptual framework for memory-efficient HSI segmentation that learns global clustering structure from local patch observations without pre-training. DGC operates on small patches with overlapping regions to enforce consistency, enabling training in under 30 minutes on consumer hardware while maintaining constant memory usage. On a leaf disease dataset, DGC achieves background-tissue separation (mean IoU 0.925) and demonstrates unsupervised disease detection through navigable semantic granularity. However, the framework suffers from optimization instability rooted in multi-objective loss balancing: meaningful representations emerge rapidly but degrade due to cluster over-merging in feature space. We position this work as intellectual scaffolding - the design philosophy has merit, but stable implementation requires principled approaches to dynamic loss balancing. Code and data are available at https://github.com/b05611038/HSI_global_clustering.&lt;/p&gt;</description></item><item><guid>2512.24193v1</guid><title>PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds</title><link>http://arxiv.org/abs/2512.24193v1</link><author>Pieter M. Blok, Haozhou Wang, Hyun Kwon Suh, Peicheng Wang, James Burridge, Wei Guo</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为 PointRAFT 的高通量点云回归网络，用于直接从不完整的点云中预测土豆块茎的重量，避免了传统三维重建导致的欠估问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 土豆产量是农业优化的重要指标，传统上通过 RGB‑D 摄像机在收获机上捕捉块茎的三维信息来估算产量，但由于自遮挡导致的点云不完整，常常低估重量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在实际收获条件下，利用部分点云直接预测土豆重量的高效方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; PointRAFT 采用点云回归网络，核心创新是加入块茎高度嵌入作为几何线索，直接从原始三维数据推断重量；网络在日本运营收获机上收集的 26,688 条部分点云数据上训练和评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 5,254 条测试点云上，PointRAFT 的平均绝对误差为 12.0 克，均方根误差为 17.2 克，显著优于线性回归和 PointNet++，推理时间仅 6.3 毫秒，可实现每秒 150 条块茎的处理速率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PointRAFT 在高通量土豆重量估计任务中表现优异，且其回归框架可推广到其他 3D 表型和机器人感知应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 土豆产量是农业中优化种植实践的重要指标。利用 RGB‑D 摄像机可以在收获机上估算土豆产量，这些摄像机捕捉到沿输送带移动的单个块茎的三维信息。然而，由于自遮挡，基于 RGB‑D 图像重建的点云是不完整的，导致块茎重量被系统性低估。为了解决这个问题，我们提出了 PointRAFT，一种高通量点云回归网络，能够直接从不完整的点云中预测连续的三维形状属性，例如块茎重量。PointRAFT 并不重建完整的三维几何，而是直接从原始三维数据推断目标值。其关键的架构创新是对象高度嵌入，它将块茎高度作为额外的几何线索，提升了在实际收获条件下的重量预测。PointRAFT 在日本一台运营收获机上收集的 26,688 条部分点云（来自 859 条块茎，涵盖四个品种和三个生长季节）上进行训练和评估。在 5,254 条来自 172 条块茎的测试点云上，PointRAFT 的平均绝对误差为 12.0 克，均方根误差为 17.2 克，显著优于线性回归基线和标准的 PointNet++ 回归网络。平均推理时间为每条点云 6.3 毫秒，支持每秒最多 150 条块茎的处理速率，满足商业土豆收获机的高通量需求。除了土豆重量估计之外，PointRAFT 还提供了一种可应用于广泛 3D 表型和机器人感知任务的通用回归网络。代码、网络权重以及部分数据集已公开发布在 https://github.com/pieterblok/pointraft.git。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Potato yield is a key indicator for optimizing cultivation practices in agriculture. Potato yield can be estimated on harvesters using RGB-D cameras, which capture three-dimensional (3D) information of individual tubers moving along the conveyor belt. However, point clouds reconstructed from RGB-D images are incomplete due to self-occlusion, leading to systematic underestimation of tuber weight. To address this, we introduce PointRAFT, a high-throughput point cloud regression network that directly predicts continuous 3D shape properties, such as tuber weight, from partial point clouds. Rather than reconstructing full 3D geometry, PointRAFT infers target values directly from raw 3D data. Its key architectural novelty is an object height embedding that incorporates tuber height as an additional geometric cue, improving weight prediction under practical harvesting conditions. PointRAFT was trained and evaluated on 26,688 partial point clouds collected from 859 potato tubers across four cultivars and three growing seasons on an operational harvester in Japan. On a test set of 5,254 point clouds from 172 tubers, PointRAFT achieved a mean absolute error of 12.0 g and a root mean squared error of 17.2 g, substantially outperforming a linear regression baseline and a standard PointNet++ regression network. With an average inference time of 6.3 ms per point cloud, PointRAFT supports processing rates of up to 150 tubers per second, meeting the high-throughput requirements of commercial potato harvesters. Beyond potato weight estimation, PointRAFT provides a versatile regression network applicable to a wide range of 3D phenotyping and robotic perception tasks. The code, network weights, and a subset of the dataset are publicly available at https://github.com/pieterblok/pointraft.git.&lt;/p&gt;</description></item><item><guid>2512.24201v1</guid><title>BATISNet: Instance Segmentation of Tooth Point Clouds with Boundary Awareness</title><link>http://arxiv.org/abs/2512.24201v1</link><author>Yating Cai, Yanghui Xu, Zehua Hu, Jiazhou Chen, Jing Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种针对牙齿点云的边界感知实例分割网络 BATISNet，旨在提高牙齿分割的准确性和鲁棒性，尤其在缺牙、牙位异常等复杂临床场景下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 牙齿点云分割对于诊断、临床辅助和治疗规划具有重要意义。现有方法多采用语义分割，关注不同牙齿类型的语义特征，但由于牙齿紧密排列、边界不清晰以及缺牙、牙位异常等复杂情况，语义分割往往难以获得满意结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决牙齿点云分割中因牙齿结构紧密、边界模糊和复杂病例导致的分割困难，提出一种能够同时学习语义特征和实例特征的网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BATISNet 由特征提取骨干网络和实例分割模块组成，能够提取不同牙齿类型的语义特征并学习单颗牙齿的实例特征；同时设计了边界感知损失函数，对实例间的边界进行专门监督，降低牙齿粘连和边界模糊。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，BATISNet 在牙齿完整性分割任务上优于现有方法，能够在缺牙、牙位异常等复杂场景下实现更稳健、更精确的分割，并有效缓解牙齿粘连和边界不清问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BATISNet 为牙齿点云分割提供了更可靠、细致的数据支持，可为临床诊断和治疗规划提供更有价值的参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 牙齿点云的准确分割对于诊断、临床辅助和治疗规划具有重要意义。现有方法大多采用语义分割，侧重于不同牙齿类型之间的语义特征。然而，由于牙齿紧密排列、边界不清晰以及缺牙、牙位异常等复杂病例的多样性，语义分割在处理复杂牙科案例时往往难以获得令人满意的结果。为解决这些问题，本文提出了 BATISNet，一种针对牙齿点云分割的边界感知实例网络。该网络模型由特征提取骨干网络和实例分割模块组成。它不仅关注提取不同牙齿类型的语义特征，还学习单颗牙齿的实例特征，从而在缺牙、牙位异常等复杂临床场景中实现更稳健、更精确的牙齿实例分割。此外，为进一步提升牙齿边界分割的完整性和准确性，本文设计了边界感知损失函数，专门监督实例间的边界分割，能够有效缓解牙齿粘连和边界模糊问题。大量实验结果表明，BATISNet 在牙齿完整性分割任务上优于现有方法，为实际临床应用提供了更可靠、更细致的数据支持。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决三维牙齿点云的实例分割问题，尤其是在缺牙、牙位异常或牙齿紧密排列等复杂临床场景下。准确的牙齿分割对于诊断、治疗规划和手术导航等口腔医学应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了 PointMLP 的点云特征提取、U‑Net 的编码解码结构以及 2D 视觉中的 OneFormer、Mask2Former 的实例分割思路，构建了一个无候选框的实例分割框架，并加入了边界感知损失来解决牙齿边界模糊的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将牙齿分割视为实例分割任务，并通过边界感知损失提升边界精度。实现流程为：输入点云 → PointMLP‑U‑Net 提取局部与全局特征 → 通过可学习查询的 Transformer 解码器生成实例掩码、类别和置信度 → 非极大抑制得到最终实例掩码，并用边界损失进行细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：①无候选框的牙齿实例分割网络 BATISNet；②全局‑局部特征聚合的 PointMLP‑U‑Net；③轻量化的边界感知损失；③端到端训练。与以往依赖语义标签或边框/中心点候选的语义/实例分割方法不同，BATISNet 能直接学习实例掩码并在复杂病例中保持高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BATISNet 提出了一种无候选框、边界感知的实例分割框架，能够在复杂牙齿点云中准确分离每颗牙齿，并在多项实验中优于现有方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate segmentation of the tooth point cloud is of great significance for diagnosis clinical assisting and treatment planning. Existing methods mostly employ semantic segmentation, focusing on the semantic feature between different types of teeth. However, due to the tightly packed structure of teeth, unclear boundaries, and the diversity of complex cases such as missing teeth, malposed teeth, semantic segmentation often struggles to achieve satisfactory results when dealing with complex dental cases. To address these issues, this paper propose BATISNet, a boundary-aware instance network for tooth point cloud segmentation. This network model consists of a feature extraction backbone and an instance segmentation module. It not only focuses on extracting the semantic features of different types of teeth but also learns the instance features of individual teeth. It helps achieve more robust and accurate tooth instance segmentation in complex clinical scenarios such as missing teeth and malposed teeth. Additionally, to further enhance the completeness and accuracy of tooth boundary segmentation, a boundary-aware loss function is designed to specifically supervise the boundary segmentation between instances. It mitigates effectively tooth adhesion and boundary ambiguity issues. Extensive experimental results show that BATISNet outperforms existing methods in tooth integrity segmentation, providing more reliable and detailed data support for practical clinical applications.&lt;/p&gt;</description></item><item><guid>2512.24212v1</guid><title>RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation</title><link>http://arxiv.org/abs/2512.24212v1</link><author>Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 RANGER 框架，利用单目摄像头实现零样本、开放词汇语义导航，克服了传统方法对深度和姿态信息的依赖，并具备强大的上下文学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在复杂环境中高效寻找目标是现实世界机器人应用的核心需求。多模态基础模型的进步使得零样本目标导航成为可能，但现有方法仍受限于对精确深度和姿态的依赖以及缺乏上下文学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决传统零样本导航方法对深度/姿态的高度依赖以及缺乏快速适应新环境的上下文学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; RANGER 通过关键帧三维重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应航路点选择以及低层动作执行，实现仅用单目摄像头完成目标导航，并通过观看短视频快速适应新环境。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 HM3D 基准和真实环境实验中，RANGER 在导航成功率和探索效率方面与现有方法竞争，并在无需预先 3D 地图的情况下表现出更优的上下文学习适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; RANGER 展示了在不依赖深度和姿态信息的情况下，利用单目摄像头实现零样本、开放词汇导航的可行性，并通过短视频实现快速环境适应，具有显著的实用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要：高效地在复杂环境中寻找目标是现实世界具身应用的基础。最近多模态基础模型的进展使得零样本目标导航成为可能，允许机器人在无需微调的情况下搜索任意对象。然而现有方法存在两个主要限制：（1）过度依赖模拟器提供的精确深度和姿态信息，限制了其在真实世界中的适用性；（2）缺乏上下文学习（ICL）能力，使得难以快速适应新环境，例如利用短视频。为了解决这些挑战，我们提出了 RANGER，一种新颖的零样本、开放词汇语义导航框架，仅使用单目摄像头即可运行。借助强大的三维基础模型，RANGER 消除了对深度和姿态的依赖，并展现出强大的上下文学习能力。仅通过观察新环境的短视频，系统即可显著提升任务效率，而无需架构修改或微调。该框架集成了关键帧三维重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应航路点选择以及低层动作执行等关键组件。在 HM3D 基准和真实环境实验中，RANGER 在导航成功率和探索效率方面表现出竞争力，同时显示出优越的上下文学习适应性，且不需要先前的 3D 环境映射。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在实现仅使用单目 RGB 摄像头的零样本语义导航，消除对深度传感器和精确位姿的依赖，并通过短视频实现快速适应。此问题在实际机器人部署中至关重要，因为它降低了硬件成本、提升了部署灵活性，并能在未知环境中高效定位目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者结合了多模态基础模型、VLM、LLM 以及现有的单目 3D 重建技术（如 MASt3R-SLAM、Grounding DINO、Mobile SAM、CLIP），并在此基础上构建了统一的关键帧记忆库。设计思路借鉴了前沿的零样本导航、语义地图构建和上下文学习方法，但将它们整合为一个无需微调、可实时运行的系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过关键帧记忆库同时存储几何、语义和探索价值信息，利用单目 RGB 实时重建、语义点云融合、2D 地图投影和 VLM 驱动的高层规划来实现导航。实现流程包括：接收 RGB 观测 → MASt3R-SLAM 进行 3D 重建与位姿估计 → Grounding DINO+Mobile SAM+CLIP 生成语义点云 → 投影为 2D 障碍/前沿/价值地图 → 高层规划器选取 waypoint → 低层控制器执行离散动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1) 仅使用 RGB 进行零样本语义导航；2) 通过短视频实现无微调的上下文学习；3) 关键帧记忆库整合几何、语义与价值；4) VLM 驱动的探索价值评估；5) 兼顾在线重建与离线视频适应。与以往依赖深度、精确位姿或全局地图、需要微调的零样本方法不同，RANGER 在硬件成本、适应性和实时性上实现了显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RANGER 通过单目 RGB、关键帧记忆库和 VLM 驱动的规划，实现了无需深度传感器、位姿或微调的零样本语义导航，并能通过短视频快速适应新环境。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.&lt;/p&gt;</description></item><item><guid>2512.24224v1</guid><title>ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation</title><link>http://arxiv.org/abs/2512.24224v1</link><author>Ziquan Liu, Zhewei Zhu, Xuyang Shi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文指出开放词汇语义分割受限于CLIP的粗粒度图像级表示，提出轻量化可学习的Attention Refinement Module（ARM）来解锁并细化CLIP内部特征，ARM通过自适应融合层次特征并采用语义引导的交叉注意力与自注意力实现细节增强；在一次性训练后可作为通用后处理器应用于多种无训练框架，实验表明ARM在多项基准上显著提升性能且推理开销极小。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 开放词汇语义分割（OVSS）受CLIP粗粒度、缺乏像素级细节的图像级表示限制；现有无训练方法要么依赖昂贵的外部基础模型，要么使用静态手工启发式，导致计算成本高或效果不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种轻量化、可学习的模块，利用CLIP内部潜能实现细粒度特征提升，从而在无训练框架下提升OVSS性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计Attention Refinement Module（ARM），通过语义引导的交叉注意力块使用深层特征选择并细化浅层细节特征，随后通过自注意力块进一步融合；ARM一次性在通用数据集上训练后可作为通用后处理器使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ARM在多项基准上持续提升基线性能，且推理开销极小，验证了其高效性与通用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ARM为训练-free OVSS提供了一种高效、有效的范式，展示了利用可学习模块解锁CLIP内部潜能的可行性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 开放词汇语义分割（OVSS）本质上受到CLIP粗粒度、缺乏像素级细节的图像级表示的限制。现有的无训练方法试图通过引入昂贵的外部基础模型（如SAM、DINO）或对CLIP内部特征应用静态、手工制定的启发式来解决这一问题，但这些方法要么计算成本高昂，要么效果不佳。我们提出了Attention Refinement Module（ARM），这是一种轻量化、可学习的模块，能够有效解锁并细化CLIP内部潜能。与静态融合方法不同，ARM学习自适应融合层次特征。它采用语义引导的交叉注意力块，使用鲁棒的深层特征（K、V）来选择并细化细节丰富的浅层特征（Q），随后再经过自注意力块。其关键创新在于“先训练一次，随处使用”的范式。ARM在一次性训练后（例如在COCO-Stuff等通用数据集上）可作为通用的即插即用后处理器，适用于多种无训练框架。大量实验表明，ARM在多个基准上持续提升基线性能，且推理开销极小，确立了一种高效且有效的无训练OVSS范式。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) is fundamentally hampered by the coarse, image-level representations of CLIP, which lack precise pixel-level details. Existing training-free methods attempt to resolve this by either importing priors from costly external foundation models (e.g., SAM, DINO) or by applying static, hand-crafted heuristics to CLIP&amp;#x27;s internal features. These approaches are either computationally expensive or sub-optimal. We propose the Attention Refinement Module (ARM), a lightweight, learnable module that effectively unlocks and refines CLIP&amp;#x27;s internal potential. Unlike static-fusion methods, ARM learns to adaptively fuse hierarchical features. It employs a semantically-guided cross-attention block, using robust deep features (K, V) to select and refine detail-rich shallow features (Q), followed by a self-attention block. The key innovation lies in a ``train once, use anywhere&amp;quot; paradigm. Trained once on a general-purpose dataset (e.g., COCO-Stuff), ARM acts as a universal plug-and-play post-processor for diverse training-free frameworks. Extensive experiments show that ARM consistently boosts baseline performance on multiple benchmarks with negligible inference overhead, establishing an efficient and effective paradigm for training-free OVSS.&lt;/p&gt;</description></item><item><guid>2512.24231v1</guid><title>MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model</title><link>http://arxiv.org/abs/2512.24231v1</link><author>Rahul Medicharla, Alper Yilmaz</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出MotivNet，一种基于Meta-Sapiens的通用面部情绪识别模型，能够在不进行跨域训练的情况下在多数据集上保持竞争性能，验证其作为Sapiens下游任务的可行性，并为真实世界应用提供更具吸引力的FER方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有最先进的FER模型在多样化数据上泛化能力弱，导致真实世界表现下降，限制了FER研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在不同域数据上保持良好泛化的面部情绪识别模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用Meta-Sapiens作为骨干网络，构建MotivNet作为下游任务，定义基准性能、模型相似度和数据相似度三项评估标准，并在不进行跨域训练的情况下进行训练和评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MotivNet在多数据集上实现了与现有SOTA模型相当的性能，满足三项评估标准，证明其在不同域上的可泛化性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MotivNet作为Sapiens的下游任务是可行的，能够提升FER在真实世界中的应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文介绍了MotivNet，一种通用的面部情绪识别模型，旨在实现稳健的真实世界应用。当前最先进的FER模型在多样化数据上测试时往往泛化能力弱，导致真实世界表现下降，阻碍了FER作为研究领域的发展。尽管研究人员提出了复杂的架构来解决泛化问题，但它们需要跨域训练才能获得可泛化的结果，这与真实世界应用本质上相矛盾。我们的模型MotivNet通过使用Meta-Sapiens作为骨干，在不进行跨域训练的情况下实现了在不同数据集上的竞争性能。Sapiens是一种人类视觉基础模型，通过大规模预训练的Masked Autoencoder实现了在真实世界中的最先进泛化能力。我们将MotivNet作为Sapiens的额外下游任务，并定义了三个标准来评估MotivNet作为Sapiens任务的可行性：基准性能、模型相似度和数据相似度。本文描述了MotivNet的组件、我们的训练方法以及展示MotivNet在跨域上具有可泛化性的结果。我们证明MotivNet可以与现有SOTA模型进行基准比较，并满足列出的标准，验证MotivNet作为Sapiens下游任务的可行性，并使FER在野外应用中更具吸引力。代码可在 https://github.com/OSUPCVLab/EmotionFromFaceImages 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this paper, we introduce MotivNet, a generalizable facial emotion recognition model for robust real-world application. Current state-of-the-art FER models tend to have weak generalization when tested on diverse data, leading to deteriorated performance in the real world and hindering FER as a research domain. Though researchers have proposed complex architectures to address this generalization issue, they require training cross-domain to obtain generalizable results, which is inherently contradictory for real-world application. Our model, MotivNet, achieves competitive performance across datasets without cross-domain training by using Meta-Sapiens as a backbone. Sapiens is a human vision foundational model with state-of-the-art generalization in the real world through large-scale pretraining of a Masked Autoencoder. We propose MotivNet as an additional downstream task for Sapiens and define three criteria to evaluate MotivNet&amp;#x27;s viability as a Sapiens task: benchmark performance, model similarity, and data similarity. Throughout this paper, we describe the components of MotivNet, our training approach, and our results showing MotivNet is generalizable across domains. We demonstrate that MotivNet can be benchmarked against existing SOTA models and meets the listed criteria, validating MotivNet as a Sapiens downstream task, and making FER more incentivizing for in-the-wild application. The code is available at https://github.com/OSUPCVLab/EmotionFromFaceImages.&lt;/p&gt;</description></item><item><guid>2512.24253v1</guid><title>Early Prediction of Sepsis using Heart Rate Signals and Genetic Optimized LSTM Algorithm</title><link>http://arxiv.org/abs/2512.24253v1</link><author>Alireza Rafiei, Farshid Hajati, Alireza Rezaee, Amirhossien Panahi, Shahadat Uddin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究开发并评估了四种新型机器学习算法，利用可穿戴设备心率数据预测败血症的发生，旨在实现ICU和普通病房以外环境的早期检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 败血症是一种免疫失调导致的严重感染，早期预测其进展对降低死亡率和医疗成本至关重要，但现有模型多针对ICU患者，缺乏针对非病房环境的早期检测方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出并验证可在可穿戴设备上实现的心率数据预测败血症的算法，以填补现有研究的空白。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 设计四种机器学习模型，并通过遗传算法优化其性能、计算复杂度和内存占用；提取各模型的性能指标；先在一小时预测窗口内评估，再通过迁移学习扩展到四小时窗口。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 四种模型在一小时预测窗口内表现良好，迁移学习后在四小时窗口内仍保持可接受的性能，表明可穿戴技术有潜力在ICU和病房外实现早期败血症检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可穿戴设备结合心率监测和优化的机器学习模型，可为非ICU环境提供早期败血症检测的可行方案，具有临床应用前景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Sepsis, characterized by a dysregulated immune response to infection, results in significant mortality, morbidity, and healthcare costs. The timely prediction of sepsis progression is crucial for reducing adverse outcomes through early intervention. Despite the development of numerous models for Intensive Care Unit (ICU) patients, there remains a notable gap in approaches for the early detection of sepsis in non-ward settings. This research introduces and evaluates four novel machine learning algorithms designed for predicting the onset of sepsis on wearable devices by analyzing heart rate data. The architecture of these models was refined through a genetic algorithm, optimizing for performance, computational complexity, and memory requirements. Performance metrics were subsequently extracted for each model to evaluate their feasibility for implementation on wearable devices capable of accurate heart rate monitoring. The models were initially tailored for a prediction window of one hour, later extended to four hours through transfer learning. The encouraging outcomes of this study suggest the potential for wearable technology to facilitate early sepsis detection outside ICU and ward environments.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Sepsis, characterized by a dysregulated immune response to infection, results in significant mortality, morbidity, and healthcare costs. The timely prediction of sepsis progression is crucial for reducing adverse outcomes through early intervention. Despite the development of numerous models for Intensive Care Unit (ICU) patients, there remains a notable gap in approaches for the early detection of sepsis in non-ward settings. This research introduces and evaluates four novel machine learning algorithms designed for predicting the onset of sepsis on wearable devices by analyzing heart rate data. The architecture of these models was refined through a genetic algorithm, optimizing for performance, computational complexity, and memory requirements. Performance metrics were subsequently extracted for each model to evaluate their feasibility for implementation on wearable devices capable of accurate heart rate monitoring. The models were initially tailored for a prediction window of one hour, later extended to four hours through transfer learning. The encouraging outcomes of this study suggest the potential for wearable technology to facilitate early sepsis detection outside ICU and ward environments.&lt;/p&gt;</description></item><item><guid>2512.24260v1</guid><title>Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT</title><link>http://arxiv.org/abs/2512.24260v1</link><author>Zhi Li, Yaqi Wang, Bingtao Ma, Yifan Zhang, Huiyu Zhou, Shuai Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 金属伪影在牙科CBCT中严重遮挡解剖结构，影响诊断。当前深度学习在金属伪影减少（MAR）方面存在局限：监督方法因回归到均值导致光谱模糊，而无监督方法存在结构幻觉风险。去噪扩散模型（DDPMs）虽能提供逼真效果，但依赖慢速、随机迭代采样，不适合临床使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 金属伪影在牙科CBCT中严重遮挡解剖结构，影响诊断。当前深度学习在金属伪影减少（MAR）方面存在局限：监督方法因回归到均值导致光谱模糊，而无监督方法存在结构幻觉风险。去噪扩散模型（DDPMs）虽能提供逼真效果，但依赖慢速、随机迭代采样，不适合临床使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出Physically-Grounded Manifold Projection (PGMP)框架，解决金属伪影减少问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先，Anatomically-Adaptive Physics Simulation (AAPS)管道通过蒙特卡洛光谱建模和患者特定数字孪生合成高保真训练对，弥合合成与真实之间的差距。其次，DMP-Former适应直接x预测范式，将修复重新定义为确定性流形投影，以单次前向传递恢复干净解剖结构，消除随机采样。最后，语义结构对齐（SSA）模块利用医学基础模型（MedDINOv3）的先验知识锚定解决方案，确保临床合理性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验在合成和多中心临床数据集上显示，PGMP在未见解剖结构上优于最先进方法，在效率和诊断可靠性方面设定新基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Physically-Grounded Manifold Projection (PGMP)框架有效解决了金属伪影减少问题，提高了牙科CBCT的诊断可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Metal artifacts in Dental CBCT severely obscure anatomical structures, hindering diagnosis. Current deep learning for Metal Artifact Reduction (MAR) faces limitations: supervised methods suffer from spectral blurring due to &amp;#x27;regression-to-the-mean&amp;#x27;, while unsupervised ones risk structural hallucinations. Denoising Diffusion Models (DDPMs) offer realism but rely on slow, stochastic iterative sampling, unsuitable for clinical use. To resolve this, we propose the Physically-Grounded Manifold Projection (PGMP) framework. First, our Anatomically-Adaptive Physics Simulation (AAPS) pipeline synthesizes high-fidelity training pairs via Monte Carlo spectral modeling and patient-specific digital twins, bridging the synthetic-to-real gap. Second, our DMP-Former adapts the Direct x-Prediction paradigm, reformulating restoration as a deterministic manifold projection to recover clean anatomy in a single forward pass, eliminating stochastic sampling. Finally, a Semantic-Structural Alignment (SSA) module anchors the solution using priors from medical foundation models (MedDINOv3), ensuring clinical plausibility. Experiments on synthetic and multi-center clinical datasets show PGMP outperforms state-of-the-art methods on unseen anatomy, setting new benchmarks in efficiency and diagnostic reliability. Code and data: https://github.com/ricoleehduu/PGMP&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Metal artifacts in Dental CBCT severely obscure anatomical structures, hindering diagnosis. Current deep learning for Metal Artifact Reduction (MAR) faces limitations: supervised methods suffer from spectral blurring due to &amp;quot;regression-to-the-mean&amp;quot;, while unsupervised ones risk structural hallucinations. Denoising Diffusion Models (DDPMs) offer realism but rely on slow, stochastic iterative sampling, unsuitable for clinical use. To resolve this, we propose the Physically-Grounded Manifold Projection (PGMP) framework. First, our Anatomically-Adaptive Physics Simulation (AAPS) pipeline synthesizes high-fidelity training pairs via Monte Carlo spectral modeling and patient-specific digital twins, bridging the synthetic-to-real gap. Second, our DMP-Former adapts the Direct x-Prediction paradigm, reformulating restoration as a deterministic manifold projection to recover clean anatomy in a single forward pass, eliminating stochastic sampling. Finally, a Semantic-Structural Alignment (SSA) module anchors the solution using priors from medical foundation models (MedDINOv3), ensuring clinical plausibility. Experiments on synthetic and multi-center clinical datasets show PGMP outperforms state-of-the-art methods on unseen anatomy, setting new benchmarks in efficiency and diagnostic reliability. Code and data: https://github.com/ricoleehduu/PGMP&lt;/p&gt;</description></item><item><guid>2512.24260v2</guid><title>Physically-Grounded Manifold Projection Model for Generalizable Metal Artifact Reduction in Dental CBCT</title><link>http://arxiv.org/abs/2512.24260v2</link><author>Zhi Li, Yaqi Wang, Bingtao Ma, Yifan Zhang, Huiyu Zhou, Shuai Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于物理驱动的流形投影框架PGMP，用于牙科CBCT金属伪影去除，结合高保真模拟、确定性投影和语义结构对齐，显著提升去伪影效果与临床可用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 牙科CBCT中的金属伪影严重遮蔽解剖结构，影响诊断；现有深度学习方法要么因监督训练导致光谱模糊，要么因无监督训练产生结构幻觉；扩散模型虽真实但采样慢。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决金属伪影去除中速度慢、结构失真和真实性不足的问题，提出一种高效、可靠的去伪影方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) Anatomically-Adaptive Physics Simulation (AAPS) 通过蒙特卡罗光谱建模和患者特定数字孪生生成高质量训练对；2) DMP-Former 将直接预测改为确定性流形投影，一次前向推理即可恢复干净解剖；3) Semantic-Structural Alignment (SSA) 模块利用医学基础模型 MedDINOv3 的先验保证结果临床可行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成和多中心临床数据上，PGMP 在未见解剖结构上优于现有最先进方法，且在效率和诊断可靠性上设立新基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PGMP 通过物理模拟、确定性投影和语义对齐，实现了快速、准确且临床可用的金属伪影去除，为牙科CBCT诊断提供了可靠工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 金属伪影在牙科CBCT中严重遮蔽解剖结构，阻碍诊断。当前用于金属伪影去除的深度学习方法存在局限：监督方法因“回归到均值”导致光谱模糊，而无监督方法则可能产生结构幻觉。去噪扩散模型（DDPM）提供逼真效果，但依赖慢速、随机迭代采样，不适合临床使用。为解决此问题，我们提出了物理驱动的流形投影（PGMP）框架。首先，我们的解剖适应物理模拟（AAPS）管道通过蒙特卡罗光谱建模和患者特定数字孪生合成高保真训练对，弥合了合成与真实之间的差距。其次，我们的 DMP-Former 采用直接 x 预测范式，将恢复改写为确定性流形投影，在单次前向传播中恢复干净解剖，消除了随机采样。最后，语义结构对齐（SSA）模块利用医学基础模型 MedDINOv3 的先验将解答锚定，确保临床可行性。对合成和多中心临床数据集的实验表明，PGMP 在未见解剖结构上优于最先进方法，在效率和诊断可靠性方面设立了新基准。代码和数据可在 https://github.com/ricoleehduu/PGMP 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Metal artifacts in Dental CBCT severely obscure anatomical structures, hindering diagnosis. Current deep learning for Metal Artifact Reduction (MAR) faces limitations: supervised methods suffer from spectral blurring due to &amp;quot;regression-to-the-mean&amp;quot;, while unsupervised ones risk structural hallucinations. Denoising Diffusion Models (DDPMs) offer realism but rely on slow, stochastic iterative sampling, unsuitable for clinical use. To resolve this, we propose the Physically-Grounded Manifold Projection (PGMP) framework. First, our Anatomically-Adaptive Physics Simulation (AAPS) pipeline synthesizes high-fidelity training pairs via Monte Carlo spectral modeling and patient-specific digital twins, bridging the synthetic-to-real gap. Second, our DMP-Former adapts the Direct x-Prediction paradigm, reformulating restoration as a deterministic manifold projection to recover clean anatomy in a single forward pass, eliminating stochastic sampling. Finally, a Semantic-Structural Alignment (SSA) module anchors the solution using priors from medical foundation models (MedDINOv3), ensuring clinical plausibility. Experiments on synthetic and multi-center clinical datasets show PGMP outperforms state-of-the-art methods on unseen anatomy, setting new benchmarks in efficiency and diagnostic reliability. Code and data: https://github.com/ricoleehduu/PGMP.&lt;/p&gt;</description></item><item><guid>2512.24271v1</guid><title>Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation</title><link>http://arxiv.org/abs/2512.24271v1</link><author>Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展，但它们存在一个关键漏洞：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。由于文本和视频之间固有的数据不平衡，这一限制很难解决，因为收集和标注反事实数据的成本很高。为了解决这个问题，我们引入了DualityForge，这是一个新颖的反事实数据合成框架，它采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，我们构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，为了充分利用我们配对数据的对比性质，我们提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。我们将开源我们的数据集和代码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展，但它们存在一个关键漏洞：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决多模态大语言模型在处理反事实视频时存在的视觉无根据的幻觉问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入DualityForge，这是一个新颖的反事实数据合成框架，采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过引入DualityForge和DualityVidQA数据集，以及Duality-Normalized Advantage Training (DNA-Train)训练机制，我们有效地减少了多模态大语言模型在处理反事实视频时的幻觉问题，并展示了强大的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态大语言模型在视频理解方面取得了显著进展。然而，它们存在一个关键问题：过度依赖语言先验，这会导致视觉无根据的幻觉，尤其是在处理违背常识的反事实视频时。由于文本和视频之间固有的数据不平衡，这一限制很难解决，因为收集和标注反事实数据的成本很高。为了解决这个问题，我们引入了DualityForge，这是一个新颖的反事实数据合成框架，它采用可控的、基于扩散的视频编辑将真实世界视频转换为反事实场景。通过将结构化的上下文信息嵌入到视频编辑和问答生成过程中，该框架自动生成高质量的问答对以及原始编辑视频对，用于对比训练。基于此，我们构建了DualityVidQA，一个大规模视频数据集，旨在减少多模态大语言模型的幻觉。此外，为了充分利用我们配对数据的对比性质，我们提出了Duality-Normalized Advantage Training (DNA-Train)，这是一种两阶段的SFT-RL训练机制，其中RL阶段应用成对的一范数优势归一化，从而实现更稳定和高效的策略优化。在DualityVidQA-Test上的实验表明，我们的方法显著减少了模型在反事实视频上的幻觉，相对于Qwen2.5-VL-7B基线，相对改进了24.0%。此外，我们的方法在幻觉和通用基准测试中都取得了显著增益，表明具有很强的泛化能力。我们将开源我们的数据集和代码。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型（MLLMs）在视频理解中存在的过度依赖语言先验的问题，导致在处理反事实视频时出现视觉无根据的幻觉。这个问题在现实研究中很重要，因为MLLMs在视频理解中的应用越来越广泛，但幻觉问题限制了它们的准确性和可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，如利用AI生成内容（AIGC）和视觉强化学习，设计出了一种新的数据合成框架DualityForge，通过可控的视频编辑将真实视频转换为反事实场景，并构建了DualityVidQA数据集。作者也借鉴了现有的视频理解数据集和视觉强化学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过嵌入结构化上下文信息到视频编辑和问答生成过程中，自动产生高质量的反事实视频和问答对，从而提高MLLMs的视频理解能力。整体实现流程包括使用DualityForge框架生成反事实视频，然后使用这些视频和原始视频构建对比性问答对，最后通过Duality-Normalized Advantage Training（DNA-Train）方法进行训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括DualityForge框架，它利用可控的视频编辑生成反事实场景，并嵌入结构化上下文信息；DualityVidQA数据集，它包含大量反事实视频和问答对；以及DNA-Train训练方法，它通过对比性问答训练提高MLLMs的视频理解能力。相比之前的工作，这些创新点提供了更系统、更自动化的方法来减少MLLMs的幻觉问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个通过反事实视频生成和对比性问答训练来提高MLLMs视频理解能力的新方法，有效减少了模型在反事实视频上的幻觉问题。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.&lt;/p&gt;</description></item><item><guid>2512.24294v1</guid><title>Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction</title><link>http://arxiv.org/abs/2512.24294v1</link><author>Md. Enamul Hoq, Linda Larson-Prior, Fred Prior</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Virtual-Eyes 是一个用于低剂量CT肺癌筛查的临床动机的16位CT质量控制管道，它对通用基础模型和专业模型的影响进行了测量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在深度学习管道中，鲁棒的预处理在低剂量CT肺癌筛查中很少被量化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发和验证Virtual-Eyes，并测量其对通用基础模型和专业模型的不同影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Virtual-Eyes执行严格的512x512平面分辨率，拒绝短或非诊断系列，并使用Hounsfield单位过滤和双侧肺覆盖评分提取连续的肺块，同时保留原始的16位网格。使用765名NLST患者（182名癌症，583名非癌症）计算切片级嵌入，冻结编码器并训练无泄漏的患者级MLP头；还评估了Sybil和2D ResNet-18基线在原始和Virtual-Eyes输入下，无需骨干重训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Virtual-Eyes提高了RAD-DINO切片级AUC从0.576到0.610，患者级AUC从0.646到0.683（平均池化）和从0.619到0.735（最大池化），并改善了校准（Brier分数从0.188到0.112）。相比之下，Sybil和ResNet-18在Virtual-Eyes下退化（Sybil AUC从0.886到0.837；ResNet-18 AUC从0.571到0.596），有证据表明上下文依赖性和捷径学习，Merlin显示出有限的迁移性（AUC大约从0.507到0.567），无论预处理如何。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 解剖学目标的质量控制可以稳定并改善通用基础模型的工作流程，但可能会破坏适应原始临床环境的专业模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Virtual-Eyes是一个用于低剂量CT肺癌筛查的临床动机的16位CT质量控制管道，它对通用基础模型和专业模型的影响进行了测量。在深度学习管道中，鲁棒的预处理在低剂量CT肺癌筛查中很少被量化。开发和验证Virtual-Eyes，并测量其对通用基础模型和专业模型的不同影响。Virtual-Eyes执行严格的512x512平面分辨率，拒绝短或非诊断系列，并使用Hounsfield单位过滤和双侧肺覆盖评分提取连续的肺块，同时保留原始的16位网格。使用765名NLST患者（182名癌症，583名非癌症）计算切片级嵌入，冻结编码器并训练无泄漏的患者级MLP头；还评估了Sybil和2D ResNet-18基线在原始和Virtual-Eyes输入下，无需骨干重训练。Virtual-Eyes提高了RAD-DINO切片级AUC从0.576到0.610，患者级AUC从0.646到0.683（平均池化）和从0.619到0.735（最大池化），并改善了校准（Brier分数从0.188到0.112）。相比之下，Sybil和ResNet-18在Virtual-Eyes下退化（Sybil AUC从0.886到0.837；ResNet-18 AUC从0.571到0.596），有证据表明上下文依赖性和捷径学习，Merlin显示出有限的迁移性（AUC大约从0.507到0.567），无论预处理如何。解剖学目标的质量控制可以稳定并改善通用基础模型的工作流程，但可能会破坏适应原始临床环境的专业模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Robust preprocessing is rarely quantified in deep-learning pipelines for low-dose CT (LDCT) lung cancer screening. We develop and validate Virtual-Eyes, a clinically motivated 16-bit CT quality-control pipeline, and measure its differential impact on generalist foundation models versus specialist models. Virtual-Eyes enforces strict 512x512 in-plane resolution, rejects short or non-diagnostic series, and extracts a contiguous lung block using Hounsfield-unit filtering and bilateral lung-coverage scoring while preserving the native 16-bit grid. Using 765 NLST patients (182 cancer, 583 non-cancer), we compute slice-level embeddings from RAD-DINO and Merlin with frozen encoders and train leakage-free patient-level MLP heads; we also evaluate Sybil and a 2D ResNet-18 baseline under Raw versus Virtual-Eyes inputs without backbone retraining. Virtual-Eyes improves RAD-DINO slice-level AUC from 0.576 to 0.610 and patient-level AUC from 0.646 to 0.683 (mean pooling) and from 0.619 to 0.735 (max pooling), with improved calibration (Brier score 0.188 to 0.112). In contrast, Sybil and ResNet-18 degrade under Virtual-Eyes (Sybil AUC 0.886 to 0.837; ResNet-18 AUC 0.571 to 0.596) with evidence of context dependence and shortcut learning, and Merlin shows limited transferability (AUC approximately 0.507 to 0.567) regardless of preprocessing. These results demonstrate that anatomically targeted QC can stabilize and improve generalist foundation-model workflows but may disrupt specialist models adapted to raw clinical context.&lt;/p&gt;</description></item><item><guid>2512.24323v1</guid><title>Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention</title><link>http://arxiv.org/abs/2512.24323v1</link><author>Haijing Liu, Zhiyuan Song, Hefeng Wu, Tao Pu, Keze Wang, Liang Lin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Ego-RVOS旨在从第一人称视频中根据语言查询分割出积极参与人类动作的特定对象，这对理解人类行为至关重要。然而，由于第一人称视频中的模糊性和训练数据中的偏差，实现鲁棒的分割具有挑战性。现有的方法常常从数据集中学习虚假的相关性，并受到第一人称视角的基本视觉混淆因素的影响，如快速运动和频繁的遮挡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Ego-RVOS任务对于理解人类行为非常重要，但由于第一人称视频的模糊性和训练数据的偏差，实现鲁棒的分割具有挑战性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决这些限制，引入了Causal Ego-REferring Segmentation (CERES)，这是一个插件式因果框架，将强预训练的RVOS骨干网络适应到第一人称领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CERES实现了双模态因果干预：应用后门调整原则来抵消从数据集统计中学习到的语言表示偏差，并利用前门调整概念来解决视觉混淆，通过智能地整合语义视觉特征与几何深度信息，创建对第一人称扭曲更鲁棒的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CERES在Ego-RVOS基准上实现了最先进的性能，突出了应用因果推理构建更可靠模型以实现更广泛的第一人称视频理解的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CERES通过因果推理方法提高了第一人称视频中对象分割的鲁棒性，展示了其在第一人称视频理解中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Ego-RVOS旨在从第一人称视频中根据语言查询分割出积极参与人类动作的特定对象，这对理解人类行为至关重要。然而，由于第一人称视频中的模糊性和训练数据中的偏差，实现鲁棒的分割具有挑战性。现有的方法常常从数据集中学习虚假的相关性，并受到第一人称视角的基本视觉混淆因素的影响，如快速运动和频繁的遮挡。为了解决这些限制，引入了Causal Ego-REferring Segmentation (CERES)，这是一个插件式因果框架，将强预训练的RVOS骨干网络适应到第一人称领域。CERES实现了双模态因果干预：应用后门调整原则来抵消从数据集统计中学习到的语言表示偏差，并利用前门调整概念来解决视觉混淆，通过智能地整合语义视觉特征与几何深度信息，创建对第一人称扭曲更鲁棒的表示。CERES在Ego-RVOS基准上实现了最先进的性能，突出了应用因果推理构建更可靠模型以实现更广泛的第一人称视频理解的潜力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是在第一人称视频中，根据语言查询分割出特定参与人类动作的对象的问题。这个问题在现实或研究中非常重要，因为它有助于机器更深入地理解人类行为和交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的不足，特别是数据集偏差和视觉混淆因素，设计了CERES框架。这个方法借鉴了因果推理的原则，并参考了现有的RVOS和因果推理相关工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是使用双模态因果干预来减少语言和视觉偏差。整体实现流程包括使用后门调整来处理语言偏差，使用前门调整和视觉-深度中介来处理视觉偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用因果推理原则来解决Ego-RVOS中的鲁棒性问题，使用后门调整来减少语言偏差，以及使用前门调整和视觉-深度中介来减少视觉偏差。与之前的工作相比，这种方法更全面地考虑了语言和视觉偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于因果推理的Ego-RVOS框架，通过双模态因果干预提高了模型的鲁棒性和泛化能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding.&lt;/p&gt;</description></item><item><guid>2512.24324v1</guid><title>Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction</title><link>http://arxiv.org/abs/2512.24324v1</link><author>Haojin Li, Anbang Zhang, Chen Sun, Chenyuan Feng, Kaiqian Qu, Tony Q. S. Quek, Haijun Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 低空经济（LAE）正在迅速发展，城市空中交通、物流无人机和空中传感技术的需求推动了这一趋势。在无人机通信中，快速准确的波束预测对于实现可靠连接至关重要。当前研究正从单一信号方法转向多模态协作方法。然而，现有的多模态方法大多采用固定或经验权重，假设在任何给定时刻各模态的可靠性相同。实际上，不同模态的重要性随着无人机运动场景的变化而波动剧烈，静态加权会放大退化模态的负面影响。此外，模态不匹配和弱对齐进一步削弱了跨场景泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 低空经济（LAE）的快速发展对无人机通信提出了高要求，特别是在波束预测方面。现有研究多采用固定或经验权重的方法，但这种方法无法适应不同场景下模态可靠性的变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种可靠性感知的动态加权方案，应用于语义感知的多模态波束预测框架SaM2B，以提高波束预测的准确性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; SaM2B利用轻量级线索（如环境视觉、飞行姿态和地理空间数据）通过可靠性感知的动态权重更新，在不同时间点自适应分配各模态的贡献。此外，通过跨模态对比学习，将与特定波束信息相关的多源表示波束语义对齐到一个共享语义空间，从而增强在模态噪声和分布变化下的判别能力和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，SaM2B在真实世界低空无人机数据集上取得了比基线方法更满意的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SaM2B通过动态加权和多模态对比学习，有效提高了波束预测的准确性和鲁棒性，适用于低空经济中的无人机通信。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; The low-altitude economy (LAE) is rapidly expanding driven by urban air mobility, logistics drones, and aerial sensing, while fast and accurate beam prediction in uncrewed aerial vehicles (UAVs) communications is crucial for achieving reliable connectivity. Current research is shifting from single-signal to multi-modal collaborative approaches. However, existing multi-modal methods mostly employ fixed or empirical weights, assuming equal reliability across modalities at any given moment. Indeed, the importance of different modalities fluctuates dramatically with UAV motion scenarios, and static weighting amplifies the negative impact of degraded modalities. Furthermore, modal mismatch and weak alignment further undermine cross-scenario generalization. To this end, we propose a reliability-aware dynamic weighting scheme applied to a semantic-aware multi-modal beam prediction framework, named SaM2B. Specifically, SaM2B leverages lightweight cues such as environmental visual, flight posture, and geospatial data to adaptively allocate contributions across modalities at different time points through reliability-aware dynamic weight updates. Moreover, by utilizing cross-modal contrastive learning, we align the &amp;#x27;multi-source representation beam semantics&amp;#x27; associated with specific beam information to a shared semantic space, thereby enhancing discriminative power and robustness under modal noise and distribution shifts. Experiments on real-world low-altitude UAV datasets show that SaM2B achieves more satisfactory results than baseline methods.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The low-altitude economy (LAE) is rapidly expanding driven by urban air mobility, logistics drones, and aerial sensing, while fast and accurate beam prediction in uncrewed aerial vehicles (UAVs) communications is crucial for achieving reliable connectivity. Current research is shifting from single-signal to multi-modal collaborative approaches. However, existing multi-modal methods mostly employ fixed or empirical weights, assuming equal reliability across modalities at any given moment. Indeed, the importance of different modalities fluctuates dramatically with UAV motion scenarios, and static weighting amplifies the negative impact of degraded modalities. Furthermore, modal mismatch and weak alignment further undermine cross-scenario generalization. To this end, we propose a reliability-aware dynamic weighting scheme applied to a semantic-aware multi-modal beam prediction framework, named SaM2B. Specifically, SaM2B leverages lightweight cues such as environmental visual, flight posture, and geospatial data to adaptively allocate contributions across modalities at different time points through reliability-aware dynamic weight updates. Moreover, by utilizing cross-modal contrastive learning, we align the &amp;quot;multi-source representation beam semantics&amp;quot; associated with specific beam information to a shared semantic space, thereby enhancing discriminative power and robustness under modal noise and distribution shifts. Experiments on real-world low-altitude UAV datasets show that SaM2B achieves more satisfactory results than baseline methods.&lt;/p&gt;</description></item><item><guid>2512.24327v1</guid><title>Topological Spatial Graph Coarsening</title><link>http://arxiv.org/abs/2512.24327v1</link><author>Anna Calissano, Etienne Lasalle</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了空间图的简化问题，旨在找到一个小型的空间图，同时保留初始图的整体结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 空间图是节点在空间中局部化的图，例如公共交通网络、分子和分支生物结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 在空间图中进行图简化，同时保留初始图的主要拓扑特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出了一种基于新框架的拓扑空间图粗化方法，该方法通过折叠短边来实现粗化，并适应了经典拓扑描述符（持久图）的构建，以捕获拓扑信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该方法是无参数的，并且对初始空间图的旋转、平移和缩放具有等变性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在合成和真实空间图上评估了性能，并显示它显著减少了图的大小，同时保留了相关的拓扑信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究了空间图的简化问题，旨在找到一个小型的空间图，同时保留初始图的整体结构。空间图是节点在空间中局部化的图，例如公共交通网络、分子和分支生物结构。在空间图中进行图简化，同时保留初始图的主要拓扑特征。提出了一种基于新框架的拓扑空间图粗化方法，该方法通过折叠短边来实现粗化，并适应了经典拓扑描述符（持久图）的构建，以捕获拓扑信息。该方法是无参数的，并且对初始空间图的旋转、平移和缩放具有等变性。该方法在合成和真实空间图上评估了性能，并显示它显著减少了图的大小，同时保留了相关的拓扑信息。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空间图简化的问题，即如何在减少节点数量的同时保留空间图的主要拓扑特征。这个问题在现实研究中非常重要，因为空间图通常包含大量节点和边，简化它们可以帮助更好地理解和分析复杂的数据结构，如交通网络、分子结构等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴拓扑数据分析（TDA）中的工具，特别是持久图和三角感知图过滤，来设计这个方法。他们考虑了现有工作在图数据上的应用，并提出了一个新的过滤方法来适应空间图数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过合并短边来简化空间图，同时保留其主要的拓扑特征。整体实现流程包括：定义一个参数θ来控制合并的边长阈值，根据这个参数将图中的节点合并成超节点，并重新计算超节点的位置，最后通过比较原始图和简化图的持久图来选择合适的简化程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出了一种新的三角感知图过滤方法来适应空间图数据，并定义了一个基于持久图的评分系统来指导简化程度的选择。与之前的工作相比，这个方法更注重在简化图的同时保留空间图的主要拓扑特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于拓扑数据分析的空间图简化方法，能够在减少节点数量的同时保留空间图的主要拓扑特征。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatial graphs are particular graphs for which the nodes are localized in space (e.g., public transport network, molecules, branching biological structures). In this work, we consider the problem of spatial graph reduction, that aims to find a smaller spatial graph (i.e., with less nodes) with the same overall structure as the initial one. In this context, performing the graph reduction while preserving the main topological features of the initial graph is particularly relevant, due to the additional spatial information. Thus, we propose a topological spatial graph coarsening approach based on a new framework that finds a trade-off between the graph reduction and the preservation of the topological characteristics. The coarsening is realized by collapsing short edges. In order to capture the topological information required to calibrate the reduction level, we adapt the construction of classical topological descriptors made for point clouds (the so-called persistent diagrams) to spatial graphs. This construction relies on the introduction of a new filtration called triangle-aware graph filtration. Our coarsening approach is parameter-free and we prove that it is equivariant under rotations, translations and scaling of the initial spatial graph. We evaluate the performances of our method on synthetic and real spatial graphs, and show that it significantly reduces the graph sizes while preserving the relevant topological information.&lt;/p&gt;</description></item><item><guid>2512.24331v1</guid><title>Spatial-aware Vision Language Model for Autonomous Driving</title><link>http://arxiv.org/abs/2512.24331v1</link><author>Weijie Wei, Zhipeng Luo, Ling Feng, Venice Erin Liong</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中展现出巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。为了解决这个问题，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。实验表明，LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中具有巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决VLMs在自动驾驶中的局限性，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 我们的工作强调了在构建可信的基于VLMs的自动驾驶系统中，明确3D度量数据的重要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Vision-Language Models (VLMs) 在自动驾驶中展现出巨大潜力，但它们依赖于2D图像进行复杂场景理解和决策，这限制了其安全性和可靠性。当前基于图像的方法在精确的度量空间推理和几何推断方面存在困难，导致驾驶策略不可靠。为了解决这个问题，我们提出了LVLDrive（LiDAR-Vision-Language）框架，通过结合LiDAR点云作为额外的输入模态，增强VLMs的3D度量空间理解能力。我们引入了Gradual Fusion Q-Former来逐步注入LiDAR特征，确保VLMs现有知识库的稳定性和保留。此外，我们开发了空间感知问答（SA-QA）数据集，以明确教授模型高级的3D感知和推理能力。实验表明，LVLDrive在场景理解、度量空间感知和可靠的驾驶决策方面优于仅使用视觉的模型。我们的工作强调了在构建可信的基于VLMs的自动驾驶系统中，明确3D度量数据的重要性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉语言模型（VLMs）在自动驾驶场景中缺乏精确的3D空间理解能力的问题。这个问题在现实研究中非常重要，因为准确的3D空间理解是自动驾驶安全规划和决策的基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有VLMs和LiDAR技术的优势，设计了一个名为LVLDrive的框架，该框架结合了图像和LiDAR点云数据，并引入了Gradual Fusion Q-Former来逐步融合LiDAR特征，同时保持VLMs的现有知识。这个设计借鉴了OmniDrive中的Q-Former 3D块。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过Gradual Fusion Q-Former逐步融合LiDAR点云特征，以增强VLMs的3D空间理解能力。整体实现流程包括使用三个预训练编码器处理文本、图像和点云数据，然后通过Gradual Fusion Q-Former融合这些数据，最后通过语言模型生成任务特定的响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括引入Gradual Fusion Q-Former来逐步融合LiDAR特征，以及构建一个空间感知的视觉问答（SA-QA）数据集来增强模型的3D空间推理能力。与之前的工作相比，LVLDrive在融合LiDAR和图像数据方面更加稳定，并且通过SA-QA数据集提供了更精确的空间理解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入Gradual Fusion Q-Former和SA-QA数据集，显著提升了视觉语言模型在自动驾驶场景中的3D空间理解能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While Vision-Language Models (VLMs) show significant promise for end-to-end autonomous driving by leveraging the common sense embedded in language models, their reliance on 2D image cues for complex scene understanding and decision-making presents a critical bottleneck for safety and reliability. Current image-based methods struggle with accurate metric spatial reasoning and geometric inference, leading to unreliable driving policies. To bridge this gap, we propose LVLDrive (LiDAR-Vision-Language), a novel framework specifically designed to upgrade existing VLMs with robust 3D metric spatial understanding for autonomous driving by incoperating LiDAR point cloud as an extra input modality. A key challenge lies in mitigating the catastrophic disturbance introduced by disparate 3D data to the pre-trained VLMs. To this end, we introduce a Gradual Fusion Q-Former that incrementally injects LiDAR features, ensuring the stability and preservation of the VLM&amp;#x27;s existing knowledge base. Furthermore, we develop a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities. Extensive experiments on driving benchmarks demonstrate that LVLDrive achieves superior performance compared to vision-only counterparts across scene understanding, metric spatial perception, and reliable driving decision-making. Our work highlights the necessity of explicit 3D metric data for building trustworthy VLM-based autonomous systems.&lt;/p&gt;</description></item><item><guid>2512.24354v1</guid><title>SeedFold: Scaling Biomolecular Structure Prediction</title><link>http://arxiv.org/abs/2512.24354v1</link><author>Zhou Yi, Lu Chan, Ma Yiming, Qu Wei, Ye Fei, Zhang Kexin, Wang Lan, Gui Minrui, Gu Quanquan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; SeedFold是一种能够成功扩展模型容量的折叠模型，它在蛋白质相关任务上表现优于AlphaFold3。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高精度生物分子结构预测是开发生物分子基础模型的关键部分，而构建基础模型最关键的一方面是确定模型扩展的配方。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出SeedFold模型，以解决生物分子结构预测的扩展问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先，为Pairformer确定一个有效的宽度扩展策略以增加表示能力；其次，引入一种新的线性三角形注意力机制以降低计算复杂度，实现高效扩展；最后，构建一个大规模的蒸馏数据集以显著扩大训练集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在FoldBench上的实验表明，SeedFold在大多数蛋白质相关任务上优于AlphaFold3。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SeedFold模型成功扩展了模型容量，并在蛋白质相关任务上表现出色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 高度精确的生物分子结构预测是开发生物分子基础模型的关键组成部分，构建基础模型最关键的一方面是确定模型扩展的配方。在这项工作中，我们提出了SeedFold，一种成功扩展模型容量的折叠模型。我们的贡献有三方面：首先，我们为Pairformer确定了一个有效的宽度扩展策略以增加表示能力；其次，我们引入了一种新的线性三角形注意力机制以降低计算复杂度，实现高效扩展；最后，我们构建了一个大规模的蒸馏数据集以显著扩大训练集。在FoldBench上的实验表明，SeedFold在大多数蛋白质相关任务上优于AlphaFold3。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Highly accurate biomolecular structure prediction is a key component of developing biomolecular foundation models, and one of the most critical aspects of building foundation models is identifying the recipes for scaling the model. In this work, we present SeedFold, a folding model that successfully scales up the model capacity. Our contributions are threefold: first, we identify an effective width-scaling strategy for the Pairformer to increase representation capacity; second, we introduce a novel linear triangular attention that reduces computational complexity to enable efficient scaling; finally, we construct a large-scale distillation dataset to substantially enlarge the training set. Experiments on FoldBench show that SeedFold outperforms AlphaFold3 on most protein-related tasks.&lt;/p&gt;</description></item><item><guid>2512.24373v1</guid><title>Skim-Aware Contrastive Learning for Efficient Document Representation</title><link>http://arxiv.org/abs/2512.24373v1</link><author>Waheed Ahmed Abro, Zied Bouraoui</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; transformer-based models 在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。相比之下，人类通常通过浏览文本，专注于重要部分来理解整体信息。基于这种人类策略，我们引入了一种新的自监督对比学习框架，以增强长文档的表示。我们的方法随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。这模拟了人类综合信息的方式，从而产生了更丰富且计算效率更高的表示。在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 虽然基于 transformer 的模型在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入一种新的自监督对比学习框架，随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过模拟人类综合信息的方式，产生了更丰富且计算效率更高的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然基于 transformer 的模型在词和句子级别的任务中表现出色，但在表示长文档方面，尤其是在法律和医学领域，仍然存在困难。稀疏注意力机制可以处理更长的输入，但资源密集且往往无法捕捉整个文档的上下文。层次化 transformer 模型提供了更好的效率，但没有清楚地解释它们如何关联文档的不同部分。相比之下，人类通常通过浏览文本，专注于重要部分来理解整体信息。基于这种人类策略，我们引入了一种新的自监督对比学习框架，以增强长文档的表示。我们的方法随机遮盖文档的一部分，并使用基于自然语言推理（NLI）的对比目标将其与相关部分对齐，同时将其与不相关部分分离。这模拟了人类综合信息的方式，从而产生了更丰富且计算效率更高的表示。在法律和生物医学文本上的实验证实了在准确性和效率方面都有显著提升。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决长文档的有效表示问题，特别是在法律和医学领域。这个问题重要，因为现有的方法在处理长文档时效率低且难以捕捉完整文档的上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了人类浏览文本的策略，设计了一种自监督对比学习框架。他们使用了现有的层次化 transformer 模型和 Longformer 模型，并提出了一个新的块预测编码器（CPE）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习来增强长文档的表示。整体流程包括随机遮蔽文档的一部分，并使用基于自然语言推理的对比目标来对齐相关部分，同时与不相关部分保持距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括提出块预测编码器（CPE），使用自监督对比学习来增强文档表示，以及利用自然语言推理（NLI）来模拟人类浏览文本的过程。与之前的工作相比，这种方法更注重捕捉文档中不同部分之间的关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的自监督对比学习框架，通过模拟人类浏览文本的策略来增强长文档的表示，从而在法律和医学领域实现了更高的准确性和效率。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.&lt;/p&gt;</description></item><item><guid>2512.24384v1</guid><title>Geometric Multi-Session Map Merging with Learned Local Descriptors</title><link>http://arxiv.org/abs/2512.24384v1</link><author>Yanlong Ma, Nakul S. Joshi, Christa S. Robison, Philip R. Osteen, Brett T. Lopez</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于学习的局部描述符框架GMLD，用于大规模多会话点云地图合并，该框架能够系统地对跨不同会话收集的地图进行对齐，特别是在重叠区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多会话地图合并对于大规模环境中的自主操作至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种能够有效合并多会话地图的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用关键点感知编码器和基于平面的几何转换器提取判别性特征，用于回环检测和相对位姿估计，并在因子图优化阶段加入会话间扫描匹配成本因素以增强全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在公开数据集和自收集数据上评估，结果显示出准确且鲁棒的地图合并，误差低，学习到的特征在回环检测和相对位姿估计中表现出色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GMLD框架能够有效地进行大规模多会话点云地图合并。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种基于学习的局部描述符框架GMLD，用于大规模多会话点云地图合并，该框架能够系统地对跨不同会话收集的地图进行对齐，特别是在重叠区域。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多会话地图合并的问题，即在大型环境中进行扩展的自主操作时，如何将不同会话收集的地图在重叠区域进行系统地对齐。这个问题在现实或研究中非常重要，因为准确的地图合并能够帮助自主系统在未知环境中进行有效的规划和导航。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的地图合并、位置识别和注册以及位姿图优化工作，设计出了一种基于学习的局部描述符框架。该方法结合了关键点感知编码器和平面几何转换器，以提取具有判别性的特征，用于回环检测和相对位姿估计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用学习到的局部描述符来提高地图合并的准确性和鲁棒性。整体实现流程包括三个主要模块：关键点和描述符生成、回环检测和注册、以及地图合并。首先，从密集点云中提取关键点和局部描述符；然后，通过计算局部描述符之间的距离来检测潜在的回环，并估计相对变换；最后，通过位姿图优化来合并地图，确保全局一致性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：关键点感知的局部描述符生成、基于平面的几何转换器编码器、以及扫描匹配成本感知的会话间位姿图优化。相比之前的工作，这些创新点提高了地图合并的准确性和鲁棒性，并能够更好地处理大型环境中的复杂情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于学习的局部描述符框架，能够有效地进行多会话地图合并，提高地图的准确性和鲁棒性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.&lt;/p&gt;</description></item><item><guid>2512.24385v1</guid><title>Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems</title><link>http://arxiv.org/abs/2512.24385v1</link><author>Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了如何从多模态车载传感器数据中构建真正的空间智能，特别是在自动驾驶车辆和无人机等自主系统中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着自主系统的快速发展，从多模态车载传感器数据中整合能力以创建统一理解的需求日益增加，但这一挑战依然存在。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文旨在提出一个全面的框架，用于多模态预训练，并识别推动该目标实现的核心技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文分析了基础传感器特征与学习策略之间的相互作用，评估了平台特定数据集在这些进步中的作用，并提出了一个统一的预训练范式分类法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 本文提出了一个统一的预训练范式分类法，从单模态基线到复杂的统一框架，这些框架学习整体表示以用于高级任务，如3D物体检测和语义占用预测。此外，本文还研究了文本输入和占用表示的集成，以促进开放世界的感知和规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文确定了关键瓶颈，如计算效率和模型可扩展性，并提出了一个路线图，以实现通用多模态基础模型，这些模型能够实现稳健的空间智能，适用于实际部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文探讨了如何从多模态车载传感器数据中构建真正的空间智能，特别是在自动驾驶车辆和无人机等自主系统中的应用。随着自主系统的快速发展，从多模态车载传感器数据中整合能力以创建统一理解的需求日益增加，但这一挑战依然存在。本文旨在提出一个全面的框架，用于多模态预训练，并识别推动该目标实现的核心技术。本文分析了基础传感器特征与学习策略之间的相互作用，评估了平台特定数据集在这些进步中的作用，并提出了一个统一的预训练范式分类法。本文提出了一个统一的预训练范式分类法，从单模态基线到复杂的统一框架，这些框架学习整体表示以用于高级任务，如3D物体检测和语义占用预测。此外，本文还研究了文本输入和占用表示的集成，以促进开放世界的感知和规划。本文确定了关键瓶颈，如计算效率和模型可扩展性，并提出了一个路线图，以实现通用多模态基础模型，这些模型能够实现稳健的空间智能，适用于实际部署。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从多模态传感器数据中训练出具有空间智能的自主系统。这个问题在现实或研究中非常重要，因为自主系统（如自动驾驶汽车和无人机）需要准确感知和理解环境，才能安全有效地运行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有单模态和跨模态学习方法，以及基础模型在多模态场景中的应用，设计了多模态数据预训练框架。他们借鉴了现有工作，特别是自监督学习、跨模态交互和知识蒸馏等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态预训练，从不同传感器（如摄像头和LiDAR）中提取统一的表示，以实现空间智能。整体流程包括单模态预训练、跨模态交互和统一框架预训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括提出统一的多模态预训练框架，以及将文本输入和占用表示整合到预训练中。与之前的工作相比，这篇论文更全面地分析了不同模态和平台的数据集，以及预训练方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文为从多模态传感器数据中训练具有空间智能的自主系统提供了一个全面的框架和路线图。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.&lt;/p&gt;</description></item><item><guid>2512.24404v1</guid><title>Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning</title><link>http://arxiv.org/abs/2512.24404v1</link><author>Soham Pahari, M. Srinivas</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 多模态智能在视觉理解和高级推理方面取得了显著进展，但大多数推理系统仍依赖文本信息进行推断，这在空间任务（如视觉导航和地理定位）中限制了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大多数推理系统依赖文本信息，这在空间任务中限制了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 讨论该领域的潜在范围，并提出一个视觉推理范式Geo-Consistent Visual Planning，即ViReLoc框架，该框架仅使用视觉表示进行规划和定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ViReLoc学习空间依赖和几何关系，通过在视觉域中编码逐步推理并使用基于强化优化的目标进行规划，同时整合对比学习和自适应特征交互来对齐跨视图视角并减少视角差异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验在不同导航和定位场景中显示出空间推理准确性和跨视图检索性能的持续改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 视觉推理作为导航和定位的强大补充方法，表明这些任务可以在没有实时全球定位系统数据的情况下完成，从而实现更安全的导航解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多模态智能在视觉理解和高级推理方面取得了显著进展，但大多数推理系统仍依赖文本信息进行推断，这在空间任务（如视觉导航和地理定位）中限制了其有效性。本文讨论了该领域的潜在范围，并提出一个视觉推理范式Geo-Consistent Visual Planning，即ViReLoc框架，该框架仅使用视觉表示进行规划和定位。ViReLoc学习空间依赖和几何关系，通过在视觉域中编码逐步推理并使用基于强化优化的目标进行规划，同时整合对比学习和自适应特征交互来对齐跨视图视角并减少视角差异。实验在不同导航和定位场景中显示出空间推理准确性和跨视图检索性能的持续改进。这些结果表明视觉推理作为导航和定位的强大补充方法，表明这些任务可以在没有实时全球定位系统数据的情况下完成，从而实现更安全的导航解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决地面到空中的定位问题，通过视觉推理引导规划。这个问题在现实研究中很重要，因为它可以实现无需GPS的导航，提高导航的安全性和隐私性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过结合视觉推理和强化学习来设计这个方法，借鉴了现有工作中的特征提取、对比学习和强化学习等技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过视觉推理引导规划，整体实现流程包括构建画布、交叉视图地理定位和通过强化学习的视觉规划三个阶段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括统一的架构、视觉推理模块和可微分的规划系统。相比之前的工作，这个方法能够同时进行检索和规划，并且通过视觉推理来引导规划。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过视觉推理引导规划的方法，实现了地面到空中的定位，为无需GPS的导航提供了一种新的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.&lt;/p&gt;</description></item><item><guid>2512.24422v1</guid><title>OmniCosmos: Transferring Particle Physics Knowledge Across the Cosmos</title><link>http://arxiv.org/abs/2512.24422v1</link><author>Vinicius Mikuni, Ibrahim Elsharkawy, Benjamin Nachman</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Foundation models能够有效地构建数据表示，并可以应用于多种下游任务。OmniLearned foundation model在粒子对撞物理中显示出显著提升发现潜力的能力。本文展示了在粒子对撞数据上训练的Foundation Models可以用于改进宇宙学参数的预测，并预测CosmoBench数据集中的暗晕和星系速度。这是首次展示粒子对撞物理模型能够跨科学领域泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Foundation models可以构建有效的数据表示，并应用于多种下游任务。OmniLearned foundation model在粒子对撞物理中显示出显著提升发现潜力的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 展示在粒子对撞数据上训练的Foundation Models可以用于改进宇宙学参数的预测，并预测CosmoBench数据集中的暗晕和星系速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在粒子对撞数据上训练Foundation Models，用于改进宇宙学参数的预测，并预测CosmoBench数据集中的暗晕和星系速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Foundation Models在粒子对撞数据上训练后，可以用于改进宇宙学参数的预测，并预测CosmoBench数据集中的暗晕和星系速度。这是首次展示粒子对撞物理模型能够跨科学领域泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 粒子对撞物理模型可以跨科学领域泛化，并应用于宇宙学参数的预测和暗晕、星系速度的预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Foundation models能够有效地构建数据表示，并可以应用于多种下游任务。OmniLearned foundation model在粒子对撞物理中显示出显著提升发现潜力的能力。本文展示了在粒子对撞数据上训练的Foundation Models可以用于改进宇宙学参数的预测，并预测CosmoBench数据集中的暗晕和星系速度。这是首次展示粒子对撞物理模型能够跨科学领域泛化。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Foundation models build an effective representations of data that can be deployed on diverse downstream tasks. Previous research developed the OmniLearned foundation model for collider physics and showed that it could significantly advance discovery potential across collider experiments. In this paper we go beyond collider physics and show that Foundation Models trained on collider data can help improve the prediction of cosmological parameters and to predict halo and galaxy velocities in different datasets from CosmoBench. This is the first time a collider physics model is shown to generalize across scientific fields.&lt;/p&gt;</description></item><item><guid>2512.24428v1</guid><title>Subsecond 3D Mesh Generation for Robot Manipulation</title><link>http://arxiv.org/abs/2512.24428v1</link><author>Qian Wang, Omar Abdellall, Tony Gao, Xiatao Sun, Daniel Rakita</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D网格是计算机科学和工程中的一种基本表示形式，在机器人学中尤其有价值，因为它们能够以与机器人如何与物理世界交互一致的方式捕捉物体，从而实现预测稳定抓取、检测碰撞和模拟动力学等核心功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 尽管自动3D网格生成方法近年来取得了显著进展，但仍然存在两个关键挑战：生成高保真网格的速度过慢，难以满足实时应用需求，通常每个物体需要数十秒；仅生成网格本身是不够的，在机器人学中，网格必须被正确地从场景中分割并注册到适当的尺度和姿态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 引入一个端到端系统，解决上述挑战，从单个RGB-D图像中在不到一秒的时间内生成高质量的、具有上下文信息的3D网格。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 集成开放词汇对象分割、加速的基于扩散的网格生成和鲁棒的点云注册，每个步骤都针对速度和准确性进行了优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在现实世界的操作任务中展示了其有效性，表明它能够使网格成为机器人感知和规划的实用按需表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该系统有效地解决了3D网格生成中的实时性和上下文信息问题，为机器人学提供了实用的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D网格是计算机科学和工程中的一种基本表示形式，在机器人学中尤其有价值，因为它们能够以与机器人如何与物理世界交互一致的方式捕捉物体，从而实现预测稳定抓取、检测碰撞和模拟动力学等核心功能。尽管自动3D网格生成方法近年来取得了显著进展，但仍然存在两个关键挑战：生成高保真网格的速度过慢，难以满足实时应用需求，通常每个物体需要数十秒；仅生成网格本身是不够的，在机器人学中，网格必须被正确地从场景中分割并注册到适当的尺度和姿态。引入一个端到端系统，解决上述挑战，从单个RGB-D图像中在不到一秒的时间内生成高质量的、具有上下文信息的3D网格。集成开放词汇对象分割、加速的基于扩散的网格生成和鲁棒的点云注册，每个步骤都针对速度和准确性进行了优化。在现实世界的操作任务中展示了其有效性，表明它能够使网格成为机器人感知和规划的实用按需表示。该系统有效地解决了3D网格生成中的实时性和上下文信息问题，为机器人学提供了实用的解决方案。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D网格生成在机器人操作中的实时性问题。在现实和研究中，这个问题非常重要，因为3D网格能够帮助机器人更好地理解和交互物理世界，实现抓取、避障和动力学模拟等关键任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合现有技术并加以改进来设计这个方法。他们借鉴了开放词汇对象分割、加速扩散模型和鲁棒点云注册等技术，并对这些技术进行了优化，以实现快速且准确的3D网格生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将开放词汇分割、加速网格生成和鲁棒点云注册三个步骤紧密集成，以实现快速且准确的3D网格生成。整体流程包括：首先使用 Florence-2 和 SAM2 进行开放词汇分割；然后使用 FlashVDM 加速的 Hunyuan3D 2.0 生成高保真网格；最后通过 RANSAC 和 ICP 进行点云注册，对齐网格与观测点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：使用 FlashVDM 加速扩散模型生成网格，减少生成时间；采用分层 SDF 解码和自适应键值选择技术进一步加速推理；结合 Florence-2 和 SAM2 实现开放词汇分割；使用 RANSAC 和 ICP 进行点云注册，无需纹理网格。这些创新点使得该方法在速度和准确性上都有显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种快速、准确的3D网格生成方法，通过整合开放词汇分割、加速扩散模型和鲁棒点云注册，实现了亚秒级的网格生成，为实时机器人应用提供了新的可能性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.&lt;/p&gt;</description></item><item><guid>2512.24438v1</guid><title>Exploring Compositionality in Vision Transformers using Wavelet Representations</title><link>http://arxiv.org/abs/2512.24438v1</link><author>Akshad Shyam Purushottamdas, Pranav K Nayak, Divya Mehul Rajparia, Deekshith Patel, Yashmitha Gogineni, Konda Reddy Mopuri, Sumohana S. Channappayya</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究通过分析视觉Transformer（ViT）编码器学习到的表示，探讨了其组合性。研究引入了一个类似于先前用于测量表示学习中组合性的框架，以测试ViT编码器的组合性。研究使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 先前对Transformer模型的工作原理的了解主要通过对它们在语言任务上的行为进行分析而获得。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本研究旨在通过组合性的视角，调查ViT编码器学习到的表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 研究引入了一个类似于先前用于测量表示学习中组合性的框架，并使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 来自一阶DWT分解的基元在潜在空间中近似地组合，产生了编码器表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ViT编码器在表示空间中尊重组合性，提供了一种新的视角来理解ViT如何结构化信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究通过分析视觉Transformer（ViT）编码器学习到的表示，探讨了其组合性。研究引入了一个类似于先前用于测量表示学习中组合性的框架，以测试ViT编码器的组合性。研究使用离散小波变换（DWT）作为工具，通过检查组合表示能否再现原始图像表示，来实证测试表示空间中组合性被尊重的程度。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想解决如何测试视觉Transformer（ViT）编码器表示的组成性。这个问题在现实或研究中很重要，因为理解ViT如何学习和组织信息可以帮助提高模型的解释性和性能，特别是在计算机视觉领域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作中关于表示学习的组成性框架，并使用离散小波变换（DWT）来生成图像的基集（输入特定的原始元素）。这个方法的设计思路是将图像分解为视觉上有意义的原始元素，并检查这些原始元素在ViT编码器中的表示是否具有组成性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是使用DWT将图像分解为原始元素，并检查这些原始元素在ViT编码器中的表示是否可以重新组合以近似原始图像的表示。整体实现流程包括使用DWT分解图像，提取原始元素，然后检查这些原始元素在ViT编码器中的表示是否具有组成性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用DWT来分析ViT编码器的组成性，以及提出一个框架来测试ViT编码器表示的组成性。与之前的工作相比，这篇论文首次将DWT应用于ViT编码器的组成性分析，并提供了实证结果来支持这一方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过使用离散小波变换来分析视觉Transformer编码器的组成性，提供了一种新的视角来理解ViT如何结构化信息。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While insights into the workings of the transformer model have largely emerged by analysing their behaviour on language tasks, this work investigates the representations learnt by the Vision Transformer (ViT) encoder through the lens of compositionality. We introduce a framework, analogous to prior work on measuring compositionality in representation learning, to test for compositionality in the ViT encoder. Crucial to drawing this analogy is the Discrete Wavelet Transform (DWT), which is a simple yet effective tool for obtaining input-dependent primitives in the vision setting. By examining the ability of composed representations to reproduce original image representations, we empirically test the extent to which compositionality is respected in the representation space. Our findings show that primitives from a one-level DWT decomposition produce encoder representations that approximately compose in latent space, offering a new perspective on how ViTs structure information.&lt;/p&gt;</description></item><item><guid>2512.24445v1</guid><title>Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics</title><link>http://arxiv.org/abs/2512.24445v1</link><author>Akash Samanta, Sheldon Williamson</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种诊断驱动的自适应学习框架，用于处理非平稳和安全关键环境中的学习系统的不稳定性、收敛缓慢和脆弱适应问题。该框架通过将误差演变分解为偏差、噪声和一致性来显式建模误差的演变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在非平稳和安全关键环境中，学习系统在动态变化的学习过程中常常面临不稳定性、收敛缓慢或脆弱适应的问题。现有的优化、强化学习和元学习方法主要适应梯度统计，而忽略了误差信号的时序结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文旨在提出一种能够显式建模误差演变并适应动态环境的学习框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文提出了一种诊断驱动的自适应学习框架，通过将误差演变分解为偏差、噪声和一致性来显式建模误差的演变。这些诊断通过轻量级的损失或时序差分误差轨迹统计在线计算，并与模型架构或任务领域无关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 本文提出的偏差-噪声-一致性分解为监督优化、演员-评论家强化学习和学习优化器提供了一个统一的控制骨干。基于该框架，本文推导出诊断驱动的实例，包括一个稳定的监督优化器、一个诊断调节的演员-评论家方案和一个诊断条件的学优化的优化器。在标准的平滑假设下，本文为所有情况建立了有界的有效更新和稳定性特性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文将误差演变提升为自适应学习中的第一类对象，并提供了一个可解释、轻量级的动态环境可靠学习基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种诊断驱动的自适应学习框架，用于处理非平稳和安全关键环境中的学习系统的不稳定性、收敛缓慢和脆弱适应问题。该框架通过将误差演变分解为偏差、噪声和一致性来显式建模误差的演变。现有的优化、强化学习和元学习方法主要适应梯度统计，而忽略了误差信号的时序结构。本文提出的偏差-噪声-一致性分解为监督优化、演员-评论家强化学习和学习优化器提供了一个统一的控制骨干。在标准的平滑假设下，本文为所有情况建立了有界的有效更新和稳定性特性。本文将误差演变提升为自适应学习中的第一类对象，并提供了一个可解释、轻量级的动态环境可靠学习基础。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Building on this framework, we derive diagnostic-driven instantiations including a stabilized supervised optimizer, a diagnostic-regulated actor-critic scheme, and a diagnostic-conditioned learned optimizer. Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to temporal-difference error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.&lt;/p&gt;</description></item><item><guid>2512.24470v1</guid><title>Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models</title><link>http://arxiv.org/abs/2512.24470v1</link><author>Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavoned, Martin Steinert</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 摘要介绍了IMO MASS Code对自主和远程监督的海上船舶的要求，包括检测操作设计域的偏离、进入预定义的回退模式通知操作员、允许立即人工覆盖以及未经批准不更改航行计划。摘要还讨论了如何在这些要求中实现快速、可人工覆盖的回退操作，并介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的海上自主系统在正确操作依赖于语义理解的情况下（例如，潜水下降标志意味着有人在水中，火势附近意味着危险）难以应对。摘要指出，视觉语言模型（VLMs）可以提供语义意识，以应对这些分布外的情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 摘要旨在展示视觉语言模型（VLMs）如何提供语义意识，以及如何通过快速-慢速异常管道和短视距、可人工覆盖的回退操作，使IMO MASS Code的要求在实际操作中可行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 摘要介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器，能够在持续的人工授权下从水有效、世界锚定的轨迹中选择一个谨慎的操作（或保持停泊）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 摘要通过在40个港口场景中测量每调用场景理解、延迟、与人类共识的一致性（模型多数投票）、火灾危险场景的短视距风险缓解以及水上警报-&amp;gt;回退操作-&amp;gt;操作员接管，验证了Semantic Lookout模型的有效性。结果表明，亚10秒的模型保留了大多数较慢的先进模型的意识，回退操作选择器优于仅基于几何学的基线，并在火灾场景中增加了安全距离。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 摘要支持视觉语言模型作为语义回退操作选择器，符合IMO MASS Code的要求，并在实际延迟预算内有效。摘要还鼓励未来研究在领域适应的混合自主系统中，将基础模型的语义与多传感器鸟瞰感知和短视距重规划相结合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 摘要介绍了IMO MASS Code对自主和远程监督的海上船舶的要求，包括检测操作设计域的偏离、进入预定义的回退模式通知操作员、允许立即人工覆盖以及未经批准不更改航行计划。摘要还讨论了如何在这些要求中实现快速、可人工覆盖的回退操作，并介绍了Semantic Lookout模型，该模型是一种仅使用摄像头的候选约束视觉语言模型回退操作选择器。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自主船舶在遇到需要语义理解的异常情况时，如何进行安全、有效的Fallback操作的问题。这个问题在现实中非常重要，因为自主船舶在遇到传统几何方法无法处理的情况（如旗帜、火灾等）时，需要能够安全地应对并等待人类接管，以确保航行安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，特别是视觉-语言模型（VLMs）和快速-慢速异常检测管道，设计出了一种基于VLM的Fallback操作选择器。该方法借鉴了Sinha等人的工作，并结合了IMO MASS Code的要求，设计出了一种短时程、可被人类覆盖的Fallback操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用VLM提供语义理解能力，选择一个谨慎的Fallback操作，直到人类接管。整体实现流程包括：使用摄像头获取图像，通过VLM理解图像内容，选择一个安全的轨迹，并在人类接管前保持船舶安全。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：使用VLM进行语义理解，选择一个谨慎的Fallback操作，并确保人类可以立即接管。相比之前的工作，这个方法更加注重语义理解，并且能够在短时程内做出反应，同时保持人类控制权。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于VLM的Fallback操作选择器，能够在自主船舶遇到需要语义理解的异常情况时，安全、有效地进行Fallback操作，并确保人类可以立即接管。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert-&amp;gt;fallback maneuver-&amp;gt;operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird&amp;#x27;s-eye-view perception and short-horizon replanning.&lt;/p&gt;</description></item><item><guid>2512.24473v1</guid><title>F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model</title><link>http://arxiv.org/abs/2512.24473v1</link><author>Devendra K. Jangid, Ripon K. Saha, Dilshan Godaliyadda, Jing Li, Seok-Jun Lee, Hamid R. Sheikh</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 生成式AI的出现使得单图像超分辨率（SISR）质量显著提高，因为文本到图像扩散（T2IDiff）基础模型（FM）学习到的强先验可以弥合高分辨率（HR）和低分辨率（LR）图像之间的差距。然而，旗舰智能手机相机迟迟未采用生成模型，因为强生成可能导致不希望的幻觉。在学术界，对于严重退化的LR图像，强生成是必要的，并且幻觉更容易被接受，因为LR和HR图像之间的差距很大。相比之下，在消费摄影中，LR图像具有更高的保真度，只需要极少的无幻觉生成。我们假设SISR中的生成受FM的条件特征严格性和丰富性的控制。首先，文本特征是高级特征，通常无法描述图像中的微妙纹理。此外，智能手机LR图像至少是12MP，而基于T2IDiff FM构建的SISR网络设计为对远小于1MP的图像进行推理。因此，SISR推理必须在小块上进行，这些小块通常无法用文本特征准确描述。为了解决这些缺点，我们引入了一个基于具有较低级特征条件的FM的SISR网络，特别是DINOv2特征，我们称之为特征到图像扩散（F2IDiff）基础模型（FM）。较低级特征提供更严格的条件，同时是描述即使是小块的丰富描述符。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成式AI的出现使得单图像超分辨率（SISR）质量显著提高，但旗舰智能手机相机未采用生成模型，因为强生成可能导致不希望的幻觉。在学术界，对于严重退化的LR图像，强生成是必要的，而在消费摄影中，LR图像具有更高的保真度，只需要极少的无幻觉生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 引入一个基于具有较低级特征条件的FM的SISR网络，特别是DINOv2特征，我们称之为特征到图像扩散（F2IDiff）基础模型（FM）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 较低级特征提供更严格的条件，同时是描述即使是小块的丰富描述符。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 我们假设SISR中的生成受FM的条件特征严格性和丰富性的控制。较低级特征可以更好地描述图像中的微妙纹理，从而提高SISR的质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成式AI的出现使得单图像超分辨率（SISR）质量显著提高，因为文本到图像扩散（T2IDiff）基础模型（FM）学习到的强先验可以弥合高分辨率（HR）和低分辨率（LR）图像之间的差距。然而，旗舰智能手机相机迟迟未采用生成模型，因为强生成可能导致不希望的幻觉。在学术界，对于严重退化的LR图像，强生成是必要的，并且幻觉更容易被接受，因为LR和HR图像之间的差距很大。相比之下，在消费摄影中，LR图像具有更高的保真度，只需要极少的无幻觉生成。我们假设SISR中的生成受FM的条件特征严格性和丰富性的控制。首先，文本特征是高级特征，通常无法描述图像中的微妙纹理。此外，智能手机LR图像至少是12MP，而基于T2IDiff FM构建的SISR网络设计为对远小于1MP的图像进行推理。因此，SISR推理必须在小块上进行，这些小块通常无法用文本特征准确描述。为了解决这些缺点，我们引入了一个基于具有较低级特征条件的FM的SISR网络，特别是DINOv2特征，我们称之为特征到图像扩散（F2IDiff）基础模型（FM）。较低级特征提供更严格的条件，同时是描述即使是小块的丰富描述符。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;With the advent of Generative AI, Single Image Super-Resolution (SISR) quality has seen substantial improvement, as the strong priors learned by Text-2-Image Diffusion (T2IDiff) Foundation Models (FM) can bridge the gap between High-Resolution (HR) and Low-Resolution (LR) images. However, flagship smartphone cameras have been slow to adopt generative models because strong generation can lead to undesirable hallucinations. For substantially degraded LR images, as seen in academia, strong generation is required and hallucinations are more tolerable because of the wide gap between LR and HR images. In contrast, in consumer photography, the LR image has substantially higher fidelity, requiring only minimal hallucination-free generation. We hypothesize that generation in SISR is controlled by the stringency and richness of the FM&amp;#x27;s conditioning feature. First, text features are high level features, which often cannot describe subtle textures in an image. Additionally, Smartphone LR images are at least $12MP$, whereas SISR networks built on T2IDiff FM are designed to perform inference on much smaller images ($&amp;lt;1MP$). As a result, SISR inference has to be performed on small patches, which often cannot be accurately described by text feature. To address these shortcomings, we introduce an SISR network built on a FM with lower-level feature conditioning, specifically DINOv2 features, which we call a Feature-to-Image Diffusion (F2IDiff) Foundation Model (FM). Lower level features provide stricter conditioning while being rich descriptors of even small patches.&lt;/p&gt;</description></item><item><guid>2512.24487v1</guid><title>Networked Markets, Fragmented Data: Adaptive Graph Learning for Customer Risk Analytics and Policy Design</title><link>http://arxiv.org/abs/2512.24487v1</link><author>Lecheng Zheng, Jian Ni, Chris Zobel, John R Birge</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 金融机构在识别大规模交易网络中的高风险客户行为方面面临挑战，欺诈活动利用市场碎片化和机构边界。本文提出一个集成客户智能框架，结合联邦学习、关系网络分析和自适应目标政策来解决问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 金融机构在识别高风险客户行为方面面临挑战，欺诈活动利用市场碎片化和机构边界。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决客户风险分析中的三个基本问题：数据孤岛、极端行为类别不平衡和次优客户干预策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 开发一个集成客户智能框架，结合联邦学习、关系网络分析和自适应目标政策。使用联邦图神经网络进行协作行为建模，而不泄露专有客户数据。引入跨银行个性化PageRank来识别协调行为集群。使用分层强化学习机制优化动态干预目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 分析140万笔客户交易，减少误报和漏报率至4.64%和11.07%，显著优于单一机构模型。框架防止79.25%的潜在损失，优于固定规则政策的49.41%。最优市场特定目标阈值反映客户群体特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 联邦客户分析显著提高风险管理和客户关系结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 金融机构在识别大规模交易网络中的高风险客户行为方面面临挑战，欺诈活动利用市场碎片化和机构边界。本文提出一个集成客户智能框架，结合联邦学习、关系网络分析和自适应目标政策来解决问题。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Financial institutions face escalating challenges in identifying high-risk customer behaviors within massive transaction networks, where fraudulent activities exploit market fragmentation and institutional boundaries. We address three fundamental problems in customer risk analytics: data silos preventing holistic relationship assessment, extreme behavioral class imbalance, and suboptimal customer intervention strategies that fail to balance compliance costs with relationship value. We develop an integrated customer intelligence framework combining federated learning, relational network analysis, and adaptive targeting policies. Our federated graph neural network enables collaborative behavior modeling across competing institutions without compromising proprietary customer data, using privacy-preserving embeddings to capture cross-market relational patterns. We introduce cross-bank Personalized PageRank to identify coordinated behavioral clusters providing interpretable customer network segmentation for risk managers. A hierarchical reinforcement learning mechanism optimizes dynamic intervention targeting, calibrating escalation policies to maximize prevention value while minimizing customer friction and operational costs. Analyzing 1.4 million customer transactions across seven markets, our approach reduces false positive and false negative rates to 4.64% and 11.07%, substantially outperforming single-institution models. The framework prevents 79.25% of potential losses versus 49.41% under fixed-rule policies, with optimal market-specific targeting thresholds reflecting heterogeneous customer base characteristics. These findings demonstrate that federated customer analytics materially improve both risk management effectiveness and customer relationship outcomes in networked competitive markets.&lt;/p&gt;</description></item><item><guid>2512.24492v1</guid><title>Automated Classification of First-Trimester Fetal Heart Views Using Ultrasound-Specific Self-Supervised Learning</title><link>http://arxiv.org/abs/2512.24492v1</link><author>Youssef Megahed, Aylin Erman, Robin Ducharme, Mark C. Walker, Steven Hawken, Adrian D. C. Chan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 先天性心脏病是最常见的先天性异常，也是导致新生儿发病率和死亡率的主要原因。虽然孕早期胎儿超声心动图提供了早期检测的机会，但由于心脏结构小、信噪比低和操作者之间差异大，自动化分析在这一阶段具有挑战性。本研究评估了自监督超声基础模型USF-MAE在孕早期胎儿心脏视图分类中的应用。USF-MAE使用超过370,000张未标记的超声图像进行预训练，涵盖40多个解剖区域，并随后进行微调以进行下游分类。作为概念验证，预训练的视觉Transformer编码器在6,720张孕早期胎儿超声心动图图像的数据集上进行了微调，以分类五个类别：主动脉、心房心室血流、V征、X征和其他。模型性能与监督卷积神经网络基线（ResNet-18和ResNet-50）以及在大自然图像（ImageNet-1k）上预训练的视觉Transformer（ViT-B/16）模型进行了基准测试。所有模型都使用相同的预处理、数据分割和优化协议进行训练和评估。在独立测试集上，USF-MAE在所有评估指标中实现了最高性能，准确率为90.57%，精确率为91.15%，召回率为90.57%，F1分数为90.71%。与最强的基线ResNet-18相比，准确率提高了+2.03%，F1分数提高了+1.98%。所提出的方法在不依赖激进图像预处理或感兴趣区域裁剪的情况下表现出稳健的性能，并显示出对非诊断帧的更好区分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 先天性心脏病是最常见的先天性异常，也是导致新生儿发病率和死亡率的主要原因。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估自监督超声基础模型USF-MAE在孕早期胎儿心脏视图分类中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用超过370,000张未标记的超声图像进行预训练，涵盖40多个解剖区域，并随后进行微调以进行下游分类。预训练的视觉Transformer编码器在6,720张孕早期胎儿超声心动图图像的数据集上进行了微调，以分类五个类别：主动脉、心房心室血流、V征、X征和其他。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; USF-MAE在所有评估指标中实现了最高性能，准确率为90.57%，精确率为91.15%，召回率为90.57%，F1分数为90.71%。与最强的基线ResNet-18相比，准确率提高了+2.03%，F1分数提高了+1.98%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的方法在不依赖激进图像预处理或感兴趣区域裁剪的情况下表现出稳健的性能，并显示出对非诊断帧的更好区分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 先天性心脏病是最常见的先天性异常，也是导致新生儿发病率和死亡率的主要原因。虽然孕早期胎儿超声心动图提供了早期检测的机会，但由于心脏结构小、信噪比低和操作者之间差异大，自动化分析在这一阶段具有挑战性。本研究评估了自监督超声基础模型USF-MAE在孕早期胎儿心脏视图分类中的应用。USF-MAE使用超过370,000张未标记的超声图像进行预训练，涵盖40多个解剖区域，并随后进行微调以进行下游分类。作为概念验证，预训练的视觉Transformer编码器在6,720张孕早期胎儿超声心动图图像的数据集上进行了微调，以分类五个类别：主动脉、心房心室血流、V征、X征和其他。模型性能与监督卷积神经网络基线（ResNet-18和ResNet-50）以及在大自然图像（ImageNet-1k）上预训练的视觉Transformer（ViT-B/16）模型进行了基准测试。所有模型都使用相同的预处理、数据分割和优化协议进行训练和评估。在独立测试集上，USF-MAE在所有评估指标中实现了最高性能，准确率为90.57%，精确率为91.15%，召回率为90.57%，F1分数为90.71%。与最强的基线ResNet-18相比，准确率提高了+2.03%，F1分数提高了+1.98%。所提出的方法在不依赖激进图像预处理或感兴趣区域裁剪的情况下表现出稳健的性能，并显示出对非诊断帧的更好区分。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Congenital heart disease remains the most common congenital anomaly and a leading cause of neonatal morbidity and mortality. Although first-trimester fetal echocardiography offers an opportunity for earlier detection, automated analysis at this stage is challenging due to small cardiac structures, low signal-to-noise ratio, and substantial inter-operator variability. In this work, we evaluate a self-supervised ultrasound foundation model, USF-MAE, for first-trimester fetal heart view classification. USF-MAE is pretrained using masked autoencoding modelling on more than 370,000 unlabelled ultrasound images spanning over 40 anatomical regions and is subsequently fine-tuned for downstream classification. As a proof of concept, the pretrained Vision Transformer encoder was fine-tuned on an open-source dataset of 6,720 first-trimester fetal echocardiography images to classify five categories: aorta, atrioventricular flows, V sign, X sign, and Other. Model performance was benchmarked against supervised convolutional neural network baselines (ResNet-18 and ResNet-50) and a Vision Transformer (ViT-B/16) model pretrained on natural images (ImageNet-1k). All models were trained and evaluated using identical preprocessing, data splits, and optimization protocols. On an independent test set, USF-MAE achieved the highest performance across all evaluation metrics, with 90.57% accuracy, 91.15% precision, 90.57% recall, and 90.71% F1-score. This represents an improvement of +2.03% in accuracy and +1.98% in F1-score compared with the strongest baseline, ResNet-18. The proposed approach demonstrated robust performance without reliance on aggressive image preprocessing or region-of-interest cropping and showed improved discrimination of non-diagnostic frames.&lt;/p&gt;</description></item><item><guid>2512.24504v1</guid><title>Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments</title><link>http://arxiv.org/abs/2512.24504v1</link><author>Zhiwei Wei, Yuxing Liu, Hua Liao, Wenjia Xu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了基础模型（FM）代理在符号地图环境中的探索、记忆和推理能力，并提出了一个交互式评估框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地图环境是表示空间结构的基本媒介，理解基础模型代理如何理解和在地图环境中行动对于实现可靠的基于地图的推理和应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文旨在分析基础模型代理如何在符号地图环境中探索、记忆和推理，并揭示不同组件的功能角色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文提出了一种交互式评估框架，代理在部分可观察的基于网格的地图上进行增量式探索，地图包含道路、交叉口和兴趣点（POI），代理在每个步骤只接收局部观察。使用六种空间任务来评估空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 探索主要影响经验获取，但对最终推理准确性的影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是顺序和基于图的表示）显著提高路径规划等结构密集型任务的性能；推理方案进一步塑造存储的空间知识的使用方式，高级提示支持更有效的多步推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 空间推理性能在模型版本和规模超过一定能力阈值后会饱和，表明改进基于地图的空间理解需要针对空间表示和推理的机制，而不仅仅是规模扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文研究了基础模型（FM）代理在符号地图环境中的探索、记忆和推理能力，并提出了一个交互式评估框架。地图环境是表示空间结构的基本媒介，理解基础模型代理如何理解和在地图环境中行动对于实现可靠的基于地图的推理和应用至关重要。本文旨在分析基础模型代理如何在符号地图环境中探索、记忆和推理，并揭示不同组件的功能角色。本文提出了一种交互式评估框架，代理在部分可观察的基于网格的地图上进行增量式探索，地图包含道路、交叉口和兴趣点（POI），代理在每个步骤只接收局部观察。使用六种空间任务来评估空间理解能力。探索主要影响经验获取，但对最终推理准确性的影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是顺序和基于图的表示）显著提高路径规划等结构密集型任务的性能；推理方案进一步塑造存储的空间知识的使用方式，高级提示支持更有效的多步推理。空间推理性能在模型版本和规模超过一定能力阈值后会饱和，表明改进基于地图的空间理解需要针对空间表示和推理的机制，而不仅仅是规模扩展。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何评估基础模型（FM）代理在地图环境中的空间认知能力问题。这个问题在现实或研究中很重要，因为理解和评估FM代理在地图环境中的空间认知能力对于开发可靠的基于地图的推理系统和应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有在物理或感知环境、地图环境中的空间能力评估方法，设计了一个交互式评估框架。这个框架让代理逐步探索部分可观察的网格地图，并通过一系列任务评估其空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过交互式探索和推理来评估FM代理的空间认知能力。整体实现流程包括代理逐步探索地图环境，构建内部空间表示，并通过一系列任务（如方向判断、距离估计、路径规划等）评估其空间理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出一个交互式评估框架，设计一套涵盖多个城市的探测任务，系统地分析不同探索策略、记忆表示和提示方法对空间理解的影响。相比之前的工作，这个方法更注重动态、经验驱动的空间认知评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过交互式评估框架，揭示了基础模型代理在地图环境中探索、记忆和推理空间的能力，为设计更可靠的基于地图的推理系统提供了重要见解。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.&lt;/p&gt;</description></item><item><guid>2512.24504v2</guid><title>Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments</title><link>http://arxiv.org/abs/2512.24504v1</link><author>Zhiwei Wei, Yuxing Liu, Hua Liao, Wenjia Xu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种交互式评估框架，用于分析基础模型代理在符号地图环境中的探索、记忆和推理能力。通过让代理逐步探索部分可观测的网格地图，并在六类空间任务中评估其表现，研究揭示了探索策略、记忆表示和推理方案对空间理解的不同影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 地图环境是表示空间结构的基本媒介，理解基础模型代理如何在其中理解和行动对于实现可靠的基于地图的推理和应用至关重要。然而，现有的空间能力评估大多依赖静态地图输入或文本查询，忽视了空间理解的交互性和经验驱动性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个交互式评估框架，以系统地研究基础模型代理在符号地图环境中的探索、记忆和推理过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 代理在部分可观测的网格地图（包含道路、交叉口和兴趣点）中逐步探索，仅接收局部观测；随后在六种空间任务中评估其空间理解；通过在多种基础模型上系统变换探索策略、记忆表示和推理方案，分析各组件的功能作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 探索策略主要影响经验获取，对最终推理准确率影响有限；记忆表示起核心作用，结构化记忆（如顺序和图结构）显著提升路径规划等结构密集任务的表现；推理方案决定如何利用存储的空间知识，先进提示支持更有效的多步推理；空间推理性能在模型版本和规模达到一定阈值后趋于饱和，说明提升地图空间理解需要针对空间表示和推理的机制，而非单纯扩大模型规模。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 要提升基于地图的空间理解，需要开发针对空间表示和推理的专门机制，而不是仅靠模型规模的扩大。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 地图环境是表示空间结构的基本媒介。了解基础模型（FM）代理如何在此类环境中理解和行动，对于实现可靠的基于地图的推理和应用至关重要。然而，现有的大多数空间能力评估依赖于静态地图输入或基于文本的查询，忽视了空间理解的交互性和经验驱动性。在本文中，我们提出了一个交互式评估框架，用以分析FM代理在符号地图环境中的探索、记忆和推理。代理在由道路、交叉口和兴趣点组成的部分可观测网格地图中逐步探索，每一步仅接收局部观测。随后使用六种空间任务评估其空间理解。通过在多种基础模型上系统地变换探索策略、记忆表示和推理方案，我们揭示了这些组件的不同功能角色。探索主要影响经验获取，但对最终推理准确率影响有限。相反，记忆表示在巩固空间经验方面起核心作用，结构化记忆（尤其是顺序和图结构表示）显著提升了路径规划等结构密集任务的表现。推理方案进一步塑造了存储空间知识的使用方式，先进提示支持更有效的多步推理。我们还观察到，空间推理性能在模型版本和规模达到一定能力阈值后趋于饱和，这表明提升基于地图的空间理解需要针对空间表示和推理的机制，而非仅靠规模扩展。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.&lt;/p&gt;</description></item><item><guid>2512.24511v1</guid><title>Understanding LLM Checkpoint/Restore I/O Strategies and Patterns</title><link>http://arxiv.org/abs/2512.24511v1</link><author>Mikaila J. Gossman, Avinash Maurya, Bogdan Nicolae, Jon C. Calhoun</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 随着大语言模型和基础模型规模的扩大，检查点/恢复已成为训练和推理的关键模式。检查点涉及许多进程，每个进程管理多个形状和大小不同的张量，必须频繁地持久化到稳定存储（例如，并行文件系统）。这使得检查点/恢复成为一个大数据I/O问题，其特点是容量、种类和速度。工作流程必须遍历整个存储堆栈——从GPU内存通过主机内存和本地存储到外部存储库——其层级在性能上相差几个数量级，即使在异步刷新/预取的情况下也会在并发下产生瓶颈。与POSIX相比，内核加速的I/O库（如liburing）可能缓解这些问题，但它们在LLM检查点中的有效性仍需探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大语言模型和基础模型规模的扩大，检查点/恢复已成为训练和推理的关键模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 开发微基准测试来量化使用liburing时的权衡，评估聚合、对齐和I/O合并在缓冲和直接I/O下的交互。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 未合并的小缓冲操作相对于合成工作负载的吞吐量减半，而文件系统感知聚合恢复了带宽并减少了元数据开销。与最先进的LLM检查点引擎相比，我们的方法比DataStates-LLM的写入吞吐量高3.9倍，比TorchSnapshot高7.6倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 这些结果突出了与现代文件系统和I/O后端保持一致的聚合和合并策略的必要性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着大语言模型和基础模型规模的扩大，检查点/恢复已成为训练和推理的关键模式。检查点涉及许多进程，每个进程管理多个形状和大小不同的张量，必须频繁地持久化到稳定存储（例如，并行文件系统）。这使得检查点/恢复成为一个大数据I/O问题，其特点是容量、种类和速度。工作流程必须遍历整个存储堆栈——从GPU内存通过主机内存和本地存储到外部存储库——其层级在性能上相差几个数量级，即使在异步刷新/预取的情况下也会在并发下产生瓶颈。与POSIX相比，内核加速的I/O库（如liburing）可能缓解这些问题，但它们在LLM检查点中的有效性仍需探索。开发微基准测试来量化使用liburing时的权衡，评估聚合、对齐和I/O合并在缓冲和直接I/O下的交互。未合并的小缓冲操作相对于合成工作负载的吞吐量减半，而文件系统感知聚合恢复了带宽并减少了元数据开销。与最先进的LLM检查点引擎相比，我们的方法比DataStates-LLM的写入吞吐量高3.9倍，比TorchSnapshot高7.6倍。这些结果突出了与现代文件系统和I/O后端保持一致的聚合和合并策略的必要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.&lt;/p&gt;</description></item><item><guid>2512.24513v1</guid><title>From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting</title><link>http://arxiv.org/abs/2512.24513v1</link><author>Zhiwei Wei, Mengzi Zhang, Boyan Lu, Zhitao Deng, Nai Yang, Hua Liao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究探讨了动态元素如行人和车辆对城市感知的影响，通过构建带和不带动态元素的街景图像对，进行感知实验和机器学习分析，发现移除动态元素会显著降低感知的活力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的城市分析研究通常将城市场景视为静态，忽略了动态元素如行人和车辆的作用，这可能导致感知偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一个控制框架，通过构建带和不带动态元素的街景图像对，研究动态元素对城市感知的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用语义分割和MLLM引导的生成性修复技术构建图像对，进行感知实验，并训练机器学习模型分析多模态视觉特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 移除动态元素导致感知活力降低30.97%，其他维度变化较小；光照条件、人类存在和深度变化是导致感知变化的关键因素；65%的参与者对活力感知有显著变化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于静态图像的城市感知评估可能低估城市的活力，动态元素对城市感知有显著影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解城市感知从街景图像已成为城市分析和以人为本的城市设计中的核心主题。然而，大多数现有研究将城市场景视为静态，很大程度上忽略了行人、车辆等动态元素的作用，这引发了基于感知的城市分析中潜在偏差的担忧。为了解决这个问题，我们提出一个控制框架，通过构建带和不带行人和车辆的街景图像对，使用语义分割和MLLM引导的生成性修复技术，隔离动态元素的感知效应。基于中国东莞的720对图像，进行感知实验，参与者评估原始和编辑场景的六个感知维度。结果表明，移除动态元素导致感知活力一致降低30.97%，其他维度变化较小且异质；为了进一步探索潜在机制，我们使用多模态视觉特征训练了11个机器学习模型，发现光照条件、人类存在和深度变化是导致感知变化的关键因素。在个体层面，65%的参与者对活力感知有显著变化，而其他维度为35-50%；性别对安全感知有边缘调节作用。除了控制实验，训练的模型被扩展到城市规模的数据集，以预测移除动态元素后的活力变化。城市级别的结果表明，这种感知变化是普遍存在的，具有空间结构，影响73.7%的位置和32.1%的图像，表明仅基于静态图像的城市感知评估可能严重低估城市的活力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有研究大多将城市场景视为静态，忽略了动态元素（如行人和车辆）对城市感知的影响。这个问题在现实中很重要，因为动态元素实际上显著影响人们对城市的感知，而忽略它们可能导致城市分析中的偏见。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有工作，如使用街景图像、语义分割和机器学习来研究城市感知。他们设计了通过生成式填充去除动态元素的方法，并构建了配对街景图像进行感知实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过控制实验来隔离动态元素对城市感知的影响。整体流程包括收集街景图像，使用语义分割和生成式填充去除动态元素，然后进行感知实验，最后分析结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括使用生成式填充技术去除动态元素，并进行配对图像的感知实验。与之前工作不同，他们特别关注动态元素对城市感知的影响，并进行了大规模的实验和分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过实验和分析揭示了动态元素对城市感知的重要影响，强调了在大型感知驱动城市研究中考虑瞬时城市特征的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding urban perception from street view imagery has become a central topic in urban analytics and human centered urban design. However, most existing studies treat urban scenes as static and largely ignore the role of dynamic elements such as pedestrians and vehicles, raising concerns about potential bias in perception based urban analysis. To address this issue, we propose a controlled framework that isolates the perceptual effects of dynamic elements by constructing paired street view images with and without pedestrians and vehicles using semantic segmentation and MLLM guided generative inpainting. Based on 720 paired images from Dongguan, China, a perception experiment was conducted in which participants evaluated original and edited scenes across six perceptual dimensions. The results indicate that removing dynamic elements leads to a consistent 30.97% decrease in perceived vibrancy, whereas changes in other dimensions are more moderate and heterogeneous. To further explore the underlying mechanisms, we trained 11 machine learning models using multimodal visual features and identified that lighting conditions, human presence, and depth variation were key factors driving perceptual change. At the individual level, 65% of participants exhibited significant vibrancy changes, compared with 35-50% for other dimensions; gender further showed a marginal moderating effect on safety perception. Beyond controlled experiments, the trained model was extended to a city-scale dataset to predict vibrancy changes after the removal of dynamic elements. The city level results reveal that such perceptual changes are widespread and spatially structured, affecting 73.7% of locations and 32.1% of images, suggesting that urban perception assessments based solely on static imagery may substantially underestimate urban liveliness.&lt;/p&gt;</description></item><item><guid>2512.24513v2</guid><title>From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes via MLLM-Guided Generative Inpainting</title><link>http://arxiv.org/abs/2512.24513v2</link><author>Zhiwei Wei, Mengzi Zhang, Boyan Lu, Zhitao Deng, Nai Yang, Hua Liao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了城市街景图像中动态元素对人类感知的影响，提出了对比实验框架，并通过机器学习模型揭示了光照、人类存在和深度变化是主要驱动因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 城市感知研究通常忽视行人和车辆等动态元素，可能导致偏差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过对比有无动态元素的街景图像，评估动态元素对感知维度的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构造720对对比图像，进行感知实验，训练11个多模态机器学习模型，扩展至城市级数据预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 去除动态元素导致活力感下降30.97%，其他维度变化较小；光照、人类存在和深度是关键因素；在城市范围内，73.7%地点受影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 仅基于静态图像的城市感知评估会低估城市活力，动态元素对感知具有显著影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 理解城市街景图像中的城市感知已成为城市分析和以人为中心的城市设计的核心议题。然而，大多数现有研究将城市场景视为静态，忽略了行人和车辆等动态元素的作用，导致基于感知的分析可能存在偏差。为解决此问题，我们提出了一个受控框架，通过语义分割和MLLM引导的生成性修补，构建了有无行人和车辆的配对街景图像。基于中国东莞的720对配对图像，我们进行了一项感知实验，参与者在六个感知维度上评估原始和编辑后的场景。结果表明，去除动态元素导致感知活力持续下降30.97%，而其他维度的变化更为温和且异质。为进一步探究机制，我们使用多模态视觉特征训练了11个机器学习模型，并发现光照条件、人类存在和深度变化是驱动感知变化的关键因素。在个体层面，65%的参与者表现出显著的活力变化，其他维度为35-50%；性别对安全感知有轻微的调节作用。除了受控实验外，我们将训练好的模型扩展到城市级数据集，预测去除动态元素后活力的变化。城市级结果显示，这种感知变化普遍且空间结构化，影响了73.7%的地点和32.1%的图像，表明仅基于静态图像的城市感知评估可能大幅低估城市的活力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Understanding urban perception from street view imagery has become a central topic in urban analytics and human centered urban design. However, most existing studies treat urban scenes as static and largely ignore the role of dynamic elements such as pedestrians and vehicles, raising concerns about potential bias in perception based urban analysis. To address this issue, we propose a controlled framework that isolates the perceptual effects of dynamic elements by constructing paired street view images with and without pedestrians and vehicles using semantic segmentation and MLLM guided generative inpainting. Based on 720 paired images from Dongguan, China, a perception experiment was conducted in which participants evaluated original and edited scenes across six perceptual dimensions. The results indicate that removing dynamic elements leads to a consistent 30.97% decrease in perceived vibrancy, whereas changes in other dimensions are more moderate and heterogeneous. To further explore the underlying mechanisms, we trained 11 machine learning models using multimodal visual features and identified that lighting conditions, human presence, and depth variation were key factors driving perceptual change. At the individual level, 65% of participants exhibited significant vibrancy changes, compared with 35-50% for other dimensions; gender further showed a marginal moderating effect on safety perception. Beyond controlled experiments, the trained model was extended to a city-scale dataset to predict vibrancy changes after the removal of dynamic elements. The city level results reveal that such perceptual changes are widespread and spatially structured, affecting 73.7% of locations and 32.1% of images, suggesting that urban perception assessments based solely on static imagery may substantially underestimate urban liveliness.&lt;/p&gt;</description></item><item><guid>2512.24532v1</guid><title>From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning</title><link>http://arxiv.org/abs/2512.24532v1</link><author>Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 在大型语言模型（LLMs）中进行空间推理的研究由于在导航和规划中的应用而受到越来越多的关注。尽管LLMs具有强大的通用语言能力，但它们在结构化环境中的空间变换和多步规划方面仍然存在困难。我们提出了一种两阶段方法，将空间推理分解为原子构建块及其组合。首先，我们对基本的空间变换（如旋转、平移和缩放）进行监督微调，以使模型具备基本的物理空间知识。然后，我们冻结这个具有物理知识的模型，并在GRPO框架内训练轻量级的LoRA适配器，以在基于谜题的环境中通过闭环方式学习组合这些构建块的多步规划策略。为了支持这一流程，我们合成了一个ASCII艺术数据集，并构建了一个相应的基于ASCII的强化学习环境。我们的方法在动态环境（具有明确的状态更新）和静态环境（模型必须依赖其内部状态跨步骤）中始终优于基线，包括通用主干、具有物理知识的模型和端到端RL模型。此外，与从头开始进行端到端强化学习相比，所提出的方法收敛更快，训练更稳定。最后，我们分析了注意力模式，以评估微调是否在空间理解方面带来了有意义的改进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; We analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在大型语言模型（LLMs）中进行空间推理的研究由于在导航和规划中的应用而受到越来越多的关注。尽管LLMs具有强大的通用语言能力，但它们在结构化环境中的空间变换和多步规划方面仍然存在困难。我们提出了一种两阶段方法，将空间推理分解为原子构建块及其组合。首先，我们对基本的空间变换（如旋转、平移和缩放）进行监督微调，以使模型具备基本的物理空间知识。然后，我们冻结这个具有物理知识的模型，并在GRPO框架内训练轻量级的LoRA适配器，以在基于谜题的环境中通过闭环方式学习组合这些构建块的多步规划策略。为了支持这一流程，我们合成了一个ASCII艺术数据集，并构建了一个相应的基于ASCII的强化学习环境。我们的方法在动态环境（具有明确的状态更新）和静态环境（模型必须依赖其内部状态跨步骤）中始终优于基线，包括通用主干、具有物理知识的模型和端到端RL模型。此外，与从头开始进行端到端强化学习相比，所提出的方法收敛更快，训练更稳定。最后，我们分析了注意力模式，以评估微调是否在空间理解方面带来了有意义的改进。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大型语言模型（LLMs）在空间推理和多步规划方面的不足。这个问题在现实或研究中很重要，因为空间推理能力对于机器人导航、语言导航任务等应用至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过将空间推理分解为原子构建块及其组合来设计这个方法，借鉴了现有工作，如使用监督微调来学习基本空间变换，并应用强化学习来优化这些构建块的组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将空间理解分解为一系列基本变换，并使用监督微调来学习这些变换，然后通过强化学习来优化这些变换的组合。整体实现流程包括两个阶段：监督微调阶段和强化学习阶段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括将空间推理分解为原子构建块，使用ASCII艺术数据集和强化学习环境，以及通过LoRA适配器来优化策略。相比之前的工作，这个方法更注重构建块的组合和强化学习的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合监督微调和强化学习的新型空间推理方法，显著提升了LLMs在多步空间规划任务中的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.&lt;/p&gt;</description></item><item><guid>2512.24542v1</guid><title>A Graph Neural Network with Auxiliary Task Learning for Missing PMU Data Reconstruction</title><link>http://arxiv.org/abs/2512.24542v1</link><author>Bo Li, Zijun Chen, Haiwang Zhong, Di Cao, Guangchun Ruan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 在广域测量系统（WAMS）中，相量测量单元（PMU）测量容易由于硬件故障、通信延迟和网络攻击而出现数据缺失。现有的数据驱动方法在电力系统中适应性有限，在高缺失率下鲁棒性差，并且依赖于不现实的完全系统可观测性假设。因此，本文提出了一种辅助任务学习（ATL）方法来重建缺失的PMU数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在广域测量系统（WAMS）中，相量测量单元（PMU）测量容易由于硬件故障、通信延迟和网络攻击而出现数据缺失。现有的数据驱动方法在电力系统中适应性有限，在高缺失率下鲁棒性差，并且依赖于不现实的完全系统可观测性假设。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种辅助任务学习（ATL）方法来重建缺失的PMU数据，以克服现有方法的局限性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先，提出一个K跳图神经网络（GNN）来直接在由PMU节点组成的子图上进行学习，克服不完全可观测系统的限制。然后，设计了一个由两个互补图网络组成的辅助学习框架，用于精确重建：一个时空GNN从PMU数据中提取时空依赖性来重建缺失值，另一个辅助GNN利用PMU数据的低秩特性实现无监督在线学习。通过这种方式，PMU数据的低秩特性在整个架构中动态利用，以确保鲁棒性和自适应性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 数值结果表明，所提出的方法在高缺失率和不可观测性下具有优越的离线和在线性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的辅助任务学习（ATL）方法能够有效地重建缺失的PMU数据，并在高缺失率和不可观测性下表现出优越的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在广域测量系统（WAMS）中，相量测量单元（PMU）测量容易由于硬件故障、通信延迟和网络攻击而出现数据缺失。现有的数据驱动方法在电力系统中适应性有限，在高缺失率下鲁棒性差，并且依赖于不现实的完全系统可观测性假设。因此，本文提出了一种辅助任务学习（ATL）方法来重建缺失的PMU数据。首先，提出一个K跳图神经网络（GNN）来直接在由PMU节点组成的子图上进行学习，克服不完全可观测系统的限制。然后，设计了一个由两个互补图网络组成的辅助学习框架，用于精确重建：一个时空GNN从PMU数据中提取时空依赖性来重建缺失值，另一个辅助GNN利用PMU数据的低秩特性实现无监督在线学习。通过这种方式，PMU数据的低秩特性在整个架构中动态利用，以确保鲁棒性和自适应性。数值结果表明，所提出的方法在高缺失率和不可观测性下具有优越的离线和在线性能。所提出的辅助任务学习（ATL）方法能够有效地重建缺失的PMU数据，并在高缺失率和不可观测性下表现出优越的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In wide-area measurement systems (WAMS), phasor measurement unit (PMU) measurement is prone to data missingness due to hardware failures, communication delays, and cyber-attacks. Existing data-driven methods are limited by inadaptability to concept drift in power systems, poor robustness under high missing rates, and reliance on the unrealistic assumption of full system observability. Thus, this paper proposes an auxiliary task learning (ATL) method for reconstructing missing PMU data. First, a K-hop graph neural network (GNN) is proposed to enable direct learning on the subgraph consisting of PMU nodes, overcoming the limitation of the incompletely observable system. Then, an auxiliary learning framework consisting of two complementary graph networks is designed for accurate reconstruction: a spatial-temporal GNN extracts spatial-temporal dependencies from PMU data to reconstruct missing values, and another auxiliary GNN utilizes the low-rank property of PMU data to achieve unsupervised online learning. In this way, the low-rank properties of the PMU data are dynamically leveraged across the architecture to ensure robustness and self-adaptation. Numerical results demonstrate the superior offline and online performance of the proposed method under high missing rates and incomplete observability.&lt;/p&gt;</description></item><item><guid>2512.24556v1</guid><title>Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs</title><link>http://arxiv.org/abs/2512.24556v1</link><author>Muhammad Abdullahi Said, Muhammad Sammani Sani</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 大型语言模型在关键全球基础设施中的整合，使得安全对齐从英语到其他语言的零样本迁移假设成为一个危险的盲点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大型语言模型（LLMs）集成到关键全球基础设施中，安全对齐从英语到其他语言的零样本迁移假设仍然是一个危险的盲点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本研究对三个最先进的模型（GPT-5.1、Gemini 3 Pro 和 Claude 4.5 Opus）进行系统审计，使用 HausaSafety 数据集，该数据集基于西非威胁场景（例如，Yahoo-Yahoo 欺诈、Dane 枪支制造）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用 2 x 4 因子设计，在 1,440 次评估中测试语言（英语与豪萨语）和时间框架的非线性相互作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 研究结果挑战了现有的多语言安全差距叙述。我们发现了复杂干扰机制，其中安全性由变量的交集决定。Claude 4.5 Opus 在豪萨语中表现显著更安全（45.0%）而不是英语（36.7%），由于不确定性驱动的拒绝，它在时间推理中遭受了灾难性失败。我们报告了深刻的时间不对称性，过去时态框架绕过了防御（15.6% 安全），而未来时态场景触发了过度保守的拒绝（57.2% 安全）。这种波动性的幅度通过最安全和最脆弱配置之间的 9.2 倍差异得到说明，证明安全性不是一个固定的属性，而是一个依赖于上下文的状态。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 当前模型依赖于肤浅的启发式方法而不是强大的语义理解，创造了安全口袋，使全球南方的用户暴露于本地化危害。我们提出了不变对齐作为确保跨语言和时间变化的安全稳定性的必要范式转变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 随着大型语言模型（LLMs）集成到关键全球基础设施中，安全对齐从英语到其他语言的零样本迁移假设仍然是一个危险的盲点。本研究对三个最先进的模型（GPT-5.1、Gemini 3 Pro 和 Claude 4.5 Opus）进行系统审计，使用 HausaSafety 数据集，该数据集基于西非威胁场景（例如，Yahoo-Yahoo 欺诈、Dane 枪支制造）。采用 2 x 4 因子设计，在 1,440 次评估中测试语言（英语与豪萨语）和时间框架的非线性相互作用。研究结果挑战了现有的多语言安全差距叙述。我们发现了复杂干扰机制，其中安全性由变量的交集决定。Claude 4.5 Opus 在豪萨语中表现显著更安全（45.0%）而不是英语（36.7%），由于不确定性驱动的拒绝，它在时间推理中遭受了灾难性失败。我们报告了深刻的时间不对称性，过去时态框架绕过了防御（15.6% 安全），而未来时态场景触发了过度保守的拒绝（57.2% 安全）。这种波动性的幅度通过最安全和最脆弱配置之间的 9.2 倍差异得到说明，证明安全性不是一个固定的属性，而是一个依赖于上下文的状态。当前模型依赖于肤浅的启发式方法而不是强大的语义理解，创造了安全口袋，使全球南方的用户暴露于本地化危害。我们提出了不变对齐作为确保跨语言和时间变化的安全稳定性的必要范式转变。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.&lt;/p&gt;</description></item><item><guid>2512.24564v1</guid><title>CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts</title><link>http://arxiv.org/abs/2512.24564v1</link><author>Shunbo Jia, Caizhi Liao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 深度学习模型在心电图诊断中取得了显著的准确性，但对平滑对抗扰动（SAP）等模仿生物形态的对抗扰动表现出脆弱性。现有的防御方法面临一个关键困境：对抗训练（AT）提供了鲁棒性但带来了巨大的计算负担，而随机平滑（RS）等认证方法引入了显著的推理延迟，使其不适用于实时临床监测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度学习模型在心电图诊断中表现出色，但对平滑对抗扰动（SAP）等模仿生物形态的对抗扰动敏感。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决深度学习模型对平滑对抗扰动（SAP）的脆弱性问题，并提出一种兼具鲁棒性、效率和临床可解释性的方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出因果生理表示学习（CPR），通过结构因果模型（SCM）模拟心电图生成，严格分离不变病理形态（P-QRS-T复合波）和非因果伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; CPR在PTB-XL数据集上显著优于标准临床预处理方法，在SAP攻击下F1分数达到0.632，比中值平滑（0.541 F1）高9.1%，同时匹配了随机平滑的认证鲁棒性并保持了单次推理效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CPR在鲁棒性、效率和临床可解释性之间提供了优越的权衡，为实时临床监测提供了实用的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 深度学习模型在心电图诊断中取得了显著的准确性，但对平滑对抗扰动（SAP）等模仿生物形态的对抗扰动表现出脆弱性。现有的防御方法面临一个关键困境：对抗训练（AT）提供了鲁棒性但带来了巨大的计算负担，而随机平滑（RS）等认证方法引入了显著的推理延迟，使其不适用于实时临床监测。提出因果生理表示学习（CPR），通过结构因果模型（SCM）模拟心电图生成，严格分离不变病理形态（P-QRS-T复合波）和非因果伪影。CPR在PTB-XL数据集上显著优于标准临床预处理方法，在SAP攻击下F1分数达到0.632，比中值平滑（0.541 F1）高9.1%，同时匹配了随机平滑的认证鲁棒性并保持了单次推理效率，为实时临床监测提供了实用的解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Deep learning models for Electrocardiogram (ECG) diagnosis have achieved remarkable accuracy but exhibit fragility against adversarial perturbations, particularly Smooth Adversarial Perturbations (SAP) that mimic biological morphology. Existing defenses face a critical dilemma: Adversarial Training (AT) provides robustness but incurs a prohibitive computational burden, while certified methods like Randomized Smoothing (RS) introduce significant inference latency, rendering them impractical for real-time clinical monitoring. We posit that this vulnerability stems from the models&amp;#x27; reliance on non-robust spurious correlations rather than invariant pathological features. To address this, we propose Causal Physiological Representation Learning (CPR). Unlike standard denoising approaches that operate without semantic constraints, CPR incorporates a Physiological Structural Prior within a causal disentanglement framework. By modeling ECG generation via a Structural Causal Model (SCM), CPR enforces a structural intervention that strictly separates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts. Empirical results on PTB-XL demonstrate that CPR significantly outperforms standard clinical preprocessing methods. Specifically, under SAP attacks, CPR achieves an F1 score of 0.632, surpassing Median Smoothing (0.541 F1) by 9.1%. Crucially, CPR matches the certified robustness of Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off between robustness, efficiency, and clinical interpretability.&lt;/p&gt;</description></item><item><guid>2512.24593v1</guid><title>3D Semantic Segmentation for Post-Disaster Assessment</title><link>http://arxiv.org/abs/2512.24593v1</link><author>Nhut Le, Maryam Rahnemoonfar</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。3D语义分割对于灾害后的评估至关重要，但现有的深度学习模型缺乏专门针对灾害后环境的数据库。为了解决这个问题，我们使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。我们评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现，揭示了现有方法在受灾地区的重大局限性。这些发现强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决现有的深度学习模型缺乏专门针对灾害后环境的数据库的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 现有方法在受灾地区的重大局限性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自然灾害的频率不断增加，对人类生命和造成巨大的经济损失。3D语义分割对于灾害后的评估至关重要，但现有的深度学习模型缺乏专门针对灾害后环境的数据库。为了解决这个问题，我们使用无人机拍摄的飓风伊恩（2022年）在受灾地区的航拍图像构建了一个专门的3D数据库，并采用Structure-from-Motion（SfM）和多视图立体（MVS）技术来重建3D点云。我们评估了最先进的3D语义分割模型，Fast Point Transformer（FPT）、Point Transformer v3（PTv3）和OA-CNNs在这个数据库上的表现，揭示了现有方法在受灾地区的重大局限性。这些发现强调了3D分割技术进步和专门3D基准数据库开发的迫切需要，以改善灾害后场景理解和响应。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有深度学习模型缺乏针对灾后环境设计的3D语义分割数据集的问题。这个问题在现实中非常重要，因为准确的灾后评估对于救援行动和减少经济损失至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的3D重建技术和深度学习模型，设计了一个针对灾后环境的3D语义分割数据集。他们使用了无人机拍摄的数据，并应用了Structure-from-Motion和Multi-View Stereo技术来重建3D点云。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是创建一个专门针对灾后环境的3D语义分割数据集，并评估现有深度学习模型在该数据集上的性能。整体流程包括数据收集、3D点云重建、生成地面真实验证标签，以及使用Fast Point Transformer、Point Transformer v3和OA-CNNs模型进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点在于构建了一个专门针对灾后环境的3D语义分割数据集，并评估了现有深度学习模型在该数据集上的性能。与之前的工作相比，这篇论文强调了现有模型在灾后环境中的局限性，并提出了改进的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建一个针对灾后环境的3D语义分割数据集，揭示了现有深度学习模型在该场景下的局限性，并强调了改进3D语义分割技术的重要性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The increasing frequency of natural disasters poses severe threats to human lives and leads to substantial economic losses. While 3D semantic segmentation is crucial for post-disaster assessment, existing deep learning models lack datasets specifically designed for post-disaster environments. To address this gap, we constructed a specialized 3D dataset using unmanned aerial vehicles (UAVs)-captured aerial footage of Hurricane Ian (2022) over affected areas, employing Structure-from-Motion (SfM) and Multi-View Stereo (MVS) techniques to reconstruct 3D point clouds. We evaluated the state-of-the-art (SOTA) 3D semantic segmentation models, Fast Point Transformer (FPT), Point Transformer v3 (PTv3), and OA-CNNs on this dataset, exposing significant limitations in existing methods for disaster-stricken regions. These findings underscore the urgent need for advancements in 3D segmentation techniques and the development of specialized 3D benchmark datasets to improve post-disaster scene understanding and response.&lt;/p&gt;</description></item><item><guid>2512.24603v1</guid><title>Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers</title><link>http://arxiv.org/abs/2512.24603v1</link><author>Zheng Liu, Jinchao Zhu, Gao Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为协同低秩适应（CLoRA）的新颖微调方法，旨在提高预训练视觉变换器在下游任务中的参数效率和性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的低秩适应（LoRA）方法在微调预训练视觉变换器方面取得了显著成功，但它们在参数效率和性能之间难以取得平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决这一问题，本文提出CLoRA，以在保持参数效率的同时提高低秩模块的学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CLoRA由基础空间共享和样本无关多样性增强（SADE）两个部分组成。基础空间共享允许所有低秩模块共享一组下/上投影空间，而SADE则通过正则化相似性来鼓励训练过程中的多样性表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，与现有方法相比，CLoRA在参数效率和性能之间取得了更好的平衡，并且在点云分析中所需的GFLOPs最少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CLoRA是一种有效的微调方法，能够在保持参数效率的同时提高预训练视觉变换器的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种名为协同低秩适应（CLoRA）的新颖微调方法，旨在提高预训练视觉变换器在下游任务中的参数效率和性能。现有的低秩适应（LoRA）方法在微调预训练视觉变换器方面取得了显著成功，但它们在参数效率和性能之间难以取得平衡。为了解决这一问题，本文提出CLoRA，以在保持参数效率的同时提高低秩模块的学习能力。CLoRA由基础空间共享和样本无关多样性增强（SADE）两个部分组成。基础空间共享允许所有低秩模块共享一组下/上投影空间，而SADE则通过正则化相似性来鼓励训练过程中的多样性表示。实验结果表明，与现有方法相比，CLoRA在参数效率和性能之间取得了更好的平衡，并且在点云分析中所需的GFLOPs最少。CLoRA是一种有效的微调方法，能够在保持参数效率的同时提高预训练视觉变换器的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Low-rank adaptation (LoRA) has achieved remarkable success in fine-tuning pre-trained vision transformers for various downstream tasks. Existing studies mainly focus on exploring more parameter-efficient strategies or more effective representation learning schemes. However, these methods either sacrifice fine-tuning performance or introduce excessive trainable parameters, failing to strike a balance between learning performance and parameter efficiency. To address this problem, we propose a novel tuning method named collaborative low-rank adaptation (CLoRA) in this paper. CLoRA consists of base-space sharing and sample-agnostic diversity enhancement (SADE) components. To maintain parameter efficiency while expanding the learning capacity of low-rank modules (LRMs), base-space sharing allows all LRMs to share a set of down/up-projection spaces. In CLoRA, the low-rank matrices obtained from the shared spaces collaboratively construct each LRM. Since the representations extracted by these matrices may contain redundant information, SADE is employed to regularize the similarities among them to encourage diverse representations in the training process. We conduct extensive experiments on widely used image and point cloud datasets to evaluate the performance of CLoRA. Experimental results demonstrate that CLoRA strikes a better balance between learning performance and parameter efficiency, while requiring the fewest GFLOPs for point cloud analysis, compared with the state-of-the-art methods.&lt;/p&gt;</description></item><item><guid>2512.24605v1</guid><title>MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding</title><link>http://arxiv.org/abs/2512.24605v1</link><author>Panquan Yang, Junfei Huang, Zongzhangbao Yin, Yingsong Hu, Anni Xu, Xinyi Luo, Xueqi Sun, Hai Wu, Sheng Ao, Zhaoxing Zhu, Chenglu Wen, Cheng Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D视觉定位旨在将自然语言句子中的对象定位到3D点云场景中。这对路边基础设施系统在复杂交通环境中解释自然语言和定位相关目标至关重要。然而，大多数现有的数据集和方法都集中在室内和室外驾驶场景，由于缺乏路边基础设施传感器捕获的配对点云-文本数据，室外监控场景仍然未被探索。本文引入了一个新的任务，即室外监控场景的3D视觉定位，它使基础设施级别的交通场景理解超越了自车视角。为了支持这个任务，我们构建了MoniRefer，这是第一个真实世界的路边级3D视觉定位多模态数据集。该数据集包含约136,018个对象和411,128个自然语言表达，收集自多个复杂的交通十字路口。为了确保数据集的质量和准确性，我们对所有语言描述和3D标签进行了人工验证。此外，我们还提出了一种新的端到端方法，名为Moni3DVG，它利用图像提供的丰富外观信息和几何信息以及点云的光学信息进行多模态特征学习和3D对象定位。在提出的基准上的大量实验和消融研究证明了我们方法的优势和有效性。我们的数据集和代码将被发布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大多数现有的3D视觉定位数据集和方法都集中在室内和室外驾驶场景，而室外监控场景由于缺乏配对点云-文本数据而未被探索。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 引入室外监控场景的3D视觉定位任务，并构建一个真实世界的路边级3D视觉定位多模态数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出了一种新的端到端方法Moni3DVG，利用图像和点云的多模态特征学习和3D对象定位。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MoniRefer数据集包含约136,018个对象和411,128个自然语言表达，通过人工验证确保了质量和准确性。Moni3DVG方法在实验中证明了其优势和有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提出的室外监控场景的3D视觉定位任务和Moni3DVG方法为交通场景理解提供了新的途径，并展示了其在真实世界数据集上的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决户外监控场景下的3D视觉定位问题，即如何根据自然语言描述在3D点云场景中定位相关对象。这个问题在现实研究中非常重要，因为对于路边基础设施系统来说，能够解释自然语言并定位复杂交通环境中的相关目标至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D视觉定位研究的局限性，特别是户外监控场景中数据稀缺、语言多样性有限和视角不一致等问题，设计出MoniRefer数据集和Moni3DVG方法。该方法借鉴了现有2D视觉定位和3D视觉定位的研究成果，并针对户外监控场景进行了优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; Moni3DVG方法的核心思想是通过整合图像和点云的多模态特征进行3D对象定位。整体实现流程包括数据采集、多模态特征学习和3D对象定位。首先，使用LiDAR和相机采集多模态数据；然后，提取图像和点云的 appearance、几何和光学信息进行特征学习；最后，利用这些特征匹配自然语言描述，定位目标对象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括构建了第一个大规模户外监控场景的多模态数据集MoniRefer，提出了Moni3DVG方法，并实现了在户外监控场景下的高效3D视觉定位。相比之前的工作，MoniRefer数据集更注重户外场景和语言多样性，Moni3DVG方法则更有效地整合了多模态信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建MoniRefer数据集和提出Moni3DVG方法，为户外监控场景下的3D视觉定位任务提供了新的基准和解决方案。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.&lt;/p&gt;</description></item><item><guid>2512.24617v1</guid><title>Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</title><link>http://arxiv.org/abs/2512.24617v1</link><author>Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 大型语言模型（LLMs）对所有的token应用均匀的计算，尽管语言表现出高度非均匀的信息密度。这种token均匀的计算方式浪费了在局部可预测的跨度上的计算能力，同时对语义关键转换分配的计算不足。我们提出了动态大型概念模型（DLCM），这是一个分层语言建模框架，它从潜在表示中学习语义边界，并将计算从token转移到压缩的概念空间，其中推理更高效。DLCM端到端地发现可变长度的概念，而不依赖于预定义的语言单位。分层压缩从根本上改变了扩展行为。我们引入了第一个压缩感知的扩展定律，它将token级别的容量、概念级别的推理容量和压缩率分离，从而在固定的FLOPs下实现原则性的计算分配。为了稳定地训练这种异构架构，我们进一步开发了一种解耦的μP参数化方法，它支持跨宽度和压缩状态的零样本超参数迁移。在一个实际设置（R=4，对应于每个概念平均四个token）下，DLCM将大约三分之一的推理计算重新分配到一个更高容量的推理主干，在匹配的推理FLOPs下，在12个零样本基准测试中实现了平均+2.69%的提高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型（LLMs）对所有的token应用均匀的计算，尽管语言表现出高度非均匀的信息密度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出动态大型概念模型（DLCM），一个分层语言建模框架，它从潜在表示中学习语义边界，并将计算从token转移到压缩的概念空间，其中推理更高效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DLCM端到端地发现可变长度的概念，而不依赖于预定义的语言单位。分层压缩从根本上改变了扩展行为。引入了第一个压缩感知的扩展定律，它将token级别的容量、概念级别的推理容量和压缩率分离，从而在固定的FLOPs下实现原则性的计算分配。开发了一种解耦的μP参数化方法，它支持跨宽度和压缩状态的零样本超参数迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DLCM将大约三分之一的推理计算重新分配到一个更高容量的推理主干，在匹配的推理FLOPs下，在12个零样本基准测试中实现了平均+2.69%的提高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DLCM能够更有效地进行推理，通过将计算从token转移到压缩的概念空间，实现了更好的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型（LLMs）对所有的token应用均匀的计算，尽管语言表现出高度非均匀的信息密度。这种token均匀的计算方式浪费了在局部可预测的跨度上的计算能力，同时对语义关键转换分配的计算不足。我们提出了动态大型概念模型（DLCM），这是一个分层语言建模框架，它从潜在表示中学习语义边界，并将计算从token转移到压缩的概念空间，其中推理更高效。DLCM端到端地发现可变长度的概念，而不依赖于预定义的语言单位。分层压缩从根本上改变了扩展行为。我们引入了第一个压缩感知的扩展定律，它将token级别的容量、概念级别的推理容量和压缩率分离，从而在固定的FLOPs下实现原则性的计算分配。为了稳定地训练这种异构架构，我们进一步开发了一种解耦的μP参数化方法，它支持跨宽度和压缩状态的零样本超参数迁移。在一个实际设置（R=4，对应于每个概念平均四个token）下，DLCM将大约三分之一的推理计算重新分配到一个更高容量的推理主干，在匹配的推理FLOPs下，在12个零样本基准测试中实现了平均+2.69%的提高。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.&lt;/p&gt;</description></item><item><guid>2512.24643v1</guid><title>A Scalable Framework for logP Prediction: From Terabyte-Scale Data Integration to Interpretable Ensemble Modeling</title><link>http://arxiv.org/abs/2512.24643v1</link><author>Malikussaid, Septian Caesar Floresko, Ade Romadhony, Isman Kurniawan, Warih Maharani, Hilal Hudan Nuha</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了一种大规模预测模型框架，用于预测logP值，使用了从PubChem、ChEMBL和eMolecules三个权威化学数据库中严格筛选的426850种生物活性化合物。我们开发了一种新的计算基础设施来解决数据集成问题，通过字节偏移索引架构，将处理时间从预计超过100天减少到3.2小时，提高了740倍。我们的综合分析揭示了亲脂性的多元性质：虽然分子量与logP呈弱双变量相关性，但SHAP分析在集成模型上确定其为全球最重要的预测因子。我们系统地评估了多种建模方法，发现线性模型存在内在的异方差性，传统的补救策略，包括加权最小二乘法和Box-Cox变换，未能解决这一问题。基于树的集成方法，包括随机森林和XGBoost，被证明对这种违反具有内在的鲁棒性，在测试集上实现了0.765的R平方和0.731的RMSE logP单位。此外，一种分层建模策略，采用专门模型处理药物样分子（数据集的91%）和极端情况（9%），实现了最佳性能：药物样子集的RMSE为0.838，极端分子的R平方为0.767，是所有评估方法中表现最好的。这些发现为分子设计提供了可行的指导，为仅使用2D描述符的亲脂性预测建立了稳健的基准，并证明了精心策划的基于描述符的集成模型与最先进的图神经网络架构具有竞争力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本研究提出了一种大规模预测模型框架，用于预测logP值，使用了从PubChem、ChEMBL和eMolecules三个权威化学数据库中严格筛选的426850种生物活性化合物。我们开发了一种新的计算基础设施来解决数据集成问题，通过字节偏移索引架构，将处理时间从预计超过100天减少到3.2小时，提高了740倍。我们的综合分析揭示了亲脂性的多元性质：虽然分子量与logP呈弱双变量相关性，但SHAP分析在集成模型上确定其为全球最重要的预测因子。我们系统地评估了多种建模方法，发现线性模型存在内在的异方差性，传统的补救策略，包括加权最小二乘法和Box-Cox变换，未能解决这一问题。基于树的集成方法，包括随机森林和XGBoost，被证明对这种违反具有内在的鲁棒性，在测试集上实现了0.765的R平方和0.731的RMSE logP单位。此外，一种分层建模策略，采用专门模型处理药物样分子（数据集的91%）和极端情况（9%），实现了最佳性能：药物样子集的RMSE为0.838，极端分子的R平方为0.767，是所有评估方法中表现最好的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 本研究提出了一种大规模预测模型框架，用于预测logP值，使用了从PubChem、ChEMBL和eMolecules三个权威化学数据库中严格筛选的426850种生物活性化合物。我们开发了一种新的计算基础设施来解决数据集成问题，通过字节偏移索引架构，将处理时间从预计超过100天减少到3.2小时，提高了740倍。我们的综合分析揭示了亲脂性的多元性质：虽然分子量与logP呈弱双变量相关性，但SHAP分析在集成模型上确定其为全球最重要的预测因子。我们系统地评估了多种建模方法，发现线性模型存在内在的异方差性，传统的补救策略，包括加权最小二乘法和Box-Cox变换，未能解决这一问题。基于树的集成方法，包括随机森林和XGBoost，被证明对这种违反具有内在的鲁棒性，在测试集上实现了0.765的R平方和0.731的RMSE logP单位。此外，一种分层建模策略，采用专门模型处理药物样分子（数据集的91%）和极端情况（9%），实现了最佳性能：药物样子集的RMSE为0.838，极端分子的R平方为0.767，是所有评估方法中表现最好的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本研究提出了一种大规模预测模型框架，用于预测logP值，使用了从PubChem、ChEMBL和eMolecules三个权威化学数据库中严格筛选的426850种生物活性化合物。我们开发了一种新的计算基础设施来解决数据集成问题，通过字节偏移索引架构，将处理时间从预计超过100天减少到3.2小时，提高了740倍。我们的综合分析揭示了亲脂性的多元性质：虽然分子量与logP呈弱双变量相关性，但SHAP分析在集成模型上确定其为全球最重要的预测因子。我们系统地评估了多种建模方法，发现线性模型存在内在的异方差性，传统的补救策略，包括加权最小二乘法和Box-Cox变换，未能解决这一问题。基于树的集成方法，包括随机森林和XGBoost，被证明对这种违反具有内在的鲁棒性，在测试集上实现了0.765的R平方和0.731的RMSE logP单位。此外，一种分层建模策略，采用专门模型处理药物样分子（数据集的91%）和极端情况（9%），实现了最佳性能：药物样子集的RMSE为0.838，极端分子的R平方为0.767，是所有评估方法中表现最好的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本研究提出了一种大规模预测模型框架，用于预测logP值，使用了从PubChem、ChEMBL和eMolecules三个权威化学数据库中严格筛选的426850种生物活性化合物。我们开发了一种新的计算基础设施来解决数据集成问题，通过字节偏移索引架构，将处理时间从预计超过100天减少到3.2小时，提高了740倍。我们的综合分析揭示了亲脂性的多元性质：虽然分子量与logP呈弱双变量相关性，但SHAP分析在集成模型上确定其为全球最重要的预测因子。我们系统地评估了多种建模方法，发现线性模型存在内在的异方差性，传统的补救策略，包括加权最小二乘法和Box-Cox变换，未能解决这一问题。基于树的集成方法，包括随机森林和XGBoost，被证明对这种违反具有内在的鲁棒性，在测试集上实现了0.765的R平方和0.731的RMSE logP单位。此外，一种分层建模策略，采用专门模型处理药物样分子（数据集的91%）和极端情况（9%），实现了最佳性能：药物样子集的RMSE为0.838，极端分子的R平方为0.767，是所有评估方法中表现最好的。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This study presents a large-scale predictive modeling framework for logP prediction using 426850 bioactive compounds rigorously curated from the intersection of three authoritative chemical databases: PubChem, ChEMBL, and eMolecules. We developed a novel computational infrastructure to address the data integration challenge, reducing processing time from a projected over 100 days to 3.2 hours through byte-offset indexing architecture, a 740-fold improvement. Our comprehensive analysis revealed critical insights into the multivariate nature of lipophilicity: while molecular weight exhibited weak bivariate correlation with logP, SHAP analysis on ensemble models identified it as the single most important predictor globally. We systematically evaluated multiple modeling approaches, discovering that linear models suffered from inherent heteroskedasticity that classical remediation strategies, including weighted least squares and Box-Cox transformation, failed to address. Tree-based ensemble methods, including Random Forest and XGBoost, proved inherently robust to this violation, achieving an R-squared of 0.765 and RMSE of 0.731 logP units on the test set. Furthermore, a stratified modeling strategy, employing specialized models for drug-like molecules (91 percent of dataset) and extreme cases (nine percent), achieved optimal performance: an RMSE of 0.838 for the drug-like subset and an R-squared of 0.767 for extreme molecules, the highest of all evaluated approaches. These findings provide actionable guidance for molecular design, establish robust baselines for lipophilicity prediction using only 2D descriptors, and demonstrate that well-curated, descriptor-based ensemble models remain competitive with state-of-the-art graph neural network architectures.&lt;/p&gt;</description></item><item><guid>2512.24645v1</guid><title>AudioFab: Building A General and Intelligent Audio Factory through Tool Learning</title><link>http://arxiv.org/abs/2512.24645v1</link><author>Cheng Zhu, Jing Han, Qianshuai Xue, Kehan Wang, Huan Zhao, Zixing Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 人工智能正在深刻改变音频领域，但许多高级算法和工具仍然分散，缺乏一个统一的、高效的框架来释放它们的全部潜力。现有的音频代理框架通常存在环境配置复杂和工具协作效率低下的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的音频代理框架存在环境配置复杂和工具协作效率低下的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决这些限制，我们引入了AudioFab，这是一个开源的代理框架，旨在建立一个开放、智能的音频处理生态系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; AudioFab采用模块化设计来解决依赖冲突，简化工具集成和扩展。它还通过智能选择和少样本学习优化工具学习，提高复杂音频任务的效率和准确性。此外，AudioFab提供了一个用户友好的自然语言界面，专为非专业用户设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; AudioFab的核心贡献在于提供一个稳定和可扩展的平台，用于未来音频和多模态人工智能的研究和开发。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AudioFab是一个开源的代理框架，旨在建立一个开放、智能的音频处理生态系统，通过模块化设计和智能工具学习优化，提高复杂音频任务的效率和准确性，并提供用户友好的自然语言界面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 当前，人工智能正在深刻改变音频领域；然而，许多高级算法和工具仍然分散，缺乏一个统一的、高效的框架来释放它们的全部潜力。现有的音频代理框架通常存在环境配置复杂和工具协作效率低下的问题。为了解决这些限制，我们引入了AudioFab，这是一个开源的代理框架，旨在建立一个开放、智能的音频处理生态系统。与现有的解决方案相比，AudioFab的模块化设计解决了依赖冲突，简化了工具集成和扩展。它还通过智能选择和少样本学习优化工具学习，提高了复杂音频任务的效率和准确性。此外，AudioFab提供了一个用户友好的自然语言界面，专为非专业用户设计。作为一个基础框架，AudioFab的核心贡献在于提供一个稳定和可扩展的平台，用于未来音频和多模态人工智能的研究和开发。代码可在https://github.com/SmileHnu/AudioFab获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Currently, artificial intelligence is profoundly transforming the audio domain; however, numerous advanced algorithms and tools remain fragmented, lacking a unified and efficient framework to unlock their full potential. Existing audio agent frameworks often suffer from complex environment configurations and inefficient tool collaboration. To address these limitations, we introduce AudioFab, an open-source agent framework aimed at establishing an open and intelligent audio-processing ecosystem. Compared to existing solutions, AudioFab&amp;#x27;s modular design resolves dependency conflicts, simplifying tool integration and extension. It also optimizes tool learning through intelligent selection and few-shot learning, improving efficiency and accuracy in complex audio tasks. Furthermore, AudioFab provides a user-friendly natural language interface tailored for non-expert users. As a foundational framework, AudioFab&amp;#x27;s core contribution lies in offering a stable and extensible platform for future research and development in audio and multimodal AI. The code is available at https://github.com/SmileHnu/AudioFab.&lt;/p&gt;</description></item><item><guid>2512.24665v1</guid><title>HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</title><link>http://arxiv.org/abs/2512.24665v1</link><author>Honglin Gao, Lan Zhao, Junhao Ren, Xiang Li, Gaoxi Xiao</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; Heterogeneous graph neural networks (HGNNs) 在实际应用中表现出色，但针对异构图的定向后门攻击研究较少。本文研究异构节点分类的后门攻击，攻击者在训练过程中注入少量触发节点和连接，迫使特定受害者节点被错误分类为攻击者选择的标签，同时保持清洁性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 异构图神经网络的性能强大，但针对异构图的后门攻击研究不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 研究异构节点分类的后门攻击，通过注入触发节点和连接，迫使特定节点被错误分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出 HeteroHBA 框架，通过显著性筛选选择有影响力的辅助邻居进行触发附件，合成多样化的触发特征和连接模式以更好地匹配局部异构上下文。结合自适应实例归一化 (AdaIN) 和最大均值差异 (MMD) 损失对齐触发特征分布与良性统计数据，减少可检测性，并优化攻击以实现攻击成功和保持清洁准确性的双重目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; HeteroHBA 在多个真实异构图上比先前的后门基线具有更高的攻击成功率，对清洁准确性的影响较小或更小。攻击在异构感知结构防御 CSD 下仍然有效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 异构图学习中存在实际的后门风险，需要更强的防御措施。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 异构图神经网络（HGNNs）在实际应用中表现出色，但针对异构图的定向后门攻击研究较少。本文研究异构节点分类的后门攻击，攻击者在训练过程中注入少量触发节点和连接，迫使特定受害者节点被错误分类为攻击者选择的标签，同时保持清洁性能。提出 HeteroHBA 框架，通过显著性筛选选择有影响力的辅助邻居进行触发附件，合成多样化的触发特征和连接模式以更好地匹配局部异构上下文。结合自适应实例归一化 (AdaIN) 和最大均值差异 (MMD) 损失对齐触发特征分布与良性统计数据，减少可检测性，并优化攻击以实现攻击成功和保持清洁准确性的双重目标。实验表明，HeteroHBA 在多个真实异构图上比先前的后门基线具有更高的攻击成功率，对清洁准确性的影响较小或更小。攻击在异构感知结构防御 CSD 下仍然有效。这些结果突出了异构图学习中的实际后门风险，并推动了更强防御措施的发展。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Heterogeneous graph neural networks (HGNNs) have achieved strong performance in many real-world applications, yet targeted backdoor poisoning on heterogeneous graphs remains less studied. We consider backdoor attacks for heterogeneous node classification, where an adversary injects a small set of trigger nodes and connections during training to force specific victim nodes to be misclassified into an attacker-chosen label at test time while preserving clean performance. We propose HeteroHBA, a generative backdoor framework that selects influential auxiliary neighbors for trigger attachment via saliency-based screening and synthesizes diverse trigger features and connection patterns to better match the local heterogeneous context. To improve stealthiness, we combine Adaptive Instance Normalization (AdaIN) with a Maximum Mean Discrepancy (MMD) loss to align the trigger feature distribution with benign statistics, thereby reducing detectability, and we optimize the attack with a bilevel objective that jointly promotes attack success and maintains clean accuracy. Experiments on multiple real-world heterogeneous graphs with representative HGNN architectures show that HeteroHBA consistently achieves higher attack success than prior backdoor baselines with comparable or smaller impact on clean accuracy; moreover, the attack remains effective under our heterogeneity-aware structural defense, CSD. These results highlight practical backdoor risks in heterogeneous graph learning and motivate the development of stronger defenses.&lt;/p&gt;</description></item><item><guid>2512.24679v1</guid><title>Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions</title><link>http://arxiv.org/abs/2512.24679v1</link><author>Pengcheng Xia, Yixiang Huang, Chengjin Qin, Chengliang Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种多模态跨域混合融合模型，用于智能故障诊断，以提高模型在未见过的工作条件下的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 现有的故障诊断方法在真实世界场景中性能显著下降，而领域适应方法依赖于目标域样本，大多数研究只依赖单模态传感信号，忽略了多模态信息的互补性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为了解决这些限制，本文提出了一种多模态跨域混合融合模型，用于提高故障诊断的模型泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本文提出了一种双重解耦框架，用于解耦模态不变和模态特定特征，以及域不变和域特定表示，实现全面的多模态表示学习和鲁棒的域泛化。此外，设计了一种跨域混合融合策略，随机混合不同域的模态信息，以增强模态和域多样性，并引入了一种三模态融合机制，以自适应地整合多模态异构信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在感应电机故障诊断的实验中，本文提出的方法在未见过的工作条件下始终优于先进方法，综合消融研究进一步验证了每个提出组件和模态融合的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 本文提出的多模态跨域混合融合模型在故障诊断中表现出色，有效提高了模型在未见过的工作条件下的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.&lt;/p&gt;</description></item><item><guid>2512.24708v1</guid><title>BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework</title><link>http://arxiv.org/abs/2512.24708v1</link><author>András Millinghoffer, András Formanek, András Antos, Péter Antal</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; BandiK是一种新的多任务辅助任务子集选择方法，使用多臂老虎机来解决问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 有效地在多个任务之间转移知识非常重要，但在基础模型下游任务中仍然存在挑战。转移的性质，特别是其可传递性和不可传递性，仍然是一个开放的问题，负迁移仍然是一个重大障碍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决选择有益辅助任务集的约束，这些约束通常受到高计算成本、大量合理的候选辅助集以及目标任务之间选择复杂性的阻碍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; BandiK引入了一种三阶段的多任务辅助任务子集选择方法，使用多臂老虎机。每个臂拉都通过在一个随机训练-测试数据集分割上训练和测试一个多输出神经网络来评估候选辅助集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; BandiK首先估计任务之间的成对迁移，这有助于识别哪些任务可能从联合学习中受益。在第二阶段，它基于初始估计为每个目标任务构建线性数量的候选辅助任务集。第三阶段，它采用多臂老虎机框架，其中臂对应于候选辅助集的性能，这些辅助集作为多个输出神经网络在训练-测试数据集分割上实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; BandiK通过将单个任务特定的MAB集成到多臂老虎机结构中来提高效率。这种解决方案利用了相同的神经网络实现了多个臂，这些臂对应于给定候选集的不同单个老虎机。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 有效地在多个任务之间转移知识非常重要，但在基础模型下游任务中仍然存在挑战。转移的性质，特别是其可传递性和不可传递性，仍然是一个开放的问题，负迁移仍然是一个重大障碍。选择有益辅助任务集的约束通常受到高计算成本、大量合理的候选辅助集以及目标任务之间选择复杂性的阻碍。BandiK引入了一种三阶段的多任务辅助任务子集选择方法，使用多臂老虎机。每个臂拉都通过在一个随机训练-测试数据集分割上训练和测试一个多输出神经网络来评估候选辅助集。BandiK首先估计任务之间的成对迁移，这有助于识别哪些任务可能从联合学习中受益。在第二阶段，它基于初始估计为每个目标任务构建线性数量的候选辅助任务集。第三阶段，它采用多臂老虎机框架，其中臂对应于候选辅助集的性能，这些辅助集作为多个输出神经网络在训练-测试数据集分割上实现。BandiK通过将单个任务特定的MAB集成到多臂老虎机结构中来提高效率。这种解决方案利用了相同的神经网络实现了多个臂，这些臂对应于给定候选集的不同单个老虎机。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多任务学习中如何高效选择辅助任务集的问题。这个问题在现实或研究中非常重要，因为选择合适的辅助任务集可以显著提高多任务学习的效率和性能，同时避免负迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴多臂老虎机（MAB）的方法，设计了一个三阶段的多任务辅助任务子集选择方法。这个方法借鉴了现有工作中关于多臂老虎机在超参数优化和神经架构搜索中的应用，以及多任务学习的相关研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用多臂老虎机来选择最佳的辅助任务集。整体实现流程包括三个阶段：首先估计任务之间的成对迁移效应，并构建正向和负向迁移图；然后基于初始估计构建候选辅助任务集；最后使用多臂老虎机框架选择每个目标任务的最佳多任务神经网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用半重叠臂的多臂老虎机框架，以及将单个任务特定的多臂老虎机集成到多臂老虎机结构中。相比之前的工作，这个方法可以更有效地选择辅助任务集，并且减少了计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于多臂老虎机的高效多任务辅助任务子集选择方法，可以显著提高多任务学习的效率和性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The challenge of effectively transferring knowledge across multiple tasks is of critical importance and is also present in downstream tasks with foundation models. However, the nature of transfer, its transitive-intransitive nature, is still an open problem, and negative transfer remains a significant obstacle. Selection of beneficial auxiliary task sets in multi-task learning is frequently hindered by the high computational cost of their evaluation, the high number of plausible candidate auxiliary sets, and the varying complexity of selection across target tasks.   To address these constraints, we introduce BandiK, a novel three-stage multi-task auxiliary task subset selection method using multi-bandits, where each arm pull evaluates candidate auxiliary sets by training and testing a multiple output neural network on a single random train-test dataset split. Firstly, BandiK estimates the pairwise transfers between tasks, which helps in identifying which tasks are likely to benefit from joint learning. In the second stage, it constructs a linear number of candidate sets of auxiliary tasks (in the number of all tasks) for each target task based on the initial estimations, significantly reducing the exponential number of potential auxiliary task sets. Thirdly, it employs a Multi-Armed Bandit (MAB) framework for each task, where the arms correspond to the performance of candidate auxiliary sets realized as multiple output neural networks over train-test data set splits. To enhance efficiency, BandiK integrates these individual task-specific MABs into a multi-bandit structure. The proposed multi-bandit solution exploits that the same neural network realizes multiple arms of different individual bandits corresponding to a given candidate set. This semi-overlapping arm property defines a novel multi-bandit cost/reward structure utilized in BandiK.&lt;/p&gt;</description></item><item><guid>2512.24724v1</guid><title>FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation</title><link>http://arxiv.org/abs/2512.24724v1</link><author>Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究模型容量对不同时间步的影响，并提出一种阶段感知的多模型采样策略FlowBlending，利用大模型和小模型在不同阶段实现加速，同时保持视觉质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在视频生成中，模型容量对不同时间步的影响不均衡，早期和后期阶段对容量敏感，而中间阶段对容量不敏感。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究模型容量在不同时间步的作用，并设计一种能够在保持质量的前提下加速推理的采样策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出FlowBlending，使用大模型在容量敏感阶段，小模型在中间阶段；通过简单的阶段边界选择准则和速度-发散分析来识别敏感区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; FlowBlending在LTX-Video和WAN 2.1上实现了最高1.65倍的推理速度提升，减少57.35% FLOPs，同时保持大模型的视觉保真度、时间连贯性和语义对齐。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 阶段感知的多模型采样策略能够显著加速视频生成，同时兼容现有加速技术，进一步提升速度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在本研究中，我们展示了模型容量在不同时间步的影响差异：它在早期和后期阶段至关重要，而在中间阶段几乎可以忽略不计。为此，我们提出了 FlowBlending，一种阶段感知的多模型采样策略，在容量敏感阶段使用大模型，在中间阶段使用小模型。我们进一步引入了简单的准则来选择阶段边界，并提供了速度-发散分析，作为识别容量敏感区域的有效代理。在 LTX-Video（2B/13B）和 WAN 2.1（1.3B/14B）上，FlowBlending 实现了最高 1.65 倍的推理速度提升，减少 57.35% FLOPs，同时保持大模型的视觉保真度、时间连贯性和语义对齐。FlowBlending 也与现有的采样加速技术兼容，能够实现额外 2 倍的加速。项目页面可在 https://jibin86.github.io/flowblending_project_page 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.&lt;/p&gt;</description></item><item><guid>2512.24763v1</guid><title>UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning</title><link>http://arxiv.org/abs/2512.24763v1</link><author>Ankit Dhiman, Srinath R, Jaswanth Reddy, Lokesh R Boregowda, Venkatesh Babu Radhakrishnan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。一个关键挑战是 2D 实例标签跨视图的不一致性，导致 3D 预测效果不佳。现有方法采用两阶段方法，一些依赖于对比学习和超参数敏感的聚类，而另一些则预处理标签以实现一致性。我们提出一个统一框架，将这些步骤合并，通过引入可学习的特征嵌入来减少训练时间并提高性能，该嵌入用于高斯原语中的分割。然后通过一种新的“嵌入到标签”过程高效解码成实例标签，有效地集成了优化。尽管这个统一框架提供了显著的好处，但我们观察到在物体边界处存在伪影。为了解决物体边界问题，我们提出沿着这些边界进行硬挖掘样本。然而，直接将硬挖掘应用于特征嵌入被证明是不稳定的。因此，我们在计算三元组损失之前对光栅化特征嵌入应用线性层，这稳定了训练并显著提高了性能。我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决 2D 实例标签跨视图的不一致性，提高 3D 预测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出一个统一框架，将对比学习和标签预处理步骤合并，引入可学习的特征嵌入来减少训练时间并提高性能，通过“嵌入到标签”过程高效解码成实例标签，并应用线性层进行硬挖掘以稳定训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 统一框架提供了显著的好处，但在物体边界处存在伪影。应用线性层进行硬挖掘稳定了训练并显著提高了性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D Gaussian Splatting (3DGS) 和 Neural Radiance Fields (NeRF) 已经推动了新视角合成的进步。最近的方法将多视角 2D 分割扩展到 3D，实现了实例/语义分割以更好地理解场景。一个关键挑战是 2D 实例标签跨视图的不一致性，导致 3D 预测效果不佳。现有方法采用两阶段方法，一些依赖于对比学习和超参数敏感的聚类，而另一些则预处理标签以实现一致性。我们提出一个统一框架，将这些步骤合并，通过引入可学习的特征嵌入来减少训练时间并提高性能，该嵌入用于高斯原语中的分割。然后通过一种新的“嵌入到标签”过程高效解码成实例标签，有效地集成了优化。尽管这个统一框架提供了显著的好处，但我们观察到在物体边界处存在伪影。为了解决物体边界问题，我们提出沿着这些边界进行硬挖掘样本。然而，直接将硬挖掘应用于特征嵌入被证明是不稳定的。因此，我们在计算三元组损失之前对光栅化特征嵌入应用线性层，这稳定了训练并显著提高了性能。我们的方法在 ScanNet、Replica3D 和 Messy-Rooms 数据集上定性和定量地优于基线。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从2D图像中不一致的实例分割标签生成一致的三维分割标签的问题。这个问题在现实或研究中非常重要，因为三维场景理解对于增强现实、自动驾驶和路径规划等领域至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有的三维表示方法如NeRF和3DGS，并利用对比学习的思想设计出这个方法。他们参考了Contrastive-Lift的工作，但对其进行了改进，以实现更高效的训练和更准确的分割。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过对比学习将二维分割标签直接解码为三维分割标签。整体实现流程包括：将二维图像渲染为三维高斯点云，为每个三维高斯点云添加一个d维向量嵌入，通过对比损失优化这些嵌入，然后通过线性层和三重损失进一步优化，最后通过“嵌入到标签”过程生成三维分割标签。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：提出了一种统一的单阶段方法，直接将学习到的三维嵌入解码为一致的三维分割标签；使用基于三重损失的新型对比损失来减少类间方差，从而实现更准确的三维分割。与之前的工作相比，这个方法更高效，因为它不需要额外的后处理步骤，并且具有更快的推理时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于对比学习的统一三维实例分割方法，该方法能够直接从二维分割标签生成一致的三维分割标签，并在多个数据集上实现了最先进的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel &amp;quot;Embedding-to-Label&amp;quot; process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.&lt;/p&gt;</description></item><item><guid>2512.24766v1</guid><title>Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow</title><link>http://arxiv.org/abs/2512.24766v1</link><author>Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 Dream2Flow 框架，将生成式视频模型与机器人控制结合，通过 3D 物体流作为中间表示，实现从视频生成到低层控制的零样本迁移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成式视频建模已成为零样本推理物理交互的有力工具，但将人类引导的运动转化为机器人低层动作仍是挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够将预训练视频模型生成的运动映射到机器人可执行动作的桥梁，克服实体化差距，实现多类别物体的零样本操控。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先从生成视频中重建 3D 物体运动，将操控视为物体轨迹跟踪；随后通过轨迹优化或强化学习，将 3D 物体流转换为可执行的低层命令，且不需要任务特定演示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 3D 物体流是一个通用且可扩展的接口，可将视频生成模型适配到开放世界机器人操控，且在仿真与真实环境中均表现良好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Dream2Flow 成功实现了从视频生成到机器人控制的零样本迁移，为开放世界机器人操控提供了新的方法论。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成式视频建模已成为一种有吸引力的工具，可在零样本条件下推理开放世界中的可行物理交互。然而，将这些由人类引导的运动转化为机器人系统所需的低层动作仍然是一个挑战。我们观察到，给定初始图像和任务指令，这些模型擅长合成合理的物体运动。因此，我们提出了 Dream2Flow，一个通过 3D 物体流作为中间表示，将视频生成与机器人控制桥接的框架。我们的方法从生成的视频中重建 3D 物体运动，并将操控表述为物体轨迹跟踪。通过将状态变化与实现这些变化的执行器分离，Dream2Flow 克服了实体化差距，使预训练视频模型能够在零样本条件下指导不同类别的物体（包括刚性、关节、可变形和颗粒状物体）的操控。通过轨迹优化或强化学习，Dream2Flow 将重建的 3D 物体流转换为可执行的低层命令，而无需任务特定的演示。仿真和真实世界实验表明，3D 物体流是一个通用且可扩展的接口，可将视频生成模型适配到开放世界机器人操控。视频和可视化可在 https://dream2flow.github.io/ 查看。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决如何利用生成式视频模型指导机器人在开放世界中进行零样本操控的问题。此问题重要，因为它能让机器人在仅通过语言描述的新任务中执行操作，减少对任务特定数据的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到视频生成器能产生合理的人类动作，但机器人与人类的执行方式不同，导致“体制差距”。他们决定提取对象的运动轨迹而非人类动作，使用3D对象流作为桥梁。该方案借鉴了视频生成、深度估计、点跟踪、对象分割等现有技术，并结合轨迹优化和强化学习。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将从生成视频中提取的3D对象轨迹视为机器人要追踪的目标。实现流程包括：1）用图像和语言指令生成视频；2）估计每帧深度；3）分割目标对象并跟踪其像素点；4）将跟踪点投影到3D得到对象流；5）使用轨迹优化或学习策略让机器人执行与该流相匹配的动作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：1）使用3D对象流作为与机器人体制无关的中间表示；2）实现零样本、跨对象类型（刚体、关节、柔性、颗粒）操控；3）将预训练视频模型与机器人控制无缝集成。与以往依赖2D流或直接模仿的方法不同，Dream2Flow将状态变化与执行动作分离，且不需要任务特定训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Dream2Flow通过提取生成式视频中的3D对象流，将预训练视频模型转化为零样本机器人操控的通用、可执行接口。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.&lt;/p&gt;</description></item><item><guid>2512.24793v1</guid><title>Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks</title><link>http://arxiv.org/abs/2512.24793v1</link><author>Shota Suzuki, Satoshi Ono</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 神经架构搜索（NAS）自动化了深度神经网络（DNN）的架构设计过程，并吸引了越来越多的关注。多模态DNN需要融合多个模态的特征，因此受益于NAS，但由于其结构复杂性，通过NAS构建多模态DNN的架构需要大量的标记训练数据。本文提出了一种用于多模态DNN架构搜索的自监督学习（SSL）方法。该方法全面应用于架构搜索和模型预训练过程。实验结果表明，该方法成功地从未标记的训练数据中设计了DNN架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 神经架构搜索（NAS）自动化了深度神经网络（DNN）的架构设计过程，并吸引了越来越多的关注。多模态DNN需要融合多个模态的特征，因此受益于NAS，但由于其结构复杂性，通过NAS构建多模态DNN的架构需要大量的标记训练数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 本文提出了一种用于多模态DNN架构搜索的自监督学习（SSL）方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 该方法全面应用于架构搜索和模型预训练过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，该方法成功地从未标记的训练数据中设计了DNN架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; unknown&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 神经架构搜索（NAS）自动化了深度神经网络（DNN）的架构设计过程，并吸引了越来越多的关注。多模态DNN需要融合多个模态的特征，因此受益于NAS，但由于其结构复杂性，通过NAS构建多模态DNN的架构需要大量的标记训练数据。本文提出了一种用于多模态DNN架构搜索的自监督学习（SSL）方法。该方法全面应用于架构搜索和模型预训练过程。实验结果表明，该方法成功地从未标记的训练数据中设计了DNN架构。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Neural architecture search (NAS), which automates the architectural design process of deep neural networks (DNN), has attracted increasing attention. Multimodal DNNs that necessitate feature fusion from multiple modalities benefit from NAS due to their structural complexity; however, constructing an architecture for multimodal DNNs through NAS requires a substantial amount of labeled training data. Thus, this paper proposes a self-supervised learning (SSL) method for architecture search of multimodal DNNs. The proposed method applies SSL comprehensively for both the architecture search and model pretraining processes. Experimental results demonstrated that the proposed method successfully designed architectures for DNNs from unlabeled training data.&lt;/p&gt;</description></item><item><guid>2512.24834v1</guid><title>GenZ: Foundational models as latent variable generators within traditional statistical models</title><link>http://arxiv.org/abs/2512.24834v1</link><author>Marko Jojic, Nebojsa Jojic</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; GenZ是一种混合模型，它通过可解释的语义特征连接了基础模型和统计模型。该方法通过迭代过程发现语义特征描述，对比通过统计建模错误识别的项目组，而不是仅仅依赖基础模型的领域理解。该方法被表述为一个通用的EM算法，联合优化语义特征描述和统计模型参数。该方法提示一个冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为对潜在二元特征的噪声观察，这些特征通过学习的统计关系预测实值目标。该方法在两个领域进行了演示：房价预测（享乐回归）和冷启动协同过滤用于电影推荐。在房价预测中，该模型使用从多模态列表数据中发现的语义特征实现了12%的中值相对误差，大大优于依赖LLM的通用领域知识的GPT-5基线（误差为38%）。对于Netflix电影嵌入，该模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式（例如，建筑细节预测当地房地产市场，系列成员身份预测用户偏好），这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 大型语言模型拥有广泛的领域知识，但它们通常无法捕捉对预测任务至关重要的数据集特定模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出GenZ模型，通过可解释的语义特征连接基础模型和统计模型，以解决大型语言模型在捕捉数据集特定模式方面的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过迭代过程对比通过统计建模错误识别的项目组，发现语义特征描述，并使用通用的EM算法联合优化语义特征描述和统计模型参数。提示一个冻结的基础模型根据发现的特征对项目进行分类，并将这些判断视为对潜在二元特征的噪声观察。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在房价预测中，GenZ模型使用发现的语义特征实现了12%的中值相对误差，远优于依赖LLM的通用领域知识的GPT-5基线。对于Netflix电影嵌入，GenZ模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式，这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GenZ模型通过可解释的语义特征有效地连接了基础模型和统计模型，在房价预测和电影推荐领域取得了显著的性能提升，并揭示了数据集特定的模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了GenZ，一种通过可解释的语义特征连接基础模型和统计模型的混合模型。虽然大型语言模型拥有广泛的领域知识，但它们通常无法捕捉对预测任务至关重要的数据集特定模式。我们的方法通过迭代过程对比通过统计建模错误识别的项目组，发现语义特征描述，而不是仅仅依赖基础模型的领域理解。我们将此表述为一个通用的EM算法，联合优化语义特征描述和统计模型参数。该方法提示一个冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为对潜在二元特征的噪声观察，这些特征通过学习的统计关系预测实值目标。我们在两个领域演示了该方法：房价预测（享乐回归）和冷启动协同过滤用于电影推荐。在房价预测中，我们的模型使用发现的语义特征实现了12%的中值相对误差，远优于依赖LLM的通用领域知识的GPT-5基线。对于Netflix电影嵌入，我们的模型仅从语义描述中预测协同过滤表示，达到了0.59的余弦相似度，这相当于传统协同过滤需要大约4000个用户评价才能达到的性能。发现的特征揭示了数据集特定的模式，这些模式与模型的领域知识不同。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将大型语言模型（LLM）的广泛领域知识与特定数据集的模式相结合，以改进预测任务。这个问题在现实或研究中很重要，因为LLM虽然知识广泛，但往往无法捕捉到特定数据集的独特模式，这限制了它们在预测任务中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴概念瓶颈模型（CBM）的思想，设计了一个混合模型，该模型通过迭代过程对比通过统计建模错误识别的项目组来发现语义特征描述，而不是仅仅依赖LLM的领域理解。这个方法借鉴了现有工作，但引入了新的元素，如不确定性参数和基于统计模型后验分布的对比。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用LLM作为解释器，通过对比统计模型识别的项目组来发现语义特征，并使用这些特征来改进预测模型。整体实现流程包括使用LLM对项目进行分类，通过统计模型的后验分布对比项目组，使用这些对比信息来更新语义特征描述，并最终联合优化语义特征和统计模型参数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括使用冻结的LLM作为解释器，通过对比统计模型识别的项目组来发现语义特征，以及支持任意映射的统计模型。与之前的工作相比，这个方法更注重发现数据集特定的模式，而不是仅仅依赖LLM的领域知识，并且能够处理高维实值目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的混合模型，通过结合LLM和统计建模来发现数据集特定的语义特征，从而提高预测任务的性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model&amp;#x27;s domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM&amp;#x27;s general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model&amp;#x27;s domain knowledge alone.&lt;/p&gt;</description></item><item><guid>2512.24845v1</guid><title>ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation</title><link>http://arxiv.org/abs/2512.24845v1</link><author>Qiuyi Gu, Yuze Sheng, Jincheng Yu, Jiahao Tang, Xiaolong Shan, Zhaoyang Shen, Tinghao Yi, Xiaodan Liang, Xinlei Chen, Yu Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。为弥补这一差距，我们提出了ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。该方法利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。我们将这些运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。大量实际实验表明，ArtiSG在功能元素召回和关节估计精度方面显著优于基线。此外，我们证明构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥补3D场景图在功能信息方面的不足，特别是对于关节对象的功能信息，通过构建功能3D场景图来指导机器人在真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。将运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; ArtiSG在功能元素召回和关节估计精度方面显著优于基线。构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ArtiSG框架能有效构建功能3D场景图，弥补现有方法的不足，并显著提升机器人在真实环境中的操作能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D场景图赋予机器人语义理解能力，但缺乏功能信息，特别是对于关节对象。现有方法在静态观察中推断关节机制易受视觉模糊影响，而从状态变化中估计参数的方法通常依赖于固定摄像头和无遮挡视图。此外，细粒度的功能元素如小把手常被通用物体检测器忽略。为弥补这一差距，我们提出了ArtiSG框架，通过将人类演示编码到结构化机器人记忆中构建功能3D场景图。该方法利用鲁棒的关节数据收集流程，即使在相机自身运动下也能准确估计6-自由度关节轨迹和轴。我们将这些运动学先验集成到分层开放词汇图中，同时利用交互数据发现视觉感知遗漏的不显眼功能元素。大量实际实验表明，ArtiSG在功能元素召回和关节估计精度方面显著优于基线。此外，我们证明构建的图作为可靠的功能记忆，能有效指导机器人在包含多样化关节对象的真实环境中执行语言指导的操作任务。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景图中缺乏功能信息的问题，特别是对于需要物理操作的关节对象。这个问题在现实研究中很重要，因为机器人需要理解物体的功能属性才能进行有效的物理操作和任务执行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，如利用人类演示进行机器人学习和3D场景图构建，设计出ArtiSG方法。他们注意到现有方法的局限性，如视觉歧义和缺乏功能细节，因此提出了一个结合人类演示和视觉基础模型的框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是将人类演示编码到结构化的机器人记忆中，以构建功能3D场景图。整体流程包括三个阶段：初始化功能场景图，利用便携式设备进行视角鲁棒的关节估计，以及交互增强的图细化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括视角鲁棒的关节跟踪、交互增强的功能元素检测和开放词汇场景构建。与之前的工作相比，ArtiSG能够更准确地估计关节机制，并识别被视觉感知忽略的细微功能元素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ArtiSG通过结合人类演示和视觉基础模型，构建了功能3D场景图，使机器人能够更好地理解和操作关节对象。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.&lt;/p&gt;</description></item><item><guid>2512.24849v1</guid><title>SSCHA-based evolutionary crystal structure prediction at finite temperatures with account for quantum nuclear motion</title><link>http://arxiv.org/abs/2512.24849v1</link><author>Daniil Poletaev, Artem Oganov</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 在有限温度下，结合量子非谐效应的准确晶体结构预测（CSP）在轻原子系统（如超导氢化物）中仍然具有挑战性但非常重要。本文将机器学习原子间势（MLIPs）与随机自洽谐波近似（SSCHA）相结合，以在量子非谐自由能景观上进行进化CSP。使用150 GPa和300 K下的LaH10作为测试案例，比较了两种基于SSCHA的CSP方法：使用轻量级主动学习MLIPs（AL-MLIPs）和Matbench项目中的基础模型或通用MLIPs（uMLIPs）。结果表明，AL-MLIPs可以正确预测实验上已知的立方Fm3m相为最稳定的多形体，但需要在热力学扰动理论中进行修正以获得一致结果。uMLIP Mattersim-5m允许进行基于SSCHA的CSP，无需结构级训练，甚至可以正确地接近全局最小值的结构排序，尽管可能需要微调以提高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在有限温度下，结合量子非谐效应的准确晶体结构预测（CSP）在轻原子系统（如超导氢化物）中仍然具有挑战性但非常重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 将机器学习原子间势（MLIPs）与随机自洽谐波近似（SSCHA）相结合，以在量子非谐自由能景观上进行进化CSP。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用150 GPa和300 K下的LaH10作为测试案例，比较了两种基于SSCHA的CSP方法：使用轻量级主动学习MLIPs（AL-MLIPs）和Matbench项目中的基础模型或通用MLIPs（uMLIPs）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; AL-MLIPs可以正确预测实验上已知的立方Fm3m相为最稳定的多形体，但需要在热力学扰动理论中进行修正以获得一致结果。uMLIP Mattersim-5m允许进行基于SSCHA的CSP，无需结构级训练，甚至可以正确地接近全局最小值的结构排序，尽管可能需要微调以提高精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 包括量子非谐性简化了自由能景观，并且对于正确的稳定性排序是必要的，特别是对于可能在经典0 K CSP中遗漏的高温相。所提出的方法扩展了CSP的范围，使其适用于量子核运动和非谐性占主导的系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在有限温度下，结合量子非谐效应的准确晶体结构预测（CSP）在轻原子系统（如超导氢化物）中仍然具有挑战性但非常重要。本文将机器学习原子间势（MLIPs）与随机自洽谐波近似（SSCHA）相结合，以在量子非谐自由能景观上进行进化CSP。使用150 GPa和300 K下的LaH10作为测试案例，比较了两种基于SSCHA的CSP方法：使用轻量级主动学习MLIPs（AL-MLIPs）和Matbench项目中的基础模型或通用MLIPs（uMLIPs）。结果表明，AL-MLIPs可以正确预测实验上已知的立方Fm3m相为最稳定的多形体，但需要在热力学扰动理论中进行修正以获得一致结果。uMLIP Mattersim-5m允许进行基于SSCHA的CSP，无需结构级训练，甚至可以正确地接近全局最小值的结构排序，尽管可能需要微调以提高精度。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Accurate crystal structure prediction (CSP) at finite temperatures with quantum anharmonic effects remains challenging but very prominent in systems with lightweight atoms such as superconducting hydrides. In this work, we integrate machine-learned interatomic potentials (MLIPs) with the stochastic self-consistent harmonic approximation (SSCHA) to enable evolutionary CSP on the quantum anharmonic free-energy landscape. Using LaH$_{10}$ at 150 GPa and 300 K as a test case, we compare two approaches for SSCHA-based CSP: using light-weight active-learning MLIPs (AL-MLIPs) trained on-the-fly from scratch, and foundation models or universal MLIPs (uMLIPs) from the Matbench project. We demonstrate that AL-MLIPs allow to correctly predict the experimentally known cubic Fm$\bar{3}$m phase as the most stable polymorph at 150 GPa but require corrections within the thermodynamic perturbation theory to get consistent results. The uMLIP Mattersim-5m allow to conduct SSCHA-based CSP without requiring per-structure training and even get correct structure ranking near the global minimum, though fine-tuning may be needed for higher accuracy. Our results show that including quantum anharmonicity simplifies the free-energy landscape and is essential for correct stability rankings, that is especially important for high-temperature phases that could be missed in classical 0 K CSP. The proposed approach extends the reach of CSP to systems where quantum nuclear motion and anharmonicity dominate.&lt;/p&gt;</description></item><item><guid>2512.24866v1</guid><title>Characterization of Transfer Using Multi-task Learning Curves</title><link>http://arxiv.org/abs/2512.24866v1</link><author>András Millinghoffer, Bence Bolgár, Péter Antal</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。我们假设通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，可以更根本地描述迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，以更根本地描述迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 我们使用多任务学习曲线来定量地建模迁移效应，这些曲线近似于不同样本数量下的归纳性能。我们描述了一种高效的方法来近似多任务学习曲线，类似于训练期间的任务亲和分组方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 我们的结果表明，学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 迁移效应在固定数据集的训练和累积数据的归纳推理中都能体现。我们假设通过增加样本数量来扰动数据集，而不是通过梯度更新来扰动模型，可以更根本地描述迁移效应。为了捕捉这种现象，我们使用多任务学习曲线来定量地建模迁移效应，这些曲线近似于不同样本数量下的归纳性能。我们描述了一种高效的方法来近似多任务学习曲线，类似于训练期间的任务亲和分组方法。我们比较了迁移的统计和计算方法，这表明之前的计算成本要高得多，但具有更好的性能和更广泛的适用性。我们使用基准药物靶点相互作用数据集进行了评估。我们的结果表明，学习曲线可以更好地捕捉多任务学习的效果，并且它们的扩展可以界定基础模型中的成对和上下文迁移效应。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多任务学习中迁移效应的表征问题。这个问题在现实或研究中非常重要，因为迁移效应直接影响机器学习模型的性能和泛化能力，特别是在处理大规模、多任务问题时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多任务学习方法，特别是任务亲和分组方法，设计出一种新的方法来表征迁移效应。他们借鉴了现有工作，但提出了新的学习曲线方法来更有效地捕捉迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是通过学习曲线来表征迁移效应，特别是通过样本数量来观察模型性能的变化。整体实现流程包括使用多任务学习曲线来建模和评估迁移效应，通过系统地比较不同任务组合下的性能变化来分析迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括提出新的多任务学习曲线方法来表征迁移效应，以及通过系统地比较统计和计算方法来分析迁移效应。相比之前的工作，这个方法更注重样本数量对模型性能的影响，并且能够更有效地捕捉迁移效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出新的多任务学习曲线方法，有效地表征和分析了多任务学习中的迁移效应。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Transfer effects manifest themselves both during training using a fixed data set and in inductive inference using accumulating data. We hypothesize that perturbing the data set by including more samples, instead of perturbing the model by gradient updates, provides a complementary and more fundamental characterization of transfer effects. To capture this phenomenon, we quantitatively model transfer effects using multi-task learning curves approximating the inductive performance over varying sample sizes. We describe an efficient method to approximate multi-task learning curves analogous to the Task Affinity Grouping method applied during training. We compare the statistical and computational approaches to transfer, which indicates considerably higher compute costs for the previous but better power and broader applicability. Evaluations are performed using a benchmark drug-target interaction data set. Our results show that learning curves can better capture the effects of multi-task learning and their multi-task extensions can delineate pairwise and contextual transfer effects in foundation models.&lt;/p&gt;</description></item><item><guid>2512.24880v1</guid><title>mHC: Manifold-Constrained Hyper-Connections</title><link>http://arxiv.org/abs/2512.24880v1</link><author>Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 近期研究，如Hyper-Connections (HC)，通过扩展残差流宽度和多样化连接模式，扩展了过去十年建立的普遍残差连接范式。虽然这带来了显著的性能提升，但这种多样化从根本上破坏了残差连接的恒等映射特性，导致严重的训练不稳定性、有限的扩展性，并增加了显著的内存访问开销。为了解决这些挑战，我们提出了Manifold-Constrained Hyper-Connections (mHC)，一个将HC的残差连接空间投影到特定流形上的通用框架，以恢复恒等映射特性，同时结合严格的架构优化以确保效率。实证实验表明，mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。我们预计，作为HC的一种灵活且实用的扩展，mHC将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; Hyper-Connections (HC)通过扩展残差流宽度和多样化连接模式提升了性能，但破坏了恒等映射特性，导致训练不稳定性、有限扩展性和内存访问开销。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决HC带来的训练不稳定性、有限扩展性和内存访问开销问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出Manifold-Constrained Hyper-Connections (mHC)，将残差连接空间投影到特定流形上，恢复恒等映射特性，并结合严格的架构优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; mHC作为HC的灵活且实用的扩展，将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 近期研究，如Hyper-Connections (HC)，通过扩展残差流宽度和多样化连接模式，扩展了过去十年建立的普遍残差连接范式。虽然这带来了显著的性能提升，但这种多样化从根本上破坏了残差连接的恒等映射特性，导致严重的训练不稳定性、有限的扩展性，并增加了显著的内存访问开销。为了解决这些挑战，我们提出了Manifold-Constrained Hyper-Connections (mHC)，一个将HC的残差连接空间投影到特定流形上的通用框架，以恢复恒等映射特性，同时结合严格的架构优化以确保效率。实证实验表明，mHC在大规模训练中非常有效，提供了显著的性能提升和优越的可扩展性。我们预计，作为HC的一种灵活且实用的扩展，mHC将有助于更深入地理解拓扑架构设计，并为基础模型的发展指明有前景的方向。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Hyper-Connections（HC）在扩展连接复杂度时导致的训练不稳定和可扩展性问题。这个问题在研究中很重要，因为HC虽然能提升性能，但其无约束的连接方式破坏了残差连接的恒等映射特性，导致大规模训练时信号放大或衰减，影响训练效率和模型稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过借鉴现有工作，特别是HC的设计思想，提出了Manifold-Constrained Hyper-Connections（mHC）方法。mHC借鉴了将残差连接空间投影到特定流形上的思想，利用Sinkhorn-Knopp算法将残差连接矩阵投影到Birkhoff多面体上，以恢复恒等映射特性，同时结合基础设施优化确保效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; mHC的核心思想是将HC的残差连接空间投影到由双随机矩阵构成的流形上，以恢复恒等映射特性。整体实现流程包括：1) 利用Sinkhorn-Knopp算法将残差连接矩阵投影到Birkhoff多面体上；2) 通过矩阵的行和列和为1的特性，确保信号在传播过程中的均值和范数得到有效控制；3) 结合基础设施优化，如内核融合、混合精度内核和通信重叠等，提高效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; mHC的关键创新点包括：1) 将残差连接空间投影到特定流形上，恢复恒等映射特性，提高训练稳定性；2) 结合基础设施优化，如内核融合、混合精度内核和通信重叠等，提高效率。相比之前的工作，mHC在保持HC拓扑复杂度的同时，解决了其训练不稳定和可扩展性问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; mHC通过将残差连接空间投影到特定流形上，恢复了恒等映射特性，同时结合基础设施优化，提高了大规模训练的稳定性和可扩展性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.&lt;/p&gt;</description></item><item><guid>2512.24896v1</guid><title>Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing</title><link>http://arxiv.org/abs/2512.24896v1</link><author>Andrii Gamalii, Daniel Górniak, Robert Nowak, Bartłomiej Olber, Krystian Radlak, Jakub Winter</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该报告介绍了DARTS项目内开发的一种半自动化数据标注流程，旨在创建大规模、多模态的波兰驾驶场景数据集。手动标注此类异构数据既昂贵又耗时。为应对这一挑战，提出的解决方案采用人机协作方法，结合人工智能与人类专业知识，以降低标注成本和时长。系统自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。核心工具依赖于3D目标检测算法生成初步标注。总体而言，开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 手动标注大规模、多模态的波兰驾驶场景数据集既昂贵又耗时。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 创建大规模、多模态的波兰驾驶场景数据集，并降低标注成本和时长。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用人机协作方法，结合人工智能与人类专业知识，自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 该报告介绍了DARTS项目内开发的一种半自动化数据标注流程，旨在创建大规模、多模态的波兰驾驶场景数据集。手动标注此类异构数据既昂贵又耗时。为应对这一挑战，提出的解决方案采用人机协作方法，结合人工智能与人类专业知识，以降低标注成本和时长。系统自动生成初始标注，支持迭代模型再训练，并包含数据匿名化和领域适应技术。核心工具依赖于3D目标检测算法生成初步标注。总体而言，开发的工具和方法在确保跨不同传感器模态的一致、高质量标注的同时，显著节省了时间。该解决方案直接支持DARTS项目，通过加速项目标准化格式的大规模标注数据集的准备，加强了波兰自动驾驶研究的技术基础。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶车辆测试中多传感器数据集的手动标注成本高、耗时长的问题。这个问题在现实或研究中很重要，因为高质量的标注数据集是训练和评估自动驾驶感知系统的关键，而手动标注难以满足大规模、多模态数据集的需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过结合人工智能和人类专业知识，设计了一个半自动化的数据标注流程。这个方法借鉴了现有的3D目标检测算法和人类在环验证技术，并对这些技术进行了改进和整合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这个方法的核心思想是利用人工智能自动生成初始标注，然后通过人类验证和修正来提高标注质量。整体实现流程包括数据预处理、自动标注生成、人类验证、质量控制和模型迭代训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括结合人工 intelligence 和人类专业知识进行半自动化标注，以及使用3D目标检测算法和领域适应技术来提高标注效率和准确性。相比之前的工作，这个方法更加注重效率和准确性，并且能够处理多模态数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种半自动化的数据标注方法，通过结合人工智能和人类专业知识，有效地提高了自动驾驶车辆测试中多传感器数据集的标注效率和准确性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project&amp;#x27;s standardized format, strengthening the technological base for autonomous vehicle research in Poland.&lt;/p&gt;</description></item><item><guid>2512.24901v1</guid><title>Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes</title><link>http://arxiv.org/abs/2512.24901v1</link><author>Debasis Maji, Arghya Banerjee, Debaditya Barman</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于光谱卷积的图神经网络模型 SpectralBrainGNN，用于从功能性磁共振成像的连接组数据中分类认知任务，实验表明该模型能够高效提取脑区之间的拓扑关系并实现约96点二五百分比的分类准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 认知任务分类是利用机器学习解码脑状态的关键技术，结合机器学习与脑网络分析可以从功能性磁共振成像的连接组中提取复杂的连接模式，将原始的血氧水平依赖信号转化为可解释的认知过程表征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种基于图傅里叶变换的光谱卷积框架，以更好地捕捉脑区节点和功能连接边之间的拓扑依赖和多尺度交互，从而提升认知任务分类的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建 SpectralBrainGNN 模型，将脑区视为节点、功能连接视为边，采用归一化拉普拉斯特征分解得到的图傅里叶变换进行光谱卷积。模型在 Human Connectome Project-Task 数据集上进行训练和评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果显示，SpectralBrainGNN 在 HCPTask 数据集上实现了约96点二五百分比的分类准确率，验证了该方法的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 基于光谱卷积的图神经网络能够有效捕获脑功能网络的结构特征，提升认知任务分类精度，并提供公开的实现代码以促进可重复性和后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 认知任务分类使用机器学习在从神经影像数据解码脑状态方面起着核心作用。通过将机器学习与脑网络分析相结合，可以从功能性磁共振成像的连接组中提取复杂的连通模式。这一过程将原始的血氧水平依赖（BOLD）信号转化为可解释的认知过程表征。图神经网络（GNN）进一步推动了这一范式，通过将脑区建模为节点、功能连接建模为边，捕捉传统方法常常遗漏的拓扑依赖和多尺度交互。我们提出的 SpectralBrainGNN 模型是一种基于图傅里叶变换（GFT）的光谱卷积框架，GFT 通过归一化拉普拉斯特征分解计算。对 Human Connectome Project-Task（HCPTask）数据集的实验表明，所提方法的有效性，分类准确率达到约96点二五百分比。实现代码已公开于 https://github.com/gnnplayground/SpectralBrainGNN，以支持可重复性和未来研究。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Cognitive task classification using machine learning plays a central role in decoding brain states from neuroimaging data. By integrating machine learning with brain network analysis, complex connectivity patterns can be extracted from functional magnetic resonance imaging connectomes. This process transforms raw blood-oxygen-level-dependent (BOLD) signals into interpretable representations of cognitive processes. Graph neural networks (GNNs) further advance this paradigm by modeling brain regions as nodes and functional connections as edges, capturing topological dependencies and multi-scale interactions that are often missed by conventional approaches. Our proposed SpectralBrainGNN model, a spectral convolution framework based on graph Fourier transforms (GFT) computed via normalized Laplacian eigendecomposition. Experiments on the Human Connectome Project-Task (HCPTask) dataset demonstrate the effectiveness of the proposed approach, achieving a classification accuracy of 96.25\%. The implementation is publicly available at https://github.com/gnnplayground/SpectralBrainGNN to support reproducibility and future research.&lt;/p&gt;</description></item><item><guid>2512.24917v1</guid><title>Frequent subgraph-based persistent homology for graph classification</title><link>http://arxiv.org/abs/2512.24917v1</link><author>Xinyang Chen, Amaël Broustet, Guoting Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于频繁子图的图过滤方法（FSF），并利用其生成频率驱动的持久同调特征（FPH），进而构建了两种图分类模型，显著提升了拓扑感知的表示能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 持久同调已成为提取拓扑特征的有力工具，然而在图数据上常用的过滤方式仅限于度或权重等简单指标，难以捕捉数据集中的重复结构信息，限制了表达能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在设计一种能够利用频繁子图信息的过滤机制，以生成更稳定、信息丰富的持久同调特征，并探索其在机器学习和图神经网络中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 提出频繁子图过滤（FSF），基于频繁子图的出现频率构造过滤序列，进而计算频率基持久同调（FPH）特征；理论上分析其性质并给出证明；实现两种分类框架：基于FPH的机器学习模型（FPH-ML）以及将FPH 与图神经网络结合的混合框架（FPH-GNN）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，FPH-ML 在准确率上可与或优于基于核函数和度过滤的方法；将FPH 融入 GCN、GIN 等图神经网络后，性能提升在 0.4% 到 21% 之间，最高提升约 8.2 个百分点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 频繁子图过滤成功将频繁子图挖掘与拓扑数据分析结合，为拓扑感知的特征提取提供了新视角，并在图分类任务中展示了显著的效果提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 持久同调（PH）最近成为提取拓扑特征的强大工具。将 PH 融入机器学习和深度学习模型可以增强拓扑感知和可解释性。然而，大多数针对图的 PH 方法仅依赖于有限的过滤集合，如基于度或权重的过滤，这忽略了数据集中重复出现的信息，从而限制了表达能力。在本工作中，我们提出了一种新颖的图过滤——频繁子图过滤（FSF），它来源于频繁子图并产生稳定且信息丰富的基于频率的持久同调（FPH）特征。我们研究了 FSF 的理论属性，并提供了证明和实验验证。除了持久同调本身，我们还引入了两种图分类方法：基于 FPH 的机器学习模型（FPH-ML）以及将 FPH 与图神经网络结合的混合框架（FPH-GNN），以增强拓扑感知的图表示学习。我们的框架桥接了频繁子图挖掘和拓扑数据分析，提供了一个新的拓扑感知特征提取视角。实验结果显示，FPH-ML 在准确率上与基于核函数和基于度的过滤方法相比具有竞争力或更优。当与图神经网络结合时，FPH 带来的相对性能提升在 0.4% 到 21% 之间，对 GCN 和 GIN 骨干网络的提升最高可达 8.2 个百分点。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Persistent homology (PH) has recently emerged as a powerful tool for extracting topological features. Integrating PH into machine learning and deep learning models enhances topology awareness and interpretability. However, most PH methods on graphs rely on a limited set of filtrations, such as degree-based or weight-based filtrations, which overlook richer features like recurring information across the dataset and thus restrict expressive power. In this work, we propose a novel graph filtration called Frequent Subgraph Filtration (FSF), which is derived from frequent subgraphs and produces stable and information-rich frequency-based persistent homology (FPH) features. We study the theoretical properties of FSF and provide both proofs and experimental validation. Beyond persistent homology itself, we introduce two approaches for graph classification: an FPH-based machine learning model (FPH-ML) and a hybrid framework that integrates FPH with graph neural networks (FPH-GNNs) to enhance topology-aware graph representation learning. Our frameworks bridge frequent subgraph mining and topological data analysis, offering a new perspective on topology-aware feature extraction. Experimental results show that FPH-ML achieves competitive or superior accuracy compared with kernel-based and degree-based filtration methods. When integrated into graph neural networks, FPH yields relative performance gains ranging from 0.4 to 21 percent, with improvements of up to 8.2 percentage points over GCN and GIN backbones across benchmarks.&lt;/p&gt;</description></item><item><guid>2512.24917v2</guid><title>Frequent subgraph-based persistent homology for graph classification</title><link>http://arxiv.org/abs/2512.24917v2</link><author>Xinyang Chen, Amaël Broustet, Guanyuan Zeng, Cheng He, Guoting Chen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种新的图过滤方法——频繁子图过滤（FSF），利用频繁子图生成稳定且信息丰富的频率基础持久同调（FPH）特征，并基于FPH构建机器学习模型和与图神经网络融合的框架，实验表明该方法在图分类任务中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 持久同调（PH）已成为提取拓扑特征的强大工具，将PH融入机器学习和深度学习可提升模型的拓扑感知和可解释性，但现有图PH方法多依赖度数或权重过滤，忽略了数据集中的重复信息，限制了表达能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出频繁子图过滤（FSF），通过频繁子图捕获更丰富的图结构信息，生成稳定且信息丰富的FPH特征，并探索其在图分类中的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 首先定义FSF并证明其理论性质；其次基于FPH构建FPH-ML机器学习模型；随后设计FPH-GNNs框架，将FPH与图神经网络结合；最后在多个基准数据集上进行实验验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; FPH-ML在准确率上与基于核函数和度数过滤的方法相当或更优；将FPH集成到图神经网络后，性能提升幅度在0.4%至21%之间，最高提升8.2个百分点，显著优于GCN和GIN骨干网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 频繁子图过滤与FPH特征为图数据提供了新的拓扑感知特征提取方式，能够有效提升图分类模型的性能，并为频繁子图挖掘与拓扑数据分析的结合提供了新的视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 持久同调（PH）最近成为提取拓扑特征的强大工具。将PH整合到机器学习和深度学习模型中可以增强拓扑感知和可解释性。然而，大多数图上的PH方法仅依赖有限的过滤方式，例如基于度数或权重的过滤，忽略了跨数据集的重复信息，从而限制了表达能力。在本研究中，我们提出了一种新颖的图过滤方法——频繁子图过滤（FSF），它来源于频繁子图，并产生稳定且信息丰富的基于频率的持久同调（FPH）特征。我们研究了FSF的理论性质，并提供了证明和实验验证。除了持久同调本身，我们还提出了两种图分类方法：基于FPH的机器学习模型（FPH-ML）和将FPH与图神经网络集成的混合框架（FPH-GNNs），以增强拓扑感知的图表示学习。我们的框架将频繁子图挖掘与拓扑数据分析相结合，为拓扑感知特征提取提供了新的视角。实验结果表明，FPH-ML在准确率上与基于核函数和基于度数过滤的方法竞争或更优。当集成到图神经网络中时，FPH相对性能提升范围为0.4%至21%，在基准测试中相对于GCN和GIN骨干网络提升高达8.2个百分点。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Persistent homology (PH) has recently emerged as a powerful tool for extracting topological features. Integrating PH into machine learning and deep learning models enhances topology awareness and interpretability. However, most PH methods on graphs rely on a limited set of filtrations, such as degree-based or weight-based filtrations, which overlook richer features like recurring information across the dataset and thus restrict expressive power. In this work, we propose a novel graph filtration called Frequent Subgraph Filtration (FSF), which is derived from frequent subgraphs and produces stable and information-rich frequency-based persistent homology (FPH) features. We study the theoretical properties of FSF and provide both proofs and experimental validation. Beyond persistent homology itself, we introduce two approaches for graph classification: an FPH-based machine learning model (FPH-ML) and a hybrid framework that integrates FPH with graph neural networks (FPH-GNNs) to enhance topology-aware graph representation learning. Our frameworks bridge frequent subgraph mining and topological data analysis, offering a new perspective on topology-aware feature extraction. Experimental results show that FPH-ML achieves competitive or superior accuracy compared with kernel-based and degree-based filtration methods. When integrated into graph neural networks, FPH yields relative performance gains ranging from 0.4 to 21 percent, with improvements of up to 8.2 percentage points over GCN and GIN backbones across benchmarks.&lt;/p&gt;</description></item><item><guid>2512.24922v1</guid><title>Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</title><link>http://arxiv.org/abs/2512.24922v1</link><author>Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于神经元激活模式的激光雷达领域适应方法，只需对目标域中少量精选且多样的样本进行标注，即可实现最先进的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 3D目标检测器是自动驾驶感知系统的核心，但在不同地区或数据分布之间的迁移能力不足，例如在美国训练的模型在亚洲或欧洲表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在开发一种能够在极小标注预算下实现跨域适应的技术，并通过后训练策略防止模型权重漂移。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 利用神经元激活模式挑选目标域的代表性样本进行少量标注，随后结合受持续学习启发的后训练技术，对模型进行微调，以保持原始模型的权重稳定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，该领域适应方法在精度上超过线性探测和现有最先进的领域适应技术，并且只需极少的标注成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 通过正确选择少量多样化的目标域样本并使用持续学习的后训练手段，可在保持原模型性能的同时实现高效的跨域适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 3D目标检测器是自动驾驶感知系统的基础组件。虽然这些检测器在标准的自动驾驶基准上取得了显著的性能，但它们常常难以在不同领域之间实现泛化——例如，在美国训练的模型在亚洲或欧洲等地区可能表现不佳。本文提出了一种基于神经元激活模式的全新激光雷达领域适应方法，证明只要正确选择目标域中少量具有代表性且多样性的样本进行标注，就能实现最先进的性能。该方法只需极少的标注预算，并且结合受持续学习启发的后训练技术，可防止模型权重从原始模型漂移。实证评估表明，所提出的领域适应方法在性能上优于线性探测和最先进的领域适应技术。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决3D目标检测模型在不同地区或传感器条件下的域适应问题，即如何用极少量标注的目标域数据使模型在新环境中保持高性能。这在自动驾驶实际部署中非常关键，因为不同地区的车辆尺寸、道路布局和LiDAR配置差异会导致模型性能大幅下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为低层特征对跨域检测至关重要，因而聚焦于细粒度的微调，并借鉴了持续学习防止灾难性遗忘的技术、L2‑SP 权重正则化以及已有的点云密度和边框尺寸对齐方法。同时，他们创新性地使用神经元激活模式来挑选最具代表性的目标域样本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过提取ROI头的激活向量，剪枝并二值化后衡量样本多样性，选出少量代表性帧进行微调。整体流程为：从预训练模型中获取激活模式 → 处理得到稀疏二进制特征 → 计算多样性并挑选样本 → 使用小学习率或L2‑SP 等正则化进行后训练，同时加入持续学习技巧以保持源域性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：① 基于神经激活模式的多样性样本选择算法；② 证明仅需少量目标域样本即可实现有效适应；③ 将持续学习正则化与微调相结合防止权重漂移；④ 在保持实现简洁的同时超越了线性探测和其他先进域适应方法。与以往依赖对抗学习、伪标签或大规模未标注数据的做法不同，本文侧重于少量高质量标注和激活驱动的选择。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于激活模式的多样性驱动微调加持续学习正则化的域适应方案，使3D LiDAR目标检测在仅使用少量精选目标域样本的情况下实现跨地区高效迁移。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.&lt;/p&gt;</description></item><item><guid>2512.25008v1</guid><title>FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</title><link>http://arxiv.org/abs/2512.25008v1</link><author>Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 FoundationSLAM，一种基于学习的单目稠密 SLAM 系统，通过将光流估计与几何推理相结合，解决了以往光流方法缺乏几何一致性的问题，实现了精准且鲁棒的跟踪与建图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 以往基于光流的 SLAM 方法在几何一致性方面存在不足，导致跟踪和映射的精度和鲁棒性受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在构建一个能够在几何一致性约束下进行准确、鲁棒且实时的单目稠密 SLAM 系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 核心思路是利用基础深度模型的引导，将光流估计与几何推理相结合。具体包括：1）设计混合光流网络，生成几何感知的对应关系，实现跨关键帧的一致深度和位姿推断；2）提出双一致性束束优化层，在多视图约束下联合优化关键帧位姿和深度，以保证全局一致性；3）引入可靠性感知的细化机制，通过区分可靠和不确定区域动态调整光流更新，形成匹配与优化的闭环反馈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 大量实验表明，FoundationSLAM 在多个具有挑战性的数据集上实现了更高的轨迹精度和稠密重建质量，且能够以 18 帧每秒的实时速度运行，展示了对各种场景的强泛化能力和实际应用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FoundationSLAM 通过融合光流与深度的几何约束，成功提升了单目稠密 SLAM 的精度、鲁棒性和实时性，具备良好的通用性和实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 FoundationSLAM，这是一种基于学习的单目稠密 SLAM 系统，解决了以往基于光流的方法在几何一致性方面的缺失，从而实现了准确且鲁棒的跟踪和建图。我们的核心思想是通过利用基础深度模型的引导，将光流估计与几何推理相结合。为此，我们首先开发了一个混合光流网络，生成几何感知的对应关系，使得在不同关键帧之间能够进行一致的深度和位姿推断。为了强制全局一致性，我们提出了双一致性束束优化层，在多视图约束下联合优化关键帧位姿和深度。此外，我们引入了可靠性感知的细化机制，通过区分可靠和不确定区域动态调整光流更新过程，在匹配和优化之间形成闭环反馈。大量实验表明，FoundationSLAM 在多个具有挑战性的数据集上实现了卓越的轨迹精度和稠密重建质量，同时以 18 FPS 的实时速度运行，展示了对各种场景的强泛化能力和我们方法的实际适用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文针对现有单目密集 SLAM 系统仅依赖光流而缺乏几何一致性的问题，导致位姿估计和稠密重建出现结构错误和不完整。几何一致性是机器人导航和三维建模的核心，直接影响系统的鲁棒性和精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为需要将几何先验引入光流匹配，并在多视图优化中强化几何约束，于是结合了已有的流式 SLAM（如 DROID‑SLAM）和基础深度模型（如 FoundationStereo）的思路，设计了融合深度特征的混合流网络和双向束调整层。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让光流估计受深度先验指导，并在闭环中通过多视图几何约束共同优化深度和位姿。整体流程是：先用混合流网络利用基础深度特征预测光流；随后双向束调整层利用流和几何残差联合优化关键帧的深度与位姿；最后可靠性感知的细化模块根据优化残差更新流的可靠性掩码并再次迭代。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新包括：① 将基础深度模型的几何特征注入光流网络，实现几何感知的对应估计；② 提出双向一致性束调整层，显式加入正向和反向几何一致性约束；③ 可靠性感知的流细化机制，根据优化残差动态调整流更新。相比仅基于像素相关性的传统流式 SLAM，这些设计实现了更强的全局几何一致性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文通过融合基础深度先验、双向几何约束和残差驱动的流细化，构建了一个几何一致且实时的单目稠密 SLAM 系统。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.&lt;/p&gt;</description></item><item><guid>2512.25008v2</guid><title>FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</title><link>http://arxiv.org/abs/2512.25008v2</link><author>Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 FoundationSLAM，一种基于学习的单目稠密 SLAM 系统，解决了以往基于光流的方法缺乏几何一致性的问题，实现了准确、鲁棒的跟踪与建图。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的光流方法在稠密 SLAM 中缺乏几何一致性，导致跟踪和重建精度受限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过将光流估计与几何推理相结合，利用基础深度模型的引导，提升单目稠密 SLAM 的精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1) 设计 Hybrid Flow Network 生成几何感知的对应关系；2) 引入 Bi-Consistent Bundle Adjustment 层，在多视角约束下联合优化关键帧位姿和深度；3) 开发 Reliability-Aware Refinement 机制，根据可靠与不确定区域动态调整光流更新，形成匹配与优化的闭环。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明 FoundationSLAM 在多组挑战性数据集上实现了更高的轨迹精度和稠密重建质量，并能以 18 FPS 的实时速度运行，展现出良好的泛化能力和实用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; FoundationSLAM 通过将光流与几何一致性结合，显著提升了单目稠密 SLAM 的性能，证明了其在多场景下的可行性和实用价值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 我们提出了 FoundationSLAM，一种基于学习的单目稠密 SLAM 系统，解决了以往基于光流的方法在实现精确和鲁棒跟踪与建图时缺乏几何一致性的问题。我们的核心思路是通过利用基础深度模型的指导，将光流估计与几何推理相结合。为此，我们首先开发了一个 Hybrid Flow Network，能够产生几何感知的对应关系，从而在不同关键帧之间实现深度和位姿的一致推断。为保证全局一致性，我们提出了 Bi-Consistent Bundle Adjustment 层，在多视角约束下联合优化关键帧位姿和深度。此外，我们引入了 Reliability-Aware Refinement 机制，通过区分可靠与不确定区域动态适配光流更新过程，形成匹配与优化之间的闭环。大量实验表明，FoundationSLAM 在多个具有挑战性的数据集上实现了更优的轨迹精度和稠密重建质量，并以 18 FPS 的实时速度运行，展示了对各种场景的强泛化能力和方法的实用性。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决了基于光流的单目稠密SLAM缺乏几何一致性的问题。缺乏几何一致性导致深度估计出现结构错误、姿态误差累积，影响机器人导航、增强现实等应用的可靠性。实现精确、鲁棒的定位与建图是SLAM研究的核心目标。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出光流估计独立且缺少多视角约束是主要瓶颈，并借鉴了基础深度模型（如FoundationStereo、DUSt3R）以及现有的端到端SLAM框架（DROID‑SLAM、DPVO）。他们将这些先验知识融入到Hybrid Flow Network、Bi‑Consistent Bundle Adjustment和Reliability‑Aware Refinement三个模块中，形成一个闭环的、可微分的系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将几何先验与光流匹配紧密耦合，并通过多视角几何约束来引导优化。流程为：1）Hybrid Flow Network利用冻结的深度先验特征与可训练分支融合，生成初始光流；2）Flow GRU迭代细化光流；3）Bi‑Consistent BA层同时最小化光流残差和几何一致性残差，更新深度与相机姿态；4）Reliability‑Aware Refinement根据残差生成可靠性掩码，动态调整光流更新，形成闭环。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) Hybrid Flow Network将冻结的基础深度模型特征与任务特定分支融合，提升光流的几何感知；2) Bi‑Consistent Bundle Adjustment层引入双向几何一致性残差和置信度加权，强化多视角约束；3) Reliability‑Aware Refinement利用优化残差自适应地调整光流更新，形成闭环反馈。与以往仅在像素层面估计光流或独立优化的系统不同，FoundationSLAM实现了端到端的几何一致性闭环，并在多数据集上实现实时（18 FPS）且更优的轨迹与重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FoundationSLAM通过将基础深度先验、光流匹配和多视角束调整紧密耦合，构建了一个可微分、实时的稠密SLAM管线，实现了更高的轨迹精度和重建质量。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.&lt;/p&gt;</description></item><item><guid>2512.25061v1</guid><title>Melting curve of correlated iron at Earth's core conditions from machine-learned DFT+DMFT</title><link>http://arxiv.org/abs/2512.25061v1</link><author>Rishi Rao, Li Zhu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种机器学习加速器，通过训练具备空间对称性的图神经网络来预测局部自能和费米能级，从而为电荷自洽的 DFT+DMFT 计算提供高效的初始值，显著降低迭代次数，并利用该方法在地核压力下获得铁的熔点曲线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确确定地球内核边界处铁的熔融曲线需要精确的有限温度电子关联计算，但传统的 DFT+DMFT 方法计算成本过高，难以进行大规模热力学采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种机器学习加速器，以提升电荷自洽 DFT+DMFT 的计算效率，使其能够用于大规模的热力学研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用高通量的 Fe、FeO 和 NiO 数据，训练 E(3) 对称的图神经网络预测原子环境中的局部自能和费米能级，作为 DMFT 自洽循环的热启动；随后生成相关能量和力，训练神经网络原子间势，并通过两相共存模拟确定熔点曲线。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该加速器将 DMFT 迭代次数降低约两到四倍；在 330 吉帕压力下预测铁的熔化温度约为 6225 开尔文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 机器学习加速器显著提升了 DFT+DMFT 的计算效率，使得在地核条件下进行精确的熔点预测成为可能，为研究地球深部物质提供了可靠的工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可靠约束地球内核边界处铁的熔融曲线需要精确的有限温度电子关联，但 DFT+DMFT 计算仍然过于昂贵，难以进行大规模热力学采样。在此，我们通过训练 E(3) 对称的图神经网络来预测原子环境中的局部自能和费米能级，提供对 DMFT 自洽循环的高效热启动，从而开发出一种机器学习加速器用于电荷自洽 DFT+DMFT。利用 Fe、FeO 和 NiO 的高通量数据，我们实现了约两到四倍的 DMFT 迭代次数减少。借助这一改进，我们在核心压力下生成了铁的相关能量和力，训练了神经网络原子间势，并通过两相共存模拟确定熔点曲线。我们预测在 330 吉帕压力下的熔化温度为 6225 开尔文。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable constraints on iron&amp;#x27;s melting curve at Earth&amp;#x27;s inner-core boundary require accurate finite-temperature electronic correlations, yet DFT+DMFT calculations remain too costly for large-scale thermodynamic sampling. Here, we develop a machine-learning accelerator for charge self-consistent DFT+DMFT by training E(3)-equivariant graph neural networks to predict the local self-energy and Fermi level from atomic environments, providing an efficient warm start to the DMFT self-consistency loop. Using high-throughput data for Fe, FeO, and NiO, we obtain a 2-4 times reuduction in DMFT iterations. Leveraging this improvement, we generate correlated energies and forces for Fe at core pressures, train a neural-network interatomic potential, and determine the melting curve via two-phase coexistence simulations. We obtain a predicted melting temperature of 6225 K at 330 GPa.&lt;/p&gt;</description></item><item><guid>2601.00041v1</guid><title>Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging</title><link>http://arxiv.org/abs/2601.00041v1</link><author>Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究评估了三种先进卷积神经网络模型在儿童肺炎胸部X光图像自动分类中的表现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 儿童肺炎是全球儿童发病率和死亡率的主要原因，及时准确的诊断至关重要，但受限于放射学专业知识和儿童影像的复杂性，诊断存在挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 比较ResNetRS、RegNet和EfficientNetV2在使用迁移学习进行儿童肺炎与正常胸片二分类时的准确率和灵敏度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 从公开数据集中挑选1000张胸片，进行预处理和二分类标注；使用ImageNet预训练权重微调三种模型，并以准确率和灵敏度评估性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; RegNet以92.4%的准确率和90.1%的灵敏度表现最佳，其次是ResNetRS（91.9%/89.3%）和EfficientNetV2（88.5%/88.1%）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 在迁移学习框架下，RegNet在儿童肺炎胸片分类任务中表现最优，显示其在临床辅助诊断中的潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Pediatric pneumonia remains a leading cause of morbidity and mortality in children worldwide. Timely and accurate diagnosis is critical but often challenged by limited radiological expertise and the physiological and procedural complexity of pediatric imaging. This study investigates the performance of state-of-the-art convolutional neural network (CNN) architectures ResNetRS, RegNet, and EfficientNetV2 using transfer learning for the automated classification of pediatric chest Xray images as either pneumonia or normal. A curated subset of 1,000 chest X-ray images was extracted from a publicly available dataset originally comprising 5,856 pediatric images. All images were preprocessed and labeled for binary classification. Each model was fine-tuned using pretrained ImageNet weights and evaluated based on accuracy and sensitivity. RegNet achieved the highest classification performance with an accuracy of 92.4 and a sensitivity of 90.1, followed by ResNetRS (accuracy: 91.9, sensitivity: 89.3) and EfficientNetV2 (accuracy: 88.5, sensitivity: 88.1).&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Pediatric pneumonia remains a leading cause of morbidity and mortality in children worldwide. Timely and accurate diagnosis is critical but often challenged by limited radiological expertise and the physiological and procedural complexity of pediatric imaging. This study investigates the performance of state-of-the-art convolutional neural network (CNN) architectures ResNetRS, RegNet, and EfficientNetV2 using transfer learning for the automated classification of pediatric chest Xray images as either pneumonia or normal.A curated subset of 1,000 chest X-ray images was extracted from a publicly available dataset originally comprising 5,856 pediatric images. All images were preprocessed and labeled for binary classification. Each model was fine-tuned using pretrained ImageNet weights and evaluated based on accuracy and sensitivity. RegNet achieved the highest classification performance with an accuracy of 92.4 and a sensitivity of 90.1, followed by ResNetRS (accuracy: 91.9, sensitivity: 89.3) and EfficientNetV2 (accuracy: 88.5, sensitivity: 88.1).&lt;/p&gt;</description></item><item><guid>2601.00075v1</guid><title>IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business</title><link>http://arxiv.org/abs/2601.00075v1</link><author>Swetha Varadarajan, Abhishek Ray, Lumina Albert</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一种名为IMBWatch的空间-时间图神经网络框架，用于大规模检测伪装成合法按摩店的非法按摩业务（IMB），并通过开放源情报构建动态图谱，显著提升检测准确率和可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 非法按摩业务以合法健康服务为幌子，参与人口贩运、性剥削和强迫劳动，且因广告加密、人员和地点频繁变动、共享基础设施等特点，传统的社区举报和监管检查难以全面揭示其运营网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种主动、可扩展的检测方法，能够捕捉IMB的空间-时间演化模式，弥补传统被动手段的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; IMBWatch利用从网络广告、营业执照记录和众包评论中提取的开放源情报，构建包含企业、别名、电话号码、地点等异构节点的动态图谱；边表示共址、重复使用电话、同步广告等时空关系；通过图卷积与时间注意力机制联合建模网络演化，识别跨城市人员流动、一次性手机轮换和广告同步等特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在美国多城市真实数据集上实验表明，IMBWatch在准确率和F1分数上均优于基线模型；其可解释性使执法部门能够获得可操作的洞察，支持主动干预。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; IMBWatch是一种可扩展、可迁移到其他非法领域的检测框架，已公开匿名数据和开源代码，促进可重复研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Illicit Massage Businesses (IMBs) are covert and persistent forms of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on. To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges. Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.   To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.   Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.&lt;/p&gt;</description></item><item><guid>2601.00092v1</guid><title>Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark</title><link>http://arxiv.org/abs/2601.00092v1</link><author>Pan Wang, Yang Liu, Guile Wu, Eduardo R. Corral-Soto, Chengjie Huang, Binbin Xu, Dongfeng Bai, Xu Yan, Yuan Ren, Xingxin Chen, Yizhe Wu, Tao Huang, Wenjun Wan, Xin Wu, Pei Zhou, Xuyang Dai, Kangbo Lv, Hongbo Zhang, Yosef Fried, Aixue Ye, Bailan Feng, Zhenyu Chen, Zhen Li, Yingcong Chen, Yiyi Liao, Bingbing Liu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 Spatial4D-Bench，一个大规模多任务评测基准，用于全面评估多模态大型语言模型在四维空间智能方面的能力。该基准包含约四万条问答对，涵盖 18 个任务，按六类认知范畴组织。实验表明，现有的开源和专有模型在路径规划、动作识别、物理可行性推理等方面仍存在显著局限。作者希望该基准能为社区提供洞见，推动更强大的模型朝人类水平的四维空间智能发展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 四维空间智能指感知和处理物体随时间移动或变化的能力，人类天然具备此能力，支持广泛的空间推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 评估多模态大型语言模型是否能达到人类水平的四维空间智能，并提供一个全面、可扩展的评测基准。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建 Spatial4D-Bench，包含约四万条问答对，覆盖 18 个定义明确的任务，按对象理解、场景理解、空间关系理解、时空关系理解、空间推理、时空推理六类组织；随后对多种先进模型进行基准测试。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 多模态大型语言模型在四维空间推理方面表现出显著不足，尤其在路径规划、动作识别和物理可行性推理等方面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该基准揭示了现有模型的局限，为进一步提升模型的四维空间智能提供了方向和参考。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.&lt;/p&gt;</description></item><item><guid>2601.00095v1</guid><title>Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning</title><link>http://arxiv.org/abs/2601.00095v1</link><author>Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; MetaJuLS 是一种元强化学习方法，能够学习通用的约束传播策略，适用于多语言和多任务的结构化推理，显著提升速度并保持高准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着大型语言模型的普及，结构化推理需求日益增长，需要满足 JSON 模式、跨语言解析等复杂约束。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出 MetaJuLS，目标是实现无需任务特定再训练即可跨语言、跨任务的约束传播。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将结构化推理视为自适应约束传播，使用图注意力网络与元学习训练策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; MetaJuLS 在 GPU 优化基线上实现 1.5 到 2.0 倍速度提升，准确率与最先进解析器相差不到 0.2%；在 10 种语言的 Universal Dependencies 以及 LLM 受限生成任务中，单一策略可在 5 到 10 次梯度更新内快速适应新语言和任务；机制分析显示策略发现类似人类的易先解析策略和新颖的非直观启发式；减少传播步骤降低了 LLM 推理的碳足迹。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; MetaJuLS 提供了一种高效、准确、绿色的跨域结构化推理解决方案，显著降低了训练和推理成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 大型语言模型越来越需要结构化推理，从 JSON 模式强制到多语言解析，输出必须满足复杂约束。我们提出 MetaJuLS，一种元强化学习方法，学习可在多语言和多任务中通用的约束传播策略，无需任务特定再训练。通过将结构化推理表述为自适应约束传播，并使用图注意力网络与元学习训练，MetaJuLS 在 GPU 优化基线上实现 1.5 到 2.0 倍速度提升，同时保持与最先进解析器相差不到 0.2% 的准确率。在 Universal Dependencies 的 10 种语言和 LLM 受限生成（LogicBench、GSM8K-受限）上，MetaJuLS 展示了快速的跨域适应：在英语解析上训练的策略可在 5 到 10 次梯度步骤（5 到 15 秒）内适应新语言和任务，而不需要数小时的任务特定训练。机制分析揭示该策略发现了类似人类的易先解析策略和新颖的非直观启发式。通过减少 LLM 部署中的传播步骤，MetaJuLS 通过直接降低推理碳足迹为绿色 AI 做出了贡献。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\times$ speedups over GPU-optimized baselines while maintaining within 0.2\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.&lt;/p&gt;</description></item><item><guid>2601.00115v1</guid><title>Adaptive Pinching Antenna Optimization via Meta-Learning for Physical-Layer Security in Dynamic Wireless Networks</title><link>http://arxiv.org/abs/2601.00115v1</link><author>Khalid T. Musri, Akram Y. Sarhan, Osamah A. Abdullah, Hayder Al-Hraishawi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于梯度的元学习框架，用于在用户位置不确定和物理层安全约束下实时控制波导式压缩天线系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在波导式压缩天线系统中，用户位置不确定和物理层安全需求导致系统性能受限，需要新的控制方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种能够在动态环境中快速适应、满足可靠性和安全性要求的控制框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建概率系统模型，提出联合天线位置和发射功率优化问题，并使用模型无关元学习（MAML）学习可迁移的初始化，实现少样本在线适应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仿真表明该框架在掉线概率、保密性能和收敛延迟方面显著优于Reptile元学习、非元强化学习、传统优化、静态天线布置和仅功率控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 元学习是实现非平稳无线环境下可重构压缩天线系统安全低延迟控制的有效工具。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种基于梯度的元学习框架，用于在用户位置不确定和物理层安全约束下实时控制波导式压缩天线系统。我们引入了一个概率系统模型，以捕捉定位不精确对掉线性能和保密性的影响。基于该模型，构建了联合天线位置和发射功率优化问题，以满足概率可靠性和保密性要求。为在高度动态环境中实现快速适应，所提出的方法采用模型无关元学习（MAML）在不同移动性和信道条件下学习可迁移的初始化，从而利用有限的导频反馈实现少样本在线适应。仿真结果表明，所提出的框架在掉线概率、保密性能和收敛延迟方面显著优于基于Reptile的元学习、非元强化学习、传统优化、静态天线布置和仅功率控制。这些结果证明了元学习是非平稳无线环境下可重构压缩天线系统安全低延迟控制的有效工具。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper develops a gradient-based meta-learning framework for real-time control of waveguided pinching-antenna systems under user-location uncertainty and physical-layer security (PLS) constraints. A probabilistic system model is introduced to capture the impact of imperfect localization on outage performance and secrecy. Based on this model, a joint antenna-positioning and transmit-power optimization problem is formulated to satisfy probabilistic reliability and secrecy requirements. To enable rapid adaptation in highly dynamic environments, the proposed approach employs model-agnostic meta-learning (MAML) to learn a transferable initialization across diverse mobility and channel conditions, allowing few-shot online adaptation using limited pilot feedback. Simulation results demonstrate that the proposed framework significantly outperforms Reptile-based meta-learning, non-meta reinforcement learning, conventional optimization, static antenna placement, and power-only control in terms of outage probability, secrecy performance, and convergence latency. These results establish meta-learning as an effective tool for secure and low-latency control of reconfigurable pinching-antenna systems in non-stationary wireless environments.&lt;/p&gt;</description></item><item><guid>2601.00133v1</guid><title>In context learning Foundation models for Materials Property Prediction with Small datasets</title><link>http://arxiv.org/abs/2601.00133v1</link><author>Qinyang Li, Rongzhi Dong, Nicholas Miklaucic, Jeffrey Hu, Sadman Sadeed Omee, Lai Wei, Sourin Dey, Ming Hu, Jianjun Hu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种统一的上下文学习基础模型框架，用于材料属性预测，结合成分和结构表示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 基础模型在多学科科学领域表现出卓越的上下文学习能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个能够高效、低成本预测材料属性的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将预训练的 TabPFN transformer 与图神经网络嵌入和新型 MagpieEX 描述符结合，MagpieEX 通过离子-阴离子相互作用数据增强特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 MatBench 基准和晶格热导率数据集上，ICL-FM 的性能与最先进模型相当或更优，且训练成本显著降低；在六个成分任务中，ICL-FM 在五个任务上优于 GNN 模型，尤其在声子频率预测上提升 9.93%；t‑SNE 分析显示模型将离散特征聚类转化为连续流形，提升插值预测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; ICL-FM 是一种可推广、数据高效的材料信息学范式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基础模型（FMs）最近在多种科学领域展示了卓越的上下文学习（ICL）能力。本文提出了一种统一的上下文学习基础模型（ICL‑FM）框架，用于材料属性预测，整合了基于成分和结构感知的表示。该方法将预训练的 TabPFN transformer 与图神经网络（GNN）生成的嵌入以及我们新颖的 MagpieEX 描述符相结合。MagpieEX 在传统特征基础上加入了阳离子-阴离子相互作用数据，显式测量键离子化程度和电荷转移不对称性，捕捉影响振动和热传输的原子间键合特征。对 MatBench 基准套件和单独的晶格热导率（LTC）数据集进行全面实验表明，ICL‑FM 在保持竞争或优于最先进模型的同时，显著降低了训练成本。值得注意的是，训练无关的 ICL‑FM 在六个代表性成分任务中有五个任务优于复杂的最先进 GNN 模型，尤其在声子频率预测上提升了 9.93%。在 LTC 数据集上，FM 有效建模了声子-声子散射和原子质量对比等复杂现象。t‑SNE 分析显示，FM 充当物理感知的特征细化器，将原始离散特征聚类转化为连续流形，实现属性平滑过渡。该重构的潜在空间提升了插值预测精度，并与基础物理规律保持一致。本文确立了 ICL‑FM 作为一种通用、数据高效的材料信息学范式。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Foundation models (FMs) have recently shown remarkable in-context learning (ICL) capabilities across diverse scientific domains. In this work, we introduce a unified in-context learning foundation model (ICL-FM) framework for materials property prediction that integrates both composition-based and structure-aware representations. The proposed approach couples the pretrained TabPFN transformer with graph neural network (GNN)-derived embeddings and our novel MagpieEX descriptors. MagpieEX augments traditional features with cation-anion interaction data to explicitly measure bond ionicity and charge-transfer asymmetry, capturing interatomic bonding characteristics that influence vibrational and thermal transport properties. Comprehensive experiments on the MatBench benchmark suite and a standalone lattice thermal conductivity (LTC) dataset demonstrate that ICL-FM achieves competitive or superior performance to state-of-the-art (SOTA) models with significantly reduced training costs. Remarkably, the training-free ICL-FM outperformed sophisticated SOTA GNN models in five out of six representative composition-based tasks, including a significant 9.93\% improvement in phonon frequency prediction. On the LTC dataset, the FM effectively models complex phenomena such as phonon-phonon scattering and atomic mass contrast. t-SNE analysis reveals that the FM acts as a physics-aware feature refiner, transforming raw, disjoint feature clusters into continuous manifolds with gradual property transitions. This restructured latent space enhances interpolative prediction accuracy while aligning learned representations with underlying physical laws. This study establishes ICL-FM as a generalizable, data-efficient paradigm for materials informatics.&lt;/p&gt;</description></item><item><guid>2601.00139v1</guid><title>Compressed Map Priors for 3D Perception</title><link>http://arxiv.org/abs/2601.00139v1</link><author>Brady Zhou, Philipp Krähenbühl</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种压缩地图先验（CMP）框架，通过学习历史行驶轨迹中的空间先验，显著提升自动驾驶视觉系统的3D目标检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类驾驶员很少驶入无人探索的新区域，自动驾驶视觉系统也往往在已被访问过的区域工作，却仍像首次遇到一样处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用历史行驶数据学习空间先验，以减少对新环境的误判并提升感知准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; CMP采用二值化哈希映射，存储量仅为每平方公里32KB，比传统稠密存储减少约20倍；该先验可无缝集成至主流3D感知系统，几乎不增加计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在nuScenes数据集上，CMP在多种架构下均实现了显著且一致的3D目标检测提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 压缩地图先验是一种简单、高效、低存储、低计算成本的方案，可显著提升自动驾驶视觉系统的检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类驾驶员很少驶入无人探索的新区域。毕竟，每天都有成千上万的驾驶员使用繁忙的城市道路，只有一个人可以声称自己是第一个。自动驾驶计算机视觉系统也同样如此。自动驾驶视觉系统的大部分部署区域已经被访问过。然而，大多数自动驾驶车辆视觉系统仍然像第一次遇到每个位置一样行事。在这项工作中，我们提出了压缩地图先验（CMP），一种简单但有效的框架，用于从历史行驶轨迹中学习空间先验。地图先验使用二值化哈希映射，仅需要每平方公里32KB，比稠密存储减少20倍。压缩地图先验易于集成到领先的3D感知系统中，几乎不增加额外计算成本，并在nuScenes数据集上跨多种架构显著且一致地提升了3D目标检测。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 自动驾驶系统大部分时间在已被多次行驶过的区域工作，但现有感知模型往往把每一次观测都当作全新场景，导致对静态环境的误判。利用历史行驶数据提供的空间先验可以显著提升检测精度，减少误报并提高系统可靠性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者意识到需要一种持久的、可学习的空间记忆，借鉴了神经隐式函数和多分辨率哈希编码的思想，并参考了 Hindsight、MapFusion 等利用历史数据的先验方法。通过将哈希嵌入量化为二进制特征，设计了可微分的压缩地图表示，并在训练时与感知网络联合优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心是用二进制哈希嵌入构建稀疏压缩的地图先验。对每个查询点先在多尺度哈希表中检索四角嵌入，做双线性插值得到多尺度特征，再用小 MLP 合并成单一先验特征。随后用轻量级融合模块（卷积或交叉注意力）将先验特征与多视角图像特征拼接或注意力融合，最终送入检测头。训练时对先验特征做随机块遮蔽，保证模型在缺失先验时仍稳健。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 采用二进制哈希嵌入实现 20 倍内存压缩；2) 先验表示可微分，能与任何 3D 检测网络端到端联合训练；3) 兼容稠密网格和基于 Transformer 的稀疏检测器；4) 仅增加约 3% 计算量。与以往使用显式地图或点云先验的方法不同，CMP 通过隐式函数学习空间上下文，既紧凑又可直接融入现有架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出一种轻量级、可微分的压缩地图先验，利用二进制哈希嵌入在极低内存和计算成本下为现有 3D 感知系统提供持久空间上下文，并显著提升检测性能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.&lt;/p&gt;</description></item><item><guid>2601.00141v1</guid><title>Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection</title><link>http://arxiv.org/abs/2601.00141v1</link><author>Lawrence Han</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; GLASS是一种结合全局视图和局部采样的检测架构，利用注意力聚合原始分辨率局部区域，提升AI生成图像检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 生成式AI快速发展，使AI生成图像逼真且高分辨率，传统检测方法往往下采样，导致细节丢失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种新的架构GLASS，以同时利用全局和局部信息，提高检测精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; GLASS通过全局缩放视图和多块随机采样的原始分辨率局部裁剪，使用空间分层采样选择裁剪，并通过注意力评分聚合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验表明，GLASS在Vision Transformer、ResNet和ConvNeXt等骨干网络上，性能优于标准迁移学习，且在可接受的计算资源下实现更高预测准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; GLASS能够在任何尺寸图像中有效整合全局与局部信息，显著提升AI生成图像检测效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 生成式人工智能的快速发展使得AI生成的图像越来越逼真且高分辨率。大多数AI生成图像检测架构通常在输入模型前对图像进行下采样，可能导致细粒度细节的丢失。本文提出GLASS（Global-Local Attention with Stratified Sampling），一种结合全局缩放视图与多块随机采样局部裁剪的架构。这些裁剪是通过空间分层采样高效选择的原始分辨率区域，并使用基于注意力的评分进行聚合。GLASS可以集成到视觉模型中，以利用任何尺寸图像的全局和局部信息。本文使用Vision Transformer、ResNet和ConvNeXt模型作为骨干，并通过实验表明GLASS在可行的计算约束下，通过实现更高的预测性能，优于标准迁移学习。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.&lt;/p&gt;</description></item><item><guid>2601.00146v1</guid><title>Combining datasets with different ground truths using Low-Rank Adaptation to generalize image-based CNN models for photometric redshift prediction</title><link>http://arxiv.org/abs/2601.00146v1</link><author>Vikram Seenivasan, Srinath Saikrishnan, Andrew Lizarraga, Jonathan Soriano, Bernie Boscoe, Tuan Do</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; LoRA技术用于结合不同星系成像数据集，以改进CNN模型的红移估计。通过先在光度红移数据集上训练基模型，再用LoRA在光谱红移数据集上微调，得到比传统迁移学习更低偏差和更小散布的模型。与在合并数据集上重新训练相比，LoRA在计算时间上更高效，提供了全重新训练与不微调之间的折中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在宇宙学中，星系红移估计是关键任务。光度红移数据量大但精度低；光谱红移精度高但样本稀少且获取耗时。需要一种方法能利用两种数据集的优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探索LoRA在天体物理回归模型中的可行性，评估其在星系红移估计中的性能，并与传统迁移学习和全重新训练进行比较。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用光度红移真值数据集训练CNN基模型；随后使用LoRA在光谱红移真值数据集上进行微调；还对比了在合并数据集上重新训练的模型。评估指标包括偏差和散布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; LoRA微调的模型比传统迁移学习降低约2.5倍偏差、2.2倍散布；在合并数据集上重新训练的模型泛化更好，但计算成本更高。LoRA在数据稀缺任务中能有效利用预训练模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; LoRA为天体物理回归任务提供了一个在全重新训练与不微调之间的实用折中方案，能够在保持计算效率的同时提升模型精度，尤其适用于数据稀缺场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在本研究中，我们演示了如何使用低秩适配（LoRA）将不同的星系成像数据集合并，以改进用于宇宙学的卷积神经网络模型的红移估计。LoRA 是一种已被广泛应用于大型语言模型的技术，它通过添加适配器网络来调整模型权重和偏置，从而在不重新训练的情况下高效微调大型基础模型。我们首先使用光度红移真值数据集训练一个基模型，该数据集包含广泛的星系类型，但精度较低。随后，我们在光谱红移真值数据集上使用 LoRA 进行微调。这些红移更为精确，但仅限于亮星系，并且获取时间比光度红移长数个数量级，因此在大规模调查中可用性较低。理想情况下，将两种数据集结合起来可以得到更精确且泛化性更好的模型。LoRA 模型的表现优于传统迁移学习方法，偏差降低约 2.5 倍，散布降低约 2.2 倍。在合并数据集上重新训练模型可以得到比 LoRA 更好的泛化性能，但计算时间更长。我们的工作表明，LoRA 对于天体物理中的回归模型微调非常有用，它在完全重新训练和不微调之间提供了一个折中方案。LoRA 在利用现有预训练天体物理模型方面显示出潜力，尤其适用于数据稀缺的任务。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this work, we demonstrate how Low-Rank Adaptation (LoRA) can be used to combine different galaxy imaging datasets to improve redshift estimation with CNN models for cosmology. LoRA is an established technique for large language models that adds adapter networks to adjust model weights and biases to efficiently fine-tune large base models without retraining. We train a base model using a photometric redshift ground truth dataset, which contains broad galaxy types but is less accurate. We then fine-tune using LoRA on a spectroscopic redshift ground truth dataset. These redshifts are more accurate but limited to bright galaxies and take orders of magnitude more time to obtain, so are less available for large surveys. Ideally, the combination of the two datasets would yield more accurate models that generalize well. The LoRA model performs better than a traditional transfer learning method, with $\sim2.5\times$ less bias and $\sim$2.2$\times$ less scatter. Retraining the model on a combined dataset yields a model that generalizes better than LoRA but at a cost of greater computation time. Our work shows that LoRA is useful for fine-tuning regression models in astrophysics by providing a middle ground between full retraining and no retraining. LoRA shows potential in allowing us to leverage existing pretrained astrophysical models, especially for data sparse tasks.&lt;/p&gt;</description></item><item><guid>2601.00202v1</guid><title>Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2601.00202v1</link><author>Wang Xing, Wei Song, Siyu Lin, Chen Wu, Zhesi Li, Man Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种专门针对时序知识图谱推理的蒸馏框架，利用大型语言模型作为教师，将结构和时序推理能力迁移到轻量化学生模型，从而在保持高准确率的同时显著降低模型规模和计算成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 时序知识图谱推理是提升智能决策系统效率与可靠性的关键技术，但现有模型往往参数庞大、计算量大，导致硬件成本高、能耗大，难以在资源受限、低功耗、分布式平台上实现实时推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决现有压缩与蒸馏技术无法充分捕捉时序依赖、导致推理性能下降的问题，构建一种高效、可部署的时序知识图谱推理模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用大型语言模型作为教师，引导蒸馏过程；将大规模公开知识与任务特定的时序信息融合，提升学生模型对时序动态的建模能力，同时保持紧凑高效的架构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多个公开基准数据集上，所提方法持续优于强基线，在推理准确率、计算效率和可部署性之间取得了良好平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该蒸馏框架有效克服了大模型的资源瓶颈，提升了时序知识图谱推理的实用性，为未来人工智能应用提供了可靠技术基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时序知识图谱（TKG）的推理是提升智能决策系统效率和可靠性的基础，并已成为未来人工智能应用的关键技术。尽管最近取得了进展，但现有的 TKG 推理模型通常依赖大量参数和高强度计算，导致硬件成本和能耗高昂。这些限制阻碍了它们在资源受限、低功耗和分布式平台上的部署，这些平台需要实时推理。此外，大多数现有的模型压缩和蒸馏技术是为静态知识图谱设计的，无法充分捕捉 TKG 中固有的时序依赖，往往导致推理性能下降。为了解决这些挑战，我们提出了一个专门针对时序知识图谱推理的蒸馏框架。我们的方法利用大型语言模型作为教师模型来指导蒸馏过程，使结构和时序推理能力能够有效迁移到轻量化学生模型。通过将大规模公开知识与任务特定的时序信息相结合，所提出的框架增强了学生模型对时序动态的建模能力，同时保持紧凑高效的架构。在多个公开可用的基准数据集上进行的广泛实验表明，我们的方法始终优于强基线，在推理准确率、计算效率和实际可部署性之间实现了良好的权衡。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model&amp;#x27;s ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.&lt;/p&gt;</description></item><item><guid>2601.00212v1</guid><title>IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation</title><link>http://arxiv.org/abs/2601.00212v1</link><author>Han Liu, Yubo Fan, Hao Li, Dewei Hu, Daniel Moyer, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于样本的风格合成方法 IntraStyler，用于在无监督域适应中捕获目标域内部多样化风格，从而提升合成数据的多样性和下游分割性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的图像级域对齐方法主要关注源域与目标域之间的域差距，而对目标域内部的风格变异研究不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决目标域内部风格多样性不足的问题，使合成数据能够覆盖更丰富的风格，从而改善域适应效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用样本图像引导风格合成，利用风格编码器通过对比学习学习仅包含风格的特征，并在图像翻译过程中生成与样本相匹配的风格。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 CrossMoDA 2023 数据集上实验表明，IntraStyler 能实现可控的风格合成，并且多样化的合成数据显著提升了下游分割任务的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 样本驱动的风格合成方法能够在无需预先指定风格的情况下捕获目标域内部多样性，为无监督域适应提供了有效手段。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 图像级域对齐是无监督域适应的事实标准方法，其中使用无配对图像翻译来最小化域差距。先前的研究主要关注源域与目标域之间的域偏移，而目标域内部的变异性仍未得到充分探索。为了解决后者，一个有效的策略是在图像翻译过程中多样化合成目标域数据的风格。然而，以往的方法通常需要预先指定目标域内部的变异性进行风格合成，这在实际中可能不切实际。本文提出了一种基于样本的风格合成方法 IntraStyler，能够在没有任何先验知识的情况下捕获多样化的目标域内部风格。具体而言，IntraStyler 使用一个样本图像来引导风格合成，使输出风格与样本风格匹配。为提取仅包含风格的特征，我们引入了一个风格编码器，基于对比学习进行判别式学习。我们在跨模态域适应的最大公开数据集 CrossMoDA 2023 上评估了所提出的方法。实验结果表明，我们的方法在可控风格合成方面具有有效性，并且多样化的合成数据对下游分割任务具有益处。代码可在 https://github.com/han-liu/IntraStyler 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.&lt;/p&gt;</description></item><item><guid>2601.00225v1</guid><title>Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions</title><link>http://arxiv.org/abs/2601.00225v1</link><author>Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li, Weisheng Dong</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文指出，使用合成数据训练的盲图像质量评估模型在特征分布上表现出离散聚类模式，导致回归性能受限。为解决此问题，作者提出了SynDR-IQA框架，通过分布感知的多样化上采样和密度感知的冗余聚类下采样两种策略，重塑合成数据分布，从而提升模型在不同数据集间的泛化能力。实验表明，该方法在三种交叉数据集设置下均有效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 盲图像质量评估已通过深度学习取得显著进展，但缺乏大规模标注数据仍是瓶颈。合成数据是潜在解决方案，但现有合成数据训练的模型泛化能力有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探究合成数据分布导致的特征聚类问题，并提出方法改善模型的泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建SynDR-IQA框架，基于样本多样性和冗余对泛化误差的理论推导，采用：1）分布感知多样化内容上采样，提升视觉多样性并保持内容分布；2）密度感知冗余聚类下采样，降低密集聚类区域样本密度，实现样本平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 合成数据学习的特征呈现离散聚类模式，主要由数据分布而非模型结构引起；通过重塑分布的SynDR-IQA显著提升了跨数据集的泛化效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SynDR-IQA在synthetic-to-authentic、synthetic-to-algorithmic和synthetic-to-synthetic三种交叉设置中均表现出优异的泛化性能，验证了其有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 盲图像质量评估（BIQA）通过深度学习取得了显著进展，但大型标注数据集的稀缺仍是挑战。合成数据提供了有前景的解决方案，但在现有合成数据集上训练的模型往往表现出有限的泛化能力。本文的关键观察是，来自合成数据集的表示往往呈现离散且聚类的模式，阻碍回归性能：高质量图像的特征聚集在参考图像周围，而低质量图像的特征则按失真类型聚集。我们的分析表明，这一问题源于合成数据的分布，而非模型架构。因此，我们提出了新框架 SynDR-IQA，通过重塑合成数据分布来提升 BIQA 的泛化能力。基于样本多样性和冗余对泛化误差影响的理论推导，SynDR-IQA 采用两种策略：分布感知的多样化内容上采样，增强视觉多样性同时保持内容分布；密度感知的冗余聚类下采样，通过降低密集聚类区域的样本密度来平衡样本。我们在三种跨数据集设置（synthetic-to-authentic、synthetic-to-algorithmic、synthetic-to-synthetic）上进行了广泛实验，证明了我们方法的有效性。代码可在 https://github.com/Li-aobo/SynDR-IQA 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy&amp;#x27;s impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.&lt;/p&gt;</description></item><item><guid>2601.00229v1</guid><title>Robust Graph Fine-Tuning with Adversarial Graph Prompting</title><link>http://arxiv.org/abs/2601.00229v1</link><author>Ziyan Zhang, Bo Jiang, Jin Tang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种新的对抗图提示（AGP）框架，用于在图神经网络的参数高效微调中提升鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 参数高效微调（PEFT）已成为将预训练图神经网络迁移到下游任务的主流方法，但现有方法对图结构和节点特征的噪声与攻击易受影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为解决PEFT方法在噪声和攻击下的脆弱性，首次将对抗学习与图提示相结合，构建AGP框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; AGP将问题建模为最小化-最大化优化，采用交替优化方案。内部最大化使用联合投影梯度下降生成强对抗噪声；外部最小化通过学习节点提示来抵消噪声。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; AGP理论上可同时处理图结构噪声和节点噪声，显示出在多种噪声场景下的通用性和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; AGP是一种通用方法，可与多种预训练图神经网络结合，显著提升下游任务的鲁棒性，并在多个基准任务上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 参数高效微调（PEFT）方法已成为将预训练图神经网络（GNN）适配到下游任务的主导范式。然而，现有的PEFT方法通常对图拓扑和节点属性/特征的各种噪声和攻击表现出显著的脆弱性。为了解决这个问题，我们首次提出将对抗学习整合到图提示中，并开发了一种新颖的对抗图提示（AGP）框架，以实现鲁棒的图微调。我们的AGP有两个关键方面。首先，我们将AGP的通用问题表述为最小化-最大化优化问题，并开发了交替优化方案来求解它。对于内部最大化，我们提出了联合投影梯度下降（JointPGD）算法来生成强对抗噪声。对于外部最小化，我们采用了一个简单而有效的模块来学习最佳节点提示，以抵消对抗噪声。其次，我们证明所提出的AGP理论上可以同时解决图拓扑和节点噪声。这证实了我们的AGP微调方法在各种图噪声下的多功能性和鲁棒性。请注意，所提出的AGP是一种通用方法，可与各种预训练GNN模型集成，以增强其在下游任务上的鲁棒性。对多个基准任务的广泛实验验证了AGP方法相对于最先进方法的鲁棒性和有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.&lt;/p&gt;</description></item><item><guid>2601.00242v1</guid><title>Neural Minimum Weight Perfect Matching for Quantum Error Codes</title><link>http://arxiv.org/abs/2601.00242v1</link><author>Yotam Peled, David Zenati, Eliya Nachmani</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种基于神经网络的量子错误校正解码器，利用图神经网络和Transformer的混合架构来预测动态边权，从而改进传统的最小权完美匹配算法，并通过代理损失实现端到端训练，显著降低了逻辑错误率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 量子计算的实现需要量子错误校正技术，常用的解码方法是最小权完美匹配算法，它通过边权来识别最可能的错误链。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种数据驱动的解码器，提升错误检测和纠正的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建混合网络，将图神经网络提取局部综合特征与Transformer捕捉全局依赖相结合，用以预测MWPM的动态边权；同时设计代理损失函数，使得非可微的MWPM算法可被端到端优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 实验结果表明，该混合解码器在逻辑错误率上相较于传统基线有显著下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将神经网络的预测能力与经典匹配算法的结构相结合的混合解码器能够有效提升量子错误校正的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 实现量子计算的全部潜力需要量子错误校正（QEC）。QEC通过在冗余物理量子比特上编码逻辑信息来降低错误率，使得错误能够被检测和纠正。用于此任务的常见解码器是最小权完美匹配（MWPM），这是一种基于图的算法，依赖边权来识别最可能的错误链。在本研究中，我们提出了一种名为神经最小权完美匹配（NMWPM）的数据驱动解码器。我们的解码器采用混合架构，集成图神经网络（GNN）以提取局部综合特征，并使用Transformer捕捉长程全局依赖，然后用来预测MWPM解码器的动态边权。为通过非可微的MWPM算法进行训练，我们制定了一种新颖的代理损失函数，使得端到端优化成为可能。我们的发现表明，与标准基线相比，逻辑错误率显著降低，突出了将神经网络的预测能力与经典匹配算法的结构相结合的混合解码器的优势。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Realizing the full potential of quantum computation requires Quantum Error Correction (QEC). QEC reduces error rates by encoding logical information across redundant physical qubits, enabling errors to be detected and corrected. A common decoder used for this task is Minimum Weight Perfect Matching (MWPM) a graph-based algorithm that relies on edge weights to identify the most likely error chains. In this work, we propose a data-driven decoder named Neural Minimum Weight Perfect Matching (NMWPM). Our decoder utilizes a hybrid architecture that integrates Graph Neural Networks (GNNs) to extract local syndrome features and Transformers to capture long-range global dependencies, which are then used to predict dynamic edge weights for the MWPM decoder. To facilitate training through the non-differentiable MWPM algorithm, we formulate a novel proxy loss function that enables end-to-end optimization. Our findings demonstrate significant performance reduction in the Logical Error Rate (LER) over standard baselines, highlighting the advantage of hybrid decoders that combine the predictive capabilities of neural networks with the algorithmic structure of classical matching.&lt;/p&gt;</description></item><item><guid>2601.00243v1</guid><title>Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture</title><link>http://arxiv.org/abs/2601.00243v1</link><author>Anirudha Ghosh, Ritam Sarkar, Debaditya Barman</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出了一种轻量级框架，用于在低资源设备上进行害虫检测和农药推荐，以帮助小农户实现精准农业。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统害虫管理依赖人工检查和化学农药，成本高、耗时长、劳动强度大，并对环境产生负面影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种可在智能手机和无人机等低资源设备上运行的害虫检测与农药推荐系统，以降低成本、提高效率并促进可持续农业。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用轻量级卷积神经网络结合原型元学习进行害虫识别，并在农药推荐模块中加入作物类型、成长阶段等环境因素；通过整合多个公开数据集构建多样化害虫图像数据集进行训练与评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 轻量级CNN在准确率上与先进模型相当，同时显著降低计算复杂度；决策支持系统通过减少化学农药使用，提升了害虫管理效果并鼓励可持续实践。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架在实时精准农业应用中具有潜力，可帮助小农户实现高效、环保的害虫管理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 有效的害虫管理对于提高农业生产力至关重要，尤其是对易受害虫侵袭的甘蔗和小麦等作物。传统害虫管理方法高度依赖人工田间检查和化学农药使用。这些方法往往成本高、耗时长、劳动强度大，并可能对环境产生负面影响。为克服这些挑战，本研究提出了一种轻量级框架，用于害虫检测和农药推荐，适用于智能手机和无人机等低资源设备，适合小农户使用。该框架包括两个主要组件。第一是害虫检测模块，使用紧凑轻量级卷积神经网络结合原型元学习，即使只有少量训练样本也能准确识别害虫。第二是农药推荐模块，结合作物类型和生长阶段等环境因素，建议安全、环保的农药。为训练和评估框架，研究人员通过整合多个公开数据集构建了综合害虫图像数据集。最终数据集包含不同视角、害虫尺寸和背景条件的样本，以确保强泛化能力。实验结果表明，所提出的轻量级CNN在准确率上与最先进模型相当，同时显著降低计算复杂度。决策支持系统通过减少对传统化学农药的依赖并鼓励可持续实践，提升了害虫管理效果，展示了其在精准农业实时应用中的潜力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.   The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.   Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.&lt;/p&gt;</description></item><item><guid>2601.00260v1</guid><title>TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models</title><link>http://arxiv.org/abs/2601.00260v1</link><author>Kohei Yamamoto, Tomohiro Kikuchi</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了TotalFM，一种基于器官分离的3D-CT基础模型，利用大规模数据集和自动化配对技术，在保持计算效率的同时实现了高质量的图像-文本对应学习，并在零样本器官和发现级别的病变分类任务中表现优异。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在放射学中，基础模型被期望应用于多种临床任务，但在3D-CT体积数据上训练时的计算成本是主要挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种既能高效学习3D-CT图像与语言表达对应关系，又能在计算资源受限的情况下实现良好表示能力的基础模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用140,000系列的大规模数据集，通过分割技术和基于大型语言模型的报告处理自动生成器官体积与发现句子对；结合VideoMAE的自监督预训练和基于体积-文本对的对比学习，以实现计算效率与表示能力的平衡。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在零样本器官级别病变分类中，TotalFM在83%（5/6）器官上F1分数高于CT-CLIP，在64%（9/14）器官上高于Merlin；在零样本发现级别分类中，AUROC在83%（25/30）发现类别上高于Merlin；在放射学报告生成任务中，性能与现有视觉-语言模型相当。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 器官分离学习框架为3D-CT基础模型的实际实现提供了可行且有效的设计指导，展示了在真实临床评估环境中的高泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然放射学中的基础模型被期望应用于各种临床任务，但在3D-CT体积数据上训练时的计算成本仍然是主要挑战。在本研究中，我们提出了TotalFM，一种基于器官分离概念的放射学基础模型，利用140,000个系列的大规模数据集高效学习3D-CT图像与语言表达之间的对应关系。通过分割技术和基于大型语言模型的放射学报告处理，自动创建器官体积与发现句子对，并结合VideoMAE的自监督预训练与基于体积-文本对的对比学习，我们旨在平衡计算效率和表示能力。在零样本器官级别病变分类任务中，所提出的模型在83%（5/6）器官上获得更高的F1分数，优于CT-CLIP；在64%（9/14）器官上优于Merlin。这些结果表明，该模型在使用真实放射学报告句子的临床评估设置中具有较高的泛化性能。此外，在零样本发现级别病变分类任务中，我们的模型在83%（25/30）发现类别上获得更高的AUROC，优于Merlin。我们还确认了在放射学报告生成任务中与现有视觉-语言模型相当的性能。我们的结果表明，器官分离学习框架可以作为3D-CT基础模型实际实现的现实且有效的设计指南。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.&lt;/p&gt;</description></item><item><guid>2601.00321v1</guid><title>Offline Multi-Agent Reinforcement Learning for 6G Communications: Fundamentals, Applications and Future Directions</title><link>http://arxiv.org/abs/2601.00321v1</link><author>Eslam Eldeeb, Hirley Alves</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文介绍了一种基于保守Q学习的离线多智能体强化学习算法，并通过元学习扩展以适应动态环境，验证了其在无线资源管理和无人机网络中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 随着5G及6G等下一代无线技术的发展，车辆编队、智慧城市、远程手术等应用需要处理大量互联无线实体，网络复杂度大幅提升，需要更先进的决策算法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种离线多智能体强化学习方法，以降低在线交互成本、提升安全性和可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建基于保守Q学习的离线MARL框架，并加入元学习机制来应对环境变化，随后在无线资源管理和无人机网络场景中进行验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 该算法在离线数据上实现了安全高效的训练，能够在动态环境中保持性能，并在实际案例中表现出优越的资源调度效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 离线MARL在无线应用中具有显著优势，但仍存在局限性，未来研究需进一步探索其适用范围和改进方向。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 下一代无线技术，包括超越5G和6G网络，为车辆编队、智慧城市和远程手术等变革性应用铺平了道路。这些创新由大量互联的无线实体推动，包括物联网设备、接入点、无人机和自动驾驶车辆，增加了网络复杂性并需要更先进的决策算法。人工智能和机器学习，尤其是强化学习，是这些网络的关键赋能技术，为高维度和复杂挑战提供解决方案。然而，随着网络扩展到多智能体环境，传统的在线强化学习方法面临成本、安全性和可扩展性限制。离线多智能体强化学习通过利用预先收集的数据，减少对实时交互的需求，提供了有前景的解决方案。本文提出了一种基于保守Q学习的离线MARL算法，确保安全高效的训练，并通过元学习扩展以应对动态环境，随后在无线资源管理和无人机网络的用例中验证了该方法。我们的工作强调了离线MARL在无线应用中的优势、局限性和未来方向。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The next-generation wireless technologies, including beyond 5G and 6G networks, are paving the way for transformative applications such as vehicle platooning, smart cities, and remote surgery. These innovations are driven by a vast array of interconnected wireless entities, including IoT devices, access points, UAVs, and CAVs, which increase network complexity and demand more advanced decision-making algorithms. Artificial intelligence (AI) and machine learning (ML), especially reinforcement learning (RL), are key enablers for such networks, providing solutions to high-dimensional and complex challenges. However, as networks expand to multi-agent environments, traditional online RL approaches face cost, safety, and scalability limitations. Offline multi-agent reinforcement learning (MARL) offers a promising solution by utilizing pre-collected data, reducing the need for real-time interaction. This article introduces a novel offline MARL algorithm based on conservative Q-learning (CQL), ensuring safe and efficient training. We extend this with meta-learning to address dynamic environments and validate the approach through use cases in radio resource management and UAV networks. Our work highlights offline MARL&amp;#x27;s advantages, limitations, and future directions in wireless applications.&lt;/p&gt;</description></item><item><guid>2601.00355v1</guid><title>The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification</title><link>http://arxiv.org/abs/2601.00355v1</link><author>Tanay Donde</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了皮肤黑色素瘤诊断中模型对病变区域关注度与诊断性能的关系，指出更高的关注度可提升准确率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 黑色素瘤是皮肤癌中致死率最高的亚型，早期准确检测能显著改善患者预后。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 分析模型对病变区域的关注度与诊断性能之间的关联。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用遮罩图像、边界框检测和迁移学习，并使用多种可解释性与敏感性分析方法评估模型关注度与精确率、召回率、F1 分数的关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 模型对病变区域关注度越高，诊断性能越好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 可解释人工智能在医学诊断中具有潜力，可为未来更准确、更可信的黑色素瘤分类模型奠定基础。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Melanoma is the most lethal subtype of skin cancer, and early and accurate detection of this disease can greatly improve patients&amp;#x27; outcomes. Although machine learning models, especially convolutional neural networks (CNNs), have shown great potential in automating melanoma classification, their diagnostic reliability still suffers due to inconsistent focus on lesion areas. In this study, we analyze the relationship between lesion attention and diagnostic performance, involving masked images, bounding box detection, and transfer learning. We used multiple explainability and sensitivity analysis approaches to investigate how well models aligned their attention with lesion areas and how this alignment correlated with precision, recall, and F1-score. Results showed that models with a higher focus on lesion areas achieved better diagnostic performance, suggesting the potential of interpretable AI in medical diagnostics. This study provides a foundation for developing more accurate and trustworthy melanoma classification models in the future.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Melanoma is the most lethal subtype of skin cancer, and early and accurate detection of this disease can greatly improve patients&amp;#x27; outcomes. Although machine learning models, especially convolutional neural networks (CNNs), have shown great potential in automating melanoma classification, their diagnostic reliability still suffers due to inconsistent focus on lesion areas. In this study, we analyze the relationship between lesion attention and diagnostic performance, involving masked images, bounding box detection, and transfer learning. We used multiple explainability and sensitivity analysis approaches to investigate how well models aligned their attention with lesion areas and how this alignment correlated with precision, recall, and F1-score. Results showed that models with a higher focus on lesion areas achieved better diagnostic performance, suggesting the potential of interpretable AI in medical diagnostics. This study provides a foundation for developing more accurate and trustworthy melanoma classification models in the future.&lt;/p&gt;</description></item><item><guid>2601.00357v1</guid><title>Traffic-MoE: A Sparse Foundation Model for Network Traffic Analysis</title><link>http://arxiv.org/abs/2601.00357v1</link><author>Jiajun Zhou, Changhui Sun, Meng Shen, Shanqing Yu, Qi Xuan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究提出了 Traffic-MoE，一种稀疏基础模型，能够在保持高检测性能的同时显著提升网络流量分析的实时推理效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 预训练的大型模型在网络流量分析中表现优异，但其巨大的计算开销限制了在实时、吞吐量敏感的网络防御环境中的部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 弥合先进表征学习与实际网络防护之间的差距，提供一种既高效又可扩展的网络流量分析方案。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; Traffic-MoE 通过动态将流量 token 路由到少量专门化专家，实现模型容量与计算开销的解耦，从而实现稀疏推理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在三项安全任务中，Traffic-MoE 的检测性能比主流稠密模型提升了 12.38%，吞吐量提升 91.62%，推理延迟降低 47.81%，峰值 GPU 内存消耗减少 38.72%；同时对对抗性流量塑造具有更强鲁棒性，并在少样本场景下保持高效检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Traffic-MoE 为可扩展且具弹性的网络流量分析树立了新的范式，兼顾高效性与鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 虽然预训练的大型模型在网络流量分析中已取得最先进的性能，但其巨大的计算成本阻碍了在实时、吞吐量敏感的网络防御环境中的部署。本研究通过引入 Traffic-MoE——一种针对高效实时推理优化的稀疏基础模型——弥合了先进表征学习与实际网络防护之间的鸿沟。Traffic-MoE 通过动态将流量 token 路由到少量专门化专家，有效地将模型容量与计算开销解耦。对三项安全导向任务的广泛评估表明，Traffic-MoE 在检测性能上相较领先的稠密竞争对手提升了多达 12.38%，并实现了 91.62% 的吞吐量提升、47.81% 的推理延迟降低以及 38.72% 的峰值 GPU 内存消耗减少。除效率外，Traffic-MoE 在对抗性流量塑造下表现出更优的鲁棒性，并在少样本场景中保持高检测效能，确立了可扩展且具弹性的网络流量分析新范式。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;While pre-trained large models have achieved state-of-the-art performance in network traffic analysis, their prohibitive computational costs hinder deployment in real-time, throughput-sensitive network defense environments. This work bridges the gap between advanced representation learning and practical network protection by introducing Traffic-MoE, a sparse foundation model optimized for high-efficiency real-time inference. By dynamically routing traffic tokens to a small subset of specialized experts, Traffic-MoE effectively decouples model capacity from computational overhead. Extensive evaluations across three security-oriented tasks demonstrate that Traffic-MoE achieves up to a 12.38% improvement in detection performance compared to leading dense competitors. Crucially, it delivers a 91.62% increase in throughput, reduces inference latency by 47.81%, and cuts peak GPU memory consumption by 38.72%. Beyond efficiency, Traffic-MoE exhibits superior robustness against adversarial traffic shaping and maintains high detection efficacy in few-shot scenarios, establishing a new paradigm for scalable and resilient network traffic analysis.&lt;/p&gt;</description></item><item><guid>2601.00427v1</guid><title>A Deep Learning-Enhanced Fourier Method for the Multi-Frequency Inverse Source Problem with Sparse Far-Field Data</title><link>http://arxiv.org/abs/2601.00427v1</link><author>Hao Chen, Yan Chang, Yukun Guo, Yuliang Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种混合计算框架，结合经典傅里叶方法和深度卷积网络，解决多频逆源问题，能够在稀疏噪声数据下实现高精度重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 逆源问题受Helmholtz方程支配，传统方法在稀疏噪声数据下效果有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过融合物理信息和深度学习，提高逆源重建的精度和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 先用经典傅里叶方法得到低频近似，再输入U-Net进行高精度重建；采用高噪声到低噪声的迁移学习策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 框架在噪声高达100%时仍能准确重建，优于传统谱方法，且对未知源几何具有良好泛化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该混合框架在稀疏测量和高噪声环境下表现优异，具有较高的计算效率和鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 本文提出了一种混合计算框架，用于由Helmholtz方程支配的多频逆源问题。通过将经典傅里叶方法与深度卷积神经网络相结合，我们解决了稀疏且噪声较大的远场数据所固有的挑战。傅里叶方法提供了物理信息驱动的低频源近似，作为U-Net的输入。该网络被训练以将粗略近似映射为高保真源重建，有效抑制截断伪影并恢复细尺度几何细节。为提高计算效率和鲁棒性，我们提出了一种从高噪声到低噪声的迁移学习策略：在高噪声环境下预训练的模型捕获全局拓扑特征，为在低噪声数据上微调提供稳健初始化。数值实验表明，该框架在噪声水平高达100%时即可实现准确重建，显著优于传统谱方法在稀疏测量约束下的表现，并能很好地推广到未见过的源几何。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper introduces a hybrid computational framework for the multi-frequency inverse source problem governed by the Helmholtz equation. By integrating a classical Fourier method with a deep convolutional neural network, we address the challenges inherent in sparse and noisy far-field data. The Fourier method provides a physics-informed, low-frequency approximation of the source, which serves as the input to a U-Net. The network is trained to map this coarse approximation to a high-fidelity source reconstruction, effectively suppressing truncation artifacts and recovering fine-scale geometric details. To enhance computational efficiency and robustness, we propose a high-to-low noise transfer learning strategy: a model pre-trained on high-noise regimes captures global topological features, offering a robust initialization for fine-tuning on lower-noise data. Numerical experiments demonstrate that the framework achieves accurate reconstructions with noise levels up to 100%, significantly outperforms traditional spectral methods under sparse measurement constraints, and generalizes well to unseen source geometries.&lt;/p&gt;</description></item><item><guid>2601.00446v1</guid><title>A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection</title><link>http://arxiv.org/abs/2601.00446v1</link><author>Miseon Park, Kijung Yoon</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了预训练的时间序列基础模型（TSFMs）在异常检测中的通用性，比较了零样本推理、完整模型微调和参数高效微调（PEFT）三种策略。实验表明，TSFMs在多项基准上均优于传统任务特定方法，尤其在类别不平衡严重的情况下表现突出。PEFT方法不仅降低了计算成本，还在大多数场景下与完整微调相当或更优。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 时间序列异常检测对于复杂系统的可靠运行至关重要，但现有方法往往需要大量针对特定任务的训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨预训练于大规模异构数据的时间序列基础模型是否能作为异常检测的通用骨干网络。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在多个基准上进行系统实验，比较零样本推理、完整模型适配以及参数高效微调（LoRA、OFT、HRA）三种策略的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; TSFMs在AUC-PR和VUS-PR指标上均优于任务特定基线，尤其在严重类别不平衡时提升显著。PEFT方法在降低计算成本的同时，往往能匹配或超过完整微调的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 预训练的时间序列基础模型是可扩展且高效的通用异常检测模型，值得在实际应用中推广。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 时间序列异常检测对于复杂系统的可靠运行至关重要，但大多数现有方法需要大量针对特定任务的训练。我们探讨了预训练于大规模异构数据的时间序列基础模型（TSFMs）是否能作为异常检测的通用骨干。通过在多个基准上的系统实验，我们比较了零样本推理、完整模型适配和参数高效微调（PEFT）策略。结果表明，TSFMs在AUC-PR和VUS-PR指标上优于任务特定基线，尤其在严重类别不平衡的情况下提升显著。PEFT方法如LoRA、OFT和HRA不仅降低了计算成本，还在大多数情况下匹配或超过完整微调的性能，表明即使预训练用于预测，TSFMs也能被高效地适配用于异常检测。这些发现将TSFMs定位为可扩展且高效的通用时间序列异常检测模型。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.&lt;/p&gt;</description></item><item><guid>2601.00488v1</guid><title>Noise-Aware Named Entity Recognition for Historical VET Documents</title><link>http://arxiv.org/abs/2601.00488v1</link><author>Alexander M. Esser, Jens Dörpinghaus</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种针对职业教育与培训文档的噪声感知命名实体识别方法，通过合成OCR错误、迁移学习和多阶段微调实现高鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 职业教育与培训领域的历史数字化文档常受OCR噪声影响，导致NER性能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在噪声环境下识别多种实体类型的NER系统，并验证其在VET文档中的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用噪声感知训练，合成OCR错误；对噪声、干净和人工数据进行三种训练策略；采用迁移学习和多阶段微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在噪声感知和领域特定微调下，模型在噪声条件下的鲁棒性和准确性显著提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法是首批在VET文档中识别多实体类型的方案，且可迁移至其他语言，公开代码支持复现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; This paper addresses Named Entity Recognition in the domain of Vocational Education and Training, focusing on historical digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.&lt;/p&gt;</description></item><item><guid>2601.00491v1</guid><title>Transfer-learned Kolosov-Muskhelishvili Informed Neural Networks for Fracture Mechanics</title><link>http://arxiv.org/abs/2601.00491v1</link><author>Shuwei Zhou, Christian Haeffner, Shuancheng Wang, Sophie Stebner, Zhen Liao, Bing Yang, Zhichao Wei, Sebastian Muenstermann</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于Kolov-Muskhelishvili理论和Williams修正的物理信息神经网络，用于裂纹传播分析。该网络通过解析形式满足偏微分方程，只需边界数据训练，精度高、误差低。并将三种裂纹传播准则通过迁移学习集成，预测路径一致，训练时间缩短70%以上。该框架为无网格、物理一致的裂纹分析提供了统一方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 物理信息神经网络已广泛用于固体力学，但在裂纹力学中平衡偏微分方程与边界条件困难，尤其需要在裂尖附近细致采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 克服传统方法在裂纹力学中的局限，开发一种能够自动满足偏微分方程、仅需边界数据、并能高效预测裂纹传播方向的神经网络框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建Kolov-Muskhelishvili信息神经网络，利用Williams修正进行裂尖修正；通过解析形式使方程满足；仅使用边界点训练；将三种裂纹传播准则通过迁移学习集成到框架中。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在多组基准问题中，该网络与解析和有限元结果高度一致，平均相对误差低于1%，R^2&amp;gt;0.99；三种准则预测的裂纹路径几乎相同；迁移学习将训练时间缩短70%以上。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 所提出的框架提供了统一、无网格、物理一致的裂纹传播分析方法，具有高精度和高效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 物理信息神经网络已被广泛应用于固体力学问题。然而，平衡治理偏微分方程与边界条件仍然具有挑战性，尤其在裂纹力学中，准确预测高度依赖于裂尖附近的细致采样。为克服这些限制，本文开发了一种基于Kolov-Muskhelishvili理论并加入Williams修正的物理信息神经网络。利用全纯表示，该网络从构造上满足治理方程，仅需边界点进行训练。在一系列基准问题中，该网络与解析解和有限元方法结果高度一致，平均相对误差低于1%，R^2超过0.99，适用于I型和II型加载。进一步地，本文将三种裂纹传播准则（最大切向应力、最大能量释放率和局部对称原理）通过迁移学习集成到框架中，用以预测裂纹传播方向。三种准则预测的路径几乎相同，迁移学习策略将训练时间缩短超过70%。总体而言，所开发的框架提供了一种统一、无网格且物理一致的高精度、高效裂纹传播分析方法。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Physics-informed neural networks have been widely applied to solid mechanics problems. However, balancing the governing partial differential equations and boundary conditions remains challenging, particularly in fracture mechanics, where accurate predictions strongly depend on refined sampling near crack tips. To overcome these limitations, a Kolosov-Muskhelishvili informed neural network with Williams enrichment is developed in this study. Benefiting from the holomorphic representation, the governing equations are satisfied by construction, and only boundary points are required for training. Across a series of benchmark problems, the Kolosov-Muskhelishvili informed neural network shows excellent agreement with analytical and finite element method references, achieving average relative errors below 1\% and $R^2$ above 0.99 for both mode I and mode II loadings. Furthermore, three crack propagation criteria (maximum tangential stress, maximum energy release rate, and principle of local symmetry) are integrated into the framework using a transfer learning strategy to predict crack propagation directions. The predicted paths are nearly identical across all criteria, and the transfer learning strategy reduces the required training time by more than 70\%. Overall, the developed framework provides a unified, mesh-free, and physically consistent approach for accurate and efficient crack propagation analysis.&lt;/p&gt;</description></item><item><guid>2601.00508v1</guid><title>ballmapper: Applying Topological Data Analysis Ball Mapper in Stata</title><link>http://arxiv.org/abs/2601.00508v1</link><author>Simon Rudkin, Wanling Rudkin</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; TDABM 是一种无模型的可视化方法，利用等半径球覆盖多维点云，保留完整数据结构，可用于多学科。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统降维方法会导致信息损失，TDABM 通过球映射提供完整结构的可视化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 介绍 TDABM 及 Stata 的 ballmapper 包。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用等半径球覆盖多维点云，只需一个半径参数，生成图形并可根据其他变量着色。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; TDABM 在金融、经济、地理、医学、化学等领域得到广泛应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; TDABM 提供了无模型、信息完整的可视化工具，适用于多学科数据分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; TDABM（Topological Data Analysis Ball Mapper）提供了一种无模型的多变量数据可视化方法，不需要降维所带来的信息损失。TDABM Dlotko（2019）使用等大小的球覆盖多维点云，球的半径是唯一的参数。TDABM 可视化保留了数据的完整结构。TDABM 生成的图形可以根据进一步的变量、模型残差或多变量数据中的变量进行着色。越来越多的文献利用 TDABM 的强大功能，涵盖金融、经济、地理、医学和化学等领域。本文提供了 TDABM 的介绍以及 Stata 的 ballmapper 包。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Topological Data Analysis Ball Mapper (TDABM) offers a model-free visualization of multivariate data which does not necessitate the information loss associated with dimensionality reduction. TDABM Dlotko (2019) produces a cover of a multidimensional point cloud using equal size balls, the radius of the ball is the only parameter. A TDABM visualization retains the full structure of the data. The graphs produced by TDABM can convey coloration according to further variables, model residuals, or variables within the multivariate data. An expanding literature makes use of the power of TDABM across Finance, Economics, Geography, Medicine and Chemistry amongst others. We provide an introduction to TDABM and the \texttt{ballmapper} package for Stata.&lt;/p&gt;</description></item><item><guid>2601.00516v1</guid><title>Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI</title><link>http://arxiv.org/abs/2601.00516v1</link><author>Laksh Advani</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; - 自动化大型语言模型代理生成多步行动计划，可能因上下文不匹配或结构不连贯而失败。- 现有异常检测方法不适用：平均池化会稀释异常步骤，单纯对比学习忽略序列结构。- 传统无监督方法在预训练嵌入上 F1 分数不超过 0.69。- 提出 Trajectory Guard：双胞胎循环自编码器，混合损失同时学习任务轨迹对齐和序列有效性。- 双重目标实现对“任务不匹配”与“结构错误”两类异常的统一检测。- 在合成扰动、RAS‑Eval 与 Who&amp;amp;When 基准上，平衡集 F1 0.88‑0.94，外部不平衡集召回 0.86‑0.92。- 推理延迟 32 毫秒，比 LLM Judge 基线快 17‑27 倍，支持实时安全验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 自动化大型语言模型代理生成多步行动计划，可能因上下文不匹配或结构不连贯而失败。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够统一检测任务不匹配和结构错误的异常检测方法，以提升多步行动计划的安全性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用双胞胎循环自编码器 Trajectory Guard，结合对比学习和重构损失，学习任务轨迹对齐和序列有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在合成扰动和真实世界失败基准上，F1 分数达到 0.88‑0.94，召回率 0.86‑0.92；推理延迟 32 毫秒，比 LLM Judge 快 17‑27 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; Trajectory Guard 能够实时、安全地验证多步行动计划，显著提升异常检测性能并实现生产部署。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 自主大型语言模型代理生成多步行动计划，但由于上下文不匹配或结构不连贯，计划可能失败。现有的异常检测方法不适用于此挑战：平均池化嵌入会稀释异常步骤，而仅使用对比学习的方法忽略了序列结构。基于预训练嵌入的标准无监督方法的 F1 分数最高也只有 0.69。我们提出 Trajectory Guard，一种双胞胎循环自编码器，采用混合损失函数，既通过对比学习学习任务轨迹对齐，又通过重构学习序列有效性。该双重目标实现了对“任务不匹配”和“结构错误”两类异常的统一检测。在涵盖合成扰动以及来自安全审计（RAS‑Eval）和多智能体系统（Who&amp;amp;When）的真实世界失败的基准上，我们在平衡集上取得 0.88‑0.94 的 F1 分数，在不平衡外部基准上取得 0.86‑0.92 的召回率。推理延迟为 32 毫秒，速度比 LLM Judge 基线快 17‑27 倍，使得在生产部署中实现实时安全验证成为可能。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both &amp;quot;wrong plan for this task&amp;quot; and &amp;quot;malformed plan structure.&amp;quot; On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&amp;amp;When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.&lt;/p&gt;</description></item><item><guid>2601.00542v1</guid><title>DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction</title><link>http://arxiv.org/abs/2601.00542v1</link><author>Jiacheng Sui, Yujie Zhou, Li Niu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了一种名为DynaDrag的拖拽式图像编辑方法，采用预测-移动框架，迭代进行运动预测与监督，并动态调整有效控制点，实验表明其在面部和人体数据集上优于现有方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 拖拽式图像编辑通过点或轨迹进行像素级图像操作，受到广泛关注。传统方法多采用移动-跟踪框架，易出现跟踪失误和模糊跟踪；其他框架则存在源图像与目标图像差距大、过渡点不合理等问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 为避免上述问题，提出DynaDrag，首个基于预测-移动框架的拖拽方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; DynaDrag在每一迭代中先进行运动预测，确定控制点应移动到的位置；随后通过运动监督将控制点拖动到预测位置；并动态调整有效控制点以提升性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在面部和人体数据集上的实验表明，DynaDrag在编辑效果和可编辑性方面优于之前的工作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; DynaDrag通过预测-移动框架和动态控制点调整，有效解决了传统方法的跟踪误差和编辑不连贯问题，展现出更优的图像编辑性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 为了实现像素级图像编辑，使用点或轨迹作为条件的拖拽式图像编辑正受到广泛关注。大多数以往方法遵循移动-跟踪框架，难免出现跟踪失误和模糊跟踪等挑战性问题。其他不同框架的方法则面临源图像与目标编辑图像之间巨大差距以及不合理的中间点，导致可编辑性低。为避免这些问题，我们提出了DynaDrag，这是首个基于预测-移动框架的拖拽方法。在DynaDrag中，运动预测和运动监督迭代进行。每一次迭代中，运动预测首先预测控制点应移动到的位置，然后运动监督相应地拖动它们。我们还提出动态调整有效控制点，以进一步提升性能。在人脸和人体数据集上的实验展示了其相较于以往工作更优的表现。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.&lt;/p&gt;</description></item><item><guid>2601.00549v1</guid><title>CoCo-Fed: A Unified Framework for Memory- and Communication-Efficient Federated Learning at the Wireless Edge</title><link>http://arxiv.org/abs/2601.00549v1</link><author>Zhiheng Guo, Zhaoyang Liu, Zihan Cen, Chenyuan Feng, Xinghua Sun, Xiang Chen, Tony Q. S. Quek, Xijun Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 该研究提出 CoCo-Fed 框架，解决 O-RAN 中大规模神经网络部署时的本地内存占用和全局通信瓶颈问题。通过双维度梯度降维和正交子空间叠加，显著降低内存和回传带宽需求，并在无线感知任务中保持收敛性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在 O-RAN 架构下，边缘智能需要在资源受限的 gNB 上本地训练大规模网络，但内存和回传链路带宽成为主要瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 设计一种既能降低本地内存占用，又能减少全局通信量的联邦学习框架，以支持 O-RAN 的原生边缘智能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 本地采用双维度降投影梯度并调整优化器以在低秩结构上工作；全局采用正交子空间叠加协议，将每个 gNB 的层级更新压缩为单一矩阵进行传输。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在角度估计任务的仿真中，CoCo-Fed 在内存和通信效率上显著优于现有基线，并在非 IID 数据下保持稳健收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; CoCo-Fed 为 O-RAN 中大规模模型部署提供了可行的内存与通信双重优化方案，并在理论上证明了其收敛性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在 Open Radio Access Network (O-RAN) 架构中部署大规模神经网络对于实现原生边缘智能至关重要。然而，这一范式面临两个关键瓶颈：在资源受限的 gNB 上进行本地训练所需的巨大内存占用，以及在全局聚合高维模型更新时带宽受限的回传链路饱和。为了解决这些挑战，我们提出了 CoCo-Fed，一种基于压缩与组合的联邦学习框架，统一了本地内存效率和全局通信减少。CoCo-Fed 在本地通过对梯度进行双维度降投影来突破内存壁垒，并调整优化器以在低秩结构上工作，而不引入额外的推理参数或延迟。在全局层面，我们引入了一种基于正交子空间叠加的传输协议，将层级更新投影并叠加为每个 gNB 的单一合并矩阵，显著降低回传流量。除了经验设计之外，我们建立了严格的理论基础，证明了 CoCo-Fed 在适用于无线感知任务的无监督学习条件下仍能收敛。对角度估计任务的大量仿真表明，CoCo-Fed 在内存和通信效率方面显著优于最先进的基线，并在非 IID 设置下保持稳健收敛。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;The deployment of large-scale neural networks within the Open Radio Access Network (O-RAN) architecture is pivotal for enabling native edge intelligence. However, this paradigm faces two critical bottlenecks: the prohibitive memory footprint required for local training on resource-constrained gNBs, and the saturation of bandwidth-limited backhaul links during the global aggregation of high-dimensional model updates. To address these challenges, we propose CoCo-Fed, a novel Compression and Combination-based Federated learning framework that unifies local memory efficiency and global communication reduction. Locally, CoCo-Fed breaks the memory wall by performing a double-dimension down-projection of gradients, adapting the optimizer to operate on low-rank structures without introducing additional inference parameters/latency. Globally, we introduce a transmission protocol based on orthogonal subspace superposition, where layer-wise updates are projected and superimposed into a single consolidated matrix per gNB, drastically reducing the backhaul traffic. Beyond empirical designs, we establish a rigorous theoretical foundation, proving the convergence of CoCo-Fed even under unsupervised learning conditions suitable for wireless sensing tasks. Extensive simulations on an angle-of-arrival estimation task demonstrate that CoCo-Fed significantly outperforms state-of-the-art baselines in both memory and communication efficiency while maintaining robust convergence under non-IID settings.&lt;/p&gt;</description></item><item><guid>2601.00551v1</guid><title>SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array</title><link>http://arxiv.org/abs/2601.00551v1</link><author>Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 SlingBAG Pro 算法，改进了传统光声成像重建方法，适用于任意非规则阵列，显著提升速度并保持质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 高质量三维光声成像在临床应用中受到关注，但传统迭代重建在非规则阵列下计算复杂度高、内存需求大、重建时间长。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决非规则阵列重建的计算瓶颈，降低换能器数量，提高重建速度与质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 基于 SlingBAG 的点云迭代概念，采用层次化优化：零梯度过滤与逐步增加时间采样率，快速去除冗余点云，加速收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SlingBAG Pro 在非规则阵列下速度提升至原算法的 2.2 倍，保持高质量，减少换能器需求。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SlingBAG Pro 为非规则阵列提供了高效、低成本的三维光声成像解决方案，已通过仿真和动物实验验证。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 高质量三维光声成像（PAI）在临床应用中受到越来越多关注。为解决空间有限和成本高昂的问题，符合特定成像区域的非规则几何换能器阵列有望在使用更少换能器的情况下实现高质量三维 PAI。然而，传统的迭代重建算法在处理非规则阵列配置时面临高计算复杂度、大量内存需求和较长重建时间的问题。在本研究中，我们提出了 SlingBAG Pro，一种基于 Sliding ball adaptive growth（SlingBAG）方法的点云迭代概念的先进重建算法，并将其兼容性扩展到任意阵列几何形状。SlingBAG Pro 在保持高重建质量、减少所需换能器数量的同时，采用层次化优化策略，将零梯度过滤与逐步增加的时间采样率相结合。该策略能够快速去除冗余空间点云，加速收敛，并显著缩短整体重建时间。与原始 SlingBAG 算法相比，SlingBAG Pro 在非规则阵列几何下的点云三维光声重建速度提升至 2.2 倍。该方法通过仿真和活体小鼠实验得到验证，源代码公开发布在 https://github.com/JaegerCQ/SlingBAG_Pro。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.&lt;/p&gt;</description></item><item><guid>2601.00573v1</guid><title>Benchmarking ERP Analysis: Manual Features, Deep Learning, and Foundation Models</title><link>http://arxiv.org/abs/2601.00573v1</link><author>Yihe Wang, Zhiqiao Kang, Bohan Chen, Yu Zhang, Xiang Zhang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文对ERP分析方法进行基准比较，评估传统手工特征、深度学习模型和预训练EEG基础模型在刺激分类和脑病检测任务上的表现，并探讨Transformer中不同补丁嵌入策略，提出统一的预处理和训练流程，为未来ERP研究提供方法选择框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; ERP是EEG的一个专门范式，用于反映对外部刺激的神经反应，广泛用于认知分析、神经疾病检测和心理状态评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 系统比较传统手工特征、深度学习模型和预训练EEG基础模型在ERP分析中的效果，并寻找更适合ERP数据的Transformer嵌入设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 构建统一的数据预处理和训练管道，在12个公开数据集上对ERP刺激分类和脑病检测两项任务进行评估，比较线性分类器、深度学习模型和预训练EEG基础模型，并研究多种补丁嵌入策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 发现不同方法在不同任务和数据集上表现差异，某些深度学习模型和预训练EEG基础模型在ERP任务上优于传统手工特征；同时，特定的补丁嵌入设计更适合ERP数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 提供了一个基准框架，帮助研究者在ERP分析中选择合适的方法和模型设计。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 事件相关电位（ERP）是脑电图（EEG）的一个专门范式，反映对外部刺激或事件的神经反应，通常与大脑对特定认知任务的处理相关。ERP在认知分析、神经疾病检测和心理状态评估中起着关键作用。近年来，基于深度学习的方法在自发EEG和其他非时间锁定的任务相关EEG信号上取得了显著进展。然而，它们在ERP数据上的有效性仍未得到充分探索，许多现有的ERP研究仍严重依赖手工提取特征。本文开展了一项全面的基准研究，系统比较了传统手工特征（随后使用线性分类器）、深度学习模型和预训练EEG基础模型在ERP分析中的表现。我们建立了统一的数据预处理和训练流程，并在12个公开数据集上评估了这些方法在ERP刺激分类和基于ERP的脑病检测两项代表性任务中的表现。此外，我们研究了在先进Transformer架构中各种补丁嵌入策略，以识别更适合ERP数据的嵌入设计。我们的研究为未来ERP分析提供了一个里程碑式的框架，以指导方法选择和定制模型设计。代码可在 https://github.com/DL4mHealth/ERP-Benchmark 获取。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Event-related potential (ERP), a specialized paradigm of electroencephalographic (EEG), reflects neurological responses to external stimuli or events, generally associated with the brain&amp;#x27;s processing of specific cognitive tasks. ERP plays a critical role in cognitive analysis, the detection of neurological diseases, and the assessment of psychological states. Recent years have seen substantial advances in deep learning-based methods for spontaneous EEG and other non-time-locked task-related EEG signals. However, their effectiveness on ERP data remains underexplored, and many existing ERP studies still rely heavily on manually extracted features. In this paper, we conduct a comprehensive benchmark study that systematically compares traditional manual features (followed by a linear classifier), deep learning models, and pre-trained EEG foundation models for ERP analysis. We establish a unified data preprocessing and training pipeline and evaluate these approaches on two representative tasks, ERP stimulus classification and ERP-based brain disease detection, across 12 publicly available datasets. Furthermore, we investigate various patch-embedding strategies within advanced Transformer architectures to identify embedding designs that better suit ERP data. Our study provides a landmark framework to guide method selection and tailored model design for future ERP analysis. The code is available at https://github.com/DL4mHealth/ERP-Benchmark.&lt;/p&gt;</description></item><item><guid>2601.00603v1</guid><title>Difference-in-Differences using Double Negative Controls and Graph Neural Networks for Unmeasured Network Confounding</title><link>http://arxiv.org/abs/2601.00603v1</link><author>Zihan Zhang, Lianyan Fu, Dehui Wang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种结合双重负控制和图神经网络的差分法框架，用于在网络干扰和未测混杂的情况下估计因果效应。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在观察性网络数据中，网络干扰和未测混杂是估计因果效应的主要难题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够同时处理网络干扰和未测混杂的差分法框架，并提供可双重稳健的估计方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 基于修改后的平行趋势假设和双重负控制，构建半参数识别；提出结合图神经网络和广义矩估计的双重稳健估计器；推导其在ψ网络依赖和近邻干扰下的渐近正态性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 仿真结果表明该估计器在有限样本下表现良好；在对中国绿色信贷政策对企业绿色创新的实证分析中，发现该政策对绿色创新具有显著正向影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法为在网络数据中估计因果效应提供了理论与实证支持，能够有效克服网络干扰和未测混杂的挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 从观察性网络数据中估计因果效应面临网络干扰和未测混杂的双重挑战。为此，我们提出了一个通用的差分法框架，结合双重负控制（DNC）和图神经网络（GNN）。基于修改后的平行趋势假设和DNC，建立了直接和间接因果效应的半参数识别。随后，我们提出了双重稳健估计器。具体而言，开发了一种将GNN与广义矩方法相结合的方法，用于估计高维协变量和网络结构的函数。进一步地，我们在ψ网络依赖和近似邻域干扰下推导了该估计器的渐近正态性。仿真表明我们的估计器在有限样本下的表现。最后，我们将该方法应用于分析中国绿色信贷政策对企业绿色创新的影响。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Estimating causal effects from observational network data faces dual challenges of network interference and unmeasured confounding. To address this, we propose a general Difference-in-Differences framework that integrates double negative controls (DNC) and graph neural networks (GNNs). Based on the modified parallel trends assumption and DNC, semiparametric identification of direct and indirect causal effects is established. We then propose doubly robust estimators. Specifically, an approach combining GNNs with the generalized method of moments is developed to estimate the functions of high-dimensional covariates and network structure. Furthermore, we derive the estimator&amp;#x27;s asymptotic normality under the $ψ$-network dependence and approximate neighborhood interference. Simulations show the finite-sample performance of our estimators. Finally, we apply our method to analyze the impact of China&amp;#x27;s green credit policy on corporate green innovation.&lt;/p&gt;</description></item><item><guid>2601.00607v1</guid><title>Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2601.00607v1</link><author>Sonia Khetarpaul, P Y Sharan</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于交通感知的图神经网络与强化学习框架，用于在大都市环境中优化出租车热点分布。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在智慧城市交通中，出租车供需匹配需要实时整合城市交通网络数据和出行模式。传统模型仅依赖历史需求，忽视了交通拥堵、道路事故和公共事件等动态因素。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够同时考虑实时交通信息和历史需求的出租车热点预测方法，以提升乘客等待时间和司机行驶距离的效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 将城市道路网络建模为图，交叉口为节点，路段为边，节点属性包含历史需求、事件接近度和实时拥堵分数；使用图神经网络生成嵌入，再由 Q‑学习代理根据奖励机制推荐最佳出租车热点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在模拟的德里出租车数据集上，该模型将乘客等待时间降低约56%，司机行驶距离降低约38%，相较于基准随机选择。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法可适用于多模式交通系统，并能集成到智慧城市平台，实现实时城市出行优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在智慧城市交通背景下，出租车供需匹配需要实时整合城市交通网络数据和出行模式。传统出租车热点预测模型往往仅依赖历史需求，忽视了交通拥堵、道路事故和公共事件等动态影响。本文提出一种交通感知的基于图的强化学习框架，用于在大都市环境中实现最佳出租车部署。城市道路网络被建模为图，交叉口为节点，路段为边，节点属性捕捉历史需求、事件接近度和实时拥堵分数。图神经网络嵌入用于编码交通网络中的时空依赖关系，随后由 Q‑学习代理根据奖励机制推荐最佳出租车热点。奖励机制同时优化乘客等待时间、司机行驶距离和拥堵规避。实验在使用真实地理边界和历史叫车请求模式生成的德里出租车模拟数据集上进行，结果显示该模型将乘客等待时间降低约56%，司机行驶距离降低约38%，相较于基准随机选择。该方法适用于多模式交通系统，可集成到智慧城市平台，实现实时城市出行优化。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.&lt;/p&gt;</description></item><item><guid>2601.00612v1</guid><title>WiFo-MUD: Wireless Foundation Model for Heterogeneous Multi-User Demodulator</title><link>http://arxiv.org/abs/2601.00612v1</link><author>Zonghui Yang, Shijian Gao, Xuesong Cai, Xiang Cheng, Liuqing Yang</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了 WiFo-MUD，一种通用的扩散模型基础框架，用于多用户信号解调，解决了传统方法在多用户环境中的准确性与复杂度平衡问题以及深度学习方法在异构配置下的适应性不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 多用户信号解调对无线通信的可靠性和效率至关重要，但现有解调器在通用多用户环境中表现不佳。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够在不同系统配置下保持高准确性且计算复杂度可控的解调模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 采用扩散模型基础框架，调整用户间信噪比不平衡，使用定制化骨干网络进行条件去噪；并引入通信感知一致性蒸馏方法和动态用户分组策略以提升推理性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在大规模异构数据集上，WiFo-MUD 达到最先进的性能，推理效率高，且在不同系统配置下具有强泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; WiFo-MUD 成为一种高效、通用的多用户解调解决方案，显著提升了无线通信系统的可靠性与效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 多用户信号解调对无线通信至关重要，直接影响传输可靠性和效率。然而，现有解调器在通用多用户环境中表现不佳：传统解调器难以在准确性和复杂度之间取得平衡，而基于深度学习的方法在异构配置下缺乏适应性。尽管已将扩散模型引入解调，但其灵活性仍有限。为解决这些问题，本文提出了 WiFo-MUD，一种通用的基于扩散的多用户解调基础模型。该模型通过对齐用户间信噪比不平衡，并通过定制化骨干网络进行条件去噪。此外，还设计了通信感知一致性蒸馏方法和动态用户分组策略以提升推理效果。WiFo-MUD 在大规模异构数据集上实现了最先进的结果，展示了高效推理和在不同系统配置下的强泛化能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Multi-user signal demodulation is critical to wireless communications, directly impacting transmission reliability and efficiency. However, existing demodulators underperform in generic multi-user environments: classical demodulators struggle to balance accuracy and complexity, while deep learning-based methods lack adaptability under heterogeneous configurations. Although diffusion models have been introduced for demodulation, their flexibility remains limited for practical use. To address these issues, this work proposes WiFo-MUD, a universal diffusion-based foundation model for multi-user demodulation. The model aligns inter-user signal-to-noise ratio imbalance and performs conditional denoising via a customized backbone. Furthermore, a communication-aware consistency distillation method and a dynamic user-grouping strategy are devised to enhance inference. WiFo-MUD achieves state-of-the-art results on large-scale heterogeneous datasets, demonstrating efficient inference and strong generalization across varying system configurations.&lt;/p&gt;</description></item><item><guid>2601.00613v1</guid><title>Personalized Forecasting of Glycemic Control in Type 1 and 2 Diabetes Using Foundational AI and Machine Learning Models</title><link>http://arxiv.org/abs/2601.00613v1</link><author>Simon Lebech Cichosz, Stine Hangaard, Thomas Kronborg, Peter Vestergaard, Morten Hasselstrøm Jensen</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本研究评估了四种现代表格学习模型在预测连续血糖监测指标方面的表现，结果显示模型整体可实现合理准确度，但低频低血糖预测仍具挑战。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 准确预测连续血糖监测指标可实现主动糖尿病管理，但现代表格学习方法的相对表现尚未完全界定。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 比较四种回归模型在预测未来一周连续血糖监测指标的准确性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用4622个病例周的数据，训练并内部验证CatBoost、XGBoost、AutoGluon、tabPFN四个模型，评估平均绝对误差和平均绝对相对差异，并用混淆矩阵热图总结分位数分类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在T1DM和T2DM中，所有模型大体表现相近；T1DM中TIR、TITR、TAR、MAGE的相对误差在8.5%到16.5%之间，TBR的相对误差高达48%；AutoGluon和tabPFN在多项指标上平均绝对误差低于XGBoost；T2DM中相对误差在7.8%到23.9%，TBR相对误差约78%；tabPFN在TIR上优于其他模型，AutoGluon/ tabPFN在TAR上优于CatBoost/XGBoost；推理时间每千个案例差异显著，tabPFN最慢，AutoGluon最快。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 使用现代表格模型可在一周前预测连续血糖监测指标，准确度合理，但低频低血糖的相对预测仍困难；高级AutoML和基础模型虽提升准确度，但计算成本显著增加。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 背景：准确预测连续血糖监测指标可实现主动糖尿病管理，但现代表格学习方法的相对表现尚未完全定义。方法：我们训练并内部验证了四个回归模型（CatBoost、XGBoost、AutoGluon、tabPFN），使用4622个病例周的数据预测六个一周前的CGM指标（TIR、TITR、TAR、TBR、CV、MAGE及相关分位数）。性能通过平均绝对误差和平均绝对相对差异评估，分位数分类通过混淆矩阵热图总结。结果：在T1DM和T2DM中，所有模型大体表现相近。T1DM中TIR、TITR、TAR和MAGE的相对误差在8.5%到16.5%之间，TBR的相对误差约48%；AutoGluon和tabPFN在多项指标上平均绝对误差低于XGBoost。T2DM中相对误差在7.8%到23.9%，TBR相对误差约78%；tabPFN在TIR上优于其他模型，AutoGluon/ tabPFN在TAR上优于CatBoost/XGBoost。推理时间每千个案例差异显著（tabPFN最慢，AutoGluon最快）。结论：一周前的CGM指标可用现代表格模型以合理准确度预测，但低频低血糖的相对预测仍具挑战。高级AutoML和基础模型在准确度上有适度提升，但计算成本显著增加。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Background: Accurate week-ahead forecasts of continuous glucose monitoring (CGM) derived metrics could enable proactive diabetes management, but relative performance of modern tabular learning approaches is incompletely defined.   Methods: We trained and internally validated four regression models (CatBoost, XGBoost, AutoGluon, tabPFN) to predict six weekahead CGM metrics (TIR, TITR, TAR, TBR, CV, MAGE, and related quantiles) using 4,622 case-weeks from two cohorts (T1DM n=3,389; T2DM n=1,233). Performance was assessed with mean absolute error (MAE) and mean absolute relative difference (MARD); quantile classification was summarized via confusion-matrix heatmaps.   Results: Across T1DM and T2DM, all models produced broadly comparable performance for most targets. For T1DM, MARD for TIR, TITR, TAR and MAGE ranged 8.5 to 16.5% while TBR showed large MARD (mean ~48%) despite low MAE. AutoGluon and tabPFN showed lower MAE than XGBoost for several targets (e.g., TITR: p&amp;lt;0.01; TAR/TBR: p&amp;lt;0.05 to 0.01). For T2DM MARD ranged 7.8 to 23.9% and TBR relative error was ~78%; tabPFN outperformed other models for TIR (p&amp;lt;0.01), and AutoGluon/ tabPFN outperformed CatBoost/XGBoost on TAR (p&amp;lt;0.05). Inference time per 1,000 cases varied markedly (PFN 699 s; AG 2.7 s; CatBoost 0.04 s, XGBoost 0.04 s).   Conclusions: Week-ahead CGM metrics are predictable with reasonable accuracy using modern tabular models, but low-prevalence hypoglycemia remains difficult to predict in relative terms. Advanced AutoML and foundation models yield modest accuracy gains at substantially higher computational cost.&lt;/p&gt;</description></item><item><guid>2601.00635v1</guid><title>SEMODS: A Validated Dataset of Open-Source Software Engineering Models</title><link>http://arxiv.org/abs/2601.00635v1</link><author>Alexandra González, Xavier Franch, Silverio Martínez-Fernández</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出了SEMODS数据集，收集并验证了3,427个适用于软件工程任务的AI模型，为SE领域提供了标准化的模型资源。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 在Hugging Face上有数百万个模型，持续产生新模型，缺乏专门的SE模型目录，导致难以识别适用于软件工程的模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 构建一个专注于软件工程的模型数据集，填补缺乏SE模型目录的空白。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 从Hugging Face自动抓取模型，结合人工标注和大语言模型辅助进行严格验证，并将模型与软件开发生命周期中的任务和活动关联，统一表示评估结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; SEMODS包含3,427个模型，已与SE任务和生命周期活动关联，提供标准化评估结果，可用于数据分析、模型发现、基准测试和模型适配等多种应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; SEMODS为将人工智能整合进软件工程提供了可检索、可验证的模型资源，支持多种后续研究和实践。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 将人工智能整合到软件工程（SE）中需要拥有一套针对SE任务的精选模型集合。Hugging Face（HF）上托管着数百万个模型，并且不断有新模型被创建，若没有专门的目录，识别SE模型是不可行的。为了解决这一缺口，我们提出了SEMODS：一个专注于SE的数据集，包含从HF提取的3,427个模型，结合自动收集与通过人工标注和大型语言模型辅助的严格验证。我们的数据集将模型与软件开发生命周期中的SE任务和活动关联，提供了其评估结果的标准化表示，并支持多种应用，如数据分析、模型发现、基准测试和模型适配。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.&lt;/p&gt;</description></item><item><guid>2601.00645v1</guid><title>Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach</title><link>http://arxiv.org/abs/2601.00645v1</link><author>Shrikant Kapse, Priyankkumar Dhrangdhariya, Priya Kedia, Manasi Patwardhan, Shankar Kausley, Soumyadipta Maiti, Beena Rai, Shirish Karande</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文通过图像深度学习实现对土豆贮藏期间质量的非侵入式监测，涵盖芽生检测、重量损失估计和保质期预测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 土豆贮藏过程中芽生、重量下降和保质期变化是影响质量的重要因素，传统检测方法耗时且不易规模化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用预训练网络设计高精度芽生检测器和多类别保质期预测模型，实现自动化质量评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 在受控温湿环境下收集200天的土豆图像和重量数据，采用ResNet、VGG、DenseNet和Vision Transformer四种预训练架构，分别训练芽生二分类器和重量损失/保质期多分类预测器。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; DenseNet在芽生检测上达到98.03%的准确率；保质期预测在粗分类（2-5类）下准确率超过89.83%，细分类（6-8类）因视觉差异细微且样本不足而下降。模型可用于自动分拣、库存管理、差异化定价和减少浪费。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 图像深度学习提供了一种成本低、无损且可扩展的土豆质量评估方法，支持高效、可持续的贮藏与分销；未来需开发适用于多品种、多条件的通用模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 基于图像的深度学习为监测土豆贮藏期间的质量提供了一种非侵入式、可扩展的解决方案，解决了芽生检测、重量损失估计和保质期预测等关键挑战。本文在受控温湿条件下收集了200天的图像和对应重量数据。利用ResNet、VGG、DenseNet和Vision Transformer（ViT）等强大的预训练架构，我们设计了两种专用模型：一是高精度芽生二分类器，二是先进的多类别预测器，用于估计重量损失并预测剩余保质期，准确率显著。DenseNet在芽生检测上表现卓越，准确率达到98.03%。保质期预测模型在粗分类（2-5类）下表现最佳，准确率超过89.83%，而细分类（6-8类）由于视觉差异细微且每类样本有限，准确率下降。这些发现证明了将基于图像的模型集成到自动分拣和库存系统中的可行性，能够实现芽生土豆的早期识别和基于贮藏阶段的动态分类。实际意义包括改进库存管理、差异化定价策略以及降低供应链中的食品浪费。虽然精确预测具体保质期区间仍具挑战，但聚焦更宽泛的类别划分可确保稳健性能。未来研究应致力于开发在多种土豆品种和贮藏条件下训练的通用模型，以提升适应性和可扩展性。总体而言，该方法提供了一种成本效益高、无损伤的质量评估手段，支持土豆贮藏和分销的效率与可持续性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.&lt;/p&gt;</description></item><item><guid>2601.00658v1</guid><title>Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network</title><link>http://arxiv.org/abs/2601.00658v1</link><author>Zhaiyu Chen, Yuanyuan Wang, Yilei Shi, Xiao Xiang Zhu</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于学习的框架，将空间雷达成像（TomoSAR）点云转换为高分辨率建筑高度图，解决噪声、点分布不均和缺失区域问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 建筑高度估计在城市应用中至关重要，传统光学方法受天气影响，TomoSAR提供了天气无关、侧向观测的建筑立面结构信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 开发一种能够从TomoSAR点云直接生成连续、高精度建筑高度图的系统。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用双拓扑网络，点分支处理不规则散射特征，网格分支保证空间一致性；两分支联合处理实现去噪和缺失区域填补。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在慕尼黑和柏林数据上实验表明，该方法能有效重建建筑高度，并且可与光学卫星影像结合进一步提升质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 首次证明了可直接从TomoSAR点云进行大规模城市高度映射的可行性，并提供了开源实现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 可靠的建筑高度估计对于各种城市应用至关重要。空间雷达成像（TomoSAR）提供了天气无关、侧向观测，能够捕捉立面级结构，成为传统光学方法的有前景的替代方案。然而，TomoSAR点云常常存在噪声、点分布不均和不相干表面上的数据空洞，这些都阻碍了精确高度重建。为了解决这些挑战，我们提出了一个基于学习的框架，将原始TomoSAR点转换为高分辨率建筑高度图。我们的双拓扑网络在点分支和网格分支之间交替，点分支建模不规则散射特征，网格分支强制空间一致性。通过联合处理这些表示，网络对输入点进行去噪并填补缺失区域，生成连续的高度估计。我们认为这是首次从TomoSAR点云直接进行大规模城市高度映射的概念验证。对慕尼黑和柏林数据的广泛实验验证了我们方法的有效性。此外，我们展示了该框架可以扩展以结合光学卫星影像，进一步提升重建质量。源代码可在 https://github.com/zhu-xlab/tomosar2height 获取。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在从空间SAR成像得到的噪声大、稀疏的Tomography点云中恢复连续、精确的建筑高度图。建筑高度信息是城市规划、灾害管理和环境监测等应用的核心数据，而传统的激光雷达或光学摄影在大范围内成本高、受天气限制，TomoSAR提供全天候、广域覆盖的替代方案，但其点云质量不足，需要新的处理方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了TomoSAR点云的噪声、异方差采样和空洞问题，认为需要同时利用点云的局部细节和网格的全局一致性。于是提出双拓扑网络：点分支捕捉不规则散射特征，网格分支强制空间一致性，并在U‑Net级联中交替融合两种特征。该设计借鉴了先前的点云到网格融合方法（如Peng等、Wang等）以及深度学习在三维重建中的U‑Net结构，并在此基础上针对2.5D高度映射进行了改造。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将不规则的TomorSAR点云映射到连续的高度图，利用点分支提取局部散射特征，网格分支提供全局平滑约束，并通过交替融合实现去噪和空洞填补。实现流程为：①将点云投影到平面并提取点特征；②在网格上构建CNN特征；③在U‑Net级联中交替将点特征投影到网格并融合；④最终输出每个网格像素的高度值；⑤可选地将光学影像特征拼接进网络以进一步提升精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) 首次提出基于深度学习的端到端框架，直接从TomorSAR点云生成连续高度图；2) 双拓扑网络将点云与网格特征交替融合，既保留细节又保证一致性；3) 通过交替U‑Net级联实现去噪和空洞填补；4) 证明可扩展到光学影像融合。与以往仅使用插值、优化或单图像深度估计的方法不同，该方法在大规模、全天候场景下实现了高分辨率建筑高度重建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 我们提出了首个深度学习框架，利用双拓扑网络将噪声大、稀疏的TomorSAR点云转换为连续、高分辨率的建筑高度图，实现了大规模、全天候城市高度映射。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.&lt;/p&gt;</description></item><item><guid>2601.00670v1</guid><title>Wave2Word: A Multimodal Transformer Framework for Joint EEG-Text Alignment and Multi-Task Representation Learning in Neurocritical Care</title><link>http://arxiv.org/abs/2601.00670v1</link><author>Argha Kamal Samanta, Deepak Mewada, Monalisa Sarma, Debasis Samanta</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了多模态EEG表征学习框架，结合信号域建模与结构化临床语言监督，提升EEG异常检测的临床可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 连续EEG在神经危重病护理中常用于监测癫痫和其他有害脑活动，但现有深度学习方法多聚焦于癫痫检测，依赖离散标签监督，且评估指标主要是准确率，未能充分反映EEG发现与临床解读之间的对应关系。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 提出一种融合时间频率表示、双Transformer编码器和对比学习的多模态EEG表征学习框架，以更好地捕捉有害EEG活动的重叠模式、专家一致性和时间持续性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; ①将原始EEG转换为纵向双极电极图和时频表示；②使用双Transformer编码器分别建模时间和频率依赖；③通过自适应门控机制融合两种编码；④利用对比目标将EEG嵌入与结构化专家共识描述对齐；⑤加入EEG条件文本重建损失作为表征层约束，并与分类损失共同训练。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在受控的训练-验证-测试划分下，六分类测试准确率达0.9797；去除对比对齐后，跨模态检索召回率从0.3390降至0.0045，分类准确率变化不大，表明单纯的分类准确率无法反映表征质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 判别准确率并不能可靠地评估临床意义深刻的EEG表征质量，需结合跨模态检索和对比学习等指标来衡量模型的临床可解释性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Continuous electroencephalography (EEG) is routinely used in neurocritical care to monitor seizures and other harmful brain activity, including rhythmic and periodic patterns that are clinically significant. Although deep learning methods have achieved high accuracy in seizure detection, most existing approaches remain seizure-centric, rely on discrete-label supervision, and are primarily evaluated using accuracy-based metrics. A central limitation of current EEG modeling practice is the weak correspondence between learned representations and how EEG findings are interpreted and summarized in clinical workflows. Harmful EEG activity exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a multimodal EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is transformed into a longitudinal bipolar montage and time-frequency representations. Second, dual transformer-based encoders model complementary temporal and frequency-centric dependencies and are fused using an adaptive gating mechanism. Third, EEG embeddings are aligned with structured expert consensus descriptions through a contrastive objective. Finally, an EEG-conditioned text reconstruction loss is introduced as a representation-level constraint alongside standard classification loss. Experimental evaluation using a controlled train-validation-test split achieves a six-class test accuracy of 0.9797. Ablation analyses show that removing contrastive alignment reduces cross-modal retrieval performance from Recall@10 of 0.3390 to 0.0045, despite minimal change in classification accuracy. These findings demonstrate that discriminative accuracy does not reliably reflect representation quality for clinically meaningful EEG modeling.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Continuous electroencephalography (EEG) is routinely used in neurocritical care to monitor seizures and other harmful brain activity, including rhythmic and periodic patterns that are clinically significant. Although deep learning methods have achieved high accuracy in seizure detection, most existing approaches remain seizure-centric, rely on discrete-label supervision, and are primarily evaluated using accuracy-based metrics. A central limitation of current EEG modeling practice is the weak correspondence between learned representations and how EEG findings are interpreted and summarized in clinical workflows. Harmful EEG activity exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a multimodal EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is transformed into a longitudinal bipolar montage and time-frequency representations. Second, dual transformer-based encoders model complementary temporal and frequency-centric dependencies and are fused using an adaptive gating mechanism. Third, EEG embeddings are aligned with structured expert consensus descriptions through a contrastive objective. Finally, an EEG-conditioned text reconstruction loss is introduced as a representation-level constraint alongside standard classification loss. Experimental evaluation using a controlled train-validation-test split achieves a six-class test accuracy of 0.9797. Ablation analyses show that removing contrastive alignment reduces cross-modal retrieval performance from Recall@10 of 0.3390 to 0.0045, despite minimal change in classification accuracy. These findings demonstrate that discriminative accuracy does not reliably reflect representation quality for clinically meaningful EEG modeling.&lt;/p&gt;</description></item><item><guid>2601.00678v1</guid><title>Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</title><link>http://arxiv.org/abs/2601.00678v1</link><author>Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种基于3D高斯场的单图像视频生成框架，能够在一次前向传播中实现快速、可控的摄像机引导视频生成，并在多个数据集上取得最先进的视频质量和推理效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 人类能够仅凭一张图像预测场景未来动态，视频生成模型若能模仿此能力，对智能系统至关重要。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 旨在提供一种能够在单图像条件下实现可控摄像机路径、保持时间一致性和几何完整性的高效视频生成方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 通过构建3D高斯场表示场景，并在一次前向传播中采样合理的物体运动，实现快速、摄像机引导的视频生成。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在四大数据集上实验表明，该方法在视频质量和推理速度上均达到或超过现有最先进水平。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该框架实现了高效、可控且时间一致的视频生成，克服了传统方法在摄像机运动建模和几何完整性方面的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 人类能够仅凭一张图像预测场景的未来动态，能够模仿这种能力的视频生成模型是智能系统的重要组成部分。最近的研究在单图像条件下的视频生成中提升了时间一致性和三维一致性。然而，这些方法往往缺乏稳健的用户可控性，例如修改摄像机路径，限制了其在实际应用中的适用性。大多数现有的摄像机控制图像到视频模型在准确建模摄像机运动、保持时间一致性和保持几何完整性方面存在困难。利用显式中间三维表示提供了一种有前景的解决方案，可实现与给定摄像机轨迹一致的连贯视频生成。虽然这些方法通常使用三维点云渲染场景并在后期引入物体运动，但这种两步过程仍然无法实现完整的时间一致性，尽管它允许精确控制摄像机运动。我们提出了一种新框架，构建三维高斯场景表示，并在一次前向传播中采样合理的物体运动。这样可以在不需要迭代去噪的情况下，快速实现摄像机引导的视频生成。我们在KITTI、Waymo、RealEstate10K和DL3DV-10K数据集上进行了广泛实验，证明我们的方法在视频质量和推理效率方面达到了最先进水平。项目页面可在 https://melonienimasha.github.io/Pixel-to-4D-Website 查看。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在解决从单张图像生成可由用户控制相机轨迹的视频，同时保持动态物体的连贯运动、几何一致性和图像风格。此问题在自动驾驶、增强现实和内容创作等领域至关重要，因为它能让系统在仅有一帧输入的情况下预测未来场景并提供可交互的视角。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了3D Gaussian Splatting的高效稠密表示和现有的单图像到3D/4D生成方法，进一步加入线性和角速度、加速度来捕捉动态。通过结合预训练的DINOv2特征、噪声采样和实例分割，设计了一个端到端的编码器-解码器网络，能够在一次前向传播中生成完整的4D场景。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用像素对齐的3D高斯分布表示场景，并为每个高斯赋予时间相关的运动参数。实现流程包括：①用图像和深度估计编码器生成潜在表示；②解码器预测静态高斯参数；③采样噪声并解码得到每个高斯的速度和加速度；④根据实例分割聚合为物体级运动；⑤计算未来时间点的高斯位置和姿态；⑥用可微分的高斯光栅化器渲染未来帧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 创新点包括：①提出了支持多层动态物体的4D像素对齐高斯表示；②在单前向传播中直接预测运动参数，避免了两阶段的点云+扩散流程；③利用DINOv2特征和噪声采样提升运动不确定性建模；④在KITTI、Waymo、RealEstate10K和DL3DV-10K上实现了更高的PSNR、LPIPS、SSIM和更低的推理时间。与以往方法相比，它消除了稀疏点云、扩散不连贯和重建误差的缺陷。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Pixel-to-4D通过一次前向传播学习密集的4D高斯场景表示，实现了可控相机轨迹下的连贯动态视频合成。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.&lt;/p&gt;</description></item><item><guid>2601.00694v1</guid><title>A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference</title><link>http://arxiv.org/abs/2601.00694v1</link><author>Qingwen Pu, Kun Xie, Hong Yang, Guocong Zhai</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出一种结合视觉特征和领域知识的 PedX-LLM 框架，用于推断行人过街行为，显著提升了模型在新场景下的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统的统计模型和监督学习方法在行人过街行为推断上表现有限，难以在新地点获得良好效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 通过引入大语言模型与视觉信息，构建一种可迁移、可解释的行为推断方法，以克服纯数据驱动方法的局限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用 LLaVA 提取视觉特征并与文本数据及交通领域知识结合，利用 LoRA 对 LLaMA-2-7B 进行微调；通过跨站点验证和少量示例的 few‑shot 学习评估泛化性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在已知数据上实现 82.0% 的平衡准确率，视觉模块提升 2.9%，知识集成再提升 4.1%；在五个未见测试站点的零样本设置下达到 66.9% 的准确率，少量示例后提升至 72.2%。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; PedX-LLM 在未见场景中表现出强大的泛化能力，证明了视觉与知识增强的推理方式能够模拟人类决策逻辑，克服了单纯数据驱动方法的不足。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 现有的行人过街行为推断范式，从统计模型到监督学习方法，普遍缺乏泛化能力，在新地点表现不佳。大型语言模型（LLM）的最新进展提供了从数值模式拟合向语义、上下文感知行为推理的转变，但现有 LLM 应用缺乏领域特定适配和视觉上下文。本文提出 Pedestrian Crossing LLM（PedX-LLM），一种视觉与知识增强的框架，旨在将行人过街推断从基于站点的模式识别转变为可泛化的行为推理。通过将 LLaVA 提取的视觉特征与文本数据和交通领域知识相结合，PedX-LLM 使用 Low‑Rank Adaptation（LoRA）对 LLaMA‑2‑7B 基础模型进行微调，以推断过街决策。PedX-LLM 在已知数据上实现 82.0% 的平衡准确率，优于最佳统计和监督学习方法。结果表明，视觉增强模块通过捕捉建成环境并整合领域知识，分别贡献了 2.9% 和 4.1% 的性能提升。为评估在未见环境中的泛化能力，本文采用基于站点的划分进行跨站点验证。零样本 PedX-LLM 在五个未见测试站点上实现 66.9% 的平衡准确率，至少比基线数据驱动方法高 18 个百分点。仅使用五个验证示例的 few‑shot 学习进一步将平衡准确率提升至 72.2%。PedX-LLM 展示了对未见场景的强大泛化能力，证实了视觉与知识增强的推理能够使模型模仿人类类似的决策逻辑，克服了纯数据驱动方法的局限。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.&lt;/p&gt;</description></item><item><guid>2601.00716v1</guid><title>Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model</title><link>http://arxiv.org/abs/2601.00716v1</link><author>Hao Guan, Li Zhou</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了在医学影像分析中，视觉-语言模型在数据分布变化时的性能下降检测方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 视觉-语言模型在医学图像诊断中表现出色，但在部署后若输入数据分布与训练时不同，模型性能可能下降。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 探讨在数据分布变化下，如何有效检测视觉-语言模型的性能下降，并提出可操作的监测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 1. 开发轻量级工具 DomainSAT，用于可视化输入数据的分布变化并集成多种检测算法。2. 研究输出层的预测置信度变化，提出无标签的置信度指标来监测性能下降。3. 在大规模肿瘤分类数据集上进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 输入数据分布检测能及时发现分布变化，但并不总能对应实际性能下降；而基于置信度的指标与性能下降高度相关，可与输入检测互补。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 将输入分布检测与输出置信度指标结合，可更可靠地监测视觉-语言模型在数据分布变化下的性能下降，为数字病理学中的基础模型可靠性提供实用框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; Vision‑Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre‑trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state‑of‑the‑art pathology VLM. We examine both input‑level data shift and output‑level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output‑based monitoring and introduce a label‑free, confidence‑based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large‑scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence‑based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.&lt;/p&gt;</description></item><item><guid>2601.00783v1</guid><title>Improving Router Security using BERT</title><link>http://arxiv.org/abs/2601.00783v1</link><author>John Carter, Spiros Mancoridis, Pavlos Protopapas, Brian Mitchell, Benji Lilley</author><pubDate>Mon, 05 Jan 2026 13:38:03 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文研究了使用高精度eBPF系统调用传感器和对比增强学习提升家用路由器恶意软件检测性能，并引入网络包抽象语言以获取网络行为信号，最终在IoT环境中实现在线异常检测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 先前研究表明，基于系统调用的Transformer模型能检测多种恶意软件，但在低误报率下效果有限。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 验证高精度eBPF传感器与对比增强学习能提升低误报率检测，并探讨网络行为信号的补充作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 使用eBPF系统调用传感器采集数据，结合对比增强学习（对负样本进行受控突变），引入网络包抽象语言构建类似网络数据的流水线，最终在IoT路由器上实现在线异常检测框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 高精度eBPF传感器与对比增强学习显著提升低误报率检测性能；网络行为信号为网络聚焦恶意软件提供互补检测信号，进一步提升检测效果；在IoT部署环境中实现的在线框架验证了方法有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 结合高精度传感器、对比增强学习和网络行为抽象，可在低误报率下有效检测家用路由器恶意软件，并在IoT环境中实现可行的在线异常检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 先前关于家用路由器安全的研究表明，利用系统调用训练基于BERT式编码器的Transformer语言模型，并采用对比学习，能够有效检测多种恶意软件，但在低误报率下的性能仍受限。在本研究中，我们证明，使用高精度的eBPF系统调用传感器，并结合对比增强学习（对负样本进行受控突变），可以在低误报率下提升检测性能。此外，我们引入了一种网络包抽象语言，能够创建类似网络包数据的流水线，并展示网络行为提供了互补的检测信号——在低误报率下显著提升针对网络聚焦恶意软件的检测效果。最后，我们将这些方法实现于在线路由器异常检测框架中，以验证该方法在物联网（IoT）部署环境中的有效性。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Previous work on home router security has shown that using system calls to train a transformer-based language model built on a BERT-style encoder using contrastive learning is effective in detecting several types of malware, but the performance remains limited at low false positive rates. In this work, we demonstrate that using a high-fidelity eBPF-based system call sensor, together with contrastive augmented learning (which introduces controlled mutations of negative samples), improves detection performance at a low false positive rate. In addition, we introduce a network packet abstraction language that enables the creation of a pipeline similar to network packet data, and we show that network behavior provides complementary detection signals-yielding improved performance for network-focused malware at low false positive rates. Lastly, we implement these methods in an online router anomaly detection framework to validate the approach in an Internet of Things (IoT) deployment environment.&lt;/p&gt;</description></item><item><guid>2601.00789v1</guid><title>Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection</title><link>http://arxiv.org/abs/2601.00789v1</link><author>Shukesh Reddy, Srijan Das, Abhijit Das</author><pubDate>Mon, 05 Jan 2026 13:36:37 +0800</pubDate><description>&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文探讨了将自监督学习作为辅助任务来提升通用深度伪造检测的效果，并通过实验验证了融合自监督特征表示能显著提升检测性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 深度伪造检测面临跨数据集泛化挑战，传统方法难以充分利用无标签数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 利用自监督学习的潜力，作为辅助任务来优化主任务——通用深度伪造检测。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 对不同训练方案组合进行探索，融合自监督辅助任务的特征表示，并在多数据集上进行实验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 融合自监督特征能提供更强的表示，提升主任务性能，并在跨数据集评估中表现出更好的泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 该方法在多数据集实验中优于现有最先进检测器，证明自监督辅助任务对深度伪造检测具有显著提升作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 在本研究中，我们尝试释放自监督学习作为辅助任务的潜力，以优化通用深度伪造检测的主任务。为此，我们探讨了不同训练方案组合，以寻找最有效的方法。我们的研究结果表明，将自监督辅助任务的特征表示融合起来，是解决该问题的强大特征表示。这种表示能够充分发挥潜力，并为自监督和主任务提供独特的表示，从而提升主任务的性能。我们在包括 DF40、FaceForensics++、Celeb-DF、DFD、FaceShifter、UADFV 在内的大量数据集上进行了实验，结果显示与当前最先进的检测器相比，在跨数据集评估中具有更好的泛化能力。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.&lt;/p&gt;</description></item><item><guid>2601.00796v1</guid><title>AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</title><link>http://arxiv.org/abs/2601.00796v1</link><author>Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu</author><pubDate>Mon, 05 Jan 2026 13:36:37 +0800</pubDate><description>&lt;p&gt;⭐ 与研究主题相关&lt;/p&gt;
&lt;h3&gt;GPT 基础摘要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结:&lt;/strong&gt; 本文提出 AdaGaR 框架，结合自适应 Gabor 表示和时间连续性约束，显著提升单目视频动态 3D 场景重建质量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景:&lt;/strong&gt; 传统单 Gaussian 方法受限于低通滤波，Gabor 函数能量不稳定，缺乏时间连续性约束导致插值时运动伪影。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目的:&lt;/strong&gt; 解决高频细节捕捉与时间连续运动的双重挑战，提升动态场景建模的精度与稳定性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法:&lt;/strong&gt; 自适应 Gabor 表示通过可学习频率权重和能量补偿扩展 Gaussian；时间连续性采用三次 Hermite 样条与曲率正则化；自适应初始化结合深度估计、点跟踪和前景掩码。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要发现:&lt;/strong&gt; 在 Tap-Vid DAVIS 数据集上，AdaGaR 在 PSNR、SSIM、LPIPS 等指标上达到最先进水平，并在帧插值、深度一致性、视频编辑和立体视图合成等任务中表现出强泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结论:&lt;/strong&gt; 自适应 Gabor 表示与时间连续性约束的结合为动态 3D 场景重建提供了有效方案，显著提升了细节保真度和运动平滑度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;翻译:&lt;/strong&gt; 重建来自单目视频的动态三维场景需要同时捕捉高频外观细节和时间连续运动。现有使用单一高斯原语的方法受限于低通滤波特性，而标准 Gabor 函数会导致能量不稳定。此外，缺乏时间连续性约束往往在插值过程中产生运动伪影。我们提出 AdaGaR，一个统一框架，解决了频率自适应和时间连续性在显式动态场景建模中的问题。我们引入自适应 Gabor 表示，通过可学习的频率权重和自适应能量补偿扩展高斯，以平衡细节捕捉与稳定性。为保证时间连续性，我们采用三次 Hermite 样条与时间曲率正则化，确保运动平滑演化。自适应初始化机制结合深度估计、点跟踪和前景掩码，在训练初期建立稳定的点云分布。对 Tap-Vid DAVIS 数据集的实验表明，AdaGaR 在 PSNR、SSIM、LPIPS 等指标上实现了最先进的性能，并在帧插值、深度一致性、视频编辑和立体视图合成等任务中表现出强大的泛化能力。&lt;/p&gt;
&lt;h3&gt;深度解读&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 该论文旨在从单目视频中重建动态 3D 场景，同时兼顾高频纹理细节和时间连续性。此问题在虚拟现实、增强现实和影视制作等领域至关重要，因为它直接影响渲染质量和用户体验。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者先分析了传统 3D 高斯原语的低通滤波限制和 Gabor 函数能量不稳定的问题，并指出缺乏时间连续性约束导致运动伪影。随后他们结合了 3D 高斯剖面、Gabor 频率调制、Hermite 样条插值以及深度与跟踪等多模态监督，形成了 AdaGaR 的整体框架。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将高斯原语扩展为可学习频率权重的 Gabor 原语，并通过能量补偿保持稳定；时间上使用 Cubic Hermite 样条与曲率正则化保证连续运动；初始化阶段融合深度估计、点跟踪和前景掩码以快速收敛。实现流程为：在正交相机坐标系中定义动态 Gabor 原语，使用多模态监督（RGB、深度、光流、掩码）对位置、颜色、频率权重和样条控制点进行联合优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 1) Adaptive Gabor Representation：可学习的频率权重和能量补偿，使原语能在 Gaussian 与 Gabor 之间自适应切换；2) Temporal Curvature Regularization：在 Hermite 样条中加入二阶导数约束，提升运动平滑度；3) Adaptive Initialization：结合深度、跟踪和掩码快速构建稳定点云。与以往仅使用低通高斯或固定频率 Gabor、缺乏显式时间约束的方法不同，AdaGaR 在保持高频细节的同时实现了更稳健的时间连续性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AdaGaR 通过可学习的 Gabor 原语与样条时间正则化，提供了一种高频细节保留且时间连续的单目视频动态 3D 重建框架。&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/&lt;/p&gt;</description></item></channel></rss>